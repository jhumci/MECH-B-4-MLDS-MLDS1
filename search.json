[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning and Data Science 1",
    "section": "",
    "text": "Preface\nDies sind die Vorlesungsnotizen für die Lehrveranstaltung Machine Learning + Data Science I (Lecture) am MCI | The Entrepreneurial School Bachelor-Studiengangs Mechatronik im Sommer-Semester 2025.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#danksagungen",
    "href": "index.html#danksagungen",
    "title": "Machine Learning and Data Science 1",
    "section": "Danksagungen",
    "text": "Danksagungen\nWir danken der Open-Source-Community für die Bereitstellung exzellenter Tutorials und Anleitungen zu Data-Science-Themen in und mit Python im Web.\nEinzelne Quellen werden an den entsprechenden Stellen im Dokument zitiert.\nBesonderer Dank gilt Peter Kandof, für das Aufsetzen eines Beispielprojekts für die Vorlesung anhand von MECH-M-DUAL-1-DBM - Grundlagen datenbasierter Methoden. Diese Notizen wurden mit Quarto erstellt.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Einleitung",
    "section": "",
    "text": "Leistungsüberprüfung\nIn dieser Lehrveranstaltung werden wir uns mit den grundlegenden Konzepten moderner Data-Science-Techniken befassen und eine solide Basis in Statistik und maschinellem Lernen legen. Wir behandeln alle essenziellen Grundlagen, die notwendig sind, um sich in diesem Bereich sicher zu bewegen. Dabei beschränken wir uns nicht nur auf die theoretischen Aspekte, sondern nutzen auch Python, um die Inhalte programmatisch zu veranschaulichen.\nDie verwendeten Referenzen wurden nach Qualität, freier Verfügbarkeit und der Nutzung von Python ausgewählt. Für die statistischen Grundlagen greifen wir auf Beispiele aus Diez, Barr, and Cetinkaya-Rundel (2019) zurück, während für die Einführung in das statistische Lernen James et al. (2013) herangezogen wird.\nDieses Skriptum richtet sich an Studierende der Ingenieurwissenschaften, weshalb mathematische Konzepte nur selten mit strengen Beweisen versehen sind.\nDie Vorlesungsteil der Lehrveranstaltung wird mit einer Klausur bewertet.",
    "crumbs": [
      "Einleitung"
    ]
  },
  {
    "objectID": "intro.html#leistungsüberprüfung",
    "href": "intro.html#leistungsüberprüfung",
    "title": "Einleitung",
    "section": "",
    "text": "Diez, David M., Christopher D. Barr, and Mine Cetinkaya-Rundel. 2019. OpenIntro Statistics. 4th ed. Boston, MA: OpenIntro. \\url{https://www.openintro.org/stat/textbook.php?stat_book=os}.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, et al. 2013. An Introduction to Statistical Learning. Vol. 112. Springer.",
    "crumbs": [
      "Einleitung"
    ]
  },
  {
    "objectID": "dataexploratory/index.html",
    "href": "dataexploratory/index.html",
    "title": "Umgang mit Daten und Explorative Analyse",
    "section": "",
    "text": "Alice: Would you tell me, please, which way I ought to go from here?\nThe Cheshire Cat: That depends a good deal on where you want to get to.\n— Carroll (2015)\n\n\nIn dieser Vorlesung erarbeiten wir gemeinsam die Kompetenzen, um Daten zu analysieren, Modelle zu erstellen und Vorhersagen zu treffen. Hierbei lernen wir alle Grundlagen kennen, die notwendig sind, um sich dabei zurechtzufinden. Zunächst gibt, es aber die Richtung zu klären, in die wir uns bewegen wollen.\n\n\n\n\n\n\nNote\n\n\n\nDiese Notizen setzen voraus, dass Sie über grundlegende Programmierkenntnisse in Python verfügen und wir bauen auf diesem Wissen auf. In diesem Sinne verwenden wir Python als Werkzeug und beschreiben die inneren Abläufe nur, wenn es uns hilft, die behandelten Themen besser zu verstehen.\nFalls dies nicht der Fall ist, schauen Sie sich MCI-MECH-B-3-SWD-SWD-ILV an, einen Kurs über Softwaredesign im selben Bachelor-Programm und von denselben Autoren.\n\n\nZusätzlich können wir die folgenden Bücher über Python empfehlen:\n\nMatthes (2023): Python Crash Course - A hands-on, project-based introduction to programming: Online Material.\nPython Cheat Sheet provided by Matthes (2023).\n\nFür die Statistik und Machine Learning ortientieren wir uns an folgenden Büchern:\n\nDiez, Barr, and Cetinkaya-Rundel (2019): OpenIntro Statistics: Online Material\nJames et al. (2013): Introduction to Statistical Learning: Online Material\n\n\n\n\n\nCarroll, Lewis. 2015. Alice’s Adventures in Wonderland Unfolded.\n\n\nDiez, David M., Christopher D. Barr, and Mine Cetinkaya-Rundel. 2019. OpenIntro Statistics. 4th ed. Boston, MA: OpenIntro. \\url{https://www.openintro.org/stat/textbook.php?stat_book=os}.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, et al. 2013. An Introduction to Statistical Learning. Vol. 112. Springer.\n\n\nMatthes, Eric. 2023. Python Crash Course - a Hands-on, Project-Based Introduction to Programming. 3rd ed. No Starch Press. https://ehmatthes.github.io/pcc_3e/.",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse"
    ]
  },
  {
    "objectID": "dataexploratory/data_science.html",
    "href": "dataexploratory/data_science.html",
    "title": "1  Data Science, Statisik, und Machine Learning",
    "section": "",
    "text": "1.1 Data Sciene: Projektvorgehen\nData Science, Statisik, Machine Learning und Küstliche Intelligenz sind Begriffe, die in den letzten Jahren immer häufiger in den Medien und in der Wissenschaft auftauchen.\nEs gibt unterschiedlichte Definitionen für diese Begriffe, die sich je nach Kontext und Anwendungsbereich unterscheiden. Damit wir uns verstehen, versuchen wir es so:\nUm uns in unseren folgenden Abenteuern zurrechtzufinden, beschäftigen wir uns zunächst hier in Chapter 1 mit dem gundlegenden Herangehen an Data Science-Probleme. In Chapter 2 mit den Daten, die wir analysieren wollen.\nFigure 1.1 zeigt einige der vielen Entscheidungen, die wir treffen müssen, wenn wir uns in unserem eigenen Projekt bewegen.\nSelbst, wenn uns das Ziel klar ist, können wir uns immernoch verlaufen. Um dies zu verhindern, gibt es verschiedene Vorgehensweisen, die uns helfen auf dem Pfad zu bleiben und uns helfen ein tiefgehendes Verständnis für einen Datensatz zu entwickeln. Hierbei unterstützen Standardvorgehensweisen wie CRISP-DM (Cross-Industry Standard Process for Data Mining Figure 1.2) Shearer (2000). Es gibts aber auch neuere und spezifische Prozesse, die in bestimmten Bereichen, wie z.B. im Feld der Zeitreihenprognosen Hyndman (2018), angewendet werden. Für den Anfang begnügen wir uns jedoch mit dem etabliertesten und verbreitesten Prozess.",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Science, Statisik, und Machine Learning</span>"
    ]
  },
  {
    "objectID": "dataexploratory/data_science.html#ssec-dataexploratory-data_science-project",
    "href": "dataexploratory/data_science.html#ssec-dataexploratory-data_science-project",
    "title": "1  Data Science, Statisik, und Machine Learning",
    "section": "",
    "text": "Figure 1.2: CRISP-DM Prozessdiagramm Wikimedia Commons contributors (retrieved 2025)\n\n\n\n\n1.1.1 CRISP-DM: Ein systematischer Ansatz für datenbezogene Projekte\nDer CRISP-DM-Prozess stellt ein generisches Rahmenwerk dar, das es ermöglicht, datengetriebene Projekte von der Problemdefinition bis hin zur operativen Anwendung zu strukturieren. Der Prozess ist in sechs zentrale Phasen unterteilt:\n\nBusiness Understanding\nZiel ist es, die geschäftlichen Anforderungen und Problemstellungen klar zu definieren. Diese Phase umfasst Fragestellungen wie: Was möchten wir mit den Daten lösen? oder Welche Ergebnisse und Metriken sind entscheidend für den Erfolg?\nData Understanding\nHier wird das vorliegende Datenmaterial genauer untersucht, einschließlich seiner Struktur, Störfaktoren und potenzieller Verzerrungen. Eine erste Erkundung der Daten kann entscheidend sein, um Hypothesen zu entwickeln.\nDatenaufbereitung\nIn dieser Phase werden die Rohdaten bereinigt und in ein Format gebracht, das für die Analyse geeignet ist (vgl. Chapter 2). Aktivitäten umfassen das Entfernen fehlender Werte, Transformation von Variablen und die Auswahl relevanter Features.\nModellierung\nAufbau eines Modells zur Beantwortung der Kernfrage. Hierbei kann z. B. ein Regressionsmodell für Prognosen oder ein Klassifikationsmodell bei Entscheidungsproblematiken im Vordergrund stehen.\nEvaluierung\nÜberprüfung, ob das Modell tatsächlich valide und praktisch anwendbar ist. Kernfragen sind: Passt das Modell zu unseren Zielen? und Sind die Ergebnisse sinnvoll und umsetzbar?\nDeployment (Inbetriebnahme)\nDas Modell wird implementiert, um tatsächliche Entscheidungen oder Vorhersagen zu unterstützen. Dies könnte z. B. bedeuten, ein automatisiertes System zu schaffen, das regelmäßig aktualisierte Prognosen liefert.\n\nTipp: Obwohl der Prozess linear erscheint, sind Rücksprünge oft unvermeidlich, z. B. wenn das Modell nicht ausreichend Performanz liefert oder die Anforderungen sich ändern. Auch werden wir mehr über der Daten und die Prozesse (Schritte 2 und 3) lernen, je mehr wir uns mit den Daten beschäftigen.\n\n\n1.1.2 Data Science als People Business?\nEs ist wichtig zu erkennen, dass Daten allein nur einen Ausschnitt der Realität darstellen und ohne Kontext wenig nützen. Ein Kernelement ist daher, sich ausreichendes Domänenwissen anzueignen, um die Semantik der zugrunde liegenden Daten zu interpretieren. Häufig kann es dabei hilfreich sein, Daten aus angrenzenden Kontexten hinzuzuziehen und den Dialog mit Expert:innen oder Personen mit Erfahrungswissen zu suchen.\nAls Standardvorgehen für viele datenbasierte Projekte empfiehlt Hyndman (2018) in seinem einflussreichen Werk zur Zeitreihenanalyse, diese Prinzipien auch auf Prognosen anzuwenden, ähnliches gibt aber für alle Probblem.\n\n\n\n\n\n\nBeispiel:\n\n\n\nWenn wir historische Verkaufsdaten eines Geschäfts analysieren, um zukünftige Trends zu prognostizieren, sollten wir sowohl mit der späteren Nutzer:in des Forecasts (z.B. Produktionsplaner:in für das Business Understanding), als auch mit den Erzeugen der Daten (z.B. Sales-Abteilung für das Data Understanding) sprechen.\n\nDieses bestimmt, wie unsere Modellierung aussehen soll.\n\nWie weit in die Zukunft müssen wir vorhersagen (Prognosehorizont)?\nWelche Auflösung benötigt unsere Prognose (z.B. tagescharf oder wöchtentlich)?\nIn welche Systeme (z.B. Dashboards) muss die Prognose später deployed werden?\nWie soll das Modell evaluiert werden (z.B. ist es wichtiger an keinem Tag große Ausreißer zu haben oder die kumulativen Absatzzahlen über das Jahr hinweg genau zu treffen?)\n\nDurch Gespräche mit weiteren Stakeholdern ergibt sich zudem Business & Data Understanding\n\nGibt es saisonale Effekte (z.B. Insekten-Schutz-Produkte)\nGibt es systematische Fehler in der Datenaufzeichntung (End-of-Year-Effecs)\nGab es Systemumstellungen in der Datenerfassung oder andere Externe Brüche (z.B. Markteintritte)\n\n\nDie Herausforderung ist hierbei jedoch, dass Prognosen fehleranfällig sind. Ein plötzlicher Markteinbruch oder ein externes Ereignis, wie ein sozioökonomischer Schock, könnte die Vorhersagen unbrauchbar machen. Evtl. gehört zum Business Understanding auch die Grenzen einer datanbasierenden Lösung zu verstehen.\n\n\n\n\n1.1.3 Ein Beispiel-Datenset: loan50\nIm Folgenden nutzen wir das loan50-Datenset, das aus dem Lehrbuch von Diez, Barr, and Cetinkaya-Rundel (2019) stammt und zur Erkundung solcher Fragestellungen dient.\nDas loan50-Datenset enthält Informationen zu 50 vergebenen Krediten, die über die Lending Club Plattform vermittelt wurden. Diese Plattform ermöglicht es Einzelpersonen, untereinander Kredite zu vergeben. Wie in vielen Finanzanwendungen sind jedoch nicht alle Kreditnehmer:innen gleich:\n\nKandidat:innen mit hoher Rückzahlungssicherheit werden bevorzugt behandelt und erhalten in der Regel Kredite mit niedrigeren Zinssätzen.\nRisikoreichere Antragsteller:innen könnten hingegen keine Angebote erhalten oder hohe Zinssätze ablehnen.\n\n\n\n\n\n\n\nWarning\n\n\n\nAchtung: Dieses Datenset enthält nur tatsächlich vergebene Kredite und repräsentiert daher nur eine Teilmenge aller möglichen Kreditanfragen. Diese Einschränkung kann dazu führen, dass wir relevante Informationen über nicht vergebene Kreditanträge nicht betrachten. Solche Probeme bezeichnet man gemeinhin als Bias (Verzerrung).\n\n\nEinige der verfügbaren Variablen:\nDie unten aufgelisteten Variablen beschreiben die Eigenschaften des umfassenderen loans_full_schema-Datensatzes, wovon eine Teilmenge im loan50-Datenset enthalten ist.\n\n\n\nVariable\nBeschreibung\n\n\n\n\nemp_title\nBerufsbezeichnung\n\n\nemp_length\nAnzahl der Jahre im Beruf (aufgerundet, Werte über 10 Jahre werden als 10 dargestellt)\n\n\nstate\nUS-Bundesstaat (zweistellige Abkürzung)\n\n\nhome_ownership\nWohnsituation der Bewerber:innen (z. B. Eigentum, gemietet)\n\n\nannual_income\nJährliches Einkommen\n\n\nverified_income\nArt der Verifikation des Einkommens\n\n\ndebt_to_income\nSchulden-Einkommens-Verhältnis\n\n\ngrade\nBewertung des Kredits, wobei A die höchste Stufe ist\n\n\n…\nWeitere Variablen finden Sie in der vollständigen Beschreibung: loan50 - OpenIntro Dataset\n\n\n\n\n\n\n\n\n\nBusiness und Data Understanding\n\n\n\n\nWelche Fragestellungen könnten mit diesem Datensatz beantwortet werden?\n\nBeispielsweise: Gibt es einen Zusammenhang zwischen dem Schulden-Einkommens-Verhältnis und der Kreditbewilligung?\n\nWas müsste noch bekannt sein, um die Daten besser zu verstehen?\n\nWelche spezifischen Regeln wurden aufgestellt, um Kredite zu vergeben oder abzulehnen?\nWo bestehen potenzielle Verzerrungen (z. B. durch die fehlenden Daten zu abgelehnten Anträgen)?\n\n\n\n\n\n\n\n\nDiez, David M., Christopher D. Barr, and Mine Cetinkaya-Rundel. 2019. OpenIntro Statistics. 4th ed. Boston, MA: OpenIntro. \\url{https://www.openintro.org/stat/textbook.php?stat_book=os}.\n\n\nHyndman, RJ. 2018. Forecasting: Principles and Practice. OTexts.\n\n\nKozyrkov, Cassie. 2018. “What on Earth Is Data Science?” medium.com. \\url{https://kozyrkov.medium.com/what-on-earth-is-data-science-eb1237d8cb37}.\n\n\nShearer, Colin. 2000. “The CRISP-DM Model: The New Blueprint for Data Mining.” Journal of Data Warehousing 5 (4): 13–22.\n\n\nWikimedia Commons contributors. retrieved 2025. “CRISP-DM Process Diagram.” https://upload.wikimedia.org/wikipedia/commons/b/b9/CRISP-DM_Process_Diagram.png.",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Science, Statisik, und Machine Learning</span>"
    ]
  },
  {
    "objectID": "dataexploratory/data_sets.html",
    "href": "dataexploratory/data_sets.html",
    "title": "2  Data Sets",
    "section": "",
    "text": "2.1 Tidy-Data\nEin sauber aufbereiteter Datensatz ist eine grundlegende Voraussetzung für jede datenbasierte Analyse und ist im CRISP-DM Teil der Datenaufbereitung. Wir starten zunächst mit dem Konzept von Tidy Data Section 2.1, welches sich mit der sauberen Strukturierung von Daten befasst. Anschließend werden wir uns mit den Typen von Variablen Section 2.2 befassen, die unter anderem den Ausschlag gibt, welche verschiedenen Visualisierungen Section 2.3 sinnvoll sind, um sich ein Data Understanding zu erarbeiten. Zum Schluss werden wir uns mit den verschiedenen Maßen für Variablen Section 2.4 auseinandersetzen, mit welchen man Datensätze beschreiben kann.\nWenn wir mit Computern automatisiert arbeiten möchten, ist neben der Semantik der Daten auch deren Syntax essenziell. Das bedeutet, dass die Daten in einer Struktur vorliegen müssen, die ihre Semantik sinnvoll abbildet.\nEin weitverbreiteter Standard, der in diesem Zusammenhang häufig genutzt wird, sind die von Wickham (2014) beschriebenen Tidy Data Conventions. Dieses Datenformat ist de facto eine Grundlage für viele Softwarepakete wie pandas, statsmodels, sklearn, tensorflow und andere Werkzeuge im Bereich Datenanalyse und des maschinelles Lernen.",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Sets</span>"
    ]
  },
  {
    "objectID": "dataexploratory/data_sets.html#sec-dataexploratory-data_tidy-data",
    "href": "dataexploratory/data_sets.html#sec-dataexploratory-data_tidy-data",
    "title": "2  Data Sets",
    "section": "",
    "text": "Hinweis: Datenbanknormalisierung\n\n\n\n\n\nIm Grunde handelt es sich bei diesem Format um ein Prinzip, das auch in der Datenbanknormalisierung nach Codd verfolgt wird. Ihnen wird dieses Konzept in relationalen Datenbanken (SQL) erneut begegnen.\n\n\n\n\n2.1.1 Was bedeutet “Tidy Data”?\nTidy Data folgt drei Grundprinzipien:\n\n\n\n\n\n\nGrundprinzipien von Tidy Data\n\n\n\n\nJede Zeile repräsentiert eine Beobachtung (bzw. eine Einheit).\nJede Spalte repräsentiert eine Variable (bzw. ein Attribut).\nJede Zelle enthält genau einen präzisen Wert (einen primitiven Datentyp wie int, float, str oder bool – keine Listen, Tupel oder geschachtelten Objekte).\n\n\n\nEin Beispiel für nicht-Tidy-Daten könnte eine Spalte enthalten, in der mehrere Werte in einer Liste zusammengefasst sind. Solche Daten sind schwerer zu verarbeiten und unflexibler beim Einsatz in Analysetools.\nTidy Data hilft uns bei der Datenbereinigung und Datenanalyse. Es erleichtert die Automatisierung und Standardisierung von Prozessen und reduziert die Wahrscheinlichkeit von Fehlern.\n\nKompatibilität: Viele Python-Bibliotheken wie pandas, statsmodels oder seaborn setzen voraus, dass die verwendeten Daten im Tidy-Format vorliegen.\nAutomatisierung: Tidy-Daten erleichtern Standardoperationen wie Filtern, Gruppieren und Pivotieren erheblich.\nFehlerprävention: Unstrukturierte oder verschachtelte Datenstrukturen sind fehleranfällig und schwer zu debuggen.\n\n\n\n\n\n\n\nDaten in das Tidy-Format transformieren\n\n\n\nEs gibt viele hilfreiche Funktionen und Methoden in pandas, um Daten zu “tidy-fizieren”. Ein Beispiel ist die Verwendung der Methoden stack, unstack und melt. Diese helfen dabei, Daten umzustrukturieren und in die gewünschte lange (viele Zeilen) oder weite (viele Spalten) Form zu bringen. Ein hilfreicher Artikel hierzu ist Reshape with Pandas.\n💡 Tipp: Wenn Sie unsicher sind, wie Sie Ihre Daten umorganisieren sollten, können Sie ein Beispiel (z.B. head() eines DataFrames) und die gewünschte Struktur (also Spaltennamen) in ein Large Language Model eingeben. Oft erhalten Sie klare Vorschläge zur Umstrukturierung!\n\n\n\n\n2.1.2 Positive Beispiele für Tidy Data\nDer folgende Beispielcode zeigt, wie Sie ein CSV-Datei laden und sich mit den ersten Zeilen vertraut machen können. Glücklicher Weise ist dieser Datensatz bereits im Tidy-Format. Jede Zeile repräsentiert eine Beobachtung (Kreditnehmer) und jede Spalte eine Variable (Attribut).\n\nimport pandas as pd\n\n# Lesen der CSV-Datei in einen DataFrame\ndf = pd.read_csv(r\"../_assets/dataexploratory/loan50.csv\")\n# Ausgabe der ersten Zeilen des Datensatzes\nprint(df.head())\n\n  state  emp_length  term homeownership  annual_income verified_income  \\\n0    NJ         3.0    60          rent          59000    Not Verified   \n1    CA        10.0    36          rent          60000    Not Verified   \n2    SC         NaN    36      mortgage          75000        Verified   \n3    CA         0.0    36          rent          75000    Not Verified   \n4    OH         4.0    60      mortgage         254000    Not Verified   \n\n   debt_to_income  total_credit_limit  total_credit_utilized  \\\n0        0.557525               95131                  32894   \n1        1.305683               51929                  78341   \n2        1.056280              301373                  79221   \n3        0.574347               59890                  43076   \n4        0.238150              422619                  60490   \n\n   num_cc_carrying_balance        loan_purpose  loan_amount grade  \\\n0                        8  debt_consolidation        22000     B   \n1                        2         credit_card         6000     B   \n2                       14  debt_consolidation        25000     E   \n3                       10         credit_card         6000     B   \n4                        2    home_improvement        25000     B   \n\n   interest_rate  public_record_bankrupt loan_status  has_second_income  \\\n0          10.90                       0     Current              False   \n1           9.92                       1     Current              False   \n2          26.30                       0     Current              False   \n3           9.92                       0     Current              False   \n4           9.43                       0     Current              False   \n\n   total_income  \n0         59000  \n1         60000  \n2         75000  \n3         75000  \n4        254000  \n\n\n\n\n2.1.3 Negative Beispiele für Tidy Data\nFolgendes Datenbeispiel zeigt, wie ein Datensatz nicht im Tidy-Format aussieht. Wir sehen die Strombedarfe von verschiedenen Netzgebieten zone_id zu verschiedenen Zeitpunkten. Allerdings ist es ungünstig, dass nicht jede Kombination aus Zone und Zeitpunkt eine eigene Zeile hat. Stattdessen sind die Werte für alle 24 Stunden in einer eigenen Spalte.\n\nimport pandas as pd\n\ndf = pd.read_csv(r\"../_assets/dataexploratory/GEFCom2012/GEFCOM2012_Data/Load/Load_history.csv\")\nprint(df.head())\n\n   zone_id  year  month  day      h1      h2      h3      h4      h5      h6  \\\n0        1  2004      1    1  16,853  16,450  16,517  16,873  17,064  17,727   \n1        1  2004      1    2  14,155  14,038  14,019  14,489  14,920  16,072   \n2        1  2004      1    3  14,439  14,272  14,109  14,081  14,775  15,491   \n3        1  2004      1    4  11,273  10,415   9,943   9,859   9,881  10,248   \n4        1  2004      1    5  10,750  10,321  10,107  10,065  10,419  12,101   \n\n   ...     h15     h16     h17     h18     h19     h20     h21     h22  \\\n0  ...  13,518  13,138  14,130  16,809  18,150  18,235  17,925  16,904   \n1  ...  16,127  15,448  15,839  17,727  18,895  18,650  18,443  17,580   \n2  ...  13,507  13,414  13,826  15,825  16,996  16,394  15,406  14,278   \n3  ...  14,207  13,614  14,162  16,237  17,430  17,218  16,633  15,238   \n4  ...  13,845  14,350  15,501  17,307  18,786  19,089  19,192  18,416   \n\n      h23     h24  \n0  16,162  14,750  \n1  16,467  15,258  \n2  13,315  12,424  \n3  13,580  11,727  \n4  17,006  16,018  \n\n[5 rows x 28 columns]\n\n\nEine Umwandlung in das Tidy-Format würde, wie folgt aussehen, wobei wird darauf achten sollten, dass der timestamp als datetime-Objekt und die load als int gespeichert wird:\n\n\n\nzone_id\ntimestamp\nload (kW)\n\n\n\n\n1\n2012-01-01 00:00:00\n1000\n\n\n1\n2012-01-01 01:00:00\n1100",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Sets</span>"
    ]
  },
  {
    "objectID": "dataexploratory/data_sets.html#sec-dataexploratory-data_variable-types",
    "href": "dataexploratory/data_sets.html#sec-dataexploratory-data_variable-types",
    "title": "2  Data Sets",
    "section": "2.2 Messniveaus von Variablen",
    "text": "2.2 Messniveaus von Variablen\nVariablen sind die Bausteine von Daten und repräsentieren die Merkmale, die wir messen oder beobachten. Im Kapitel Section 2.2 werden wir uns noch tiefer mit Variablen auseinandersetzen. In der Datenanalyse ist es wichtig, die Art der Variablen zu kennen, da dies beeinflusst, welche Methoden und Visualisierungen für die Daten geeignet sind. Variablen lassen sich nach ihrem Messniveau klassifizieren, was wiederum die Art der Informationen beschreibt, die sie enthalten. Die bekannteste Klassifikation von Variablen basiert auf den vier Messniveaus von Stanley Smith Stevens: Nominal, Ordinal, Intervall und Ratio (Verhältnis).\n\nNutzen Sie den oben gezeigten Datensatz loan50, um die folgenden Aufgaben für unterschiedliche Variablenarten zu lösen:\n\n\nSortieren: Wie könnte man die Werte der Variablen\n\nstate,\ngrade,\nein Beispiel für ein Intervallniveau (aber kein Ratio),\nannual_income\nsinnvoll in aufsteigender Reihenfolge anordnen?\n\nZentrale Werte bestimmen: Wie lässt sich ein zentraler Wert bestimmen, sei es durch den Modus, die Median oder den Mittelwert?\nBeziehungen beschreiben: Welche Aussage könnte man über die Beziehung zwischen zwei Werten einer Variablen machen?\n\n\n2.2.1 Nominale Variablen\nDefinition: Nominale Variablen kategorisieren Daten ohne eine festgelegte Reihenfolge.\n\ndf = pd.read_csv(r\"../_assets/dataexploratory/loan50.csv\")\nprint(df[\"state\"].head())\n\n0    NJ\n1    CA\n2    SC\n3    CA\n4    OH\nName: state, dtype: object\n\n\n\nprint(df[\"state\"].value_counts().head())\n\nstate\nCA    9\nTX    5\nIL    4\nNJ    3\nMD    3\nName: count, dtype: int64\n\n\n\nprint(df[\"state\"].mode().head())\n\n0    CA\nName: state, dtype: object\n\n\n\nSortieren: Es gibt keine inhärente Methode, diese Werte zu sortieren. Dies liegt daran, dass nominale Daten keine Reihenfolge implizieren.\nZentraler Wert: Modus, da dieser Wert am häufigsten vorkommt.\nBeziehungen: Die Beziehung zwischen zwei Werten kann nur beschreiben, ob sie in derselben Kategorie sind oder nicht.\n\n\n\n2.2.2 Ordinale Variablen\nOrdinale Variablen haben eine natürliche Ordnung, aber der Abstand zwischen den Werten ist nicht zwingend gleichmäßig.\n\nprint(df[\"grade\"].head())\n\n0    B\n1    B\n2    E\n3    B\n4    B\nName: grade, dtype: object\n\n\n\nprint(df[\"grade\"].sort_values().head())\n\n49    A\n18    A\n33    A\n36    A\n14    A\nName: grade, dtype: object\n\n\n\nprint(df[\"grade\"].value_counts().head())\n\ngrade\nB    19\nA    15\nD     8\nC     6\nE     2\nName: count, dtype: int64\n\n\n\nprint(df[\"grade\"].mode())\n\n0    B\nName: grade, dtype: object\n\n\n\n# Define the order for the categorical values\ngrade_order = sorted(df[\"grade\"].unique())\n\n# Convert the 'grade' column to a categorical type with the specified order\ndf['grade'] = pd.Categorical(df['grade'], categories=grade_order, ordered=True)\n\n# Convert categorical data to numerical codes\ngrade_codes = df['grade'].cat.codes\n\nprint(grade_codes.median())\n\n1.0\n\n\n\nSortieren: Mit geeigneten Regeln ist es möglich, diese Werte in aufsteigender Reihenfolge zu ordnen: [\"C\", \"B\", \"A\"].\nZentraler Wert:: Der Modus ist geeignet, und der Median zeigt auf, dass 50 % der Werte gleich oder niedriger als \"B\" sind.\nBeziehungen: Zwei Werte lassen sich nach ihrer Position der Reihenfolge vergleichen: höher oder niedriger.\n\n\n\n2.2.3 Intervallskalierte Variablen\nIntervallskalierte Variablen haben geordnete Werte mit gleichmäßigen Abständen zwischen ihnen, aber sie besitzen keinen absoluten Nullpunkt. Ein Beispiel ist das jährliche Einkommen.\n\nprint(df[\"annual_income\"].head())\n\nprint(df[\"annual_income\"].mean())\n\n0     59000\n1     60000\n2     75000\n3     75000\n4    254000\nName: annual_income, dtype: int64\n86170.0\n\n\n\nSortieren:: Daten können numerisch in aufsteigender Reihenfolge sortiert werden.\nZentraler Wert:: Der Modus und Median sind geeignete Maße. Der arithmetische Mittelwert berechnet sich als: \\(\\mu = \\frac{1}{n} \\sum x_i\\)\nBeziehungen: Der Abstand (Intervall) zwischen zwei Werten kann quantifiziert werden.\n\n\n\n\n\n\n\nUnterschied zwischen Intervall- und Ratiodaten\n\n\n\nIm Gegensatz zum Ratio-Messniveau besitzen Intervall-Daten keinen absoluten Nullpunkt. Aussagen wie “das Doppelte” sind daher nicht sinnvoll.\n\n\n\n\n\n\n\n\nTip\n\n\n\nEs ist oft hilfreich, sich das Messniveau einer Variablen vor Beginn der Analyse klar zu machen. Das Messniveau entscheidet auch, welche Visualisierung sinnvoll sind. Mögliche Fehleinschätzungen können zu falschen oder unzulässigen Berechnungen führen, z. B. Mittelwerte bei Nominaldaten. Daten sollten entsprechend ihrem Typ gereinigt und transformiert werden.",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Sets</span>"
    ]
  },
  {
    "objectID": "dataexploratory/data_sets.html#sec-dataexploratory-data_visualization",
    "href": "dataexploratory/data_sets.html#sec-dataexploratory-data_visualization",
    "title": "2  Data Sets",
    "section": "2.3 Visualisierungen",
    "text": "2.3 Visualisierungen\n\n\n\n\n\n\nTip\n\n\n\nEs gibt viele Möglichkeiten, Daten zu visualisieren, um Muster und Trends zu erkennen. Zwei weit verbreitete Pakete sind matplotlib und plotly. Im folgenden benutzen wir vorallem seaborn, welches eine Erweiterung von matplotlib ist und speziell für statistische Visualisierungen entwickelt wurde.\n\n\n\n2.3.1 Histogramme\nEin Histogramm ist eine angenäherte Darstellung der Verteilung einer intervallskalierten Variable. Es liefert wertvolle Informationen über:\n\nZentralwert: Wo liegen die Daten?\nVarianz: Wie stark streuen die Daten?\nVerteilung: Wie häufig treten bestimmte Werte auf?\n\n(fig:sec-dataexploratory-sets-histogram?) zeigt ein Histogramm des jährlichen Einkommens aus dem Datensatz loan50.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create a histogram of the annual income\nsns.histplot(df[\"annual_income\"], bins=10, stat = 'count')\n\n\n\n\nHistogramm des jährlichen Einkommens\n\n\n\n\n\n2.3.1.1 Konstruktion eines Histogramms\nEin Histogramm wird in wenigen Schritten erstellt. Meinst wird dies bereits für uns wie in seaborn erledigt, es ist jedoch hilfreich, die Schritte zu kennen, um die Visualisierung besser zu verstehen, da sie manchmal abgewandelt wird.\n\nBinning: Teilen Sie die Werte der beobachteten Variablen \\(x_i\\) in eine Reihe von Intervallen (Bins oder Buckets) auf.\nZählen: Erfassen Sie, wie viele Werte in jedes Intervall fallen (z. B. 5% der Werte).\nIntervall-Eigenschaften: Die Intervalle der Bins sollten aufeinander folgen, sich nicht überlappen und idealerweise die gleiche Breite haben.\nDarstellung: Die Anzahl der Werte in jedem Intervall wird entlang der y-Achse aufgetragen. Für relative Häufigkeiten wird durch die Stichprobengröße geteilt.\nWenn die Intervalle gleich breit sind, wird die y-Achse als Häufigkeit interpretiert. Wenn die Intervalle unterschiedlich breit sind, wird die y-Achse als Dichte interpretiert. Dazu wird die Höhe der Balken so skaliert, dass die Fläche unter dem Histogramm \\(1\\) ergibt.",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Sets</span>"
    ]
  },
  {
    "objectID": "dataexploratory/data_sets.html#sec-dataexploratory-data_variable-measures",
    "href": "dataexploratory/data_sets.html#sec-dataexploratory-data_variable-measures",
    "title": "2  Data Sets",
    "section": "2.4 Maße für Variablen",
    "text": "2.4 Maße für Variablen\nVariablen lassen sich auf verschiedene Weisen beschreiben. Lagemaße bzw. die zentrale Tendenz gibt an, wo die Daten liegen, während die Streuung angibt, wie weit die Daten von diesem Wert entfernt sind. Die Zusammenhänge zwischen Variablen können durch Korrelationen und Kovarianzen beschrieben werden.\n\n2.4.1 Lagemaße\nVariablen können auf verschiedene Weisen beschrieben werden. Beispielweise können Lagemaße wie der Arithmetischer Mittelwert (Mean), Median oder Modus genutzt werden, um die zentrale Tendenz der Daten zu beschreiben. Welche wir einsetzen, hängt vom Messniveau der Variablen ab.\nBetrachten wir eine mindestens intervall-skalierte Variable \\(x \\in \\mathbb{R}^n\\) aus den Datensatz, so können wir die folgenden Lagemaße berechnen:\n\ndas maximale Element bzw. der Höchstwert: \\[\nx^{max} = \\max_i x_i,\n\\]\n\n\nincome_max = df[\"annual_income\"].max()\nprint(f\"{income_max=}\")\n\nincome_max=np.int64(325000)\n\n\n\nder minimale Wert bzw. das Minimum:\n\n\nincome_min = df[\"annual_income\"].min()\nprint(f\"{income_min=}\")\n\nincome_min=np.int64(28800)\n\n\n\\[\nx^{min} = \\min_i x_i,\n\\] - der arithmetische Mittelwert: \\[\n\\overline{x} = \\frac1n \\sum_{i=1}^n x_i = \\frac{x_1 + x_2 + \\cdots + x_n}{n},\n\\]\n\nincome_mean = df[\"annual_income\"].mean()\nprint(f\"{income_mean=}\")\n\nincome_mean=np.float64(86170.0)\n\n\n\nder Median ist der Wert, der die Daten in zwei gleich große Teile teilt:\n\n\\[\n\\widetilde{x} = \\begin{cases}\n                x_{(n+1)/2}& n\\quad \\text{odd}\\\\\n                \\frac{x_{n/2} + x_{n/2+1}}{2}& n\\quad \\text{even}\n                \\end{cases},\n\\]\n\nincome_median = df[\"annual_income\"].median()\nprint(f\"{income_median=}\")\n\nincome_median=np.float64(74000.0)\n\n\n\nVerallgemeinert für \\(p\\in(0,1)\\) ist das p-Quantil \\(\\overline{x}_p\\) der Wert, der die Daten in zwei Teile teilt, wobei \\(p\\) der Anteil der Daten ist, die kleiner oder gleich \\(\\overline{x}_p\\) sind.\n\n\nincome_quartiles = df[\"annual_income\"].quantile([0.25, 0.5, 0.75])\nprint(f\"{income_quartiles=}\")\n\nincome_quartiles=0.25    55750.0\n0.50    74000.0\n0.75    99500.0\nName: annual_income, dtype: float64\n\n\n\\[\n\\overline{x}_p = \\begin{cases}\n                 \\frac12\\left(x_{np} + x_{np+1}\\right) & pn \\in \\mathbb{N}\\\\\n                x_{\\lfloor np+1\\rfloor} & pn \\not\\in \\mathbb{N}\n                \\end{cases}.\n\\]\nEinige Quantile haben spezielle Namen, wie der Median für \\(p=0.5\\), das untere und obere Quartil für \\(p=0.25\\) und \\(p=0.75\\) (oder erstes, zweites (Median) und drittes Quartil), respektive.\n\n\n\n\n\n\nCaution\n\n\n\nWie gut lassen sich Arithmetischer Mittelwert, Median und Mode aus dem Histogramm ablesen?\n\n\n\n\n2.4.2 Kumulative Histogramme und Empirische Verteilungsfunktionen\nAls Alternative haben sich kumulative Histogramme, wie in ?fig-sec-dataexploratory-sets-kum-histogram, etabliert, die die kumulative Verteilungsfunktion (Cumulative Density Function / CDF) visualisieren. Diese Funktion gibt an, wie viele Werte kleiner oder gleich einem bestimmten Wert sind. Zur Konstruktion der CDF werden die Daten in aufsteigender Reihenfolge sortiert und die relative Häufigkeit der Werte berechnet.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create a histogram of the annual income\nsns.histplot(df[\"annual_income\"], bins=10, stat = 'density', cumulative=True)\n\n\n\n\n\n\n\n\n\n\n2.4.3 Steuungsmaße\nSteuungsmaße beschreiben die Streuung der Daten um den zentralen Wert. Beispiele sind die Spannweite, Varianz und die Standardabweichung.\nDie Spannweite ist die Differenz zwischen dem größten und kleinsten Wert: \\[\n\\text{Spannweite} = x^{max} - x^{min}.\n\\] Die Varianz ist ein Maß für die mittlere quadratische Abweichung der Daten vom Mittelwert. Die Einheit der Varianz ist das Quadrat der Einheit der Daten: \\[\n\\sigma = \\sqrt{\\operatorname{Var}(x)}.\n\\] \\[\n\\operatorname{Var}(x) = \\frac1n \\sum_{i=1}^n (x_i - \\mu)^2.\n\\]\nDie Standardabweichung ist die Quadratwurzel der Varianz. Damit hat sie die gleiche Einheit wie die Daten:\n\\[\n\\sigma = \\sqrt{\\operatorname{Var}(x)}.\n\\]\nIn Python können wir die Varianz und Standardabweichung mit pandas oder numpy berechnen:\n\nprint(f\"Varianz: {df['annual_income'].var()}\")\nprint(f\"Standardabweichung: {df['annual_income'].std()}\")\n\nVarianz: 3313901734.6938777\nStandardabweichung: 57566.49837096119\n\n\n\n\n\n\n\n\nKorrigerte Stichproben-Varianz\n\n\n\nDie Varianz einer Stichprobe ist kein erwartungstreuer Schätzer für die Varianz der Grundgesamtheit. Die Begriffe werden wir in Section 5.1 noch genauer betrachten. Einfach gesagt, die Varianz einer Stichprobe ist tendenziell kleiner als die Varianz der Grundgesamtheit, da wird beim zufälligen Ziehen wahrscheinlich eher aus der Mitte als von den Extremen ziehen. Die korrigierte Stichproben-Varianz wird durch \\(n-1\\) statt \\(n\\) im Nenner definiert. In pandas wird die korrigierte Stichproben-Varianz als Standard verwendet, die unkorrigierte Varianz kann mit dem Parameter ddof=0 berechnet werden.\n\n\n\n\n2.4.4 Zusammenhangsmaße\nIn der Statistik beschreiben Zusammenhangsmaße die Beziehung zwischen zwei Variablen. Beispiele sind die Kovarianz und der Korrelationskoeffizient. Diese geben einen Hinweis darauf, ob und wie stark zwei Variablen zusammenhängen.\n\n2.4.4.1 Korrrelation\nIn der Statistik beschreibt der Begriff Korrelation oder Abhängigkeit jede statistische Beziehung zwischen bivariaten Daten (gepaarte Daten) oder Zufallsvariablen.\nIn unserem Datensatz können wir beispielsweise untersuchen: - emp_length: Anzahl der Jahre im Beruf - annual_income: Jährliches Einkommen - debt_to_income: Schulden-Einkommens-Verhältnis\nUm die Daten besser zu verstehen können wir zunächst einen Scatterplot ?fig-sec-dataexploratory-sets-scatterplot erstellen:\n\nimport seaborn as sns\n\ndf_reduced = df[[\"emp_length\", \"annual_income\", \"debt_to_income\"]]\n\nsns.pairplot(df_reduced)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiskussion\n\n\n\nWie interpretieren Sie den Zusammenhang zwischen den Variablen emp_length, annual_income und debt_to_income? Was würde entsprechend Ihres Domänenwissens Sinn ergeben?\n\n\n\n\n2.4.4.2 Kovarianz\nDie Kovarianz ist ein Maß für die gemeinsame Variabilität zweier Variablen. Sie ist definiert als der Erwartungswert des Produkts der Abweichungen der Zufallsvariablen von ihren Erwartungswerten:\n\\[\n\\operatorname{cov}(x, y) = \\frac1n \\sum_{i=1}^n (x_i - \\overline{x})(y_i - \\overline{y}).\n\\]\nIn Python können wir die Kovarianz-Matrix mit pandas direkt berechnen:\n\ndf_reduced.cov()\n\n\n\n\n\n\n\n\nemp_length\nannual_income\ndebt_to_income\n\n\n\n\nemp_length\n12.393174\n1.924082e+04\n-0.051111\n\n\nannual_income\n19240.824468\n3.313902e+09\n-8584.039227\n\n\ndebt_to_income\n-0.051111\n-8.584039e+03\n0.918269\n\n\n\n\n\n\n\nDie Kovarianz kann Wertebereiche von \\(-\\infty\\) bis \\(+\\infty\\) annehmen und ist nicht normiert. Um die Stärke der Beziehung zu quantifizieren, verwenden wir den Korrelationskoeffizienten, der leichter zu interpretieren ist.\n\n\n2.4.4.3 Korrelationskoeffizient\nDer Korrelationskoeffizient nach Pearson ist ein Maß für den linearen Zusammenhang zwischen zwei Variablen. Er ist definiert als das Verhältnis der Kovarianz der beiden Variablen zur Multiplikation ihrer Standardabweichungen:\n\\[\n\\rho_{x,y} = \\operatorname{corr}(x, y) = \\frac{\\operatorname{cov}(x, y)}{\\sigma_x \\sigma_y},\n\\]\nwobei \\(\\sigma_x\\) und \\(\\sigma_y\\) die Standardabweichungen der Variablen sind.\nIn Python können wir den Korrelationskoeffizienten mit numpy berechnen:\n\ndf_reduced.corr()\n\n\n\n\n\n\n\n\nemp_length\nannual_income\ndebt_to_income\n\n\n\n\nemp_length\n1.000000\n0.093156\n-0.014857\n\n\nannual_income\n0.093156\n1.000000\n-0.155610\n\n\ndebt_to_income\n-0.014857\n-0.155610\n1.000000\n\n\n\n\n\n\n\nEin Korrelationskoeffizient von \\(1\\) bedeutet eine perfekte positive Korrelation, \\(-1\\) eine perfekte negative Korrelation und \\(0\\) keine Korrelation. In diesem fall beobachten wir eine leichte negative Korrelation zwischen emp_length und debt_to_income und eine leichte positive Korrelation zwischen emp_length und annual_income. Eine Variable kann auch mit sich selbst perfekt korreliert sein, was zu einem Korrelationskoeffizienten von \\(1\\) führt.\n\n\n\n\n\n\nVorsicht\n\n\n\nDer Korrelationskoeffizient misst nur lineare Zusammenhänge. Nicht-lineare Zusammenhänge werden nicht erfasst. Es kann auch Zusammenhänge geben, die nicht durch den Korrelationskoeffizienten erfasst werden, z.B. wenn die Daten nicht normalverteilt sind. In Figure 2.1 sehen wir einige Beispiele in denen definitiv Korrelationen bestehen, die aber nicht durch den Korrelationskoeffizienten erfasst werden.\n\n\n\n\n\n\nFigure 2.1: Beispiele Korrelationskoeffizient DATAtab (retrieved 2025)\n\n\n\n\n\n\n\n\n\n\n\nKorrelation vs. Kausalität\n\n\n\nEine hohe Korrelation bedeutet nicht notwendigerweise Kausalität. Es ist wichtig, die Daten und den Kontext zu verstehen, um sinnvolle Schlussfolgerungen zu ziehen. Ansonsten besteht die Gefahr, dass Zusammenhänge fehlinterpretiert werden. Ein bekanntes Beispiel ist die Korrelation zwischen der Anzahl der Piraten und der globalen Temperatur, die in Figure 2.2 dargestellt ist. In der Wissenschaft begegnet man diesem Problem mit kontrollierten Experimenten.\n\n\n\n\n\n\nFigure 2.2: Korrelation Piraten Klima RedAndr and Mikhail Ryazanov (2011)\n\n\n\n\n\n\n\n\n\nDATAtab. retrieved 2025. “Korrelationskoeffizient Tutorial Image.” https://datatab.de/assets/tutorial/Korrelationskoeffizient.png.\n\n\nRedAndr and Mikhail Ryazanov. 2011. “Satirical diagram illustrating the influence of pirates decreasing on global warming as per Pastafarian beliefs.” https://commons.wikimedia.org/wiki/File:PiratesVsTemp(en).svg.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59: 1–23.",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Sets</span>"
    ]
  },
  {
    "objectID": "dataexploratory/tutorial.html",
    "href": "dataexploratory/tutorial.html",
    "title": "Tutorial: Getting Data Tidy and finding Correlation",
    "section": "",
    "text": "Objective\nThe Global Energy Forecasting Competition Hong, Pinson, and Fan (2014) is a data science competition that aims to advance the field of energy forecasting. The competition is held every two years and the data is made available to the public for research purposes. Different teams from around the world participate in the competition and the best models are selected based on their performance.\nIn 2012 one of the goals was to forecast the load of a power system. The data consists of hourly load data for a period of 5 years. The data was not provided in a tidy format and we need to clean it up before we can start working with it. The main objective of the load forecasting competition was to predict an accurate system load for each hour in each of the zones.",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse",
      "Tutorial: Getting Data Tidy and finding Correlation"
    ]
  },
  {
    "objectID": "dataexploratory/tutorial.html#data",
    "href": "dataexploratory/tutorial.html#data",
    "title": "Tutorial: Getting Data Tidy and finding Correlation",
    "section": "Data",
    "text": "Data\nThe data is provided in a zip file that contains different files:\n\nHoliday_List.csv\nLoad_history.csv\ntemperature_history.csv\n\nYour task is to make sense from the data an bring it into a tidy format. Store the data in a pandas DataFrame and save it as a csv file. Also reload it, to make shure it works.",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse",
      "Tutorial: Getting Data Tidy and finding Correlation"
    ]
  },
  {
    "objectID": "dataexploratory/tutorial.html#crisp-dm",
    "href": "dataexploratory/tutorial.html#crisp-dm",
    "title": "Tutorial: Getting Data Tidy and finding Correlation",
    "section": "Crisp-DM",
    "text": "Crisp-DM\nWe start by using the first four steps of the CRISP-DM process to make sense of the data using exploratory data analysis.\n\nBusiness Understanding\n\nWhat is the system load of a power system and why is it important to forecast it?\nWhat are the benefits of accurate load forecasting?\nWhat factors should influence the load of a power system?\n\nData Understanding\nHow many systems are there in the data? What are the features of the data? What is the time period of the data?\nDatenaufbereitung\nWhat is a meaningful structure for the data? What should be colums and what should be rows? How can we bring the data into a tidy format?\nModellierung\nIs there a seasionality in the data? Plot the average load for each hour of the day, day of the week and month of the year in a Boxplot. How do the distributions between the different systems compare? Are there any outliers? Are there any correlations between the load and the temperature? Plot the load against the temperature and compute the correlation coefficient.\n\n\n\n\n\n\n\nTip\n\n\n\nMake sure to store not only the processed, but also the processed data in a csv file. This way you can always go back to the original data and start over if you make a mistake. Also store the preprocessing steps in a separate script, so you can reproduce the results later. It is not uncommon, to recieve new data that needs to be processed in the same way. This is also the time to think of a proper folder structure for your project. Do not forget all the things you learned in the software design courses. You can also use modules like cookiecutter to create a project structure like this:\n├── LICENSE            &lt;- Open-source license if one is chosen\n├── Makefile           &lt;- Makefile with convenience commands like `make data` or `make train`\n├── README.md          &lt;- The top-level README for developers using this project.\n├── data\n│   ├── external       &lt;- Data from third party sources.\n│   ├── interim        &lt;- Intermediate data that has been transformed.\n│   ├── processed      &lt;- The final, canonical data sets for modeling.\n│   └── raw            &lt;- The original, immutable data dump.\n│\n├── docs               &lt;- A default mkdocs project; see www.mkdocs.org for details\n│\n├── models             &lt;- Trained and serialized models, model predictions, or model summaries\n│\n├── notebooks          &lt;- Jupyter notebooks. Naming convention is a number (for ordering),\n│                         the creator's initials, and a short `-` delimited description, e.g.\n│                         `1.0-jqp-initial-data-exploration`.\n...\n\n\n\n\n\n\nHong, Tao, Pierre Pinson, and Shu Fan. 2014. “Global Energy Forecasting Competition 2012.” International Journal of Forecasting 30 (2): 357–63. https://doi.org/https://doi.org/10.1016/j.ijforecast.2013.07.001.",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse",
      "Tutorial: Getting Data Tidy and finding Correlation"
    ]
  },
  {
    "objectID": "statistics/index.html",
    "href": "statistics/index.html",
    "title": "Statistik",
    "section": "",
    "text": "“Ich bin Ingenieur und gewohnt, in Wahrscheinlichkeiten zu denken, nach der Mathematik der Vernunft.” - Frisch (1957)\n\nDie klassische Statistik ist ein Teilbereich der Mathematik, der sich mit der Sammlung, Analyse, Interpretation, Präsentation und Modellierung von Daten beschäftigt. Ein großen Teil der Statistik ist die Wahrscheinlichkeitstheorie, die sich mit dem Verständnis von Zufallsereignissen befasst.\nIn Kapitel 3  Stichproben und Zufallsvariablen beschäftigen wir uns zunöchst damit, wie Daten erhoben werden und wie man die Wahrscheinlichkeiten von Ereignissen berechnet. In zweiten Kapitel 4  Verteilungen betrachten wir typische Verteilungen, die geeigenet sind zufällige Ereignisse zu beschreiben.\n?sec-statistics-estimates\n?sec-statistics-hypothesis\n\n\n\n\nFrisch, Max. 1957. Homo Faber. Ein Bericht. Frankfurt am Main: Suhrkamp.",
    "crumbs": [
      "Statistik"
    ]
  },
  {
    "objectID": "statistics/sampling.html",
    "href": "statistics/sampling.html",
    "title": "3  Stichproben und Zufallsvariablen",
    "section": "",
    "text": "3.1 Sampling aus einer Population\nIn diesem Abschnitt werden wir uns mit Stichproben und Zufallsvariablen beschäftigen.\nDie Auswahl unserer \\(n\\) Beobachtungen aus einer Grundgesamtheit wird als Stichprobe bezeichnet. Die Grundgesamtheit ist die Menge \\(N\\) aller möglichen Beobachtungen, die wir untersuchen möchten. Die Stichprobe ist eine Teilmenge der Grundgesamtheit. Im besten Fall zwischen den Beobachtungen in der Stichprobe und der Grundgesamtheit besteht eine Beziehung, die es uns erlaubt, Rückschlüsse auf die Grundgesamtheit zu ziehen.\nAls Grundgesamtheit (population) bezeichnen wir die Menge aller möglichen Beobachtungen, die wir untersuchen möchten. Die Stichprobe (sample) ist eine Teilmenge der Grundgesamtheit. Im besten Fall besteht eine Beziehung zwischen den Beobachtungen in der Stichprobe und der Grundgesamtheit, die es uns erlaubt, Rückschlüsse auf die Grundgesamtheit zu ziehen. In der Regel können wir nicht alle Daten einer Grundgesamtheit sammeln, da dies zu aufwändig und teuer wäre. Stattdessen sammeln wir eine Stichprobe von Daten und ziehen Rückschlüsse auf die Grundgesamtheit (vgl. Figure @fig-sec-dataexploratory-sampling-sampling). Die Stichprobe sollte möglichst repräsentativ für die Grundgesamtheit sein, damit wir verallgemeinern können. Hierzu kommen wir in der Regel, wenn wir eine große Stichprobe ziehen und diese zufällig auswählen.\nFigure 3.1: Sampling from a population.\nAllerdings kommt es hier zu einen Unterschied zwischen der Sichtweise der klassischen Statistik und dem Ansatz den den viele Data Scientists verfolgen. In der klassischen Statistik wird die Stichprobe so gewählt, dass sie repräsentativ für die Grundgesamtheit ist.\nAls Data Scientist hingegen, sind wir oft an den Daten interessiert, die uns zur Verfügung stehen. Wir haben keine Möglichkeit, die Grundgesamtheit zu beeinflussen. Wir müssen also mit den Daten arbeiten, die wir haben und uns dabei bewusst sein, dass wir einem Sampling-Bias unterliegen.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Stichproben und Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "statistics/sampling.html#sampling-aus-einer-population",
    "href": "statistics/sampling.html#sampling-aus-einer-population",
    "title": "3  Stichproben und Zufallsvariablen",
    "section": "",
    "text": "Note\n\n\n\nWenn wir die Leistungsfähigkeit in Mathematik unter Studierenden auswerten wollen, dann sollten wir unsere Stichprobe nicht nur im Studiengang Mechatronik nachfragen.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWenn wir die die Lebensdauer eines Werkzeugs auf einer 5-Achs-Fräsmaschinene prognostizieren wollen, können wir die Modelle nicht zwischen Betrieben vergleichen, die die Maschine regelmäßig warten und solchen, die das nicht tun.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Stichproben und Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "statistics/sampling.html#gruppieren-von-daten",
    "href": "statistics/sampling.html#gruppieren-von-daten",
    "title": "3  Stichproben und Zufallsvariablen",
    "section": "3.2 Gruppieren von Daten",
    "text": "3.2 Gruppieren von Daten\nWir könnten in unserem Beispiel von letztem Mal bewusst einen Bias (Verzerrung) bei der Auswahl der Daten einführen.\nWenn wir Beispielweise nur die Einkommen von personen mit einem Kredit auf ihr Haus laufen haben (mortgage) nach ihrem Einkommen fragen und nicht die von Personen die zur Miete wohnen (rent), dann haben wir einen Bias in unseren Daten und erhalten keine repräsentative Stichprobe.\n\nimport pandas as pd\nimport seaborn as sns\n\n# Lesen der CSV-Datei in einen DataFrame\ndf = pd.read_csv(\"../_assets/dataexploratory/loan50.csv\")\ndf.columns\n\nsns.histplot(data=df, x=\"annual_income\", bins=30, hue=\"homeownership\")\n\n\n\n\n\n\n\n\n\ndf.groupby(\"homeownership\")[\"annual_income\"].mean()\n\nhomeownership\nmortgage    99807.692308\nown         67666.666667\nrent        71928.571429\nName: annual_income, dtype: float64\n\n\n\n\n\n\n\n\nNote\n\n\n\nInwiefern entsprechen die Daten hier unten Erwartungen?\n\n\nIm vorliegenden beispiel haben wir Beobachtungen, die eine ordinal skalierte Variable (homeownership) und eine metrisch skalierte Variable (annual_income) enthalten. Wir interessieren und für den Zusammenhang. Die in der letzten Einheit besprochenen Metriken Korrelation und Kausalität sind hier nicht anwendbar, da die Variable homeownership eine ordinal Variable ist.\n\n3.2.1 Boxplot\nFür den Vergleich von ordinalen und metrischen Variablen eignet sich der Boxplot. Hier können wir die Verteilung der metrischen Variable für die verschiedenen Ausprägungen der ordinalen Variable darstellen. Die Box zeigt den Median und das erste und dritte Quartil. Die Whisker zeigen die Ausdehnung der Daten und die Punkte sind Ausreißer. Die Whisker werden in der Regel als 1.5-fache der Interquartilsdistanz definiert und starten am 1. bzw. 3. Quartil.\n\n# Box plot\nsns.boxplot(data=df, x=\"homeownership\", y=\"annual_income\")\n\n# Actual observations\nsns.stripplot(data=df, x=\"homeownership\", y=\"annual_income\", color=\"black\", size=3, alpha=0.5)\n\n\n\n\nBoxplot der jährlichen Einkommen nach Art des Wohneigentums.\n\n\n\n\nDer Boxplot lässt sich einfach berechnen und interpretieren. Allerdings ist er nicht sehr aussagekräftig, wenn die Verteilungen der Daten sehr unterschiedlich sind. In diesem Fall ist es sinnvoll, die Daten zu transformieren oder eine andere Visualisierung zu wählen.\n\n\n\n\n\n\nTip\n\n\n\nEine moderne Alternative zum Boxplot ist der Violinplot. Dieser zeigt die Verteilung der Daten als geschätzte Wahrscheinlichkeitsdichte (für uns bisher geglättetes Histogram). Der Violinplot ist informativer als der Boxplot, da er die Verteilung der Daten besser darstellt. \n\n\n\n\n3.2.2 Experimente\nIn der Statistik unterscheiden wir zwischen Beobachtungsstudien und Experimenten. In Beobachtungsstudien beobachten wir die Daten, ohne sie zu beeinflussen. In Experimenten hingegen manipulieren wir die Daten, um einen Effekt zu beobachten. In der Regel sind Experimente aufwendiger und teurer als Beobachtungsstudien, aber sie erlauben uns, Kausalzusammenhänge zu untersuchen.\n\n\n\n\n\n\nNote\n\n\n\nEine Beobachtungsstudie könnte longitudinal sein, d.h. über einen längeren Zeitraum Beobachtungen sammeln: z.B. wie sich die Leistungsfähigkeit im Mathematik von Studierenden über die Zeit verändert. Oder Querschnittsstudien, bei denen wir zu einem bestimmten Zeitpunkt Daten sammeln: z.B. wie die Leistungsfähigkeit in Mathematik von Studierenden in verschiedenen Studiengängen ist.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWenn wir klären wollen, ob unterschiedliche Studiengänge eine Auswirkung auf die Leistungsfähigkeit in Mathematik haben, könnten wir eine experimentelle Studie durchführen, bei der wir Studierende zufällig verschiedenen Studiengängen zuweisen und ihre Leistungsfähigkeit in Mathematik messen. Hierzu müssten wir die Studierenden zufällig auswählen und ihnen zufällig einen Studiengang zuweisen (ethische Bedenken beachten) und ihre Leistungsfähigkeit in Mathematik vor und nach dem Studium messen. Die Messung vor dem Studium könnten wir uns sparen, wenn wir die Studierenden zufällig zuweisen. Gute wissenschaftliche Praxis erfordert, wäre es auch noch eine Kontrollgruppe zu haben, die keinen Studiengang zugeteilt bekommt. Damit kann ausgeschlossen werden, dass die Leistungsfähigkeit in Mathematik durch andere Faktoren (z.B. Alter) beeinflusst wird.\n\n\nExperimente sind der Goldstandard, um Kausalzusammenhänge zu untersuchen, da hierbei versucht wird einen unabhängigen (welches Studium wird abgeschlossen) zu identifizieren und gezielt zu manipulieren. Alle anderen Variablen werden konstant gehalten oder durch große Stichproben ausgeglichen. Finden wir dann eine Korrelation zwischen dem unabhängigen Variable (Studiengang) und einen abhängigen Variable (Leistungsfähigkeit in Mathe) können wir einen Kausalzusammenhang vermuten.\nIm bereich von Data Science gilt häufig ein pragmatischerer but it works Ansatz. Hier wird oft versucht, mit den vorhandenen Daten zu arbeiten und Rückschlüsse zu ziehen. Dies ist in der Regel einfacher und schneller, aber auch weniger zuverlässig.\n\n3.2.2.1 Beispiel: Ist ein Würfel gezinkt?\nStellen wir uns vor, unsere Kolleg:in besteht darauf mit ihrem selbst mitgebrachten Würfel zu spielen. Wir sind uns aber nicht sicher, ob der Würfel gezinkt ist. Wir könnten nun eine Beobachtungsstudie durchführen, indem wir die Augenzahlen des Würfels beobachten und mit unserer Erwartung oder einem fairen Würfel vergleichen. um eine Stichprobe zu erstellen, können wir beide Würfel 1000 mal werfen. Die Würfe des fairen Würfeln nennen wir dabei die Kontrollgruppe.\nWenn wir ein eine Häufigkeitsverteilung, wie in Histogram Figure 3.2 beobachten würden, würden wir misstrauisch werden.\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Würfeln\nnp.random.seed(42)\nfair_dice_rolls = np.random.randint(1, 7, 1000)\n# A manipulated dice that has a higher probability of rolling a 6\nother_dice_rolls = np.random.choice([1, 2, 3, 4, 5, 6], 1000, p=[1.9/12, 1.9/12, 1.9/12, 1.9/12, 1.9/12, 2.5/12])\n\n# Histogramm\nsns.histplot(fair_dice_rolls, bins=6, discrete=True, color=\"lightblue\", alpha = 0.2, label=\"Fair Dice\")\nsns.histplot(other_dice_rolls, bins=6, discrete=True, color=\"red\", alpha = 0.2, label=\"Manipulated Dice\")\n\n# Add ledgend\nplt.legend()\n\n\n\n\n\n\n\nFigure 3.2: Verteilung der Würfelergebnisse.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Stichproben und Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "statistics/sampling.html#blickpunkte-auf-variablen",
    "href": "statistics/sampling.html#blickpunkte-auf-variablen",
    "title": "3  Stichproben und Zufallsvariablen",
    "section": "3.3 Blickpunkte auf Variablen",
    "text": "3.3 Blickpunkte auf Variablen\nWie haben uns nun Variablen (Spalten in tidy data) angeschaut und gesehen und können diese aus verschiedenen Blickwinkeln betrachten.\n\n3.3.1 Skalenniveaus\nSkalenniveaus sind eine Möglichkeit, Variablen zu klassifizieren. Es gibt vier Skalenniveaus: nominal, ordinal, metrisch und verhältnisskaliert. Diese Skalenniveaus geben uns Hinweise darauf, welche statistischen Methoden wir verwenden können, um die Daten zu analysieren.\n\n\n3.3.2 Im Kontext von Experimenten\nIm Kontext von Experimenten und Beobachtungsstudien unterscheiden wir zwischen unabhängigen und abhängigen Variablen. Die unabhängige Variable ist die Variable, welche uns als Einflussgröße interessiert. Die abhängige Variable ist die Variable, die wir messen, um den Effekt zu beobachten. Leider gibt es hier je nach Sprache und Fachgebiet unterschiedliche Bezeichnungen. Die folgende Tabelle gibt einen Überblick über verschiedene Bezeichnungen. Später werden wir merken, dass es nicht immer nur genau eine unabhängige und eine abhängige Variable gibt, sondern auch mehrere unabhängige und abhängige Variablen geben kann. Der Einfacheit halber sprechen wir aber zunächst nur im Singular.\n\n\n\nAnwendungsfeld\nUnabhängige Variable\nAbhängige Variable\n\n\n\n\nStatistik\nExplanatory Variable\nResponse Variable\n\n\nMachine Learning\nFeatures\nTarget\n\n\nExperimente\nTreatment\nOutcome\n\n\nPsychologie\nIndependent Variable\nDependent Variable\n\n\nForecasts\nPredictor\nPredicted Variable\n\n\nÖkonometrie\nExplanatory Variable\nDependent Variable\n\n\nInformatik\nInput\nOutput\n\n\nProgramming\nArgument\nReturn Value\n\n\nProgramming\nX\ny",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Stichproben und Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "statistics/sampling.html#wahrscheinlichkeitsrechnung",
    "href": "statistics/sampling.html#wahrscheinlichkeitsrechnung",
    "title": "3  Stichproben und Zufallsvariablen",
    "section": "3.4 Wahrscheinlichkeitsrechnung",
    "text": "3.4 Wahrscheinlichkeitsrechnung\nEine weitere Betrachtunsweise von Variablen sind die Prozesse, die zu den Beobachtungen führen. Hierbei unterscheiden wir zwischen deterministischen und zufälligen Prozessen. Deterministische Prozesse führen immer zum gleichen Ergebnis, wenn wir die gleichen Bedingungen haben. Zufällige Prozesse führen zu unterschiedlichen Ergebnissen, auch wenn die Bedingungen gleich sind. Zufällige Prozesse können durch Wahrscheinlichkeiten beschrieben werden.\n\n\n\n\n\n\nImportant\n\n\n\nHierbei ist es irrelevant, ob ob wir uns das Universum als vorherbestimmt (deteministisch) oder zufällig vorstellen. Zufälligkeit bedeutet uns in der Folge nur, dass wir das Ergebnis nicht a-priori vorhersagen können. Die Zufälligkeit durch einen echten Zufallsprozess (z.B. Würfeln) oder durch eine unvollständige Modellierung (z.B. durch fehlende Variablen) entsteht.\n\n\n\n3.4.1 Zufallsvariablen\nEine Zufallsvariable ist eine Variable, die zufällig einen von mehreren verschiedenen Werten annimmt. Wenn wir eine Münze werfen, ist die Zahl, die auf dem Würfel erscheint, eine Zufallsvariable. Zufallsvariablen können diskret oder kontinuierlich sein. Diskrete Zufallsvariablen können nur bestimmte Werte annehmen, während kontinuierliche Zufallsvariablen jeden Wert in einem Intervall annehmen können.\nDie Augenzahl eines Würfels ist eine diskrete Zufallsvariable, da sie nur die Werte 1, 2, 3, 4, 5 oder 6 annehmen kann. Wenn wir den Würfel werfen, ist die Augenzahl jedes mal eine zufällige Realisierung der Zufallsvariable.\n\nimport numpy as np\nimport seaborn as sns\n\n# Würfeln\nnp.random.seed(42)\ndice_rolls = np.random.randint(1, 7, 1000)\n\n# Histogramm\nsns.histplot(dice_rolls, bins=6, discrete=True)\n\n\n\n\n\n\n\nFigure 3.3: Verteilung der Würfelergebnisse.\n\n\n\n\n\nÄnlich verhält es sich bei einem Münzwurf. Hier ist die Zufallsvariable die Seite der Münze, die oben liegt. Diese kann entweder Kopf oder Zahl sein. Damit wir sinnvoll mit dem katregorischen Datentyp umgehen können, können wir die Werte in 0 und 1 umwandeln. Das bedeutet, dass wird aus dem Sample Space (Kopf, Zahl) einen numerischen Wert (0, 1) machen.\nHierzu definieren wir nun einige Begriffe:\n\nZufallsexperiment/-Prozess: Ein Experiment, bei dem das Ergebnis zufällig ist. Zufällig bedeutet, dass wir das Ergebnis nicht vorhersagen können. Zum Beispiel das Werfen einer Münze, eines Würfels oder das Ziehen einer Karte aus einem Kartenspiel.\nEreignisraum (sample space): Die Menge aller möglichen Ergebnisse (outcomes) eines Zufallsexperiments. Zum Beispiel beim Werfen einer Münze ist der Ereignisraum \\({\\text{Kopf}, \\text{Zahl}}\\).\n(messbare) Zufallsvariable: Eine Funktion, die jedem Ergebnis eines Zufallsexperiments eine Zahl zuordnet. Zum Beispiel die \\(1\\) für Kopf und \\(0\\) für Zahl beim Münzwurf.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Stichproben und Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "statistics/sampling.html#frequentischtische-definition-der-wahrscheinlichkeit",
    "href": "statistics/sampling.html#frequentischtische-definition-der-wahrscheinlichkeit",
    "title": "3  Stichproben und Zufallsvariablen",
    "section": "3.5 Frequentischtische Definition der Wahrscheinlichkeit",
    "text": "3.5 Frequentischtische Definition der Wahrscheinlichkeit\n\n\n\n\n\n\nImportant\n\n\n\nDie Wahrscheinlichkeit eines Ergebnisses ist der Anteil, wie oft das Ergebnis eintreten würde, wenn wir den Zufallsprozess unendlich oft beobachten würden. Die Wahrscheinlichkeit ist als Anteil definiert und nimmt immer Werte zwischen 0 und 1 (einschließlich) an. Sie kann auch als Prozentsatz zwischen 0% und 100% angezeigt werden.\n\n\nIn Figure Figure 3.3 kam die Augenzahl 2 in etwa 167/1000 der Fälle vor. Die Wahrscheinlichkeit, dass die Augenzahl 1 beträgt, ist also 1/6 oder etwa 0,167. Gleiches gibt für die anderen Augenzahlen.\nEs is offentsichtlich, dass das bei einer endlichen Anzahl von Beobachtungen nicht immer so sein muss. Werfen wir den Würfel nur einmal, so kann die Augenzahl nur entweder 1, 2, 3, 4, 5 oder 6 sein. Für eine endliche Anzahl von Beobachtungen kann die Wahrscheinlichkeit also nur eine Schätzung sein. Umgekehrt ist die Wahrscheinlichkeit eine Grenzwertbetrachtung, die nur für unendlich viele Beobachtungen gilt.\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nnuber_of_rolls = 1000\nnp.random.seed(12)\ndice_rolls = np.random.randint(1, 7, nuber_of_rolls)\n\n# Anteil der 1er bei jedem Schritt\nproportion_ones = np.cumsum(dice_rolls == 1) / np.arange(1, nuber_of_rolls+1)\n\n# Plot\n\nsns.lineplot(x=np.arange(1, nuber_of_rolls+1), y=proportion_ones)\nsns.lineplot(x=[1, nuber_of_rolls+1], y=[1/6, 1/6], color=\"red\", linestyle=\"--\")\n\nplt.xlabel(\"$n$ - Number of rolls\")\nplt.ylabel(\"$\\hat{p}_n$ - Proportion of 1s \")\n\n&lt;&gt;:18: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n&lt;&gt;:18: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n/tmp/ipykernel_3035/1774020984.py:18: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n\n\n\n\n\n\n\nText(0, 0.5, '$\\\\hat{p}_n$ - Proportion of 1s ')\n\n\n(a) Der Anteil der Würfelergebnisse, die 1 sind, bei jedem Schritt einer Simulation.\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 3.4\n\n\n\n\nFigure Figure 3.4 zeigt den Anteil \\(\\hat{p}_n\\) der Augenzahl 1 bei jedem Schritt \\(n\\) einer Simulation. Der Anteil konvergiert gegen die Wahrscheinlichkeit von 1/6 oder etwa 0,167.\nDer beobeachtete Anteil \\(\\hat{p}_n\\) ist eine Schätzung der Wahrscheinlichkeit \\(p\\) und wird genauer, je mehr Beobachtungen wir haben. Die Wahrscheinlichkeit \\(p\\) ist der Grenzwert des Anteils.\n\n\n\n\n\n\nImportant\n\n\n\nWir werden in der Statistik oft mit Schätzungen arbeiten, da wir nicht unendlich viele Beobachtungen haben. Es ist wichtig, die Unsicherheit unserer Schätzungen zu kennen und zu berücksichtigen. Schätzungen kennzeichnen wir oft mit einem Dach über dem Symbol, z.B. \\(\\hat{p}\\).\n\n\n\n\n\n\n\n\nImportant\n\n\n\nGesetz der großen Zahlen: Mit zunehmender Anzahl von Beobachtungen konvergiert der Anteil der Beobachtungen mit einem bestimmten Ergebnis gegen die Wahrscheinlichkeit dieses Ergebnisses.\n\n\nDa wir verschiedene Ergebnisse haben, führen wir eine neue Schreibweise ein. Wir schreiben \\(P(X = x)\\) für die Wahrscheinlichkeit, dass die Zufallsvariable \\(X\\) (z.B. Münzwürf) den Wert \\(x\\) annimmt.\nFür einen fairen Münzwurf ist: \\[\nP(X = \\text{Kopf}) = 0.5,\n\\]\nund\n\\[\nP(X = \\text{Zahl}) = 0.5.\n\\]\nbzw.\n\\[\nP(X = 1) = 0.5,\n\\]\nund\n\\[\nP(X = 0) = 0.5.\n\\]\nFür einen fairen Würfelwurf ist: \\[\nP(X = 1) = P(X = 2) = P(X = 3) = P(X = 4) = P(X = 5) = P(X = 6) = 1/6.\n\\]\nDie Summe der Wahrscheinlichkeiten aller möglichen Ergebnisse eines Zufallsexperiments ist immer 1. Das bedeutet, dass mindestens eines der möglichen Ergebnisse eintreten muss. Vergleiche dies mit der Fläche unter einem normalisieren Histogramm oder einer Dichtefunktion.\n\n3.5.1 Disjunkte bzw. Sich-Ausschließende Ereignisse\nZwei Ereignisse \\(A\\) und \\(B\\) sind disjunkt oder sich ausschließend, wenn sie nicht gleichzeitig eintreten können. Das bedeutet, dass wenn \\(A\\) eintritt, \\(B\\) nicht eintreten kann und umgekehrt. Beispielsweise kann bei einem Wurf mit einem fairen Würfel die Augenzahl nicht gleichzeitig 1 und 2 sein. Die Ereignisse “Augenzahl ist 1” und “Augenzahl ist 2” sind sich ausschließend.\nDie Wahrscheinlichkeit, dass eines der beiden Ereignisse auftritt, ist die Summe der Wahrscheinlichkeiten der beiden Ereignisse. Als Notation schreiben wir \\(P(A \\cup B)\\) für die Wahrscheinlichkeit, dass eines der beiden Ereignisse eintritt. Dies entspricht einem logischen oder. Wir lesen A vereinigt B.\nFür disjunkte Ereignisse gilt:\n\\[\nP(X=1 \\cup X=2) = P(X=1) + P(X=2) = \\frac{1}{6} + \\frac{1}{6} = \\frac{1}{3}.\n\\]\n\n\n\n\n\n\nImportant\n\n\n\nDie Additionsregel besagt, dass die Wahrscheinlichkeit, dass eines von zwei sich ausschließenden Ereignissen (\\(A\\), \\(B\\)) eintritt, die Summe der Wahrscheinlichkeiten der beiden Ereignisse ist.\n\\[\nP(A \\cup B) = P(A) + P(B).\n\\]\nBzw. für mehrere sich ausschließende Ereignisse (\\(A_*\\)):\n\\[\nP(A_1 \\cup A_2 \\cup ... \\cup A_n ) = P(A_1) + P(A_2) + ... + P(A_n).\n\\]\n\n\n\n3.5.1.1 Beispiel\nIm Datensatz der Kredite in Kapitel 2 beschreibt die Variable homeownership, ob der Kreditnehmer mietet, eine Hypothek hat oder Eigentümer seiner Immobilie ist. Von den 10.000 Kreditnehmern mieteten 3858, 4789 hatten eine Hypothek und 1353 besaßen ihr Zuhause.\n\nimport pandas as pd\n\ndf = pd.read_csv(\"../_assets/dataexploratory/loan50.csv\")\nprint(f\"Number of Observations: {df[\"homeownership\"].shape[0]}\")\nprint(df[\"homeownership\"].value_counts())\n\nNumber of Observations: 50\nhomeownership\nmortgage    26\nrent        21\nown          3\nName: count, dtype: int64\n\n\n\nSind die Ergebnisse Miete, Hypothek und Eigentum disjunkt?\nBestimmen Sie den Anteil der Kredite mit dem Wert Hypothek und Eigentum separat.\nVerwenden Sie die Additionsregel für disjunkte Ergebnisse, um die Wahrscheinlichkeit zu berechnen, dass ein zufällig ausgewählter Kredit aus dem Datensatz für jemanden ist, der eine Hypothek hat oder Eigentümer seines Hauses ist.\n\n\n\nJa, die Ergebnisse sind disjunkt, da ein Kreditnehmer nur eine der drei Kategorien haben kann.\nDer Anteil der Kredite mit Hypothek beträgt \\(\\frac{26}{50}\\) und der Anteil der Kredite mit Eigentum beträgt \\(\\frac{3}{50}\\).\nDie Wahrscheinlichkeit, dass ein zufällig ausgewählter Kreditnehmer eine Hypothek hat oder Eigentümer seines Hauses ist, beträgt \\(\\frac{26}{50}\\) + \\(\\frac{3}{50}\\) = \\(\\frac{29}{50}\\). Dies entspricht der Wahrscheinlichkeit, dass ein zufällig ausgewählter Kreditnehmer nicht mietet.\n\n\n\n\n\n3.5.2 Das Komplement eines Ereignisses\nDas Komplement eines Ereignisses \\(A\\) ist das Ereignis, dass \\(A\\) nicht eintritt. Das Komplement von \\(A\\) wird als \\(A^c\\) oder \\(\\bar{A}\\) bezeichnet. Das Komplement eines Ereignisses ist das Ereignis, dass alle anderen Ergebnisse eintreten, die nicht \\(A\\) sind. Das Komplement eines Ereignisses ist disjunkt zu diesem Ereignis.\nDie Wahrscheinlichkeit des Komplements eines Ereignisses ist die Wahrscheinlichkeit, dass das Ereignis nicht eintritt. Die Wahrscheinlichkeit des Komplements eines Ereignisses ist \\(1 - P(A)\\).\nBeispielsweise ist die Wahrscheinlichkeit, dass die Augenzahl eines fairen Würfels nicht 1 ist,\n\\[\n1 - P(X=1) = 1 - \\frac{1}{6} = \\frac{5}{6}.\n\\]\nDie Summe der Wahrscheinlichkeiten eines Ereignisses und seines Komplements ist immer 1. Das bedeutet, dass entweder das Ereignis oder sein Komplement eintreten muss.\n\n\n3.5.3 Nicht-disjunkte Ereignisse\nNicht immer haben wir den Luxus, disjunkte Ereignisse zu betrachten. In diesem Fall können die Ereignisse überlappen. Beispielsweise wollen wir beim Kartenspiel wissen, wie hoch die Wahrscheinlichkeit ist, dass wir eine Karte mit einem Bild (Face) oder einer Karo-Farbe (Diamond) ziehen. Die Ereignisse “Bild” und “Karo” sind nicht disjunkt, da es Karten gibt, die beides sind. Wenn wird die Wahrscheinlichkeiten einfach addieren, zählen wir die Karten, die beides sind, doppelt und überschätzen die Wahrscheinlichkeit.\n\n\n\n\n\n\nFigure 3.5: Deck of 52 Cards\n\n\n\nUm um das zu verbildlichen können wie die Ereignisse in einem Venn-Diagramm darstellen. Hierin werden alle Ereignisse (z.B. Farbe ist Karo bzw. Karte ist Bildkarte) in einem Diagramm dargestellt. Die Schnittmenge (als \\(\\cap\\) geschrieben) der beiden Ereignisse ist die Menge der Karten, die beides sind. Dies entspricht dem logischen und und wird als Schnitt bezeichnet.\nDie Vereinigungsmenge (logisches oder / \\(\\cup\\)) ist die Menge aller Karten, die entweder das eine oder das andere Ereignis sind.\n\n\n\n\n\n\nFigure 3.6: Venn Diagramm of Cards\n\n\n\nVon den \\(N=52\\) Karten sind \\(12\\) Bildkarten (Bube, Dame, König) und \\(13\\) Karo-Karten. Davon sind \\(k=3\\) Karten sowohl Bildkarten als auch Karo-Karten. Die Wahrscheinlichkeit, dass eine Karte entweder eine Bildkarte oder eine Karo-Karte ist, ist die Summe der Wahrscheinlichkeiten der beiden Ereignisse abzüglich der Wahrscheinlichkeit des Schnitts:\n\\[\nP(\\text{Bild} \\cup \\text{Karo}) = P(\\text{Bild}) + P(\\text{Karo}) - P(\\text{Bild} \\cap \\text{Karo}).\n= \\frac{12}{52} + \\frac{13}{52} - \\frac{3}{52} = \\frac{22}{52}.\n\\]\nHierraus können wir die Additionsregel für nicht-disjunkte Ereignisse formulieren:\n\n\n\n\n\n\nImportant\n\n\n\nDie Generelle Additionsregel besagt, dass die Wahrscheinlichkeit, dass eines von zwei Ereignissen (\\(A\\), \\(B\\)) eintritt, die Summe der Wahrscheinlichkeiten der beiden Ereignisse ist, abzüglich der Wahrscheinlichkeit, dass beide Ereignisse eintreten.\n\\[\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B).\n\\]\n\n\n\n3.5.3.1 Beispiel\nSei \\(A\\) ein Ereignis, bei dem zwei faire Würfel geworfen werden und dabei die Summe der Augenzahl kleiner als 12 ist.\n\n\nWas ist das Komplement von \\(A\\)?\n\n\nWie groß ist die Wahrscheinlichkeit von \\(A\\)?\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nDas Komplement von \\(A\\) ist das Ereignis, bei dem die Summe der Augenzahl 12 oder mehr beträgt.\n\n\nDie Wahrscheinlichkeit von \\(A\\) ist die Summe der Wahrscheinlichkeiten der Ergebnisse, bei denen die Summe der Augenzahl kleiner als 12 ist. Das sind die Ergebnisse 2 bis 11.\n\n\nWir müssen die Wahrscheinlichkeiten der einzelnen Ergebnisse addieren:\nSei \\(W_1\\) das Ereignis von Würfel 1 und \\(W_2\\) das Ereignis von Würfel 2. Wir können nun eine Tabelle bauen, in denen wir sammeln, welche Ereignisse für welche Augenzahlen sorgen:\n\n\n\n\n\n\n\n\\(A = W_1 + W_2\\)\nMögliche Kombinationen\n\n\n\n\n2\n\\((W_1=1 \\cap W_2=1)\\)\n\n\n3\n\\((W_1=1 \\cap W_2=2) \\cup (W_1=2 \\cap W_2=1)\\)\n\n\n4\n\\((W_1=1 \\cap W_2=3)  \\cup (W_1=2 \\cap W_2=2)  \\cup  (W_1=3 \\cap W_2=1)\\)\n\n\n5\n\\((W_1=1 \\cap W_2=4) \\cup (W_1=2 \\cap W_2=3) \\cup (W_1=3 \\cap W_2=2) \\cup (W_1=4 \\cap W_2=1)\\)\n\n\n6\n\\((W_1=1 \\cap W_2=5) \\cup (W_1=2 \\cap W_2=4) \\cup (W_1=3 \\cap W_2=3) \\cup (W_1=4 \\cap W_2=2) \\cup (W_1=5 \\cap W_2=1)\\)\n\n\n7\n\\((W_1=1 \\cap W_2=6) \\cup (W_1=2 \\cap W_2=5) \\cup (W_1=3 \\cap W_2=4) \\cup (W_1=4 \\cap W_2=3) \\cup (W_1=5 \\cap W_2=2) \\cup (W_1=6 \\cap W_2=1)\\)\n\n\n8\n\\((W_1=2 \\cap W_2=6) \\cup (W_1=3 \\cap W_2=5) \\cup (W_1=4 \\cap W_2=4) \\cup (W_1=5 \\cap W_2=3) \\cup (W_1=6 \\cap W_2=2)\\)\n\n\n9\n\\((W_1=3 \\cap W_2=6) \\cup (W_1=4 \\cap W_2=5) \\cup (W_1=5 \\cap W_2=4) \\cup (W_1=6 \\cap W_2=3)\\)\n\n\n10\n\\((W_1=4 \\cap W_2=6) \\cup (W_1=5 \\cap W_2=5) \\cup (W_1=6 \\cap W_2=4)\\)\n\n\n11\n\\((W_1=5 \\cap W_2=6) \\cup (W_1=6 \\cap W_2=5)\\)\n\n\n12\n\\((W_1=6 \\cap W_2=6)\\)\n\n\n\nWir wissen, wie wir \\(P(W_1=6)\\) und \\(P(W_2=6)\\) einzeln ausrechen. Was uns aktuell noch fehlt ist wie wir die Wahrscheinlichkeit \\(P(W_1=x \\cap W_2=y)\\) ausrechnen, die uns sagt, wie wahrscheinlich es ist, dass zwei 6er gewürfelt werden. Wenn wir diese hätten könnten wie diese als Ereignisse zusammenzählen und die Wahrscheinlichkeit über die Additionregel berechnen. Was wird aber aus der Tabelle mitnehmen können ist, dass die Wahrscheinlichkeit für die Augenzahl \\(P(A=2)=P(W_1=1) \\cdot P(W_2=1)\\) ist und im vergleich zu \\(P(A=7)\\) wohl eine kleinere Wahrscheinlichkeit hat.\n\n\nImmer wenn die Wahrscheinlichkeitsberechung etwas schwieriger wird können wir auf die Simulation zurückgreifen. Bei einer sogenannten Monte-Carlo-Simulation führen wir das Experiment mehrfach durch und zählen die Anzahl der Fälle, die das Ereignis erfüllen. Die Wahrscheinlichkeit ist dann die Anzahl der Fälle, die das Ereignis erfüllen, geteilt durch die Anzahl der Durchläufe.\n\nimport numpy as np\nimport pandas as pd\ndf = pd.DataFrame(np.random.randint(1, 7, (10000, 2)), columns=[\"W1\", \"W2\"])\n\n# Summe der Augenzahlen\n\ndf[\"Sum\"] = df[\"W1\"] + df[\"W2\"]\n\n# Wahrscheinlichkeit\n\np = (df[\"Sum\"] &lt; 12).mean()\n\nprint(f\"Die Wahrscheinlichkeit, dass die Summe der Augenzahlen kleiner als 12 ist, beträgt {p:.2f}\")\n\nDie Wahrscheinlichkeit, dass die Summe der Augenzahlen kleiner als 12 ist, beträgt 0.97\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.histplot(df[\"Sum\"], bins=11, discrete=True, stat = 'density')\nplt.axvline(11, color=\"red\", linestyle=\"--\")\n\nplt.xlabel(\"Sum of dice rolls\")\nplt.ylabel(\"Frequency\")\n\n\n\n\n\n\nText(0, 0.5, 'Frequency')\n\n\n(a) Die Wahrscheinlichkeit, dass die Summe der Augenzahlen kleiner als 12 ist, beträgt 0.97.\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 3.7\n\n\n\n\nFigure Figure 3.7 zeigt die Verteilung der Summe der Augenzahlen bei einem Wurf mit zwei Würfeln. Die Wahrscheinlichkeit, dass die Summe der Augenzahlen kleiner als 12 ist, beträgt 0,97.\n\n\nDie Wahrscheinlichkeit, dass die Summe der Augenzahlen 12 oder mehr beträgt, ist \\(1 - P(A) = 1 - 0.97 = 0.03\\).",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Stichproben und Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "statistics/sampling.html#unabhängige-ereignisse",
    "href": "statistics/sampling.html#unabhängige-ereignisse",
    "title": "3  Stichproben und Zufallsvariablen",
    "section": "3.6 Unabhängige Ereignisse",
    "text": "3.6 Unabhängige Ereignisse\nZwei Ereignisse \\(A\\) und \\(B\\) sind unabhängig, wenn das Eintreten eines Ereignisses nicht das Eintreten des anderen Ereignisses beeinflusst (vgl. Korrelation und Kausalität in Chapter 2). Das bedeutet, dass die Wahrscheinlichkeit eines Ereignisses nicht von dem anderen Ereignis abhängt. Ein typisches Beispiel für unabhängige Ereignisse ist das Werfen einer Münze und das Werfen eines Würfels. Das Ergebnis des Münzwurfs beeinflusst das Ergebnis des Würfelwurfs nicht. Gleiches gibt dafür, dass wird wie im Beispiel oben die Augenzahl zweiter Würfel betrachten. Das Ergebnis des ersten Würfels beeinflusst das Ergebnis des zweiten Würfels nicht.\nDie Wahrscheinlichkeit, dass zwei unabhängige Ereignisse eintreten, ist das Produkt der Wahrscheinlichkeiten der beiden Ereignisse. Die Wahrscheinlichkeit, dass zwei unabhängige Ereignisse eintreten, ist \\(P(A \\cap B) = P(A) \\cdot P(B)\\).\n\n\n\n\n\n\nImportant\n\n\n\nDie Multiplikationsregel für unabhängige Ereignisse besagt, dass die Wahrscheinlichkeit, dass zwei unabhängige Ereignisse (\\(A\\), \\(B\\)) eintreten, das Produkt der Wahrscheinlichkeiten der beiden Ereignisse ist.\n\\[\nP(A \\cap B) = P(A) \\cdot P(B).\n\\]\nBzw. für mehrere unabhängige Ereignisse (\\(A_*\\)):\n\\[\nP(A_1 \\cap A_2 \\cap ... \\cap A_n ) = P(A_1) \\cdot P(A_2) \\cdot ... \\cdot P(A_n).\n\\]\n\n\n\n3.6.1 Beispiel 1\nMit diesem Puzzelstein können wir nun auch die Wahrscheinlichkeit jeder Augenzahl beim Würfeln berechnen. Die Ergebnisse der beiden Würfel 1 und 2 sind unabhängig und können deshalb multipliziert werden.\n\n\n\n\\(A = W_1 + W_2\\)\nMögliche Kombinationen\n\n\n\n\n2\n\\(W_1=1, W_2=1\\)\n\n\n3\n\\(W_1=1, W_2=2\\), \\(W_1=2, W_2=1\\)\n\n\n4\n\\(W_1=1, W_2=3\\), \\(W_1=2, W_2=2\\), \\(W_1=3, W_2=1\\)\n\n\n\n\n\n3.6.2 Beispiel 2\nWahr oder falsch. Bestimmen Sie, ob die folgenden Aussagen wahr oder falsch sind, und erklären Sie Ihre Begründung.\n\n\nWenn eine faire Münze viele Male geworfen wird und die letzten acht Würfe alle Kopf sind, dann ist die Wahrscheinlichkeit, dass der nächste Wurf Kopf ist, etwas weniger als 50%.\n\n\nDas Ziehen einer Bildkarte (Bube, Dame oder König) und das Ziehen einer roten Karte aus einem vollständigen Kartenspiel sind sich gegenseitig ausschließende Ereignisse.\n\n\nDas Ziehen einer Bildkarte und das Ziehen eines Asses aus einem vollständigen Kartenspiel sind sich gegenseitig ausschließende Ereignisse.\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n\nFalsch. Die Wahrscheinlichkeit, dass eine faire Münze Kopf zeigt, beträgt 50%. Die Ergebnisse der vorherigen Würfe beeinflussen die Wahrscheinlichkeit des nächsten Wurfs nicht, da die Würfe unabhängig sind.\n\n\nFalsch. Bildkarten können rote oder schwarze Farben haben. Die Ereignisse “Bildkarte” und “rote Karte” sind nicht disjunkt.\n\n\nWahr, da ein Ass keine Bildkarte ist.\n\n\n\n\n\n\n\n3.6.3 Bedingte Wahrscheinlichkeit\nBedingte Wahrscheinlichkeiten geben uns die Wahrscheinlichkeit eines Ergebnisses unter der Bedinung an, dass ein anderes Ereignis eingetreten ist oder ebenfall eintreten würde.\nWenn wir uns das das Kartenbeispiel vorstellen können wir uns Fragen, wie wahrscheinlich es ist eine Bildkarte zu ziehen, wenn wir schon wissen, dass die Karte eine Karo-Karte ist. Aus dem Venn-Diagramm ergibt sich, dass die Wahrscheinlichkeit hierfür wie folgt lautet:\n\\[\nP(\\text{Bild} | \\text{Karo}) = \\frac{3}{13}.\n\\]\nDas \\(|\\) wird hierbei als “gegeben” gelesen. Die Abhängigkeit lässt sich in einer Kreuztballe (contingency table) darstellen, die die Häufigkeiten aller Ereigniss-Kombinationen gegenüberstellt.\n\nimport pandas as pd\n\ndf = pd.read_csv(\"../_assets/dataexploratory/loan50.csv\")\n\n# Create a contingency table\ncontingency_table = pd.crosstab(df['homeownership'], df['has_second_income'])\n\n# Display the result\nprint(contingency_table)\n\nhas_second_income  False  True \nhomeownership                  \nmortgage              20      6\nown                    3      0\nrent                  19      2\n\n\nHierraus lässt sich z.B. ablegen, dass es 26 Haus-Kredite gibt,bei denen der Kreditnehmer eine Hypothek hat. Von diesen haben 6 ein zweites Einkommen. Die bedingte Wahrscheinlichkeit, dass ein Kreditnehmer mit einer Hypothek ein zweites Einkommen hat, beträgt also \\(P(\\text{Zweiteinkommen} | \\text{Hypothek}) = \\frac{6}{26}\\).\n\n\n\n\n\n\nImportant\n\n\n\nDie bedingte Wahrscheinlichkeit eines Ereignisses \\(A\\) unter der Bedingung, dass ein anderes Ereignis \\(B\\) eingetreten ist, ist die Wahrscheinlichkeit, dass \\(A\\) eintritt, wenn wir wissen, dass \\(B\\) eingetreten ist. Die bedingte Wahrscheinlichkeit wird als \\(P(A|B)\\) geschrieben und berechnet sich als:\n\\[\nP(A|B) = \\frac{P(A \\cap B)}{P(B)}.\n\\]\nIm Fall unseres Beispiels ist die bedingte Wahrscheinlichkeit, dass ein Kreditnehmer ein zweites Einkommen hat, wenn er eine Hypothek hat,\n\\[\nP(\\text{Zweiteinkommen} | \\text{Hypothek}) = \\frac{\\text{Zweiteinkommen} \\cap \\text{Hypothek}}{\\text{Hypothek}}=\\frac{\\frac{6}{50}}{\\frac{26}{50}} = \\frac{6}{26}.\n\\]\n\n\n\n\n\n\n\n\nWarning\n\n\n\nDie Wahrscheinlichkeit von \\(P(A \\cap B)\\) kann nicht einfach mittels der Multiplikationsregel berechnet werden, da die Ereignisse nicht unabhängig sind. Stattdessen muss sie aus den beobachteten Häufigkeiten berechnet werden.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nSumme von bedingten Wahrscheinlichkeiten\nSeien \\(A_1, ..., A_k\\) alle disjunkten Ergebnisse für eine Variable oder einen Prozess. Dann gilt für ein Ereignis \\(B\\), möglicherweise für eine andere Variable oder einen anderen Prozess:\n\\[\nP(A_1|B) + · · · + P(A_k|B) = 1\n\\]\nDie Regel für Komplemente gilt auch, wenn ein Ereignis und sein Komplement unter denselben Informationen bedingt sind:\n\\[\nP(A|B) = 1 − P(A^c|B).\n\\]\n\n\n\n\n3.6.4 Beispiel\n\n\n\n\n\n\nFigure 3.8: Meme Test Cancer\n\n\n\nWie hoch ist die Wahrscheinlichkeit, dass ein Patient Krebs hat, wenn wir folgendes wissen:\n\nDie Wahrscheinlichkeit, dass eine Person Krebs hat, beträgt 0.1%.\nIm Falle, dass eine Person Krebs hat, beträgt die Wahrscheinlichkeit, dass der Test positiv ist, 99%. (dies nennt man auch ist die Sensitivität des Tests)\nIm Falle, dass eine Person keinen Krebs hat, beträgt die Wahrscheinlichkeit, dass der Test positiv ist, 5%. (dies nennt man auch ist die Spezifität des Tests)\n\n\nZunächst können wir das ganze als Baumdiagramm darstellen. Hierbei ist die Wahrscheinlichkeit, dass eine Person Krebs hat \\(P(\\text{Krebs}) = 0.001\\) und die Wahrscheinlichkeit, dass eine Person keinen Krebs hat \\(P(\\text{Kein Krebs}) = 0.999\\). Die Wahrscheinlichkeit, dass der Test positiv ist, wenn eine Person Krebs hat, ist \\(P(\\text{Positiv} | \\text{Krebs}) = 0.99\\) und die Wahrscheinlichkeit, dass der Test positiv ist, wenn eine Person keinen Krebs hat, ist \\(P(\\text{Positiv} | \\text{Kein Krebs}) = 0.05\\).\n\n\n\n\n\ngraph LR\n    U[Person] --&gt;|0.001| A[Krebs] \n    U[Person] --&gt;|0.999| D[Kein Krebs]\n    A[Krebs] --&gt;|0.99| B[Positiv]\n    A[Krebs] --&gt;|0.01| C[Negativ]\n    D[Kein Krebs] --&gt;|0.05| E[Positiv]\n    D[Kein Krebs] --&gt;|0.95| F[Negativ]\n\n\n\n\n\n\nWir können nun die alle Pfade, die mit einem positiven Test enden, zusammenzählen und die Wahrscheinlichkeit berechnen, dass eine Person Krebs hat, wenn der Test positiv ist. Wir dürfen dieses addieren, da die Pfade disjunkt sind.\nFür den oberen Pfad ist die Wahrscheinlichkeit, dass eine Person Krebs hat und der Test positiv ist, \\(P(\\text{Krebs} \\cap \\text{Positiv}) = P(\\text{Krebs}) \\cdot P(\\text{Positiv} | \\text{Krebs}) = 0.001 \\cdot 0.99 = 0.00099\\).\nFür den unteren Pfad ist die Wahrscheinlichkeit, dass eine Person keinen Krebs hat und der Test positiv ist, \\(P(\\text{Kein Krebs} \\cap \\text{Positiv}) = P(\\text{Kein Krebs}) \\cdot P(\\text{Positiv} | \\text{Kein Krebs}) = 0.999 \\cdot 0.05 = 0.04995\\).\nDie Wahrscheinlichkeit, dass der Test positiv ist, beträgt also \\(P(\\text{Positiv}) = 0.00099 + 0.04995 = 0.05094\\).\nTrotz eines positiven Test ist es also sehr unwahrscheinlich, dass eine Person Krebs hat. Die Wahrscheinlichkeit, dass eine Person Krebs hat, wenn der Test positiv ist, beträgt \\(P(\\text{Krebs} | \\text{Positiv}) = \\frac{P(\\text{Krebs} \\cap \\text{Positiv})}{P(\\text{Positiv})} = \\frac{0.00099}{0.05094} = 0.0194\\).\nIn der Realität werden Personen mit einem positiven Testergebnis oft weiteren Tests unterzogen, um die Wahrscheinlichkeit zu erhöhen, dass die Diagnose korrekt ist. Dies wird als Bestätigungstest bezeichnet. Zudem werden nur Personen getestet, wenn es schon Anzeichen für eine Erkrankung gibt. Dies wird als Prävalenz bezeichnet und ist dann in der Stichprobe höher als in der Gesamtbevölkerung.\n\n\n\n3.6.5 Satz von Bayes\nDer Satz von Bayes ist ein mathematischer Satz, der uns erlaubt, die Wahrscheinlichkeit eines Ereignisses unter der Bedingung eines anderen Ereignisses zu berechnen. Der Satz von Bayes ist besonders nützlich, wenn wir die bedingte Wahrscheinlichkeit eines Ereignisses unter der Bedingung eines anderen Ereignisses kennen, aber die bedingte Wahrscheinlichkeit des anderen Ereignisses unter der Bedingung des ersten Ereignisses berechnen möchten.\nDer Satz von Bayes lautet:\n\\[\nP(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}.\n\\]\nDer Satz von Bayes kann auch als Multiplikation der bedingten Wahrscheinlichkeit eines Ereignisses unter der Bedingung eines anderen Ereignisses und der Wahrscheinlichkeit des anderen Ereignisses geteilt durch die Wahrscheinlichkeit des ersten Ereignisses geschrieben werden.\nDer Satz von Bayes wird oft in der Medizin, der Wirtschaft und der Technik verwendet, um die Wahrscheinlichkeit eines Ereignisses zu berechnen, wenn wir die Wahrscheinlichkeit eines anderen Ereignisses kennen. Hierbei wird auch von Prior, Likelihood und Posterior gesprochen. Der Prior ist die Wahrscheinlichkeit des Ereignisses, bevor wir die Daten haben. Der Likelihood ist die Wahrscheinlichkeit der Daten unter der Bedingung des Ereignisses. Der Posterior ist die Wahrscheinlichkeit des Ereignisses unter der Bedingung der Daten.\n\n\n\n\n\n\nTip\n\n\n\nYoutube-Videos: - Three Blue One Brown: Bayes Theorem - Veritasium: The Bayesian Trap\n\n\n\n\n\n\n\n\nAssoziationsanalyse mit A-Priori-Algorithmus\n\n\n\nDie Assoziationsanalyse ist ein Verfahren, um Zusammenhänge in Daten zu finden. Ein bekannter Algorithmus ist der A-Priori-Algorithmus, der z.B. in für Predictive Maintainance oder in der Warenkorbanalyse verwendet wird. Der A-Priori-Algorithmus findet heraus, welche Produkte oft zusammen gekauft werden. Ein Beispiel ist, dass Kunden, die Windeln kaufen, oft auch Bier kaufen.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Stichproben und Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "statistics/distributions.html",
    "href": "statistics/distributions.html",
    "title": "4  Verteilungen",
    "section": "",
    "text": "4.1 Diskrete Verteilungen\nIn diesem Abschnitt werden die Verteilungen der einzelnen Variablen untersucht. Dazu werden die Verteilungen der einzelnen Variablen in Form von Histogrammen dargestellt. Die Histogramme zeigen die Häufigkeitsverteilung der einzelnen Variablen. Die Verteilungen der einzelnen Variablen werden in den folgenden Abbildungen dargestellt.\nDiskrete Verteilungen sind Wahrscheinlichkeitsverteilungen, die nur diskrete Werte annehmen können. Die diskreten Verteilungen, die in diesem Abschnitt untersucht werden, sind die Bernoulli-Verteilung, die Binomialverteilung und die Poisson-Verteilung.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Verteilungen</span>"
    ]
  },
  {
    "objectID": "statistics/distributions.html#diskrete-verteilungen",
    "href": "statistics/distributions.html#diskrete-verteilungen",
    "title": "4  Verteilungen",
    "section": "",
    "text": "4.1.1 Bernoulli-Verteilung\nDie Bernoulli-Verteilung ist eine diskrete Wahrscheinlichkeitsverteilung, die nur zwei mögliche Werte annehmen kann. Ein typisches Beispiel für eine Bernoulli-Verteilung ist das Werfen einer Münze. Die Münze kann entweder Kopf oder Zahl zeigen. Die Wahrscheinlichkeit, dass die Münze Kopf zeigt, wird mit \\(p\\) bezeichnet. Die Wahrscheinlichkeit, dass die Münze Zahl zeigt, wird mit \\(1-p\\) bezeichnet. Die Wahrscheinlichkeitsfunktion der Bernoulli-Verteilung ist gegeben durch:\nNun können wir die Wahrscheinlichsverteilung durch eine Formel darstellen.\n\\[\nP(X=x) = \\begin{cases}\np & \\text{für } x=1, \\\\\n1-p & \\text{für } x=0.\n\\end{cases}\n\\]\nWir schreiben eine Zufallsvariable \\(X\\) als \\(X \\sim \\text{Bernoulli}(p)\\), um auszudrücken, dass \\(X\\) eine Bernoulli-verteilte Zufallsvariable mit Parameter \\(p\\) ist.\nFür den Wurf einer fairen Münze nimmt den Wert 1 an, wenn die Münze Kopf zeigt, und den Wert 0 an, wenn die Münze Zahl zeigt. Die Wahrscheinlichkeitsfunktion der Bernoulli-Verteilung für die faire Münze ist gegeben durch:\n\\(X \\sim \\text{Bernoulli}(p=0.5)\\)\nDie folgende Abbildung zeigt die Wahrscheinlichkeitsverteilung der Bernoulli-Verteilung für die faire Münze.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameter der Bernoulli-Verteilung\np = 0.5\n\n# Werte der Zufallsvariablen\nx = np.array([0, 1])\n\n# Wahrscheinlichkeitsfunktion der Bernoulli-Verteilung\nP_X = np.array([1-p, p])\n\n# Plot der Wahrscheinlichkeitsverteilung\nplt.bar(x, P_X, color='skyblue')\nplt.xlabel('Werte der Zufallsvariablen')\nplt.ylabel('Wahrscheinlichkeit')\nplt.title('Wahrscheinlichkeitsverteilung der Bernoulli-Verteilung')\nplt.ylim(0, 1)\nplt.xticks(x)\nplt.show()\n\n\n\n\n\n\n\n\nIn der Abbildung (fig:sec-dataexploratory-distributions-bernoulli-distribution?) wird nun jeder möglichen Realisierung der Zufallsvariablen \\(X\\) die entsprechende Wahrscheinlichkeit zugeordnet. Im Gegensatz zu, Histogrammen, in denen reale Häufigkeiten dargestellt werden, zeigt die Abbildung die theoretischen Wahrscheinlichkeiten der Zufallsvariablen.\n\n4.1.1.1 Erwartungswert und Varianz der Bernoulli-Verteilung\n\n\n\n\n\n\nImportant\n\n\n\nDer Erwartungswert einer Verteilung ist ein Maß für die zentrale Tendenz der Verteilung. Er gibt den durchschnittlichen Wert der Zufallsvariablen an. Der Erwartungswert einer Zufallsvariablen \\(X\\) wird durch \\(E(X)\\) bezeichnet. Allgemein ergibt sich der Erwartungswert einer Zufallsvariablen \\(X\\) durch die Summe der Produkte der Werte der Zufallsvariablen und deren Wahrscheinlichkeiten.\nDer Erwartungswert einer diskreten Zufallsvariablen \\(X\\) ist definiert als:\n\\[\nE(X) = \\sum_{x} x \\cdot P(X=x),\n\\]\n\n\nDer Erwartungswert und die Varianz der Bernoulli-Verteilung können durch die folgenden Formeln berechnet werden:\nDer Erwartungswert der Bernoulli-Verteilung ist gegeben durch:\n\\[\nE(X) = p.\n\\]\n\n\n\n\n\n\nImportant\n\n\n\nGrundsätzlich berechnet sich der Erwartungswert einer diskreten Zufallsvariablen \\(X\\) durch die Summe der Produkte der Werte der Zufallsvariablen und deren Wahrscheinlichkeiten. Für die Bernoulli-Verteilung ist der Erwartungswert gleich dem Parameter \\(p\\).\n\\[\nE(X) = \\sum_{x} x \\cdot P(X=x) = 0 \\cdot (1-p) + 1 \\cdot p = p.\n\\]\n\n\n\n\n4.1.1.2 Varianz der Bernoulli-Verteilung\n\n\n\n\n\n\nImportant\n\n\n\nDie Varianz einer Verteilung ist ein Maß für die Streuung der Verteilung. Sie gibt an, wie weit die Werte der Zufallsvariablen von ihrem Erwartungswert abweichen. Die Varianz einer Zufallsvariablen \\(X\\) wird durch \\(\\text{Var}(X)\\) bezeichnet.\n\n\nDie Varianz der Bernoulli-Verteilung ist gegeben durch:\n\\[\n\\text{Var}(X) = p \\cdot (1-p).\n\\]\n\n\n\n\n\n\nImportant\n\n\n\nGrundsätzlich berechnet sich die Varianz einer Zufallsvariablen \\(X\\) durch die Summe der quadrierten Abweichungen der Werte der Zufallsvariablen von ihrem Erwartungswert, gewichtet mit den Wahrscheinlichkeiten der Zufallsvariablen. Für die Bernoulli-Verteilung ist die Varianz gleich \\(p \\cdot (1-p)\\).\n\\[\n\\text{Var}(X) = \\sum_{x} (x - E(X))^2 \\cdot P(X=x) = (0 - p)^2 \\cdot (1-p) + (1 - p)^2 \\cdot p = p \\cdot (1-p).\n\\]\n\n\n\n\n\n4.1.2 Binomialverteilung\nStellen wir uns vor, dass wird nun den Wurf einer Münze \\(n\\)-mal wiederholen. Die Würfe sind unabhängig voneinander. Die Wahrscheinlichkeit, dass die Münze Kopf zeigt, ist zudem immer \\(p\\). Die Wahrscheinlichkeit, dass die Münze Zahl zeigt, ist \\(1-p\\). Wir Interessieren uns nun dafür, wie oft die Münze Kopf zeigt. Die Anzahl der Kopfwürfe ist eine Zufallsvariable \\(Y\\). Die Zufallsvariable \\(Y\\) ist binomialverteilt. Die Binomialverteilung beschreibt die Anzahl der Erfolge in \\(n\\) unabhängigen Bernoulli-Experimenten.\n\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\n# Parameter der Binomialverteilung\nn = 10\np = 0.5\n\ndef binomial_distribution(n, p):\n    # Werte der Zufallsvariablen\n    y = np.arange(0, n+1)\n\n    # Wahrscheinlichkeitsfunktion der Binomialverteilung\n    P_Y = np.array([math.comb(n, y_i) * p**y_i * (1-p)**(n-y_i) for y_i in y])\n\n    # Plot der Wahrscheinlichkeitsverteilung\n    plt.bar(y, P_Y, color='skyblue')\n\n    plt.xlabel('Werte der Zufallsvariablen $Y$')\n    plt.ylabel('Wahrscheinlichkeit')\n    plt.title('Wahrscheinlichkeitsverteilung der Binomialverteilung')\n    plt.xticks(y)\n    plt.show()\n\nbinomial_distribution(n, p)\n\n\n\n\n\n\n\n\nMan kann auch sagen die Würfe sind unabhängig und identisch verteilt. Abgekürzt \\(Y \\sim \\text{Bin}(n, p)\\). Die Wahrscheinlichkeitsfunktion der Binomialverteilung ist gegeben durch:\n\\[\nP(Y=y) = \\binom{n}{y} \\cdot p^y \\cdot (1-p)^{n-y}.\n\\]\nDas Symbol \\(\\binom{n}{y}\\) bezeichnet den Binomialkoeffizienten. Der Binomialkoeffizient gibt an, auf wie viele verschiedene Arten \\(y\\) Erfolge in \\(n\\) Versuchen auftreten können. Der Binomialkoeffizient ist definiert als:\n\\[\n\\binom{n}{y} = \\frac{n!}{y! \\cdot (n-y)!}.\n\\]\nWenn wir die Werte anpassen und z.B. ein Los ziehen, dann ist die Wahrscheinlichkeit, dass wir ein Gewinnlos ziehen \\(p=0.1\\). Die Wahrscheinlichkeitsfunktion der Binomialverteilung für das Ziehen eines Gewinnloses aus 10 Losen ist gegeben durch:\n\\(Y \\sim \\text{Bin}(n=10, p=0.1)\\)\nDie folgende Abbildung zeigt die Wahrscheinlichkeitsverteilung der Binomialverteilung für das Ziehen eines Gewinnloses aus 10 Losen.\n\nbinomial_distribution(n = 10, p = 0.1)\n\n\n\n\n\n\n\n\n\n4.1.2.1 Erwartungswert und Varianz der Binomialverteilung\nDer Erwartungswert und die Varianz der Binomialverteilung können durch die folgenden Formeln berechnet werden:\nDer Erwartungswert der Binomialverteilung ist gegeben durch:\n\\[\nE(Y) = n \\cdot p.\n\\]\nDie Varianz der Binomialverteilung ist gegeben durch:\n\\[\n\\text{Var}(Y) = n \\cdot p \\cdot (1-p).\n\\]\n\n\n\n4.1.3 Diskrete Gleichverteilung\nDie diskrete Gleichverteilung ist eine diskrete Wahrscheinlichkeitsverteilung, bei der alle möglichen Werte der Zufallsvariablen die gleiche Wahrscheinlichkeit haben. Klassisches Beispiel ist hier der Wurf eines Würfels. Die diskrete Gleichverteilung ist auch als diskrete Uniformverteilung bekannt. Die diskrete Gleichverteilung ist definiert auf einem endlichen Intervall von ganzen Zahlen. Die Wahrscheinlichkeitsfunktion der diskreten Gleichverteilung ist gegeben durch:\n\\[\nP(X=x) = \\frac{1}{n},\n\\]\nfür \\(x=1, 2, \\ldots, n\\). Die Zufallsvariable \\(X\\) wird als \\(X \\sim \\text{DU}(n)\\) bezeichnet, um auszudrücken, dass \\(X\\) eine diskret gleichverteilte Zufallsvariable auf dem Intervall \\([1, n]\\) ist.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameter der diskreten Gleichverteilung\nn = 6\n\n# Werte der Zufallsvariablen\nx = np.arange(1, n+1)\n\n# Wahrscheinlichkeitsfunktion der diskreten Gleichverteilung\nP_X = np.array([1/n for x_i in x])\n\n# Plot der Wahrscheinlichkeitsverteilung\nplt.bar(x, P_X, color='skyblue')\nplt.xlabel('Werte der Zufallsvariablen $X$')\nplt.ylabel('Wahrscheinlichkeit')\nplt.title('Wahrscheinlichkeitsverteilung der diskreten Gleichverteilung')\nplt.xticks(x)\nplt.show()\n\n\n\n\n\n\n\n\nIn der Abbildung (fig:sec-dataexploratory-distributions-discrete-uniform-distribution?) wird die Wahrscheinlichkeitsverteilung der diskreten Gleichverteilung für das Würfeln eines Würfels dargestellt. Die Wahrscheinlichkeitsverteilung zeigt, dass alle möglichen Werte der Zufallsvariablen die gleiche Wahrscheinlichkeit haben.\n\n4.1.3.1 Erwartungswert und Varianz der diskreten Gleichverteilung\nDer Erwartungswert und die Varianz der diskreten Gleichverteilung können durch die folgenden Formeln berechnet werden:\nDer Erwartungswert der diskreten Gleichverteilung ist gegeben durch:\n\\[\nE(X) = \\frac{n+1}{2}.\n\\]\nDie Varianz der diskreten Gleichverteilung ist gegeben durch:\n\\[\n\\text{Var}(X) = \\frac{n^2-1}{12}.\n\\]",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Verteilungen</span>"
    ]
  },
  {
    "objectID": "statistics/distributions.html#stetige-verteilungen",
    "href": "statistics/distributions.html#stetige-verteilungen",
    "title": "4  Verteilungen",
    "section": "4.2 Stetige Verteilungen",
    "text": "4.2 Stetige Verteilungen\nStetige Verteilungen sind Wahrscheinlichkeitsverteilungen, die kontinuierliche Werte annehmen können. Die stetigen Verteilungen, die in diesem Abschnitt untersucht werden, sind die Normalverteilung, die Exponentialverteilung und die Gleichverteilung.\n\n4.2.1 Stetige Gleichverteilung\nDie stetige Gleichverteilung ist eine stetige Wahrscheinlichkeitsverteilung, bei der alle Werte innerhalb eines Intervalls die gleiche Wahrscheinlichkeit haben. Die stetige Gleichverteilung ist definiert auf einem Intervall von reellen Zahlen. Die Wahrscheinlichkeitsdichte der stetigen Gleichverteilung ist gegeben durch:\n\\[\nf(x) = \\begin{cases}\n\\frac{1}{b-a} & \\text{für } a \\leq x \\leq b, \\\\\n0 & \\text{sonst}.\n\\end{cases}\n\\]\nDie Zufallsvariable \\(X\\) wird als \\(X \\sim \\text{U}(a, b)\\) bezeichnet, um auszudrücken, dass \\(X\\) eine stetig gleichverteilte Zufallsvariable auf dem Intervall \\([a, b]\\) ist.\nBei stetigen Verteilungen geben die Wahrscheinlichkeitsdichten die Wahrscheinlichkeiten an. Die Wahrscheinlichkeiten werden durch die Fläche unter der Wahrscheinlichkeitsdichte bestimmt. Die Fläche unter der Wahrscheinlichkeitsdichte ist gleich 1.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameter der stetigen Gleichverteilung\na = 0\nb = 1\n\n# Werte der Zufallsvariablen\nx = np.linspace(a-0.5, b+0.5, 1000)\n\n# Wahrscheinlichkeitsdichte der stetigen Gleichverteilung\nf_X = np.array([1/(b-a) if a &lt;= x_i &lt;= b else 0 for x_i in x])\n\n# Plot der Wahrscheinlichkeitsdichte\nplt.plot(x, f_X, color='skyblue')\nplt.xlabel('Werte der Zufallsvariablen $X$')\nplt.ylabel('Wahrscheinlichkeitsdichte')\nplt.title('Wahrscheinlichkeitsdichte der stetigen Gleichverteilung')\nplt.show()\n\n\n\n\n\n\n\n\nWir können nun keine Aussagen mehr über die Wahrscheinlichkeit eines bestimmten Wertes machen, da die Wahrscheinlichkeit eines einzelnen Wertes 0 ist. Wir können jedoch Aussagen über die Wahrscheinlichkeit eines Wertebereichs machen. Die Wahrscheinlichkeit, dass die Zufallsvariable \\(X\\) einen Wert im Intervall \\([a, b]\\) annimmt, ist gegeben durch:\n\\[\nP(a \\leq X \\leq b) = \\int_{a}^{b} f(x) \\, dx.\n\\]\n\n4.2.1.1 Beispiel: Die Ausfallwahrscheinlichkeit eines elektronischen Bauteils\nDer Ausfall eines elektronischen Bauteils erfolgt nach einer stetigen Gleichverteilung auf dem Intervall \\([0, 3650]\\) Tagen. Wie wahrscheinlich ist es, dass das Bauteil innerhalb der ersten 1000 Tage ausfällt?\n\n\n\n\n\n\nTip\n\n\n\n\n\nDie Ausfallwahrscheinlichkeit des elektronischen Bauteils ist \\(X \\sim \\text{U}(0, 3650)\\)\nDie Wahrscheinlichkeitsdichte der stetigen Gleichverteilung ist gegeben durch:\n\\[\nf(x) = \\begin{cases}\n\\frac{1}{3650} & \\text{für } 0 \\leq x \\leq 3650, \\\\\n0 & \\text{sonst}.\n\\end{cases}\n\\]\nDie Wahrscheinlichkeit für einen Ausfall innerhalb der ersten 1000 Tage ist gegeben durch:\n\\[\nP(0 \\leq X \\leq 1000) = \\int_{0}^{1000} f(x) \\, dx = \\int_{0}^{1000} \\frac{1}{3650} \\, dx = \\frac{1000}{3650} = 0.274.\n\\]\n\n\n\n\n\n\n4.2.2 Normalverteilung\nDie Normalverteilung ist eine stetige Wahrscheinlichkeitsverteilung, die in vielen Anwendungen in der Statistik und den Naturwissenschaften vorkommt. Die Normalverteilung ist durch ihre Glockenkurve gekennzeichnet. Die Glockenkurve zeigt, dass die Werte der Zufallsvariablen um den Erwartungswert herum symmetrisch verteilt sind. Die Wahrscheinlichkeitsdichte der Normalverteilung ist gegeben durch:\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi} \\sigma} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right),\n\\]\nfür \\(-\\infty &lt; x &lt; \\infty\\). Die Zufallsvariable \\(X\\) wird als \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) bezeichnet, um auszudrücken, dass \\(X\\) eine normalverteilte Zufallsvariable mit Erwartungswert \\(\\mu\\) und Varianz \\(\\sigma^2\\) ist.\nSofern \\(\\mu=0\\) und \\(\\sigma=1\\) ist, spricht man von einer Standardnormalverteilung. \\(X \\sim \\mathcal{N}(0, 1)\\). Die Standardnormalverteilung ist eine spezielle Form der Normalverteilung. Die Standardnormalverteilung hat einen Erwartungswert von 0 und eine Varianz von 1.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameter der Normalverteilung\nmu = 0\nsigma = 1\n\n# Werte der Zufallsvariablen\nx = np.linspace(-5, 5, 1000)\n\n# Wahrscheinlichkeitsdichte der Normalverteilung\nf_X = 1/(np.sqrt(2*np.pi)*sigma) * np.exp(-(x-mu)**2/(2*sigma**2))\n\n# Plot der Wahrscheinlichkeitsdichte\nplt.plot(x, f_X, color='skyblue')\nplt.xlabel('Werte der Zufallsvariablen $X$')\nplt.ylabel('Wahrscheinlichkeitsdichte')\nplt.title('Wahrscheinlichkeitsdichte der Normalverteilung')\nplt.show()\n\n\n\n\n\n\n\n\nIn Zeiten als es noch keine Computer gab, war es schwierig die Fläche unter der Kurve zu (\\(P(X \\leq x)\\)) zu berechnen. Entsprechend konnte man die Fläche unter der Kurve in Tabellen nachschlagen. \\(Z\\)-Wert-Tabellen sind Tabellen, die die Fläche unter der Standardnormalverteilungskurve für verschiedene Werte von \\(Z\\) anzeigen. Die \\(Z\\)-Tabelle gibt die Wahrscheinlichkeit an, dass eine Zufallsvariable \\(Z\\) einen Wert kleiner oder gleich einem bestimmten Wert annimmt.\nBeispielsweise können wir ablesen, dass\n\n\\(P(Z \\leq 0) = 0.5\\), d.h. die Wahrscheinlichkeit, dass eine Zufallsvariable \\(Z\\) einen Wert kleiner oder gleich 0 annimmt, beträgt 50%.\n\\(P(Z \\leq 1) = 0.8413\\), d.h. die Wahrscheinlichkeit, dass eine Zufallsvariable \\(Z\\) einen Wert kleiner oder gleich 1 annimmt, beträgt 84.13%.\n\\(P(Z \\leq -1) = 1 - P(Z \\leq 1) = 1 - 0.8413 = 0.1587\\), d.h. die Wahrscheinlichkeit, dass eine Zufallsvariable \\(Z\\) einen Wert kleiner oder gleich -1 annimmt, beträgt 15.87%.\n\nUmgekehrt können wir auch die Z-Werte für eine bestimmte Wahrscheinlichkeit ablesen. Beispielsweise können wir ablesen, dass\n\n\\(P(Z \\leq z) = 0.95\\) für \\(z = 1.645\\), d.h. die Wahrscheinlichkeit, dass eine Zufallsvariable \\(Z\\) einen Wert kleiner oder gleich 1.645 annimmt, beträgt 95%.\n\n\n4.2.2.1 Erwartungswert und Varianz der Normalverteilung\n\n\n\n\n\n\nImportant\n\n\n\nIm allgemeinen Fall berechnet sich der Erwartungswert einer stetigen Zufallsvariablen \\(X\\) durch die Fläche unter der Kurve der Wahrscheinlichkeitsdichte. Der Erwartungswert einer stetigen Zufallsvariablen \\(X\\) ist definiert als:\n\\[\nE(X) = \\mu = \\int_{-\\infty}^{\\infty} x \\cdot f(x) \\, dx,\n\\]\ndie Varianz einer stetigen Zufallsvariablen \\(X\\) ist definiert als:\n\\[\n\\text{Var}(X) = \\sigma^2 = E(X-\\mu_x)^2= \\int_{-\\infty}^{\\infty} (x - \\mu)^2 \\cdot f(x) \\, dx.\n\\]\n\n\n\n4.2.2.1.1 Standardisierung der Normalverteilung\nIn der Realität verhalten sich viele Zufallsvariablen nicht wie die Standardnormalverteilung, sondern eine Normalverteilung mit einem anderen Erwartungswert und einer anderen Varianz. Um die Normalverteilung auf eine Standardnormalverteilung zu überführen, wird die Zufallsvariable standardisiert. Die Standardisierung einer Zufallsvariable \\(X\\) erfolgt durch die folgende Formel:\n\\[\nZ = \\frac{X - \\mu}{\\sigma},\n\\]\nwobei \\(Z\\) die standardisierte Zufallsvariable ist, \\(\\mu\\) der Erwartungswert der Zufallsvariable \\(X\\) und \\(\\sigma\\) die Standardabweichung der Zufallsvariable \\(X\\). Die standardisierte Zufallsvariable \\(Z\\) hat einen Erwartungswert von 0 und eine Varianz von 1. Hier können dann die Werte in der \\(Z\\)-Tabelle nachgeschlagen werden.\nHeute können wir die Fläche unter der Kurve mit Hilfe von Computerprogrammen berechnen. Der Intelligenz-Quotient (IQ) ist ein Beispiel für eine normalverteilte Zufallsvariable. Der IQ hat einen Erwartungswert von 100 und eine Standardabweichung von 15.\nDer IQ ist also \\(X \\sim \\mathcal{N}(100, 15^2)\\). Wie wahrscheinlich ist es, dass eine Person einen IQ von 130 oder mehr hat?\n\n\n\n4.2.2.2 Analytische Berechnung von Wahrscheinlichkeiten\nWir können die Wahrscheinlichkeitsdichte-Funktion direkt aufschreiben:\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi} \\cdot 15} \\exp\\left(-\\frac{(x-100)^2}{2 \\cdot 15^2}\\right),\n\\]\nund mit dieser dann die Wahrscheinlichkeit, dass eine Person einen IQ geringer als 130 hat, berechnen \\(P(X \\leq 130)\\). Wir benötigen jedoch die Gegenwahrscheinlichkeit, dass eine Person einen IQ von 130 oder mehr hat: \\[\nP(X \\geq 130) = 1 - P(X \\leq 130) = 1 - \\int_{-\\infty}^{130} f(x) \\, dx.\n\\]\nDies ergibt:\n\\[\nP(X \\geq 130) = 1 - \\int_{-\\infty}^{130} \\frac{1}{\\sqrt{2\\pi} \\cdot 15} \\exp\\left(-\\frac{(x-100)^2}{2 \\cdot 15^2}\\right) \\, dx = 0.0227501.\n\\]\nDie Wahrscheinlichkeit, dass eine Person hochbegabt ist, beträgt also 2.27501%.\n\n\n4.2.2.3 Berechnung von Wahrscheinlichkeiten mit Python\nIm Python kommen wir z.B. das Modul scipy.stats verwenden, um die Wahrscheinlichkeit zu berechnen. Hier gibt es die Funktion norm.cdf(x, loc=mu, scale=sigma), die die kumulative Verteilungsfunktion der Normalverteilung berechnet. Die kumulative Verteilungsfunktion gibt die Wahrscheinlichkeit an, dass eine Zufallsvariable einen Wert kleiner oder gleich einem bestimmten Wert annimmt.\n\nimport numpy as np\nfrom scipy.stats import norm\n\n# Parameter der Normalverteilung\nmu = 100\nsigma = 15\n\n# Wahrscheinlichkeit, dass eine Person\n# einen IQ von 130 oder mehr hat\nP_X_geq_130 = 1 - norm.cdf(130, loc=mu, scale=sigma)\n\nprint(f'P(X &gt;= 130) = {P_X_geq_130:.4f}')\n\nP(X &gt;= 130) = 0.0228\n\n\n\n\n4.2.2.4 Visualisierung der Wahrscheinlichkeiten\nHäufig stellt sich wie frage, welcher Anteil einer Population innerhalb von einer oder zwei Standardabweichungen liegt. Hierzu müssen wir folgendes berechnen:\n\\[\nP(\\mu - \\sigma \\leq X \\leq \\mu + \\sigma) = P(X \\leq \\mu + \\sigma) - P(X \\leq \\mu - \\sigma) = \\text{cdf}(\\mu + \\sigma) - \\text{cdf}(\\mu - \\sigma).\n\\]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Parameter der Normalverteilung\nmu = 100\nsigma = 15\n\n# Werte der Zufallsvariablen\nx = np.linspace(mu - 4*sigma, mu + 4*sigma, 1000)\n\n# Wahrscheinlichkeitsdichte der Normalverteilung\nf_X = 1/(np.sqrt(2*np.pi)*sigma) * np.exp(-(x-mu)**2/(2*sigma**2))\n\n# Wahrscheinlichkeit, dass eine Person\n# einen IQ von 2 Standardabweichungen vom Mittelwert hat\nP_X_2_sigma = norm.cdf(mu + 2*sigma, loc=mu, scale=sigma) - norm.cdf(mu - 2*sigma, loc=mu, scale=sigma)\n\nprint(f'P({mu - 2*sigma} &lt;= X &lt;= {mu + 2*sigma}) = {P_X_2_sigma:.4f}')\n\n# Plot der Wahrscheinlichkeitsdichte\nplt.plot(x, f_X, color='skyblue')\nplt.fill_between(x, f_X, where=(x &gt;= mu - 2*sigma) & (x &lt;= mu + 2*sigma), color='lightblue')\nplt.xlabel('Werte der Zufallsvariablen $X$')\nplt.ylabel('Wahrscheinlichkeitsdichte')\nplt.title('Wahrscheinlichkeitsdichte der Normalverteilung')\n# Text mit der Wahrscheinlichkeit\nplt.text(mu, 0.02, f'P({mu - 2*sigma} &lt;= X &lt;= {mu + 2*sigma}) = {P_X_2_sigma:.4f}', horizontalalignment='center', verticalalignment='center')\nplt.show()\n\nP(70 &lt;= X &lt;= 130) = 0.9545\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nDer IQ ist willkürlich so skaliert, dass der Mittelwert bei 100 liegt und die Standardabweichung bei 15. Die Wahrscheinlichkeit, dass eine Person einen IQ von 130 oder mehr hat, beträgt 2.23%. Die Wahrscheinlichkeit, dass eine Person einen IQ von 2 Standardabweichungen vom Mittelwert hat, beträgt 95.45%. Dies bedeutet, dass 95.45% der Bevölkerung einen IQ zwischen 70 und 130 haben.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Verteilungen</span>"
    ]
  },
  {
    "objectID": "statistics/distributions.html#weitere-verteilungen",
    "href": "statistics/distributions.html#weitere-verteilungen",
    "title": "4  Verteilungen",
    "section": "4.3 Weitere Verteilungen",
    "text": "4.3 Weitere Verteilungen\nNeben der Normalverteilung gibt es noch weitere stetige Verteilungen, die in der Statistik und den Naturwissenschaften vorkommen. Dazu gehören die Exponentialverteilung und die Gleichverteilung. Diese kommen zum Beispiel in der Zuverlässigkeitsanalyse und der Simulation von Zufallsprozessen vor.\n\nDie Exponentialverteilung ist eine stetige Wahrscheinlichkeitsverteilung, die die Zeit zwischen unabhängigen Ereignissen beschreibt. Anwendungsfälle sind die Zuverlässigkeitsanalyse und die Warteschlangentheorie.\nDie Poissionverteilung ist eine diskrete Wahrscheinlichkeitsverteilung, die die Anzahl der Ereignisse in einem festen Zeitintervall beschreibt. Anwendungsfälle sind die Modellierung von Zufallsprozessen und die Warteschlangentheorie.\n\nVerschiedene Verteilungen finden sich im Python-Modul scipy.stats. Das Modul enthält Funktionen zur Berechnung von Wahrscheinlichkeiten und zur Erzeugung von Zufallszahlen für verschiedene Verteilungen.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Verteilungen</span>"
    ]
  },
  {
    "objectID": "statistics/distributions.html#fitting-von-verteilungen",
    "href": "statistics/distributions.html#fitting-von-verteilungen",
    "title": "4  Verteilungen",
    "section": "4.4 Fitting von Verteilungen",
    "text": "4.4 Fitting von Verteilungen\nIn der Praxis ist es oft notwendig, die Verteilung von Daten zu bestimmen. Dieser Prozess wird als Fitting bezeichnet. Das Ziel des Fittings ist es, die Verteilung zu finden, die die Daten am besten beschreibt. Es gibt verschiedene Methoden, um die Verteilung von Daten zu bestimmen. Dazu gehören die Methode der kleinsten Quadrate, Maximum-Likelihood-Schätzung und die Methode der Momente.\nAls Beispiel nehmen wir unsere Lottrie-Daten, wobei die Anzahl der Gewinnlose \\(X \\sim \\text{Bin}(10, 0.1)\\) binomialverteilt ist. Wir können nun die Verteilung der Daten bestimmen, indem wir die Binomialverteilung anpassen.\n\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nfrom scipy.stats import binom, norm, uniform\nimport seaborn as sns\n\n# Simulierte Daten für die Anzahl der Gewinnlose pro Ziehung\nnp.random.seed(42)  # Für Reproduzierbarkeit\ndata = np.random.binomial(n=10, p=0.1, size=1000)  # 1000 Ziehungen mit Binomialverteilung\n\n# Schätzung der Binomialparameter\nn_est = np.max(data)  # Annahme: n entspricht dem höchsten beobachteten Wert\np_est = np.mean(data) / n_est  # Erwartungswert von p = Mittelwert/n\n\n# Wahrscheinlichkeitswerte für Binomialverteilung\nbinom_x = np.arange(0, n_est + 1)\nbinom_y = binom.pmf(binom_x, n=n_est, p=p_est)\n\n# Schätzung und Werte für Normalverteilung\nmu, sigma = norm.fit(data)\nnorm_x = np.linspace(0, n_est, 1000)\nnorm_y = norm.pdf(norm_x, mu, sigma)\n\n# Schätzung und Werte für Gleichverteilung\na, b = uniform.fit(data)\nuniform_x = np.linspace(0, n_est, 1000)\nuniform_y = uniform.pdf(uniform_x, a, b-a)  # Höhe der Gleichverteilung = 1/(b-a)\n\n# Plot der Histogramme und Verteilungen\nplt.hist(data, bins=n_est + 1, density=True, color='skyblue', alpha=0.7, label='Daten')\nplt.plot(binom_x, binom_y, color='red', marker='o', linestyle='dashed', label='Binomialverteilung')\nplt.plot(norm_x, norm_y, color='green', label='Normalverteilung')\nplt.plot(uniform_x, uniform_y, color='purple', label='Gleichverteilung')\n\nplt.xlabel('Anzahl der Gewinnlose')\nplt.ylabel('Dichte')\nplt.title('Fitting von Verteilungen')\nplt.legend()\nplt.show()",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Verteilungen</span>"
    ]
  },
  {
    "objectID": "statistics/distributions.html#rechnen-mit-verteilungen-zufallsvariablen-und-erwartungswert",
    "href": "statistics/distributions.html#rechnen-mit-verteilungen-zufallsvariablen-und-erwartungswert",
    "title": "4  Verteilungen",
    "section": "4.5 Rechnen mit Verteilungen, Zufallsvariablen und Erwartungswert",
    "text": "4.5 Rechnen mit Verteilungen, Zufallsvariablen und Erwartungswert\nDer zentrale Grenzwertsatz besagt, dass die Summe einer großen Anzahl von unabhängigen und identisch verteilten Zufallsvariablen einer Normalverteilung folgt, unabhängig von der Form der Verteilung der Zufallsvariablen. Der zentrale Grenzwertsatz spielt eine wichtige Rolle in der Statistik, da er die Grundlage für viele statistische Methoden bildet.\n\n4.5.1 Recheneregeln für Erwartungswert und Varianz\n\n\n\n\n\n\nImportant\n\n\n\nFür die Summe von unabhängigen Zufallsvariablen \\(X_1, X_2, \\ldots, X_n\\) gelten die folgenden Rechenregeln für den Erwartungswert und die Varianz:\n\nDer Erwartungswert der Summe von Zufallsvariablen ist gleich der Summe der Erwartungswerte der Zufallsvariablen, unabängig davon, ob die Zufallsvariablen unabhängig sind oder nicht:\n\n\\[\nE(X_1 + X_2 + \\ldots + X_n) = E(X_1) + E(X_2) + \\ldots + E(X_n).\n\\]\n\nDie Varianz der Summe von Zufallsvariablen ist gleich der Summe der Varianzen der Zufallsvariablen:\n\n\\[\n\\text{Var}(X_1 + X_2) = \\text{Var}(X_1) + \\text{Var}(X_2) + 2 \\text{Cov}(X_1, X_2),\n\\]\nwenn die Zufallsvariablen unabhängig sind, ist die Kovarianz 0.\n\nFür Linearkombinationen von Zufallsvariablen \\(aX + bY\\) gelten die folgenden Rechenregeln für den Erwartungswert und die Varianz:\n\n\\[\nE(aX + bY) = aE(X) + bE(Y).\n\\]\n\\[\n\\text{Var}(aX + bY) = a^2 \\text{Var}(X) + b^2 \\text{Var}(Y) + 2ab \\text{Cov}(X, Y),\n\\]\nwobei \\(\\text{Cov}(X, Y)\\) die Kovarianz zwischen den Zufallsvariablen \\(X\\) und \\(Y\\) ist und 0 ist, wenn \\(X\\) und \\(Y\\) unabhängig sind.\n\n\n\n\n4.5.2 Beispiel: Summen von Zufallsvariablen aus verschiedenen Verteilungen\nEs existieren auch Verallgemeinerungen, z.B., dass die Zufallsvariablen nicht identisch verteilt sein müssen. So zeigt, das folgende Beispiel, dass auch die Summe von Zufallsvariablen aus verschiedenen Verteilungen einer Normalverteilung folgen kann, wenn die Anzahl der Zufallsvariablen groß genug ist.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set seed\nnp.random.seed(42)\n\n# Anzahl der Zufallswerte\nN = 10000\n\n# Uniformverteilung\nlow1, high1 = np.random.uniform(0, 10, 2)\nlow2, high2 = np.random.uniform(0, 10, 2)\nX_uni_1 = np.random.uniform(low1, high1, N)\nX_uni_2 = np.random.uniform(low2, high2, N)\n\n# Exponentialverteilung\nscale1, scale2 = np.random.uniform(0.5, 5, 2)\nX_exp_1 = np.random.exponential(scale1, N)\nX_exp_2 = np.random.exponential(scale2, N)\n\n# Binomialverteilung\nn1, p1 = np.random.randint(10, 50), np.random.uniform(0.1, 0.2)\nn2, p2 = np.random.randint(10, 50), np.random.uniform(0.1, 0.2)\nX_bin_1 = np.random.binomial(n1, p1, N) \nX_bin_2 = np.random.binomial(n2, p2, N)\n\nX_sum = X_uni_1 + X_uni_2 + X_exp_1 + X_exp_2 + X_bin_1 + X_bin_2\n\n# Histogramme plotten\nplt.figure(figsize=(12, 6))\nplt.hist(X_uni_1, bins=50, alpha=0.6, label=\"Uniform 1\", density=True)\nplt.hist(X_uni_2, bins=50, alpha=0.6, label=\"Uniform 2\", density=True)\nplt.hist(X_exp_1, bins=50, alpha=0.6, label=\"Exponential 1\", density=True)\nplt.hist(X_exp_2, bins=50, alpha=0.6, label=\"Exponential 2\", density=True)\nplt.hist(X_bin_1, bins=50, alpha=0.6, label=\"Binomial 1\", density=True)\nplt.hist(X_bin_2, bins=50, alpha=0.6, label=\"Binomial 2\", density=True)\nplt.hist(X_sum, bins=50, alpha=0.6, label=\"Total Sum\", density=True)\nplt.legend()\nplt.xlabel(\"Wert\")\nplt.ylabel(\"Dichte\")\nplt.title(\"Summen von Zufallsvariablen aus verschiedenen Verteilungen\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom scipy.stats import norm\n\nX_sum_mu, X_sum_sigma = norm.fit(X_sum)\n\n# Histogramm der Summe der Zufallsvariablen\nplt.hist(X_sum, bins=50, alpha=0.6, label=\"Total Sum\", density=True)\n\n# Wahrscheinlichkeitsdichte der Normalverteilung\nx = np.linspace(0, 100, 1000)\ny = norm.pdf(x, X_sum_mu, X_sum_sigma)\nplt.plot(x, y, color='red', label='Normalverteilung')\n\nplt.legend()\nplt.xlabel(\"Wert\")\nplt.ylabel(\"Dichte\")\nplt.title(\"Fit der Summe von Zufallsvariablen mit einer Normalverteilung\")\nplt.show()",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Verteilungen</span>"
    ]
  },
  {
    "objectID": "statistics/tutorial.html",
    "href": "statistics/tutorial.html",
    "title": "Tutorial: Monte Carlo Simulation of Equipment Failure Time",
    "section": "",
    "text": "Objective\nStudents will build a Monte Carlo simulation to model the time until equipment failure in machinery. The goal is to understand probability concepts like conditional probability, addition and multiplication laws, statistical independence, and to visualize the results using a histogram.",
    "crumbs": [
      "Statistik",
      "Tutorial: Monte Carlo Simulation of Equipment Failure Time"
    ]
  },
  {
    "objectID": "statistics/tutorial.html#story-and-use-case",
    "href": "statistics/tutorial.html#story-and-use-case",
    "title": "Tutorial: Monte Carlo Simulation of Equipment Failure Time",
    "section": "Story and Use Case",
    "text": "Story and Use Case\n\nStory:\nA manufacturing plant relies on an automated production line with three robotic arms. Each robotic arm consists of several critical components: motors, sensors, and control units. If any of the motors or control units fail, the robotic arm stops functioning, leading to production downtime. The factory manager wants to estimate the expected downtime caused by failures and identify weak points in the system to optimize maintenance schedules.\n\n\nUse Case:\nThe students must develop a simulation that models the failure behavior of a robotic arm in the production line. Each robotic arm consists of four components:\n\nMotor fail according to an exponential distribution with a mean failure time of 500 hours.\nSensor 1 fail according to a normal distribution with a mean of 600 hours and a standard deviation of 100 hours.\nSensor 2 fail according to a normal distribution with a mean of 600 hours and a standard deviation of 100 hours.\nControl Unit fail according to a uniform distribution between 400 and 800 hours.\nIf a Sensor fails before the Motor, the the failure time of the motor is reduced by 100 hours.\nIf a Control Unit or Motor fails, the robotic arm stops working.\n\n\nSimulate random failure times for each component.\nDetermine the time untils the first robitic arm fails, stopping the production line.\nRepeat the process for 10,000 trials.\nPlot the histogram of failure times.\nAnalyze the contingency tables of the variables (motor_fails, sensor_1_fails, sensor_1_fails, control_unit_fails, line_fails) and discuss the conditional probabilities and independence.\nIs there a correlation between the time-to-failure of the the Sensors and the Robotic Arm? Create the scatter plot of the time-to-failure of the Sensors and the Robotic Arm and compute the correlation coefficient.",
    "crumbs": [
      "Statistik",
      "Tutorial: Monte Carlo Simulation of Equipment Failure Time"
    ]
  },
  {
    "objectID": "statistics/tutorial.html#application-of-bayes-theorem",
    "href": "statistics/tutorial.html#application-of-bayes-theorem",
    "title": "Tutorial: Monte Carlo Simulation of Equipment Failure Time",
    "section": "Application of Bayes Theorem",
    "text": "Application of Bayes Theorem\nCan You use Bayes Theorem to compute the probability a Control Unit causes the failure, given that the robotic arm has stopped working.",
    "crumbs": [
      "Statistik",
      "Tutorial: Monte Carlo Simulation of Equipment Failure Time"
    ]
  },
  {
    "objectID": "statistics/interference_basics.html",
    "href": "statistics/interference_basics.html",
    "title": "5  Interferenz",
    "section": "",
    "text": "5.1 Punktschätzer und Konfidenzintervalle\nBei der Frage der Interferenz geht es darum, was wir mit ausreichender Sicherheit über eine Population aussagen können, wenn wir nur eine Stichprobe haben. Ein Beispiel hierfür könnte sein, ob ein Parameter einer Verteilung signifikant von einem bestimmten Wert abweicht oder ob sich die Mittelwerte zweier Verteilungen signifikant unterscheiden.\nEin Punktschätzer ist eine Schätz-Funktion, die eine Schätzung für einen Parameter (z.B. Mittelwert oder Varianz) einer Verteilung liefert. Da wir die wahre Verteilung der Grundgesamtheit nie direkt beobachten können, müssen wir Rückschlüsse daraus aus unserer Stichprobe ziehen. Dabei hilft uns wieder der Zentrale Grenzwertsatz.\nWir beginnen mit einem Beispiel einer Zufallsvariable \\(X \\sim \\text{Bernoulli}(p)\\), wobei uns der Parameter \\(p\\) unbekannt ist. Diesen Parameter wollen wir nun anhand unserer Stichprobe schätzen. Eine Schätzung für \\(p\\) ist der Anteil der Einsen in der Stichprobe \\(\\hat{p}\\). Diesen Schätzer können wir berechnen als\n\\[\n\\hat{p} = \\frac{1}{n} \\sum_{i=1}^n X_i,\n\\]\nUm dies zu illustrieren, schauen wir uns die Grundgesamtheit und eine Stichprobe an. Wir ziehen \\(N=1000\\) Zufallszahlen aus einer Bernoulli-Verteilung mit \\(p=0.3\\) und wählen eine Stichprobe der Größe \\(n=100\\). Diesen Vorgang wiederholen wir \\(m=100\\) mal. Den Anteil der Einsen in der Stichprobe speichern wir als Stichprobenhäufigkeit \\(\\hat{p}\\).\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\n\n# Definiere Anzahl der Versuche \nm = 100\n\nresults = np.zeros(m)\n\n# Grundgesamtheit ist Bernoulli verteilt\np = 0.3\nN = 1000\n\nfor i in range(m):\n    # Grundgesamtheit\n    X = np.random.binomial(1, p, N)\n\n    # Stichprobe mit n=100\n    n = 100\n    X_sample = np.random.choice(X, n)\n\n    # Speichere das Anzahl der Einsen\n    results[i] = np.sum(X_sample)/n\n\n# Wie verteilt sich die Anzahl der Einsen in der Stichprobe\nsns.histplot(results)\n\n# Wie gut passt eine Normalverteilung\nmu, std = norm.fit(results)\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = norm.pdf(x, mu, std)\nplt.plot(x, p, 'k', linewidth=2)\n\nplt.title('Verteilung der Anzahl der Einsen in der Stichprobe')\nplt.show()\n\nprint(f'Mittelwert von p_hat: {np.mean(results)/n:.2f}')\nprint(f'Standardabweichung von p_hat: {np.std(results)/n:.2f}')\n\n\n\n\n\n\n\n\nMittelwert von p_hat: 0.00\nStandardabweichung von p_hat: 0.00\nVerglichen mit den theoretischen Werten übergibt dies eine hohe Übereinstimmung.\n\\[\n\\mu_{\\hat{p}} = p = 0.3,\n\\]\n\\[\nSE_{\\hat{p}}=\\sigma_{\\hat{p}} = \\sqrt{\\frac{p(1-p)}{n}} = 0.046.\n\\]",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Interferenz</span>"
    ]
  },
  {
    "objectID": "statistics/interference_basics.html#sec-statistics-pointestimates",
    "href": "statistics/interference_basics.html#sec-statistics-pointestimates",
    "title": "5  Interferenz",
    "section": "",
    "text": "Bestimmung von Punktschätzern mittels Maximum-Likelihood-Schätzung\n\n\n\n\n\nUm den Maximum-Likelihood-Schätzer (MLE) für den Parameter \\(p\\) einer Bernoulli-Verteilung zu finden, geht man wie folgt vor. Man stellt die Likelihood-Funktion der Beobachteten Werte in der Stichprobe auf und sucht dann nach dem Wert von \\(p\\), bei dem dieses Auftreten am wahrscheinlichsten ist.\n\n5.1.0.0.1 1. Likelihood-Funktion aufstellen:\nAngenommen, du hast \\(n\\) unabhängige und identisch verteilte Bernoulli-Zufallsvariablen \\(X_1, X_2, \\ldots, X_n\\). Jede dieser Variablen \\(X_i\\) hat eine Bernoulli-Verteilung mit Parameter \\(p\\), was bedeutet, dass die Wahrscheinlichkeit, dass \\(X_i = 1\\) ist, gleich \\(p\\) ist und die Wahrscheinlichkeit, dass \\(X_i = 0\\) ist, gleich \\(1-p\\) ist.\nDie Likelihood-Funktion ist das Produkt der Wahrscheinlichkeiten aller beobachteten Werte. Falls wir \\(k\\) Einsen und \\(n-k\\) Nullen in unserer Stichprobe haben, ist die Likelihood-Funktion gegeben durch:\n\\[\nL(p) = \\prod_{i=1}^{n} p^{x_i} (1-p)^{1-x_i} = P(X_1=x_1) \\cdot P(X_2=x_2) \\cdot \\ldots \\cdot P(X_n=x_n)\n\\]\nDas vereinfacht sich zu:\n\\[\nL(p) = p^k (1-p)^{n-k}\n\\]\nwobei \\(k = \\sum_{i=1}^{n} x_i\\) die Anzahl der Einsen in der Stichprobe ist.\n\n\n5.1.0.0.2 2. Log-Likelihood-Funktion aufstellen:\nUm die Berechnungen zu vereinfachen, nimmt man den natürlichen Logarithmus der Likelihood-Funktion, da nun die Produkte zu Summen werden und dies die Ableitungen vereinfacht:\n\\[\n\\log L(p) = k \\log p + (n-k) \\log (1-p)\n\\]\n\n\n5.1.0.0.3 3. Log-Likelihood maximieren:\nUm die Log-Likelihood-Funktion zu maximieren, leiten wir sie nach \\(p\\) ab und setzen diese Ableitung gleich null:\n\\[\n\\frac{d}{dp} \\log L(p) = \\frac{k}{p} - \\frac{n-k}{1-p} = 0\n\\]\n\n\n5.1.0.0.4 4. Gleichung nach \\(p\\) auflösen:\nUm diese Gleichung nach \\(p\\) aufzulösen, bringen wir sie auf einen gemeinsamen Nenner:\n\\[\n\\frac{k(1-p) - (n-k)p}{p(1-p)} = 0\n\\]\nDer Zähler muss null sein, damit der Bruch null wird und wir die Nullstellen finden:\n\\[\nk - kp - np + kp = k - np = 0\n\\]\nDas ergibt:\n\\[\nk = np\n\\]\nDaraus folgt, dass der MLE für \\(p\\):\n\\[\n\\hat{p} = \\frac{k}{n} = \\frac{1}{n} \\sum_{i=1}^{n} x_i.\n\\]\n\n\n5.1.0.0.5 Fazit:\nDer Maximum-Likelihood-Schätzer \\(\\hat{p}\\) für den Parameter \\(p\\) einer Bernoulli-Verteilung ist also gleich dem Stichprobenmittelwert \\(frac{k}{n}\\), was intuitiv der Anteil der beobachteten Einsen in der Stichprobe ist. Der Maximum-Likelihood-Schätzer ergibt sich, aus dem Gedanken, dass der Parameter \\(p\\) der Wert ist, bei dem die beobachteten Werte in der Stichprobe am wahrscheinlichsten sind.\n\n\n\n\n\n\n\n\n\n\nAnwendund des Zentralen Grenzwertsatzes für Schätzer\n\n\n\nWenn eine Stichprobe groß genug ist und die Ziehungen unabhängig und identisch Verteilt sind, dann ist \\(\\hat{p}\\) normalverteilt mit\n\\[\n\\mu_{\\hat{p}} = p,\n\\]\nwas bedeutet, dass wird bei unendlich vielen Wiederholungen im Mittel den wahren Wert von \\(p\\) schätzen. Einen Schätzer mit dieser Eigenschaft nennen wir erwartungstreu.\nDie Standardabweichung, welche wir bei einem Schätzer als Standardfehler bezeichnen, gegeben durch \\[\nSE_{\\hat{p}}=\\sigma_{\\hat{p}} = \\sqrt{\\frac{p(1-p)}{n}}.\n\\]\nDamit dies gilt muss \\(np \\geq 10\\) und \\(n(1-p) \\geq 10\\) sein.\n\n\n\n\n\n\n\n\n5.1.1 Konfidenzintervalle\nEin Konfidenzintervall ist ein Intervall, das den wahren Wert eines Parameters mit einer bestimmten Wahrscheinlichkeit enthält. Bei einer Normalverteilung haben wir schon beobachtet, dass ca. 95% der Werte innerhalb von zwei Standardabweichungen liegen. Wir können dieses Wissen nutzen, um ein Konfidenzintervall zu berechnen.\n\n\n\n\n\n\nImportant\n\n\n\nEin Konfidenzintervall \\(\\text{CI}_{1-\\alpha}\\) gibt für einen Schätzer \\(\\hat{\\theta}\\) eines Parameters \\(\\theta\\) einen Intervall an, in dem der wahre Wert von \\(\\theta\\) mit einer bestimmten Wahrscheinlichkeit (\\(1-\\alpha\\)) liegt. Das Konfidenzintervall für den Mittelwert \\(\\mu\\) einer Normalverteilung mit bekannter Varianz \\(\\sigma^2\\) ist gegeben durch\n\\[\n\\left(\\bar{\\theta} - z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}, \\bar{\\theta} + z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\right),\n\\]\nwobei \\(z_{\\alpha/2}\\) das \\(\\alpha/2\\)-Quantil der Standardnormalverteilung ist und wir dieses durch die Multiplikation mit der Standardabweichung \\(\\sigma\\) und Division durch die Wurzel der Stichprobengröße \\(n\\) auf unsere Ursprungsverteilung skalieren.\nFür den Fall, dass die Varianz unbekannt ist, können wir die Stichprobenvarianz \\(S^2\\) verwenden und erhalten\n\n\n\n\n\n\n\n\nErklärung\n\n\n\nZunächst überführen wir die Normalverteilung von \\(\\theta\\) in eine Standardnormalverteilung. Dazu subtrahieren wir den Erwartungswert \\(\\mu\\) und teilen durch die Standardabweichung \\(\\sigma\\). Damit erhalten wir eine Standardnormalverteilung mit Erwartungswert 0 und Varianz 1.\nNun können wir die Wahrscheinlichkeit berechnen, dass der Wert innerhalb eines Intervalls liegt. Für ein Konfidenzniveau von 95% ist \\(\\alpha = 0.05\\). Das bedeutet, dass wird an beiden Seiten der Verteilung \\(\\alpha/2 = 0.025\\) abtrennen.\nIn der Tabelle \\(\\Phi(z)\\) der Standardnormalverteilung suchen wir nun den Wert, ab dem 97.5% der Werte unterhalb liegen. Dieser Wert ist \\(z_{\\alpha/2} = 1.96\\). Aus der Symmetrie der Normalverteilung folgt, dass der Wert für \\(-z_{\\alpha/2}\\) ebenfalls 1.96 ist.\n\n\nIn unserem Beispiel ist die Stichprobenhäufigkeit \\(\\hat{p}\\) normalverteilt mit \\(\\mu_{\\hat{p}} = p\\) und \\(\\sigma_{\\hat{p}} = \\sqrt{\\frac{p(1-p)}{n}}\\). Wir können also ein Konfidenzintervall für \\(p\\) berechnen.\n\\[\n\\left(\\hat{p} - z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}, \\hat{p} + z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\right).\n\\]\nAus der Tabelle der Standardnormalverteilung können wir den Wert für \\(z_{\\alpha/2}\\) ablesen. Für ein Konfidenzniveau von 95% ist \\(\\alpha = 0.05\\) und damit \\(z_{\\alpha/2} = 1.96\\). Alternativ können wir dieses auch mit der Funktion norm.ppf berechnen.\n\n# Konfidenzintervall\nalpha = 0.05\nz = norm.ppf(1-alpha/2)\nprint(f'z_alpha/2: {z:.2f}')\n\nz_alpha/2: 1.96\n\n\nDamit ergibt sich für unser Beispiel ein Konfidenzintervall von\n\\[\n\\left(0.3 - 1.96 \\sqrt{\\frac{0.3(1-0.3)}{100}}, 0.3 + 1.96 \\sqrt{\\frac{0.3(1-0.3)}{100}}\\right) = (0.21, 0.39).\n\\]\nWir sind uns also zu 95% sicher, dass der wahre Wert von \\(p\\) in diesem Intervall liegt.\n\n\n\n\n\n\nCaution\n\n\n\nWelchen Faktor können wir anpassen, wenn wir den Konfidenzintervall verkleineren wollen?\nWelcher Konfidenzintervall würde sich für ein Konfidenzniveau von 99% ergeben?\n\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\nDer Faktor, den wir anpassen können, die größe der Stichprobe \\(n\\), da diese im Nennern steht.\nFür ein Konfidenzniveau von 99% ist \\(\\alpha = 0.01\\) und damit \\(z_{\\alpha/2} = 2.58\\). Damit ergibt sich für unser Beispiel ein Konfidenzintervall von\n\\[\n\\left(0.3 - 2.58 \\sqrt{\\frac{0.3(1-0.3)}{100}}, 0.3 + 2.58 \\sqrt{\\frac{0.3(1-0.3)}{100}}\\right) = (0.18, 0.42).\n\\]\n\n5.1.2 Darstellung des Konfidenzintervalls eines normalverteilten Parameters\n\n# Konfidenzniveau\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Konfidenzniveaus\nalphas = [0.05, 0.01]\n\ncmap = plt.get_cmap('tab10')\ncolors = cmap(np.linspace(0, 1, len(alphas)))\n\n# Normalverteilung\nmu = 0.3\nsigma = np.sqrt(0.3*(1-0.3)/100)\n\n# Konfidenzintervall\nx = np.linspace(0.1, 0.5, 100)\ny = norm.pdf(x, mu, sigma)\nplt.plot(x, y)\n\nfor alpha in alphas:\n    z = norm.ppf(1-alpha/2)\n    lower = mu - z*sigma\n    upper = mu + z*sigma\n\n    print(f'Konfidenzintervall für alpha={alpha}: ({lower:.2f}, {upper:.2f})')\n\n    # Darstellung\n    plt.fill_between(x, y, where=(x &gt;= lower) & (x &lt;= upper), alpha=0.1, color=colors[alphas.index(alpha)])\n    # Text\n    plt.text(mu+0.1, norm.pdf(mu, mu, sigma) * (0.2+ 0.1 * alphas.index(alpha)), f'CI {1-alpha} %', color=colors[alphas.index(alpha)])\n    \n\n\nplt.legend()\nplt.title('Konfidenzintervall für p')\nplt.show()\n\nKonfidenzintervall für alpha=0.05: (0.21, 0.39)\nKonfidenzintervall für alpha=0.01: (0.18, 0.42)\n\n\n/tmp/ipykernel_3111/598074221.py:35: UserWarning:\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.1.3 Punktschätzer und Konfidenzintervall für andere Fälle\nFür andere Verteilungen und Schätzer können wir die gleichen Prinzipien anwenden. Wir können die Schätzer und Konfidenzintervalle für den Mittelwert, die Varianz oder andere Parameter berechnen. Dabei müssen wir nur die Verteilung des Schätzers kennen und die entsprechenden Formeln anwenden.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Interferenz</span>"
    ]
  },
  {
    "objectID": "statistics/interference_basics.html#sec-statistics-hypothesistests",
    "href": "statistics/interference_basics.html#sec-statistics-hypothesistests",
    "title": "5  Interferenz",
    "section": "5.2 Hypothesentests",
    "text": "5.2 Hypothesentests\nEin Hypothesentest ist ein statistisches Verfahren, um zu entscheiden, ob eine Hypothese über eine Population auf Basis einer Stichprobe abgelehnt oder beibehalten wird. Dabei wird eine Nullhypothese \\(H_0\\) aufgestellt, die wir widerlegen wollen. Die Nullhypothese ist meist eine Aussage über den Wert eines Parameters, z.B. dass der Mittelwert einer Verteilung gleich einem bestimmten Wert ist. Die Alternative Hypothese \\(H_1\\) ist die Aussage, die wir beweisen wollen.\n\n\n\n\n\n\nDialektik und Falsifikationismus\n\n\n\nZunächst wirkt es ungewohnt, dass wir etwas aufstellen, nur um es zu widerlegen. Dieser Ansatz ist jedoch ein zentraler Bestandteil der wissenschaftlichen Methode und fußt tief in der westlichen Philosophie.\nDie Dialektik ist eine Methode, um Wahrheit zu finden, indem eine These aufgestellt wird und diese durch eine Antithese widerlegt wird. Die Synthese ist dann der nächste Schritt in Richtung Wahrheit. Dieser Prozess wird so lange wiederholt, bis die Wahrheit gefunden ist.\nDer Philisoph Karl Popper hat diesen Ansatz weiterentwickelt und den Falsifikationismus geprägt, um den wissenschaftlichen Erkennungsprozess zu beschreiben. Laut Popper lässt sich eine These nie beweisen, sondern nur widerlegen. Wenn eine These widerlegt wird, muss sie verworfen werden. Wenn sie widersteht, ist sie nicht bewiesen, sondern nur nicht widerlegt.\n\n\nEnsprechend gehen wir in der Statistik wie folgt vor:\n\nWir formulieren die Nullhypothese \\(H_0\\) und die Alternativhypothese \\(H_1\\). Die Nullhypothese ist die Aussage, die wir widerlegen wollen. Die Alternativhypothese ist die Aussage, die wir beweisen wollen.\nWir wählen ein Signifikanzniveau \\(\\alpha\\), das die Wahrscheinlichkeit angibt, mit der wir die Nullhypothese ablehnen.\nWir berechnen den Teststatistik \\(t\\) und bestimmen die Verteilung der Teststatistik unter der Nullhypothese.\n\n\n5.2.1 Entscheidungsfehler\nUm uns das Problem noch einmal zu vergegenwärtigen, betrachten wir nochmal das Beispiel eines medizinischen Tests. Wir haben eine Person, die entweder Krebs hat oder nicht. Der Test kann entweder positiv oder negativ sein.\n\n\n\n\n\ngraph LR\n    U[Person] --&gt;|0.001| A[Krebs] \n    U[Person] --&gt;|0.999| D[Kein Krebs]\n    A[Krebs] --&gt;|0.99| B[Positiv]\n    A[Krebs] --&gt;|0.01| C[Negativ]\n    D[Kein Krebs] --&gt;|0.05| E[Positiv]\n    D[Kein Krebs] --&gt;|0.95| F[Negativ]\n\n\n\n\n\n\nNun vergegenwärtigen wir noch einmal unser Business Understanding: Wir wollen die Person mit Krebs identifizieren. Der Worst-Case ist, dass wir die Person mit Krebs nicht erkennen. Wir wollen also die Wahrscheinlichkeit, dass wir die Person mit Krebs erkennen maximieren.\nDas bedeutet, dass wir die Nullhypothese \\(H_0\\) formulieren, dass die Person keinen Krebs hat. Die Alternativhypothese \\(H_1\\) ist, dass die Person Krebs hat. Bei einem Hypothesentest können wir zwei Arten von Fehlern machen:\n\nTyp I Fehler (\\(\\alpha\\)): Wir lehnen die Nullhypothese ab, obwohl sie wahr ist.\nTyp II Fehler (\\(\\beta\\)): Wir akzeptieren die Nullhypothese, obwohl sie falsch ist.\n\noder als Tabelle\n\nTable of error types\n\n\n\n\n\n\n\n\nTable of error types\nNull hypothesis (H0) is\n\n\nTrue\nFalse\n\n\nDecision\nabout null\nhypothesis (H0)\nNot reject\nCorrect inference\n(true negative)\n(probability = 1-α)\nType II error\n(false negative)\n(probability = β)\n\n\nReject\nType I error\n(false positive)\n(probability = α)\nCorrect inference\n(true positive)\n(probability = 1-β)\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nDer kritischere Fehler ist der Typ I Fehler, da wir die Nullhypothese ablehnen, obwohl sie wahr ist. Das Signifikanzniveau \\(\\alpha\\) gibt die Wahrscheinlichkeit an, mit der wir die Nullhypothese fälschlicherweise ablehnen. Das Signifikanzniveau wird vor dem Test festgelegt und sollte möglichst klein sein.\nÜblicherweise wird ein Signifikanzniveau von 5% oder 1% gewählt. Das bedeutet, dass wir die Nullhypothese nur ablehnen, wenn die Wahrscheinlichkeit, dass die Nullhypothese wahr ist, kleiner als 5% oder 1% ist.\nIm Beispiel würde dies bedeuten, dass wir eine Person als gesund einstufen, obwohl sie Krebs hat. Dies kann fatale Folgen haben, wenn die Person nicht rechtzeitig behandelt wird.\n\n\n\n\n5.2.2 Am Beispiel eines Punktschätzer für den Mittelwert einer Normalverteilung\nAngenommen wir wollen untersuchen, ob Studierende der Mechatronik im Schnitt intelligenter sind als der Durchschnitt. Wie haben bereits etabliert, dass die durchschnittliche Intelligenz in der Bevölkerung bei einem IQ von 100 liegt. Nehmen wir nun wohlwollend an, dass der IQ der Mechatronikstudierenden normalverteilt ist mit einem Mittelwert von 110 und einer Standardabweichung von 15:\n\\[\nX \\sim \\mathcal{N}(110, 15).\n\\]\nWir haben eine Stichprobe von 100 Studierenden und wollen wissen, ob der Mittelwert signifikant von 100 abweicht.\n\n# Hypothesentest\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Stichprobe der Mechatronikstudierenden\nmu = 110\nsigma = 15\n\n# Nullhypothese\nmu_0 = 100\n\n# Stichprobe\nn = 100\nX = np.random.normal(mu, sigma, n)\n\n# Schätzer für den Mittelwert und Standardabweichung\nX_hat = np.mean(X)\nmu_hat = np.std(X)\n\n# Normale Verteilung aus der Schätzung\nx = np.linspace(90, 130, 100)\ny = norm.pdf(x, X_hat, mu_hat)\n\n# Plot der geschätzten Verteilung und Nullhypothese\n\nplt.plot(x, y, label='Verteilung der Stichprobe')\nplt.axvline(mu_0, color='r', linestyle='--', label='Nullhypothese')\nplt.legend()\nplt.title('Verteilung der Stichprobe und Nullhypothese')\nplt.show()\n\n\n\n\n\n\n\n\nWir können nun sehen, dass die Stichprobe der Mechatronikstudierenden eine höhere Intelligenz aufweist als der Durchschnitt von 100. Wahrscheinlich werden wir auch einige Mechatronikstudierende finden, die weniger intelligent sind als der Durchschnitt.\nWas wir nun aber testen wollen, ist ob der Mittelwert der Stichprobe signifikant von 100 abweicht. Dazu brauchen wir jetzt einen Schätzer des Mittelwertes und der Standardabweichung der Stichprobe. Die Ziehungen sind unabängig und identisch verteilt, daher können wir den zentralen Grenzwertsatz anwenden. Dieser besagt, dass der Schätzer für den Mittelwert gegeben ist durch\n\\[\n\\hat{\\mu}=\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i,\n\\]\ndessen Standardfehler ist gegeben durch\n\\[\nSE(\\hat{\\mu})=\\sigma_{\\hat{\\mu}} = \\frac{\\sigma}{\\sqrt{n}}.\n\\]\nDies können wir nun anhand der Stichprobe berechnen und visualisieren.\n\n# Schätzer für den Mittelwert und Standardabweichung\nX_hat = np.mean(X)\nmu_hat = np.std(X)/np.sqrt(n)\n\n# Normale Verteilung aus der Schätzung\nx = np.linspace(90, 130, 100)\ny = norm.pdf(x, X_hat, mu_hat)\n\n# mit zweiseitigem Signifikanzniveau von 5%\nalpha = 0.05\nz = norm.ppf(1-alpha/2)\n\n# Plot der geschätzten Verteilung und Nullhypothese\n\nplt.plot(x, y, label='Verteilung der Stichprobe')\nplt.axvline(mu_0, color='r', linestyle='--', label='Nullhypothese')\nplt.axvline(X_hat + z*mu_hat, color='g', linestyle='--', label='Signifikanzniveau')\nplt.axvline(X_hat - z*mu_hat, color='g', linestyle='--')\nplt.fill_between(x, y, where=(x &gt;= X_hat - z*mu_hat) & (x &lt;= X_hat + z*mu_hat), alpha=0.1, color='g')\n\nplt.legend()\nplt.title('Verteilung von Mittelwert-Schätzer und Nullhypothese')\nplt.show()\n\nprint(f'Mittelwert der Stichprobe: {X_hat:.2f}')\nprint(f'Standardabweichung der Stichprobe: {mu_hat:.2f}')\n\n\n\n\n\n\n\n\nMittelwert der Stichprobe: 109.59\nStandardabweichung der Stichprobe: 1.48\n\n\nAn Fig. können wir erkennen, dass die Wahrscheinlichkeit, dass der Mittelwert der Stichprobe von 100 abweicht, sehr hoch ist. Der grüner Bereich, der das Signifikanzniveau darstellt, liegt weit rechts von der Nullhypothese.\n\n\n5.2.3 T-Verteilung\nFür endlich große Stichproben-Umfänge funktionert unsere Annäherung des Strichprobenmittelwerts durch eine Normalverteilung. Für kleine Stichproben-Umfänge ist die T-Verteilung besser geeignet.\nDie T-Verteilung ist gegeben durch\n\\[\nt = \\frac{\\bar{X} - \\mu_0}{\\frac{S}{\\sqrt{n}}},\n\\]\nund hängt vorallem vom Stichprobenumfang \\(n\\) ab. Je kleiner der Stichprobenumfang, desto breiter ist die T-Verteilung. Für große Stichprobenumfänge konvergiert die T-Verteilung gegen die Normalverteilung. \\(S\\) ist die Stichprobenstandardabweichung. Man bezeichnet \\(df = n-1\\) als die Freiheitsgrade der T-Verteilung.\n\n\n\n\n\n\nNote\n\n\n\nDie T-Verteilung wurde von William Sealy Gosset entwickelt. Er arbeitete für die Guinness-Brauerei und entwickelte die T-Verteilung, um die Qualität des Bieres zu verbessern. Um die Geheimhaltung der Brauerei zu wahren, veröffentlichte er seine Ergebnisse unter dem Pseudonym Student. Dies führte zur Bezeichnung der T-Verteilung als Student’s T-Verteilung.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Interferenz</span>"
    ]
  },
  {
    "objectID": "statistics/interference_advanced.html",
    "href": "statistics/interference_advanced.html",
    "title": "6  Tests",
    "section": "",
    "text": "6.1 T-Test\nNach dem zuvor beschreibenen Prinzip der Hypothesentests, gibt es verschiedene Tests, die auf unterschiedliche Fragestellungen zugeschnitten sind. In diesem Abschnitt werden einige dieser Tests vorgestellt.\nWir werden uns nur mit einigen typischen Tests beschäftigen. Es gibt noch viele weitere Tests, die auf spezielle Fragestellungen zugeschnitten sind. Die hier vorgestellten Tests sind jedoch die wichtigsten und werden in der Praxis am häufigsten verwendet.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tests</span>"
    ]
  },
  {
    "objectID": "statistics/interference_advanced.html#t-test",
    "href": "statistics/interference_advanced.html#t-test",
    "title": "6  Tests",
    "section": "",
    "text": "6.1.1 One-Sample Student’s T-Test\nBeim One-Sample Student’s T-Test wird die Mittelwert einer Stichprobe mit einem vorgegebenen Wert verglichen. Der Test wird verwendet, wenn die Varianz der Grundgesamtheit unbekannt ist.\nWir möchten den Typ I Fehler (\\(\\alpha\\)) festlegen und dann sicherstellen, dass die Wahrscheinlichkeit, dass wir die Nullhypothese fälschlicherweise ablehnen, kleiner oder gleich \\(\\alpha\\) ist.\nUm zu beurteilen, wie wahrscheinlich die Stichprobe ist, wenn die Nullhypothese wahr ist, wird die Teststatistik \\(t\\) berechnet. Diese berechnet den Unterschied zwischen \\(\\bar{x}\\) und \\(\\mu_0\\) in Einheiten der Standardabweichung der Stichprobe. Wörtlich bedeutet die Teststatistik: Um wie viele Standardfehler unterscheidet sich der Stichprobenmittelwert \\(\\bar{x}\\) vom vorgegebenen Wert \\(\\mu_0\\).\n\\[\nt = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}\n\\]\nwobei \\(\\bar{x}\\) der Stichprobenmittelwert, \\(s\\) die Stichprobenstandardabweichung und \\(n\\) die Stichprobengröße ist.\nDie Teststatistik \\(t\\) folgt einer t-Verteilung mit \\(n-1\\) Freiheitsgraden.\nWenn die Teststatistik \\(t\\) größer ist als der kritische Wert \\(t_{\\alpha}\\), dann lehnen wir die Nullhypothese ab. Der kritische Wert \\(t_{\\alpha}\\) wird aus der t-Verteilungstabelle abgelesen.\n\n\n6.1.2 Beispiel: Intelligenz von Mechatronikstudierenden\n\n# Hypothesentest\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Stichprobe der Mechatronikstudierenden\nmu = 110\nsigma = 15\n\n# Nullhypothese\nmu_0 = 100\n\n# Stichprobe\nn = 100\nX = np.random.normal(mu, sigma, n)\n\n# Schätzer für den Mittelwert und Standardabweichung\nX_hat = np.mean(X)\nmu_hat = np.std(X)\n\n# Normale Verteilung aus der Schätzung\nx = np.linspace(90, 130, 100)\n\n\n# Plot der geschätzten Verteilung und Nullhypothese\n\nplt.hist(X, bins=20, alpha=0.5, label='Stichprobe', color='blue', edgecolor='black')\n\nplt.axvline(mu_0, color='r', linestyle='--', label='Nullhypothese')\nplt.legend()\nplt.title('Verteilung der Stichprobe und Nullhypothese')\nplt.xlabel('Wert')\nplt.ylabel('Häufigkeit')\nplt.show()\n\n\n\n\n\n\n\n\n\nDefinition der Hypothesen:\n\n\\(H_0: \\mu_x = 100\\) (Der Mittelwert der Mechatronikstudierenden ist gleich dem Durchschnitt)\n\\(H_1: \\mu_x &gt; 100\\) (Der Mittelwert der Mechatronikstudierenden ist größer als der Durchschnitt)\n\nDefinition des Signifikanzniveaus: \\(\\alpha = 0.05\\)\n\nDie Wahrscheinlichkeit, dass wir die Nullhypothese fälschlicherweise ablehnen, soll maximal 5% betragen oder \\(P(H_1 | H_0) \\leq 0.05\\).\n\nBerechnung des Teststatistik \\(t\\):\n\n\n\\(T = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}= 6.50\\)\n\n\nX_bar = np.mean(X)\ns = np.std(X)\nt = (X_bar - mu_0) / (s / np.sqrt(n))\nprint(f't = {t}')\n\nt = 9.31922935528903\n\n\n\nBerechnung des kritischen Wertes \\(t_{\\alpha}\\):\n\n\nZunächst müssen wir uns entscheiden, ob wir ein- oder zweiseitig testen. Da wir wissen, dass der Mittelwert größer ist, wählen wir einen einseitigen Test.\nAus der t-Verteilungstabelle können wir für \\(n-1 = 99\\) Freiheitsgrade und \\(\\alpha = 0.05\\) den kritischen Wert \\(t_{\\alpha} = 1.660\\) ablesen.\n\n\nfrom scipy.stats import t as t_table\n\nalpha = 0.05\nt_alpha = t_table.ppf(1 - alpha, n-1)\nprint(f't_alpha = {t_alpha}')\n\nt_alpha = 1.6603911559963895\n\n\nDies können wir, wie folgt visualisieren:\n\nx = np.linspace(-7, 7, 100)\ny = t_table.pdf(x, n-1)\nplt.plot(x, y, label='t-Verteilung')\nplt.fill_between(x, 0, y, where=(x &gt; t_alpha), color='red', alpha=0.5, label='Ablehnungsbereich')\n\nplt.axvline(t, color='black', linestyle='--', label='T')\nplt.legend()\nplt.title('t-Verteilung und Ablehnungsbereich')\nplt.xlabel('t')\nplt.ylabel('Dichte')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAuf den ersten Blick kann es so wirken, als hätten wir eine Verteilung, um unsere Null-Hypothese gefittet, für die wir ja gar keine Daten haben (vgl. Beispiel im vorherigen Kapitel). Tatsächlich haben wird jedoch eine neue Zufallsvariable \\(T\\) definiert, die die Differenz zwischen dem Stichprobenmittelwert und dem Mittelwert der Nullhypothese in Einheiten der Standardabweichung der Stichprobe angibt. Diese Zufallsvariable \\(T\\) folgt einer t-Verteilung, die wir aus der Stichprobe berechnen können. Die Daten aus der Stichprobe können wir zum Fitten der Verteilung nutzen, da wir ja annehmen, dass alles was wird auf der Stichprobe basiert, auch auf der Grundgesamtheit basiert.\n\n\n\nDie Entscheidung basiert auf \\(T &gt; t_{\\alpha}\\),\n\n\nda \\(6.50 &gt; 1.660\\) ist, lehnen wir die Nullhypothese ab.\nDie Wahrscheinlichkeit, dass wir einen Mittelwert von 110 erhalten, wenn der Mittelwert tatsächlich 100 beträgt, ist deutlich kleiner als 5%.\nDie Wahrscheinlichkeit, dass wir die Nullhypothese fälschlicherweise ablehnen, obwohl Sie wahr ist \\(P(\\text{Entscheidung für }H_1 | H_0) &lt; 0.05\\)\nWir können diese Wahrscheinlichkeit auch genau bestimmen, indem wir sie z.B. aus der t-Verteilung berechnen bzw. aus der Tabelle ablesen. \\(P(\\text{Entscheidung für }H_1 | H_0) = 1.63\\cdot 10^{-9}\\)\nWir können unsere Null-Hypothese also mit einer Wahrscheinlichkeit von 99.999999837% ablehnen. Vorerst ist unsere These, dass die Mechatronikstudierenden intelligenter sind als der Durchschnitt, nicht falsifiziert.\n\n\np_value = 1 - t_table.cdf(t, n-1)\nprint(f'p-value = {p_value}')\n\np-value = 1.6653345369377348e-15\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWir bezeichen dern Wert \\(p\\) als p-Wert. Der p-Wert gibt die Wahrscheinlichkeit an, dass wir die Nullhypothese fälschlicherweise ablehnen, wenn sie tatsächlich wahr ist. Je kleiner der p-Wert, desto unwahrscheinlicher ist es, dass wir die Nullhypothese fälschlicherweise ablehnen.\n\n\n\n6.1.2.1 Zweiseitiger Test\nWir sind bisher davon ausgegaben, dass die Intelligenz von Mechantronikstudierenden entweder gleich oder größer als der Durchschnitt ist. Deswegen, haben wir den Ablehnungsbereich nur für \\(T &gt; t_{\\alpha}\\) definiert und einen einseitigen Test durchgeführt. Wir könnten aber auch einen Studiengang untersuchen, über den wir weniger wissen. In diesem Fall, könnten wir auch einen zweiseitigen Test durchführen, der einen Ablehnungsbereich für \\(T &gt; t_{\\alpha}\\) und \\(T &lt; -t_{\\alpha}\\) definiert.\n\nt_alpha = t_table.ppf(1 - alpha/2, n-1)\nprint(f't_alpha = {t_alpha}')\n\nt_alpha = 1.9842169515086827\n\n\n\nx = np.linspace(-7, 7, 100)\ny = t_table.pdf(x, n-1)\nplt.plot(x, y, label='t-Verteilung')\nplt.fill_between(x, 0, y, where=(x &gt; t_alpha), color='red', alpha=0.5, label='Ablehnungsbereich')\nplt.fill_between(x, 0, y, where=(x &lt; -t_alpha), color='red', alpha=0.5)\n\nplt.axvline(t, color='black', linestyle='--', label='T')\nplt.legend()\nplt.title('t-Verteilung und Ablehnungsbereich')\nplt.xlabel('t')\nplt.ylabel('Dichte')\nplt.show()\n\n\n\n\n\n\n\n\nDie Rechte Grenze des Ablehnungsbereichs ist \\(t_{\\alpha} = 1.984\\) wandert nun etwas nach rechts, da die Fläche unter der Kurve nun nur noch \\(0.025\\) beträgt. Wenn wir zweiseitig testen, testen wir also streger, da wir extremere Werte beobachten müssen, um die Nullhypothese abzulehnen.\nAm Vorgehen ändert sich jedoch nichts. Wir berechnen die Teststatistik \\(T\\) und vergleichen sie mit den kritischen Werten \\(t_{\\alpha}\\) und \\(-t_{\\alpha}\\), die wird nun aber aus der zweiseitigen t-Verteilungstabelle ablesen.\n\nDefinition der Hypothesen:\n\n\\(H_0: \\mu_x = 100\\) (Der Mittelwert der Mechatronikstudierenden ist gleich dem Durchschnitt)\n\\(H_1: \\mu_x \\neq 100\\) (Der Mittelwert der Mechatronikstudierenden ist größer als der Durchschnitt)\n\nDefinition des Signifikanzniveaus: \\(\\alpha = 0.05\\)\n\nDie Wahrscheinlichkeit, dass wir die Nullhypothese fälschlicherweise ablehnen, soll maximal 5% betragen oder \\(P(H_1 | H_0) \\leq 0.05\\).\n\nBerechnung des Teststatistik \\(t\\):\n\n\n\\(T = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}= 6.50\\)\n\n\nX_bar = np.mean(X)\ns = np.std(X)\nt = (X_bar - mu_0) / (s / np.sqrt(n))\nprint(f't = {t}')\n\nt = 9.31922935528903\n\n\n\nBerechnung des kritischen Wertes \\(t_{\\alpha}\\):\n\n\nNun müssen wir uns entscheiden, ob wir ein- oder zweiseitig testen. In diesem Fall müssen wir einen zweiseitigen Test durchführen.\nAus der t-Verteilungstabelle können wir für \\(n-1 = 99\\) Freiheitsgrade und \\(\\alpha = 0.05\\) den kritischen Wert \\(t_{\\alpha} = 1.984\\) ablesen.\n\n\nDie Entscheidung basiert auf \\(T &gt; t_{\\alpha}\\)\n\n\nda \\(6.50 &gt; 1.984\\) ist, lehnen wir die Nullhypothese weiterhin ab.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tests</span>"
    ]
  },
  {
    "objectID": "statistics/interference_advanced.html#zwei-stichproben-t-test",
    "href": "statistics/interference_advanced.html#zwei-stichproben-t-test",
    "title": "6  Tests",
    "section": "6.2 Zwei Stichproben T-Test",
    "text": "6.2 Zwei Stichproben T-Test\nHäufig ist man daran interessiert, ob sich die Mittelwerte zweier Stichproben unterscheiden. Auch in diesem Fall kann man einen T-Test einsetzen und die Teststatistik \\(t\\) berechnen (vgl. Fig. ?fig-sec-statistics-hypothesistests-two-sample-t-test)\nVorraussetzung ist jedoch, dass die Varianzen der beiden Stichproben gleich sind und die Stichproben unabhängig voneinander sind.\n {#fig-sec-statistics-hypothesistests-two-sample-t-test}\n\nPrüfen der Vorraussetzungen:\n\nDie Varianzen der beiden Stichproben sind gleich\nDie Stichproben sind unabhängig\n\nDefinition der Hypothesen:\n\n\\(H_0: \\mu_1 = \\mu_2\\) (Die Mittelwerte der beiden Stichproben sind gleich)\n\\(H_1: \\mu_1 \\neq \\mu_2\\) (Die Mittelwerte der beiden Stichproben sind ungleich)\n\nBerechnung der Teststatistik \\(t\\):\n\n\n\\(T = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\)\nwobei \\(s_p = \\sqrt{\\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}}\\) die gepoolte Standardabweichung ist.\n\n\nBerechnung des kritischen Wertes \\(t_{\\alpha}\\):\n\n\nAus der t-Verteilungstabelle können wir für \\(n_1 + n_2 - 2\\) Freiheitsgrade und \\(\\alpha = 0.05\\) den kritischen Wert \\(t_{\\alpha}\\) ablesen.\n\n\nDie Entscheidung basiert auf \\(T &gt; t_{\\alpha}\\) oder \\(T &lt; -t_{\\alpha}\\)\n\n\n\n\n\n\n\nTest auf Differenz der Erwartungswerte\n\n\n\nStatt auf Gleichheit der Mittelwerte zu testen, könnte man auch auf einen bestimmten Unterschied \\(\\omega_0\\) testen. In diesem Fall, würde die Nullhypothese \\(H_0: \\mu_1 - \\mu_2 = \\omega_0\\) lauten. Die Teststatistik \\(T\\) würde dann wie folgt berechnet werden: \\[\nT = \\frac{\\bar{x}_1 - \\bar{x}_2 - \\omega_0}{s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}.\n\\]\nDie Freiheitsgrade der t-Verteilung wären weiterhin \\(n_1 + n_2 - 2\\).\n\n\n\n\n\n\n\n\nTest bei gepaarten Stichproben\n\n\n\nIn manchen Fällen, haben wir zwei Stichproben, die nicht unabhängig voneinander sind. Ein Beispiel wäre, wenn wir ein Experiment haben, bei dem wir die gleiche Bauteil vor und nach einer Behandlung messen. In diesem Fall, können wir die Differenz der beiden Stichproben berechnen und dann einen One-Sample T-Test durchführen.\nZum Beispiel könnte man ein Verfahren zum Härten eines metallischen Bauteils untersuchen. Im Experiment würde man den Härtungsgrad vor und nach der Behandlung messen.\nTabelle: Härtungsgrad in HR (Rockwell) vor und nach der Behandlung\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nVor der Behandlung\n49.1\n49.2\n49.3\n49.4\n49.5\n49.6\n49.7\n49.8\n49.0\n50.0\n\n\nNach der Behandlung\n50.2\n50.3\n50.3\n50.2\n50.7\n50.7\n50.8\n50.9\n51.0\n51.1\n\n\nDifferenz\n1.1\n1.1\n1.0\n0.9\n1.2\n1.1\n1.1\n1.1\n1.0\n1.1\n\n\n\nDie durchschnittliche Differenz der Stichproben beträgt \\(\\bar{d} = 1.07\\) und die Standardabweichung der Differenz beträgt \\(s_d = 0.08\\). Wir könnten nun einen One-Sample T-Test durchführen, um zu testen, ob die Behandlung den Härtungsgrad signifikant erhöht.\n\nimport numpy as np\nd = np.array([1.1, 1.1, 1.0, 0.9, 1.2, 1.1, 1.1, 1.1, 1.0, 1.1])\nd_bar = np.mean(d)\ns_d = np.std(d)\nn = len(d)\nprint(f'd_bar = {d_bar}, s_d = {s_d}, n = {n}')\nt = d_bar / (s_d / np.sqrt(n))\nprint(f't = {t}')\n\nd_bar = 1.0699999999999998, s_d = 0.07810249675906655, n = 10\nt = 43.323033664571994\n\n\n\nPrüfen der Vorraussetzungen:\n\nDie Differenz der Stichproben ist normalverteilt\nDie Stichproben sind gepaart\n\nDefinition der Hypothesen:\n\n\\(H_0: \\mu = 0\\) (Die Behandlung hat keinen Effekt)\n\\(H_1: \\mu &gt; 0\\) (Die Behandlung erhöht den Härtungsgrad)\n\nBerechnung der Teststatistik \\(t\\):\n\n\\(T = \\frac{\\bar{x}}{s / \\sqrt{n}} = \\frac{1.07}{0.08/\\sqrt{10}} = 43.2\\)\n\nBerechnung des kritischen Wertes \\(t_{\\alpha}\\):\n\n\nAus der t-Verteilungstabelle können wir für \\(n-1 = 9\\) Freiheitsgrade und \\(\\alpha = 0.05\\) den kritischen Wert \\(t_{\\alpha} = 1.833\\) ablesen.\n\n::: {#a142d68c .cell .styled-output execution_count=10} ``` {.python .cell-code} from scipy.stats import t as t_table alpha = 0.05\nt_alpha = t_table.ppf(1 - alpha, n-1) print(f’t_alpha = {t_alpha}’) ```\n::: {.cell-output .cell-output-stdout} t_alpha = 1.8331129326536335 ::: :::\n\nEntscheidung basiert auf \\(T &gt; t_{\\alpha}\\)\n\n\nDa \\(43.2 &gt; 1.833\\) ist, lehnen wir die Nullhypothese ab. Die Behandlung hat den Härtungsgrad signifikant erhöht.\n\n\n6.3 Chi-Quadrat-Test\nNicht alle Test basieren auf der Normalverteilung oder einer T-Verteilung. Einige Schätzer folgen einer anderen Verteilung, wie z.B. der Chi-Quadrat(\\(\\mathcal{X}^2\\))-Verteilung. Der Chi-Quadrat-Test ist ein statistischer Test, der für verschiedene Fragestellungen eingesetzt werden kann.\n\nVerteilungstest (auch Anpassungstest genannt): Hier wird geprüft, ob vorliegende Daten auf eine bestimmte Weise verteilt sind.\nUnabhängigkeitstest: Hier wird geprüft, ob zwei Merkmale stochastisch unabhängig sind.\nHomogenitätstest: Hier wird geprüft, ob zwei oder mehr Stichproben derselben Verteilung bzw. einer homogenen Grundgesamtheit entstammen.\n\n\n6.3.1 Verteilungs-Test\nDer Chi-Quadrat-Test kann verwendet werden, um zu prüfen, ob zwei oder mehr Stichproben derselben Verteilung entstammen. Als Beispiel mit mechantronischem Anwendungsfall könnten wir die Qualität eines Lieferanten mit denen unserer bisherigen vergleichen. Wir können dabei die Häufigkeiten von OK-Teilen und verschiedenen Arten von Fehlern zählen und dann prüfen, ob der Lieferant die gleiche Qualität liefert.\n\n\n\nLieferant\nOK\nFehler 1\nFehler 2\nFehler 3\n\n\n\n\nA\n100\n10\n5\n5\n\n\nBisherige\n0.81\n0.08\n0.06\n0.05\n\n\n\nWir können die beobachteten Häufigkeiten in einer Kontingenztafel darstellen und dann die erwarteten Häufigkeiten berechnen, wenn die beiden Lieferanten die gleiche Qualität liefern würden.\n\n\n\nLieferant\nOK\nFehler 1\nFehler 2\nFehler 3\nSumme\n\n\n\n\nA (Observed)\n100\n10\n5\n5\n120\n\n\nBisherige (Expected)\n97.2\n9.6\n7.2\n6.0\n120\n\n\n\n\nÜberprüfen der Vorraussetzungen:\n\nDie Stichproben sind unabhängig\nDie erwarteten Häufigkeiten sind größer als 5\n\nDefintion der Hypothesen:\n\n\\(H_0\\): Der neue Lieferant liefert die gleiche Qualität (Identische Verteilung)\n\\(H_1\\): Die neue Lieferanten liefert abweichende Qualität\n\n\n\n\n\n\nBerechnung der Teststatistik \\(Z\\)\n\n\nDie Test-Statistik \\(Z\\) wird über die Summe der quadrierten Differenzen zwischen beobachteten \\(O_i\\) und erwarteten Häufigkeiten \\(E_i\\) berechnet. Dabei liegt der Ablehnungsbereich immer rechts. \\[\nZ = \\sum{\\frac{(O_i - E_i)^2}{E_i}}\n\\]\n\n\nimport numpy as np\nfrom scipy.stats import chi2\nO = np.array([100, 10, 5, 5])\nE = np.array([0.81, 0.08, 0.06, 0.05]) * np.sum(O)\n\nprint(f'O = {O}, E = {E}')\n\nZ = np.sum((O - E)**2 / E)\nprint(f'Z = {Z}')\n\nO = [100  10   5   5], E = [97.2  9.6  7.2  6. ]\nZ = 0.9362139917695469\n\n\n\nBerechnung des kritischen Wertes \\(Z_{\\alpha}\\):\n\n\nDer kritische Wert \\(Z_{\\alpha}\\) wird aus der Chi-Quadrat-Verteilungstabelle abgelesen. Die Anzahl der Freiheitsgrade \\(df\\) entspricht der Anzahl der Kategorien minus 1.\nIn diesem Fall haben wir 4 Kategorien, also \\(df = 4 - 1 = 3\\) Freiheitsgrade.\nFür \\(\\alpha = 0.05\\) beträgt der kritische Wert \\(Z_{\\alpha} = 7.815\\).\n\nDie Chi-Quadrat-Verteilung ist asymmetrisch und hat eine hohe Wahrscheinlichkeit für Werte nahe 0. Abbildung ?fig-sec-statistics-hypothesistests-chi2-distribution zeigt die Chi-Quadrat-Verteilung und den Ablehnungsbereich für \\(\\alpha = 0.05\\). Der Ablehnungsbereich ist rot markiert.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nx = np.linspace(0, 20, 100)\ny = chi2.pdf(x, 3)\nplt.plot(x, y, label='Chi-Quadrat-Verteilung')\nplt.fill_between(x, 0, y, where=(x &gt; 7.815), color='red', alpha=0.5, label='Ablehnungsbereich')\n\nplt.axvline(Z, color='black', linestyle='--', label='Z')\nplt.legend()\nplt.title('Chi-Quadrat-Verteilung und Ablehnungsbereich')\nplt.xlabel('Z')\nplt.ylabel('Dichte')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nDie Entscheidung basiert auf \\(Z &gt; Z_{\\alpha}\\)\n\n\nDa \\(Z = 0.936 &lt; 7.815 =  Z_{\\alpha}\\) ist, können wir die Nullhypothese nicht ablehnen. Der neue Lieferant liefert die gleiche Qualität wie der bisherige Lieferant.\n\n\n\n\n\n\n\nTest auf bestimmte Verteilung\n\n\n\nAnstelle einer Empirischen Grundgesamtheit, könnten wir auch eine bestimmte Verteilung annehmen und prüfen, ob die Daten dieser Verteilung entsprechen. In diesem Fall, würden wir die erwarteten Häufigkeiten basierend auf der angenommenen Verteilung berechnen und dann den Chi-Quadrat-Test durchführen.\n\n\n\n\n6.3.2 Unabhängigkeitstest\nDer Chi-Quadrat-Test kann auch verwendet werden, um zu prüfen, ob zwei Merkmale \\(X\\) mit \\(m\\) Kategorien und \\(Y\\) mit \\(k\\) Kategorien stochastisch unabhängig sind.\nHierzu kann man eine Kontingenztafel erstellen, die die Häufigkeiten der Kombinationen der beiden Merkmale enthält:\n\n\n\n\n\\( Y_1 \\)\n\\( Y_2 \\)\n\\(\\dots\\)\n\\( Y_r \\)\n\\( \\sum \\)\n\n\n\\( X_1 \\)\n\\( n_{11} \\)\n\\( n_{12} \\)\n\\(\\dots\\)\n\\( n_{1r} \\)\n\\( n_{1.} \\)\n\n\n\\( X_2 \\)\n\\( n_{21} \\)\n\\( n_{22} \\)\n\\(\\dots\\)\n\\( n_{2r} \\)\n\\( n_{2.} \\)\n\n\n\\( \\vdots \\)\n\\( \\vdots \\)\n\\( \\vdots \\)\n\\( \\ddots \\)\n\\( \\vdots \\)\n\\( \\vdots \\)\n\n\n\\( X_m \\)\n\\( n_{m1} \\)\n\\( n_{m2} \\)\n\\(\\dots\\)\n\\( n_{mr} \\)\n\\( n_{m.} \\)\n\n\n\\( \\sum \\)\n\\( n_{.1} \\)\n\\( n_{.2} \\)\n\\(\\dots\\)\n\\( n_{.r} \\)\n\\( n \\)\n\n\n\nFür die Zeilen und Spalten der Kontingenztafel werden die Randhäufigkeiten berechnet:\n\\[\nn_{i.} = \\sum_{j=1}^{r} n_{ij} \\quad \\text{und} \\quad n_{.j} = \\sum_{i=1}^{m} n_{ij}\n\\]\nDie erwarteten Häufigkeiten \\(p_{ij}\\) für jede Zelle der Kontingenztafel werden berechnet als das Produkt der Randhäufigkeiten geteilt durch die Gesamtanzahl der Beobachtungen:\n\\[\np_{ij} = \\frac{n_{i.} \\cdot n_{.j}}{n},\n\\]\nund die relativen Randhäufigkeiten als\n\\[\np_{i.} = \\frac{n_{i.}}{n} \\quad \\text{und} \\quad p_{.j} = \\frac{n_{.j}}{n}.\n\\]\nAus den Rechenregeln der Wahrscheinlichkeit in Kapitel Chapter 3 folgt für unabhängige Ereignisse, dass die Wahrscheinlichkeit des gemeinsamen Eintretens der beiden Ereignisse gleich dem Produkt der Einzelwahrscheinlichkeiten ist:\n\\[\nP(A \\cap B) = P(A) \\cdot P(B).\n\\]\nFür die Kontingenztafel bedeutet dies, dass die erwarteten Häufigkeiten \\(p_{ij}\\) gleich dem Produkt der Randhäufigkeiten \\(p_{i.}\\) und \\(p_{.j}\\) sind, wenn die beiden Merkmale unabhängig voneinander sind. Wenn dies für alle Zellen nahezu zutrifft, dann können wir die Nullhypothese annehmen, dass die beiden Merkmale unabhängig voneinander sind.\nDie Testgröße \\(\\mathcal{X}^2\\) wird dann berechnet als die Summe der quadrierten Differenzen zwischen beobachteten und erwarteten Häufigkeiten, normiert durch die erwarteten Häufigkeiten:\n\\[\n\\mathcal{X}^2 = \\sum_{i=1}^{m} \\sum_{j=1}^{r} \\frac{(n_{ij} - p_{ij})^2}{p_{ij}}.\n\\]\nDie Testgröße \\(\\mathcal{X}^2\\) folgt einer Chi-Quadrat-Verteilung mit \\((m-1)(r-1)\\) Freiheitsgraden.\n\n\n6.3.3 Homogenitätstest\nAuf ähnliche Weise kann der Chi-Quadrat-Test auch verwendet werden, um zu prüfen, ob zwei oder mehr Stichproben derselben Verteilung bzw. einer homogenen Grundgesamtheit entstammen. In diesem Fall wird eine Kontingenztafel erstellt, die die Häufigkeiten der Kombinationen der Stichproben enthält. Die Testgröße \\(\\mathcal{X}^2\\) wird dann berechnet als die Summe der quadrierten Differenzen zwischen beobachteten und erwarteten Häufigkeiten, normiert durch die erwarteten Häufigkeiten.\n\n\n\n6.4 Kolmogorov-Smirnov-Test\nDer \\(\\mathcal{X}^2\\)-Test funktionier auf der Basis von Häufigkeiten und ist daher nur für diskrete Daten geeignet. Der Kolmogorov-Smirnov-Test hingegen kann auch für kontinuierliche Daten verwendet werden. Der Kolmogorov-Smirnov-Test, kann z.B. angewendet werden, um zu prüfen ob,\n\nzwei Zufallsvariablen eine identische Verteilung besitzen oder\neine Zufallsvariable einer zuvor angenommenen Wahrscheinlichkeitsverteilung folgt.\n\n\n\n6.5 Übersicht über Statistische Tests\nGrundsätzlich setzen wir statischtische Tests ein, wenn wir eine Hypothese über eine Grundgesamtheit aufstellen und diese Hypothese auf Basis einer Stichprobe überprüfen wollen. Die Hypothese wird in eine Nullhypothese \\(H_0\\) und eine Alternativhypothese \\(H_1\\) aufgeteilt. Die Nullhypothese ist die Hypothese, die wir widerlegen wollen, während die Alternativhypothese die Hypothese ist, die wir annehmen, wenn die Nullhypothese widerlegt wird.\nTest ermöglichen und zu unterschieden, ob die durch uns auf Grund der Stichprobe getroffene Aussage auf die Grundgesamtheit übertragbar oder möglicherweise nur Zufall sind.\n\n\n\n\n\n\n\nInductiveload, based on original work by Jhguch. n.d. “Two Sample t-Test.” https://commons.wikimedia.org/wiki/File:Two_sample_ttest.svg.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tests</span>"
    ]
  },
  {
    "objectID": "statistics/interference_advanced.html#chi-quadrat-test",
    "href": "statistics/interference_advanced.html#chi-quadrat-test",
    "title": "6  Tests",
    "section": "6.3 Chi-Quadrat-Test",
    "text": "6.3 Chi-Quadrat-Test\nNicht alle Test basieren auf der Normalverteilung oder einer T-Verteilung. Einige Schätzer folgen einer anderen Verteilung, wie z.B. der Chi-Quadrat(\\(\\mathcal{X}^2\\))-Verteilung. Der Chi-Quadrat-Test ist ein statistischer Test, der für verschiedene Fragestellungen eingesetzt werden kann.\n\nVerteilungstest (auch Anpassungstest genannt): Hier wird geprüft, ob vorliegende Daten auf eine bestimmte Weise verteilt sind.\nUnabhängigkeitstest: Hier wird geprüft, ob zwei Merkmale stochastisch unabhängig sind.\nHomogenitätstest: Hier wird geprüft, ob zwei oder mehr Stichproben derselben Verteilung bzw. einer homogenen Grundgesamtheit entstammen.\n\n\n6.3.1 Verteilungs-Test\nDer Chi-Quadrat-Test kann verwendet werden, um zu prüfen, ob zwei oder mehr Stichproben derselben Verteilung entstammen. Als Beispiel mit mechantronischem Anwendungsfall könnten wir die Qualität eines Lieferanten mit denen unserer bisherigen vergleichen. Wir können dabei die Häufigkeiten von OK-Teilen und verschiedenen Arten von Fehlern zählen und dann prüfen, ob der Lieferant die gleiche Qualität liefert.\n\n\n\nLieferant\nOK\nFehler 1\nFehler 2\nFehler 3\n\n\n\n\nA\n100\n10\n5\n5\n\n\nBisherige\n0.81\n0.08\n0.06\n0.05\n\n\n\nWir können die beobachteten Häufigkeiten in einer Kontingenztafel darstellen und dann die erwarteten Häufigkeiten berechnen, wenn die beiden Lieferanten die gleiche Qualität liefern würden.\n\n\n\nLieferant\nOK\nFehler 1\nFehler 2\nFehler 3\nSumme\n\n\n\n\nA (Observed)\n100\n10\n5\n5\n120\n\n\nBisherige (Expected)\n97.2\n9.6\n7.2\n6.0\n120\n\n\n\n\nÜberprüfen der Vorraussetzungen:\n\nDie Stichproben sind unabhängig\nDie erwarteten Häufigkeiten sind größer als 5\n\nDefintion der Hypothesen:\n\n\\(H_0\\): Der neue Lieferant liefert die gleiche Qualität (Identische Verteilung)\n\\(H_1\\): Die neue Lieferanten liefert abweichende Qualität\n\n\n\n\n\n\nBerechnung der Teststatistik \\(Z\\)\n\n\nDie Test-Statistik \\(Z\\) wird über die Summe der quadrierten Differenzen zwischen beobachteten \\(O_i\\) und erwarteten Häufigkeiten \\(E_i\\) berechnet. Dabei liegt der Ablehnungsbereich immer rechts. \\[\nZ = \\sum{\\frac{(O_i - E_i)^2}{E_i}}\n\\]\n\n\nimport numpy as np\nfrom scipy.stats import chi2\nO = np.array([100, 10, 5, 5])\nE = np.array([0.81, 0.08, 0.06, 0.05]) * np.sum(O)\n\nprint(f'O = {O}, E = {E}')\n\nZ = np.sum((O - E)**2 / E)\nprint(f'Z = {Z}')\n\nO = [100  10   5   5], E = [97.2  9.6  7.2  6. ]\nZ = 0.9362139917695469\n\n\n\nBerechnung des kritischen Wertes \\(Z_{\\alpha}\\):\n\n\nDer kritische Wert \\(Z_{\\alpha}\\) wird aus der Chi-Quadrat-Verteilungstabelle abgelesen. Die Anzahl der Freiheitsgrade \\(df\\) entspricht der Anzahl der Kategorien minus 1.\nIn diesem Fall haben wir 4 Kategorien, also \\(df = 4 - 1 = 3\\) Freiheitsgrade.\nFür \\(\\alpha = 0.05\\) beträgt der kritische Wert \\(Z_{\\alpha} = 7.815\\).\n\nDie Chi-Quadrat-Verteilung ist asymmetrisch und hat eine hohe Wahrscheinlichkeit für Werte nahe 0. Abbildung ?fig-sec-statistics-hypothesistests-chi2-distribution zeigt die Chi-Quadrat-Verteilung und den Ablehnungsbereich für \\(\\alpha = 0.05\\). Der Ablehnungsbereich ist rot markiert.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nx = np.linspace(0, 20, 100)\ny = chi2.pdf(x, 3)\nplt.plot(x, y, label='Chi-Quadrat-Verteilung')\nplt.fill_between(x, 0, y, where=(x &gt; 7.815), color='red', alpha=0.5, label='Ablehnungsbereich')\n\nplt.axvline(Z, color='black', linestyle='--', label='Z')\nplt.legend()\nplt.title('Chi-Quadrat-Verteilung und Ablehnungsbereich')\nplt.xlabel('Z')\nplt.ylabel('Dichte')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nDie Entscheidung basiert auf \\(Z &gt; Z_{\\alpha}\\)\n\n\nDa \\(Z = 0.936 &lt; 7.815 =  Z_{\\alpha}\\) ist, können wir die Nullhypothese nicht ablehnen. Der neue Lieferant liefert die gleiche Qualität wie der bisherige Lieferant.\n\n\n\n\n\n\n\nTest auf bestimmte Verteilung\n\n\n\nAnstelle einer Empirischen Grundgesamtheit, könnten wir auch eine bestimmte Verteilung annehmen und prüfen, ob die Daten dieser Verteilung entsprechen. In diesem Fall, würden wir die erwarteten Häufigkeiten basierend auf der angenommenen Verteilung berechnen und dann den Chi-Quadrat-Test durchführen.\n\n\n\n\n6.3.2 Unabhängigkeitstest\nDer Chi-Quadrat-Test kann auch verwendet werden, um zu prüfen, ob zwei Merkmale \\(X\\) mit \\(m\\) Kategorien und \\(Y\\) mit \\(k\\) Kategorien stochastisch unabhängig sind.\nHierzu kann man eine Kontingenztafel erstellen, die die Häufigkeiten der Kombinationen der beiden Merkmale enthält:\n\n\n\n\n\\( Y_1 \\)\n\\( Y_2 \\)\n\\(\\dots\\)\n\\( Y_r \\)\n\\( \\sum \\)\n\n\n\\( X_1 \\)\n\\( n_{11} \\)\n\\( n_{12} \\)\n\\(\\dots\\)\n\\( n_{1r} \\)\n\\( n_{1.} \\)\n\n\n\\( X_2 \\)\n\\( n_{21} \\)\n\\( n_{22} \\)\n\\(\\dots\\)\n\\( n_{2r} \\)\n\\( n_{2.} \\)\n\n\n\\( \\vdots \\)\n\\( \\vdots \\)\n\\( \\vdots \\)\n\\( \\ddots \\)\n\\( \\vdots \\)\n\\( \\vdots \\)\n\n\n\\( X_m \\)\n\\( n_{m1} \\)\n\\( n_{m2} \\)\n\\(\\dots\\)\n\\( n_{mr} \\)\n\\( n_{m.} \\)\n\n\n\\( \\sum \\)\n\\( n_{.1} \\)\n\\( n_{.2} \\)\n\\(\\dots\\)\n\\( n_{.r} \\)\n\\( n \\)\n\n\n\nFür die Zeilen und Spalten der Kontingenztafel werden die Randhäufigkeiten berechnet:\n\\[\nn_{i.} = \\sum_{j=1}^{r} n_{ij} \\quad \\text{und} \\quad n_{.j} = \\sum_{i=1}^{m} n_{ij}\n\\]\nDie erwarteten Häufigkeiten \\(p_{ij}\\) für jede Zelle der Kontingenztafel werden berechnet als das Produkt der Randhäufigkeiten geteilt durch die Gesamtanzahl der Beobachtungen:\n\\[\np_{ij} = \\frac{n_{i.} \\cdot n_{.j}}{n},\n\\]\nund die relativen Randhäufigkeiten als\n\\[\np_{i.} = \\frac{n_{i.}}{n} \\quad \\text{und} \\quad p_{.j} = \\frac{n_{.j}}{n}.\n\\]\nAus den Rechenregeln der Wahrscheinlichkeit in Kapitel Chapter 3 folgt für unabhängige Ereignisse, dass die Wahrscheinlichkeit des gemeinsamen Eintretens der beiden Ereignisse gleich dem Produkt der Einzelwahrscheinlichkeiten ist:\n\\[\nP(A \\cap B) = P(A) \\cdot P(B).\n\\]\nFür die Kontingenztafel bedeutet dies, dass die erwarteten Häufigkeiten \\(p_{ij}\\) gleich dem Produkt der Randhäufigkeiten \\(p_{i.}\\) und \\(p_{.j}\\) sind, wenn die beiden Merkmale unabhängig voneinander sind. Wenn dies für alle Zellen nahezu zutrifft, dann können wir die Nullhypothese annehmen, dass die beiden Merkmale unabhängig voneinander sind.\nDie Testgröße \\(\\mathcal{X}^2\\) wird dann berechnet als die Summe der quadrierten Differenzen zwischen beobachteten und erwarteten Häufigkeiten, normiert durch die erwarteten Häufigkeiten:\n\\[\n\\mathcal{X}^2 = \\sum_{i=1}^{m} \\sum_{j=1}^{r} \\frac{(n_{ij} - p_{ij})^2}{p_{ij}}.\n\\]\nDie Testgröße \\(\\mathcal{X}^2\\) folgt einer Chi-Quadrat-Verteilung mit \\((m-1)(r-1)\\) Freiheitsgraden.\n\n\n6.3.3 Homogenitätstest\nAuf ähnliche Weise kann der Chi-Quadrat-Test auch verwendet werden, um zu prüfen, ob zwei oder mehr Stichproben derselben Verteilung bzw. einer homogenen Grundgesamtheit entstammen. In diesem Fall wird eine Kontingenztafel erstellt, die die Häufigkeiten der Kombinationen der Stichproben enthält. Die Testgröße \\(\\mathcal{X}^2\\) wird dann berechnet als die Summe der quadrierten Differenzen zwischen beobachteten und erwarteten Häufigkeiten, normiert durch die erwarteten Häufigkeiten.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tests</span>"
    ]
  },
  {
    "objectID": "statistics/interference_advanced.html#kolmogorov-smirnov-test",
    "href": "statistics/interference_advanced.html#kolmogorov-smirnov-test",
    "title": "6  Tests",
    "section": "6.4 Kolmogorov-Smirnov-Test",
    "text": "6.4 Kolmogorov-Smirnov-Test\nDer \\(\\mathcal{X}^2\\)-Test funktionier auf der Basis von Häufigkeiten und ist daher nur für diskrete Daten geeignet. Der Kolmogorov-Smirnov-Test hingegen kann auch für kontinuierliche Daten verwendet werden. Der Kolmogorov-Smirnov-Test, kann z.B. angewendet werden, um zu prüfen ob,\n\nzwei Zufallsvariablen eine identische Verteilung besitzen oder\neine Zufallsvariable einer zuvor angenommenen Wahrscheinlichkeitsverteilung folgt.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tests</span>"
    ]
  },
  {
    "objectID": "statistics/interference_advanced.html#übersicht-über-statistische-tests",
    "href": "statistics/interference_advanced.html#übersicht-über-statistische-tests",
    "title": "6  Tests",
    "section": "6.5 Übersicht über Statistische Tests",
    "text": "6.5 Übersicht über Statistische Tests\nGrundsätzlich setzen wir statischtische Tests ein, wenn wir eine Hypothese über eine Grundgesamtheit aufstellen und diese Hypothese auf Basis einer Stichprobe überprüfen wollen. Die Hypothese wird in eine Nullhypothese \\(H_0\\) und eine Alternativhypothese \\(H_1\\) aufgeteilt. Die Nullhypothese ist die Hypothese, die wir widerlegen wollen, während die Alternativhypothese die Hypothese ist, die wir annehmen, wenn die Nullhypothese widerlegt wird.\nTest ermöglichen und zu unterschieden, ob die durch uns auf Grund der Stichprobe getroffene Aussage auf die Grundgesamtheit übertragbar oder möglicherweise nur Zufall sind.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tests</span>"
    ]
  },
  {
    "objectID": "statistics/tutorial_2.html",
    "href": "statistics/tutorial_2.html",
    "title": "Tutorial 2: Understanding Patterns in Grid Loads",
    "section": "",
    "text": "Objective\nAgain, we work witht the Global Energy Forecasting Competition Hong, Pinson, and Fan (2014) data, you have cleaned before. This time, we want to make a deeper analysis of the data and try to find patterns in the data. In particular, we want to find out about the following characteristics of the data:",
    "crumbs": [
      "Statistik",
      "Tutorial 2: Understanding Patterns in Grid Loads"
    ]
  },
  {
    "objectID": "statistics/tutorial_2.html#objective",
    "href": "statistics/tutorial_2.html#objective",
    "title": "Tutorial 2: Understanding Patterns in Grid Loads",
    "section": "",
    "text": "Weekday Effects: Grid load varies by day of the wee, as this changes the behavior of private households and industries. You want to find out, whether there is a difference in the load between weekdays and weekends. This is important, for later forecasting steps, as you might want to include this information in your model or use different models for weekdays and weekends.\n\nVisualize the load of each Weekday in a Boxplot, Violin-Plot or a Histogram and color the weekends differently.\nAdd holidays to the plot and see, if there is a difference in the load on holidays.\nMake a statistical test to see, if the load is different between weekdays and weekends. You can use either a t-test to compare the means or the Kolmogorov-Smirnov (KS) Test to compare the distributions.\nIs there a significant difference (\\(\\alpha&gt;0.05\\)) in the load on weekends?\n\nExtreme Values: Extreme values are values that are far away from the mean of the data. In electricity grids, peak loads, which are the maximum load in a certain time period, are important to know, as they determine the capacity of the grid. If the peak load get to high, this might lead to equipment failure and blackouts. You want to find out, if there are extreme values in the data and how they are distributed.\n\nIdentify the peak loads for each day in the data and visualize them in a histogram.\nFit a Generalized Extreme Value Distribution to the extreme values and compare it to a fitted normal distribution in a plot. Use (from scipy.stats import genextreme)\nHow would You use this distribution to predict the probability peak loads higher, than a certain threshold?\n\n\n\n\n\n\nHong, Tao, Pierre Pinson, and Shu Fan. 2014. “Global Energy Forecasting Competition 2012.” International Journal of Forecasting 30 (2): 357–63. https://doi.org/https://doi.org/10.1016/j.ijforecast.2013.07.001.",
    "crumbs": [
      "Statistik",
      "Tutorial 2: Understanding Patterns in Grid Loads"
    ]
  },
  {
    "objectID": "regression/index.html",
    "href": "regression/index.html",
    "title": "Regressions-Analyse",
    "section": "",
    "text": "Im Allgemeinen kann die Regressionsanalyse als eine Sammlung von Werkzeugen verstanden werden, die dazu verwendet werden, eine Beziehung zwischen einer abhängigen Variable \\(Y\\)$ (auch Zielvariable, Antwortvariable oder Label genannt) und der unabhängigen Variable \\(X\\) (auch Regressor, Prädiktoren, Kovariaten, erklärende Variable oder Feature genannt) zu schätzen oder festzustellen.\nWenn wir eine Regressionsfunktion \\(f\\) als Modell für die Beziehung zwischen \\(X\\) und \\(Y\\) annehmen, können wir die Regressionsanalyse als die Suche nach den Parametern \\(\\theta\\) verstehen, die die Funktion \\(f\\) am besten an die Daten anpassen.\nDas Problem der Regressionsanalyse kann also wie folgt formuliert werden: \\[\nY = f(X, \\theta) + \\epsilon,\n\\]\nwobei \\(\\theta\\) durch die Optimierung für eine gute Anpassung von \\(f\\) an die Daten gefunden wird. In der Regel erhalten wir dabei einen Fehlerterm \\(\\epsilon\\), der – wenn wir Glück haben – normalverteilt mit einem Erwartungswert von null und konstanter Varianz ist.\nRegessionen sind ein mächtiges Werkzeug für die Interpetation von Daten und die Vorhersage von Werten. In der Praxis gibt es viele verschiedene Arten von Regressionsmodellen, die sich in ihrer Komplexität und den Annahmen, die sie machen, unterscheiden. In diesem Kapitel werden wir uns auf die lineare Regression konzentrieren, die eine der einfachsten und am häufigsten verwendeten Formen der Regressionsanalyse ist und sich für Zusammenhänge zwischen einer abhängigen und einer unabhängigen Variablen mit intervallskalierten Daten eignet.\nZuerst führen wir eine einfache Lineare Regression mit einer unabhängigen Variable durch. Anschließend erweitern wir das Modell auf mehrere unabhängige und auch kategorische Variablen. Hierzu nutzen wird die Matrix-Schreibweise.\nErweiterte Themen Intercepts und Regularisierung\nDiskussion von Statistischem Lernen und Resampling\nLogistische Regression als Beispiel für Klassifikation",
    "crumbs": [
      "Regressions-Analyse"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "This concludes our introduction to the basis of data science with a focus on engineering topics.\nThese notes introduced the main concepts with a clear focus of accurate mathematical representation and close illustration with programmatic examples.\nIf we need to sum up the main concept that is present in large sections of these notes it is that the correct representation is key in finding good concepts of computer based processing, To this extend the second key concept is to make sure the concepts can be handled via a computer and the student, so theory and programmatic application go hand in hand.\nBoth concepts stay true if we move to more evolved data science methods in further classes.",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Carroll, Lewis. 2015. Alice’s Adventures in Wonderland\nUnfolded.\n\n\nDATAtab. retrieved 2025. “Korrelationskoeffizient Tutorial\nImage.” https://datatab.de/assets/tutorial/Korrelationskoeffizient.png.\n\n\nDiez, David M., Christopher D. Barr, and Mine Cetinkaya-Rundel. 2019.\nOpenIntro Statistics. 4th ed. Boston, MA: OpenIntro. \\url{https://www.openintro.org/stat/textbook.php?stat_book=os}.\n\n\nFrisch, Max. 1957. Homo Faber. Ein Bericht. Frankfurt am Main:\nSuhrkamp.\n\n\nHong, Tao, Pierre Pinson, and Shu Fan. 2014. “Global Energy\nForecasting Competition 2012.” International Journal of\nForecasting 30 (2): 357–63. https://doi.org/https://doi.org/10.1016/j.ijforecast.2013.07.001.\n\n\nHyndman, RJ. 2018. Forecasting: Principles and Practice.\nOTexts.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, et al.\n2013. An Introduction to Statistical Learning. Vol. 112.\nSpringer.\n\n\nKozyrkov, Cassie. 2018. “What on Earth Is Data Science?”\nmedium.com. \\url{https://kozyrkov.medium.com/what-on-earth-is-data-science-eb1237d8cb37}.\n\n\nMatthes, Eric. 2023. Python Crash Course - a Hands-on, Project-Based\nIntroduction to Programming. 3rd ed. No Starch Press. https://ehmatthes.github.io/pcc_3e/.\n\n\nRedAndr and Mikhail Ryazanov. 2011. “Satirical diagram illustrating the influence of pirates\ndecreasing on global warming as per Pastafarian beliefs.”\nhttps://commons.wikimedia.org/wiki/File:PiratesVsTemp(en).svg.\n\n\nShearer, Colin. 2000. “The CRISP-DM Model: The New Blueprint for\nData Mining.” Journal of Data Warehousing 5 (4): 13–22.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of\nStatistical Software 59: 1–23.\n\n\nWikimedia Commons contributors. retrieved 2025. “CRISP-DM\nProcess Diagram.” https://upload.wikimedia.org/wikipedia/commons/b/b9/CRISP-DM_Process_Diagram.png.",
    "crumbs": [
      "References"
    ]
  }
]