[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning and Data Science 1",
    "section": "",
    "text": "Preface\nDies sind die Vorlesungsnotizen für die Lehrveranstaltung Machine Learning + Data Science I (Lecture) am MCI | The Entrepreneurial School Bachelor-Studiengangs Mechatronik im Sommer-Semester 2025.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#danksagungen",
    "href": "index.html#danksagungen",
    "title": "Machine Learning and Data Science 1",
    "section": "Danksagungen",
    "text": "Danksagungen\nWir danken der Open-Source-Community für die Bereitstellung exzellenter Tutorials und Anleitungen zu Data-Science-Themen in und mit Python im Web.\nEinzelne Quellen werden an den entsprechenden Stellen im Dokument zitiert.\nBesonderer Dank gilt Peter Kandof, für das Aufsetzen eines Beispielprojekts für die Vorlesung anhand von MECH-M-DUAL-1-DBM - Grundlagen datenbasierter Methoden. Diese Notizen wurden mit Quarto erstellt.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Einleitung",
    "section": "",
    "text": "Leistungsüberprüfung\nIn dieser Lehrveranstaltung werden wir uns mit den grundlegenden Konzepten moderner Data-Science-Techniken befassen und eine solide Basis in Statistik und maschinellem Lernen legen. Wir behandeln alle essenziellen Grundlagen, die notwendig sind, um sich in diesem Bereich sicher zu bewegen. Dabei beschränken wir uns nicht nur auf die theoretischen Aspekte, sondern nutzen auch Python, um die Inhalte programmatisch zu veranschaulichen.\nDie verwendeten Referenzen wurden nach Qualität, freier Verfügbarkeit und der Nutzung von Python ausgewählt. Für die statistischen Grundlagen greifen wir auf Beispiele aus Diez, Barr, and Cetinkaya-Rundel (2019) zurück, während für die Einführung in das statistische Lernen James et al. (2013) herangezogen wird.\nDieses Skriptum richtet sich an Studierende der Ingenieurwissenschaften, weshalb mathematische Konzepte nur selten mit strengen Beweisen versehen sind.\nDie Vorlesungsteil der Lehrveranstaltung wird mit einer Klausur bewertet.",
    "crumbs": [
      "Einleitung"
    ]
  },
  {
    "objectID": "intro.html#leistungsüberprüfung",
    "href": "intro.html#leistungsüberprüfung",
    "title": "Einleitung",
    "section": "",
    "text": "Diez, David M., Christopher D. Barr, and Mine Cetinkaya-Rundel. 2019. OpenIntro Statistics. 4th ed. Boston, MA: OpenIntro. \\url{https://www.openintro.org/stat/textbook.php?stat_book=os}.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, et al. 2013. An Introduction to Statistical Learning. Vol. 112. Springer.",
    "crumbs": [
      "Einleitung"
    ]
  },
  {
    "objectID": "dataexploratory/index.html",
    "href": "dataexploratory/index.html",
    "title": "Umgang mit Daten und Explorative Analyse",
    "section": "",
    "text": "Alice: Would you tell me, please, which way I ought to go from here?\nThe Cheshire Cat: That depends a good deal on where you want to get to.\n— Carroll (2015)\n\n\nIn dieser Vorlesung erarbeiten wir gemeinsam die Kompetenzen, um Daten zu analysieren, Modelle zu erstellen und Vorhersagen zu treffen. Hierbei lernen wir alle Grundlagen kennen, die notwendig sind, um sich dabei zurechtzufinden. Zunächst gibt, es aber die Richtung zu klären, in die wir uns bewegen wollen.\n\n\n\n\n\n\nNote\n\n\n\nDiese Notizen setzen voraus, dass Sie über grundlegende Programmierkenntnisse in Python verfügen und wir bauen auf diesem Wissen auf. In diesem Sinne verwenden wir Python als Werkzeug und beschreiben die inneren Abläufe nur, wenn es uns hilft, die behandelten Themen besser zu verstehen.\nFalls dies nicht der Fall ist, schauen Sie sich MCI-MECH-B-3-SWD-SWD-ILV an, einen Kurs über Softwaredesign im selben Bachelor-Programm und von denselben Autoren.\n\n\nZusätzlich können wir die folgenden Bücher über Python empfehlen:\n\nMatthes (2023): Python Crash Course - A hands-on, project-based introduction to programming: Online Material.\nPython Cheat Sheet provided by Matthes (2023).\n\nFür die Statistik und Machine Learning ortientieren wir uns an folgenden Büchern:\n\nDiez, Barr, and Cetinkaya-Rundel (2019): OpenIntro Statistics: Online Material\nJames et al. (2013): Introduction to Statistical Learning: Online Material\n\n\n\n\n\nCarroll, Lewis. 2015. Alice’s Adventures in Wonderland Unfolded.\n\n\nDiez, David M., Christopher D. Barr, and Mine Cetinkaya-Rundel. 2019. OpenIntro Statistics. 4th ed. Boston, MA: OpenIntro. \\url{https://www.openintro.org/stat/textbook.php?stat_book=os}.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, et al. 2013. An Introduction to Statistical Learning. Vol. 112. Springer.\n\n\nMatthes, Eric. 2023. Python Crash Course - a Hands-on, Project-Based Introduction to Programming. 3rd ed. No Starch Press. https://ehmatthes.github.io/pcc_3e/.",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse"
    ]
  },
  {
    "objectID": "dataexploratory/data_science.html",
    "href": "dataexploratory/data_science.html",
    "title": "1  Data Science, Statisik, und Machine Learning",
    "section": "",
    "text": "1.1 Data Sciene: Projektvorgehen\nData Science, Statisik, Machine Learning und Küstliche Intelligenz sind Begriffe, die in den letzten Jahren immer häufiger in den Medien und in der Wissenschaft auftauchen.\nEs gibt unterschiedlichte Definitionen für diese Begriffe, die sich je nach Kontext und Anwendungsbereich unterscheiden. Damit wir uns verstehen, versuchen wir es so:\nUm uns in unseren folgenden Abenteuern zurrechtzufinden, beschäftigen wir uns zunächst hier in Chapter 1 mit dem gundlegenden Herangehen an Data Science-Probleme. In Chapter 2 mit den Daten, die wir analysieren wollen.\nFigure 1.1 zeigt einige der vielen Entscheidungen, die wir treffen müssen, wenn wir uns in unserem eigenen Projekt bewegen.\nSelbst, wenn uns das Ziel klar ist, können wir uns immernoch verlaufen. Um dies zu verhindern, gibt es verschiedene Vorgehensweisen, die uns helfen auf dem Pfad zu bleiben und uns helfen ein tiefgehendes Verständnis für einen Datensatz zu entwickeln. Hierbei unterstützen Standardvorgehensweisen wie CRISP-DM (Cross-Industry Standard Process for Data Mining Figure 1.2) Shearer (2000). Es gibts aber auch neuere und spezifische Prozesse, die in bestimmten Bereichen, wie z.B. im Feld der Zeitreihenprognosen Hyndman (2018), angewendet werden. Für den Anfang begnügen wir uns jedoch mit dem etabliertesten und verbreitesten Prozess.",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Science, Statisik, und Machine Learning</span>"
    ]
  },
  {
    "objectID": "dataexploratory/data_science.html#ssec-dataexploratory-data_science-project",
    "href": "dataexploratory/data_science.html#ssec-dataexploratory-data_science-project",
    "title": "1  Data Science, Statisik, und Machine Learning",
    "section": "",
    "text": "Figure 1.2: CRISP-DM Prozessdiagramm Wikimedia Commons contributors (retrieved 2025)\n\n\n\n\n1.1.1 CRISP-DM: Ein systematischer Ansatz für datenbezogene Projekte\nDer CRISP-DM-Prozess stellt ein generisches Rahmenwerk dar, das es ermöglicht, datengetriebene Projekte von der Problemdefinition bis hin zur operativen Anwendung zu strukturieren. Der Prozess ist in sechs zentrale Phasen unterteilt:\n\nBusiness Understanding\nZiel ist es, die geschäftlichen Anforderungen und Problemstellungen klar zu definieren. Diese Phase umfasst Fragestellungen wie: Was möchten wir mit den Daten lösen? oder Welche Ergebnisse und Metriken sind entscheidend für den Erfolg?\nData Understanding\nHier wird das vorliegende Datenmaterial genauer untersucht, einschließlich seiner Struktur, Störfaktoren und potenzieller Verzerrungen. Eine erste Erkundung der Daten kann entscheidend sein, um Hypothesen zu entwickeln.\nDatenaufbereitung\nIn dieser Phase werden die Rohdaten bereinigt und in ein Format gebracht, das für die Analyse geeignet ist (vgl. Chapter 2). Aktivitäten umfassen das Entfernen fehlender Werte, Transformation von Variablen und die Auswahl relevanter Features.\nModellierung\nAufbau eines Modells zur Beantwortung der Kernfrage. Hierbei kann z. B. ein Regressionsmodell für Prognosen oder ein Klassifikationsmodell bei Entscheidungsproblematiken im Vordergrund stehen.\nEvaluierung\nÜberprüfung, ob das Modell tatsächlich valide und praktisch anwendbar ist. Kernfragen sind: Passt das Modell zu unseren Zielen? und Sind die Ergebnisse sinnvoll und umsetzbar?\nDeployment (Inbetriebnahme)\nDas Modell wird implementiert, um tatsächliche Entscheidungen oder Vorhersagen zu unterstützen. Dies könnte z. B. bedeuten, ein automatisiertes System zu schaffen, das regelmäßig aktualisierte Prognosen liefert.\n\nTipp: Obwohl der Prozess linear erscheint, sind Rücksprünge oft unvermeidlich, z. B. wenn das Modell nicht ausreichend Performanz liefert oder die Anforderungen sich ändern. Auch werden wir mehr über der Daten und die Prozesse (Schritte 2 und 3) lernen, je mehr wir uns mit den Daten beschäftigen.\n\n\n1.1.2 Data Science als People Business?\nEs ist wichtig zu erkennen, dass Daten allein nur einen Ausschnitt der Realität darstellen und ohne Kontext wenig nützen. Ein Kernelement ist daher, sich ausreichendes Domänenwissen anzueignen, um die Semantik der zugrunde liegenden Daten zu interpretieren. Häufig kann es dabei hilfreich sein, Daten aus angrenzenden Kontexten hinzuzuziehen und den Dialog mit Expert:innen oder Personen mit Erfahrungswissen zu suchen.\nAls Standardvorgehen für viele datenbasierte Projekte empfiehlt Hyndman (2018) in seinem einflussreichen Werk zur Zeitreihenanalyse, diese Prinzipien auch auf Prognosen anzuwenden, ähnliches gibt aber für alle Probblem.\n\n\n\n\n\n\nBeispiel:\n\n\n\nWenn wir historische Verkaufsdaten eines Geschäfts analysieren, um zukünftige Trends zu prognostizieren, sollten wir sowohl mit der späteren Nutzer:in des Forecasts (z.B. Produktionsplaner:in für das Business Understanding), als auch mit den Erzeugen der Daten (z.B. Sales-Abteilung für das Data Understanding) sprechen.\n\nDieses bestimmt, wie unsere Modellierung aussehen soll.\n\nWie weit in die Zukunft müssen wir vorhersagen (Prognosehorizont)?\nWelche Auflösung benötigt unsere Prognose (z.B. tagescharf oder wöchtentlich)?\nIn welche Systeme (z.B. Dashboards) muss die Prognose später deployed werden?\nWie soll das Modell evaluiert werden (z.B. ist es wichtiger an keinem Tag große Ausreißer zu haben oder die kumulativen Absatzzahlen über das Jahr hinweg genau zu treffen?)\n\nDurch Gespräche mit weiteren Stakeholdern ergibt sich zudem Business & Data Understanding\n\nGibt es saisonale Effekte (z.B. Insekten-Schutz-Produkte)\nGibt es systematische Fehler in der Datenaufzeichntung (End-of-Year-Effecs)\nGab es Systemumstellungen in der Datenerfassung oder andere Externe Brüche (z.B. Markteintritte)\n\n\nDie Herausforderung ist hierbei jedoch, dass Prognosen fehleranfällig sind. Ein plötzlicher Markteinbruch oder ein externes Ereignis, wie ein sozioökonomischer Schock, könnte die Vorhersagen unbrauchbar machen. Evtl. gehört zum Business Understanding auch die Grenzen einer datanbasierenden Lösung zu verstehen.\n\n\n\n\n1.1.3 Ein Beispiel-Datenset: loan50\nIm Folgenden nutzen wir das loan50-Datenset, das aus dem Lehrbuch von Diez, Barr, and Cetinkaya-Rundel (2019) stammt und zur Erkundung solcher Fragestellungen dient.\nDas loan50-Datenset enthält Informationen zu 50 vergebenen Krediten, die über die Lending Club Plattform vermittelt wurden. Diese Plattform ermöglicht es Einzelpersonen, untereinander Kredite zu vergeben. Wie in vielen Finanzanwendungen sind jedoch nicht alle Kreditnehmer:innen gleich:\n\nKandidat:innen mit hoher Rückzahlungssicherheit werden bevorzugt behandelt und erhalten in der Regel Kredite mit niedrigeren Zinssätzen.\nRisikoreichere Antragsteller:innen könnten hingegen keine Angebote erhalten oder hohe Zinssätze ablehnen.\n\n\n\n\n\n\n\nWarning\n\n\n\nAchtung: Dieses Datenset enthält nur tatsächlich vergebene Kredite und repräsentiert daher nur eine Teilmenge aller möglichen Kreditanfragen. Diese Einschränkung kann dazu führen, dass wir relevante Informationen über nicht vergebene Kreditanträge nicht betrachten. Solche Probeme bezeichnet man gemeinhin als Bias (Verzerrung).\n\n\nEinige der verfügbaren Variablen:\nDie unten aufgelisteten Variablen beschreiben die Eigenschaften des umfassenderen loans_full_schema-Datensatzes, wovon eine Teilmenge im loan50-Datenset enthalten ist.\n\n\n\nVariable\nBeschreibung\n\n\n\n\nemp_title\nBerufsbezeichnung\n\n\nemp_length\nAnzahl der Jahre im Beruf (aufgerundet, Werte über 10 Jahre werden als 10 dargestellt)\n\n\nstate\nUS-Bundesstaat (zweistellige Abkürzung)\n\n\nhome_ownership\nWohnsituation der Bewerber:innen (z. B. Eigentum, gemietet)\n\n\nannual_income\nJährliches Einkommen\n\n\nverified_income\nArt der Verifikation des Einkommens\n\n\ndebt_to_income\nSchulden-Einkommens-Verhältnis\n\n\ngrade\nBewertung des Kredits, wobei A die höchste Stufe ist\n\n\n…\nWeitere Variablen finden Sie in der vollständigen Beschreibung: loan50 - OpenIntro Dataset\n\n\n\n\n\n\n\n\n\nBusiness und Data Understanding\n\n\n\n\nWelche Fragestellungen könnten mit diesem Datensatz beantwortet werden?\n\nBeispielsweise: Gibt es einen Zusammenhang zwischen dem Schulden-Einkommens-Verhältnis und der Kreditbewilligung?\n\nWas müsste noch bekannt sein, um die Daten besser zu verstehen?\n\nWelche spezifischen Regeln wurden aufgestellt, um Kredite zu vergeben oder abzulehnen?\nWo bestehen potenzielle Verzerrungen (z. B. durch die fehlenden Daten zu abgelehnten Anträgen)?\n\n\n\n\n\n\n\n\nDiez, David M., Christopher D. Barr, and Mine Cetinkaya-Rundel. 2019. OpenIntro Statistics. 4th ed. Boston, MA: OpenIntro. \\url{https://www.openintro.org/stat/textbook.php?stat_book=os}.\n\n\nHyndman, RJ. 2018. Forecasting: Principles and Practice. OTexts.\n\n\nKozyrkov, Cassie. 2018. “What on Earth Is Data Science?” medium.com. \\url{https://kozyrkov.medium.com/what-on-earth-is-data-science-eb1237d8cb37}.\n\n\nShearer, Colin. 2000. “The CRISP-DM Model: The New Blueprint for Data Mining.” Journal of Data Warehousing 5 (4): 13–22.\n\n\nWikimedia Commons contributors. retrieved 2025. “CRISP-DM Process Diagram.” https://upload.wikimedia.org/wikipedia/commons/b/b9/CRISP-DM_Process_Diagram.png.",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Science, Statisik, und Machine Learning</span>"
    ]
  },
  {
    "objectID": "dataexploratory/data_sets.html",
    "href": "dataexploratory/data_sets.html",
    "title": "2  Data Sets",
    "section": "",
    "text": "2.1 Tidy-Data\nEin sauber aufbereiteter Datensatz ist eine grundlegende Voraussetzung für jede datenbasierte Analyse und ist im CRISP-DM Teil der Datenaufbereitung. Wir starten zunächst mit dem Konzept von Tidy Data Section 2.1, welches sich mit der sauberen Strukturierung von Daten befasst. Anschließend werden wir uns mit den Typen von Variablen Section 2.2 befassen, die unter anderem den Ausschlag gibt, welche verschiedenen Visualisierungen Section 2.3 sinnvoll sind, um sich ein Data Understanding zu erarbeiten. Zum Schluss werden wir uns mit den verschiedenen Maßen für Variablen Section 2.4 auseinandersetzen, mit welchen man Datensätze beschreiben kann.\nWenn wir mit Computern automatisiert arbeiten möchten, ist neben der Semantik der Daten auch deren Syntax essenziell. Das bedeutet, dass die Daten in einer Struktur vorliegen müssen, die ihre Semantik sinnvoll abbildet.\nEin weitverbreiteter Standard, der in diesem Zusammenhang häufig genutzt wird, sind die von Wickham (2014) beschriebenen Tidy Data Conventions. Dieses Datenformat ist de facto eine Grundlage für viele Softwarepakete wie pandas, statsmodels, sklearn, tensorflow und andere Werkzeuge im Bereich Datenanalyse und des maschinelles Lernen.",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Sets</span>"
    ]
  },
  {
    "objectID": "dataexploratory/data_sets.html#sec-dataexploratory-data_tidy-data",
    "href": "dataexploratory/data_sets.html#sec-dataexploratory-data_tidy-data",
    "title": "2  Data Sets",
    "section": "",
    "text": "Hinweis: Datenbanknormalisierung\n\n\n\n\n\nIm Grunde handelt es sich bei diesem Format um ein Prinzip, das auch in der Datenbanknormalisierung nach Codd verfolgt wird. Ihnen wird dieses Konzept in relationalen Datenbanken (SQL) erneut begegnen.\n\n\n\n\n2.1.1 Was bedeutet “Tidy Data”?\nTidy Data folgt drei Grundprinzipien:\n\n\n\n\n\n\nGrundprinzipien von Tidy Data\n\n\n\n\nJede Zeile repräsentiert eine Beobachtung (bzw. eine Einheit).\nJede Spalte repräsentiert eine Variable (bzw. ein Attribut).\nJede Zelle enthält genau einen präzisen Wert (einen primitiven Datentyp wie int, float, str oder bool – keine Listen, Tupel oder geschachtelten Objekte).\n\n\n\nEin Beispiel für nicht-Tidy-Daten könnte eine Spalte enthalten, in der mehrere Werte in einer Liste zusammengefasst sind. Solche Daten sind schwerer zu verarbeiten und unflexibler beim Einsatz in Analysetools.\nTidy Data hilft uns bei der Datenbereinigung und Datenanalyse. Es erleichtert die Automatisierung und Standardisierung von Prozessen und reduziert die Wahrscheinlichkeit von Fehlern.\n\nKompatibilität: Viele Python-Bibliotheken wie pandas, statsmodels oder seaborn setzen voraus, dass die verwendeten Daten im Tidy-Format vorliegen.\nAutomatisierung: Tidy-Daten erleichtern Standardoperationen wie Filtern, Gruppieren und Pivotieren erheblich.\nFehlerprävention: Unstrukturierte oder verschachtelte Datenstrukturen sind fehleranfällig und schwer zu debuggen.\n\n\n\n\n\n\n\nDaten in das Tidy-Format transformieren\n\n\n\nEs gibt viele hilfreiche Funktionen und Methoden in pandas, um Daten zu “tidy-fizieren”. Ein Beispiel ist die Verwendung der Methoden stack, unstack und melt. Diese helfen dabei, Daten umzustrukturieren und in die gewünschte lange (viele Zeilen) oder weite (viele Spalten) Form zu bringen. Ein hilfreicher Artikel hierzu ist Reshape with Pandas.\n💡 Tipp: Wenn Sie unsicher sind, wie Sie Ihre Daten umorganisieren sollten, können Sie ein Beispiel (z.B. head() eines DataFrames) und die gewünschte Struktur (also Spaltennamen) in ein Large Language Model eingeben. Oft erhalten Sie klare Vorschläge zur Umstrukturierung!\n\n\n\n\n2.1.2 Positive Beispiele für Tidy Data\nDer folgende Beispielcode zeigt, wie Sie ein CSV-Datei laden und sich mit den ersten Zeilen vertraut machen können. Glücklicher Weise ist dieser Datensatz bereits im Tidy-Format. Jede Zeile repräsentiert eine Beobachtung (Kreditnehmer) und jede Spalte eine Variable (Attribut).\n\nimport pandas as pd\n\n# Lesen der CSV-Datei in einen DataFrame\ndf = pd.read_csv(r\"../_assets/dataexploratory/loan50.csv\")\n# Ausgabe der ersten Zeilen des Datensatzes\nprint(df.head())\n\n  state  emp_length  term homeownership  annual_income verified_income  \\\n0    NJ         3.0    60          rent          59000    Not Verified   \n1    CA        10.0    36          rent          60000    Not Verified   \n2    SC         NaN    36      mortgage          75000        Verified   \n3    CA         0.0    36          rent          75000    Not Verified   \n4    OH         4.0    60      mortgage         254000    Not Verified   \n\n   debt_to_income  total_credit_limit  total_credit_utilized  \\\n0        0.557525               95131                  32894   \n1        1.305683               51929                  78341   \n2        1.056280              301373                  79221   \n3        0.574347               59890                  43076   \n4        0.238150              422619                  60490   \n\n   num_cc_carrying_balance        loan_purpose  loan_amount grade  \\\n0                        8  debt_consolidation        22000     B   \n1                        2         credit_card         6000     B   \n2                       14  debt_consolidation        25000     E   \n3                       10         credit_card         6000     B   \n4                        2    home_improvement        25000     B   \n\n   interest_rate  public_record_bankrupt loan_status  has_second_income  \\\n0          10.90                       0     Current              False   \n1           9.92                       1     Current              False   \n2          26.30                       0     Current              False   \n3           9.92                       0     Current              False   \n4           9.43                       0     Current              False   \n\n   total_income  \n0         59000  \n1         60000  \n2         75000  \n3         75000  \n4        254000  \n\n\n\n\n2.1.3 Negative Beispiele für Tidy Data\nFolgendes Datenbeispiel zeigt, wie ein Datensatz nicht im Tidy-Format aussieht. Wir sehen die Strombedarfe von verschiedenen Netzgebieten zone_id zu verschiedenen Zeitpunkten. Allerdings ist es ungünstig, dass nicht jede Kombination aus Zone und Zeitpunkt eine eigene Zeile hat. Stattdessen sind die Werte für alle 24 Stunden in einer eigenen Spalte.\n\nimport pandas as pd\n\ndf = pd.read_csv(r\"../_assets/dataexploratory/GEFCom2012/GEFCOM2012_Data/Load/Load_history.csv\")\nprint(df.head())\n\n   zone_id  year  month  day      h1      h2      h3      h4      h5      h6  \\\n0        1  2004      1    1  16,853  16,450  16,517  16,873  17,064  17,727   \n1        1  2004      1    2  14,155  14,038  14,019  14,489  14,920  16,072   \n2        1  2004      1    3  14,439  14,272  14,109  14,081  14,775  15,491   \n3        1  2004      1    4  11,273  10,415   9,943   9,859   9,881  10,248   \n4        1  2004      1    5  10,750  10,321  10,107  10,065  10,419  12,101   \n\n   ...     h15     h16     h17     h18     h19     h20     h21     h22  \\\n0  ...  13,518  13,138  14,130  16,809  18,150  18,235  17,925  16,904   \n1  ...  16,127  15,448  15,839  17,727  18,895  18,650  18,443  17,580   \n2  ...  13,507  13,414  13,826  15,825  16,996  16,394  15,406  14,278   \n3  ...  14,207  13,614  14,162  16,237  17,430  17,218  16,633  15,238   \n4  ...  13,845  14,350  15,501  17,307  18,786  19,089  19,192  18,416   \n\n      h23     h24  \n0  16,162  14,750  \n1  16,467  15,258  \n2  13,315  12,424  \n3  13,580  11,727  \n4  17,006  16,018  \n\n[5 rows x 28 columns]\n\n\nEine Umwandlung in das Tidy-Format würde, wie folgt aussehen, wobei wird darauf achten sollten, dass der timestamp als datetime-Objekt und die load als int gespeichert wird:\n\n\n\nzone_id\ntimestamp\nload (kW)\n\n\n\n\n1\n2012-01-01 00:00:00\n1000\n\n\n1\n2012-01-01 01:00:00\n1100",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Sets</span>"
    ]
  },
  {
    "objectID": "dataexploratory/data_sets.html#sec-dataexploratory-data_variable-types",
    "href": "dataexploratory/data_sets.html#sec-dataexploratory-data_variable-types",
    "title": "2  Data Sets",
    "section": "2.2 Messniveaus von Variablen",
    "text": "2.2 Messniveaus von Variablen\nVariablen sind die Bausteine von Daten und repräsentieren die Merkmale, die wir messen oder beobachten. Im Kapitel Section 2.2 werden wir uns noch tiefer mit Variablen auseinandersetzen. In der Datenanalyse ist es wichtig, die Art der Variablen zu kennen, da dies beeinflusst, welche Methoden und Visualisierungen für die Daten geeignet sind. Variablen lassen sich nach ihrem Messniveau klassifizieren, was wiederum die Art der Informationen beschreibt, die sie enthalten. Die bekannteste Klassifikation von Variablen basiert auf den vier Messniveaus von Stanley Smith Stevens: Nominal, Ordinal, Intervall und Ratio (Verhältnis).\n\nNutzen Sie den oben gezeigten Datensatz loan50, um die folgenden Aufgaben für unterschiedliche Variablenarten zu lösen:\n\n\nSortieren: Wie könnte man die Werte der Variablen\n\nstate,\ngrade,\nein Beispiel für ein Intervallniveau (aber kein Ratio),\nannual_income\nsinnvoll in aufsteigender Reihenfolge anordnen?\n\nZentrale Werte bestimmen: Wie lässt sich ein zentraler Wert bestimmen, sei es durch den Modus, die Median oder den Mittelwert?\nBeziehungen beschreiben: Welche Aussage könnte man über die Beziehung zwischen zwei Werten einer Variablen machen?\n\n\n2.2.1 Nominale Variablen\nDefinition: Nominale Variablen kategorisieren Daten ohne eine festgelegte Reihenfolge.\n\ndf = pd.read_csv(r\"../_assets/dataexploratory/loan50.csv\")\nprint(df[\"state\"].head())\n\n0    NJ\n1    CA\n2    SC\n3    CA\n4    OH\nName: state, dtype: object\n\n\n\nprint(df[\"state\"].value_counts().head())\n\nstate\nCA    9\nTX    5\nIL    4\nNJ    3\nMD    3\nName: count, dtype: int64\n\n\n\nprint(df[\"state\"].mode().head())\n\n0    CA\nName: state, dtype: object\n\n\n\nSortieren: Es gibt keine inhärente Methode, diese Werte zu sortieren. Dies liegt daran, dass nominale Daten keine Reihenfolge implizieren.\nZentraler Wert: Modus, da dieser Wert am häufigsten vorkommt.\nBeziehungen: Die Beziehung zwischen zwei Werten kann nur beschreiben, ob sie in derselben Kategorie sind oder nicht.\n\n\n\n2.2.2 Ordinale Variablen\nOrdinale Variablen haben eine natürliche Ordnung, aber der Abstand zwischen den Werten ist nicht zwingend gleichmäßig.\n\nprint(df[\"grade\"].head())\n\n0    B\n1    B\n2    E\n3    B\n4    B\nName: grade, dtype: object\n\n\n\nprint(df[\"grade\"].sort_values().head())\n\n49    A\n18    A\n33    A\n36    A\n14    A\nName: grade, dtype: object\n\n\n\nprint(df[\"grade\"].value_counts().head())\n\ngrade\nB    19\nA    15\nD     8\nC     6\nE     2\nName: count, dtype: int64\n\n\n\nprint(df[\"grade\"].mode())\n\n0    B\nName: grade, dtype: object\n\n\n\n# Define the order for the categorical values\ngrade_order = sorted(df[\"grade\"].unique())\n\n# Convert the 'grade' column to a categorical type with the specified order\ndf['grade'] = pd.Categorical(df['grade'], categories=grade_order, ordered=True)\n\n# Convert categorical data to numerical codes\ngrade_codes = df['grade'].cat.codes\n\nprint(grade_codes.median())\n\n1.0\n\n\n\nSortieren: Mit geeigneten Regeln ist es möglich, diese Werte in aufsteigender Reihenfolge zu ordnen: [\"C\", \"B\", \"A\"].\nZentraler Wert:: Der Modus ist geeignet, und der Median zeigt auf, dass 50 % der Werte gleich oder niedriger als \"B\" sind.\nBeziehungen: Zwei Werte lassen sich nach ihrer Position der Reihenfolge vergleichen: höher oder niedriger.\n\n\n\n2.2.3 Intervallskalierte Variablen\nIntervallskalierte Variablen haben geordnete Werte mit gleichmäßigen Abständen zwischen ihnen, aber sie besitzen keinen absoluten Nullpunkt. Ein Beispiel ist das jährliche Einkommen.\n\nprint(df[\"annual_income\"].head())\n\nprint(df[\"annual_income\"].mean())\n\n0     59000\n1     60000\n2     75000\n3     75000\n4    254000\nName: annual_income, dtype: int64\n86170.0\n\n\n\nSortieren:: Daten können numerisch in aufsteigender Reihenfolge sortiert werden.\nZentraler Wert:: Der Modus und Median sind geeignete Maße. Der arithmetische Mittelwert berechnet sich als: \\(\\mu = \\frac{1}{n} \\sum x_i\\)\nBeziehungen: Der Abstand (Intervall) zwischen zwei Werten kann quantifiziert werden.\n\n\n\n\n\n\n\nUnterschied zwischen Intervall- und Ratiodaten\n\n\n\nIm Gegensatz zum Ratio-Messniveau besitzen Intervall-Daten keinen absoluten Nullpunkt. Aussagen wie “das Doppelte” sind daher nicht sinnvoll.\n\n\n\n\n\n\n\n\nTip\n\n\n\nEs ist oft hilfreich, sich das Messniveau einer Variablen vor Beginn der Analyse klar zu machen. Das Messniveau entscheidet auch, welche Visualisierung sinnvoll sind. Mögliche Fehleinschätzungen können zu falschen oder unzulässigen Berechnungen führen, z. B. Mittelwerte bei Nominaldaten. Daten sollten entsprechend ihrem Typ gereinigt und transformiert werden.",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Sets</span>"
    ]
  },
  {
    "objectID": "dataexploratory/data_sets.html#sec-dataexploratory-data_visualization",
    "href": "dataexploratory/data_sets.html#sec-dataexploratory-data_visualization",
    "title": "2  Data Sets",
    "section": "2.3 Visualisierungen",
    "text": "2.3 Visualisierungen\n\n\n\n\n\n\nTip\n\n\n\nEs gibt viele Möglichkeiten, Daten zu visualisieren, um Muster und Trends zu erkennen. Zwei weit verbreitete Pakete sind matplotlib und plotly. Im folgenden benutzen wir vorallem seaborn, welches eine Erweiterung von matplotlib ist und speziell für statistische Visualisierungen entwickelt wurde.\n\n\n\n2.3.1 Histogramme\nEin Histogramm ist eine angenäherte Darstellung der Verteilung einer intervallskalierten Variable. Es liefert wertvolle Informationen über:\n\nZentralwert: Wo liegen die Daten?\nVarianz: Wie stark streuen die Daten?\nVerteilung: Wie häufig treten bestimmte Werte auf?\n\n(fig:sec-dataexploratory-sets-histogram?) zeigt ein Histogramm des jährlichen Einkommens aus dem Datensatz loan50.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create a histogram of the annual income\nsns.histplot(df[\"annual_income\"], bins=10, stat = 'count')\n\n\n\n\nHistogramm des jährlichen Einkommens\n\n\n\n\n\n2.3.1.1 Konstruktion eines Histogramms\nEin Histogramm wird in wenigen Schritten erstellt. Meinst wird dies bereits für uns wie in seaborn erledigt, es ist jedoch hilfreich, die Schritte zu kennen, um die Visualisierung besser zu verstehen, da sie manchmal abgewandelt wird.\n\nBinning: Teilen Sie die Werte der beobachteten Variablen \\(x_i\\) in eine Reihe von Intervallen (Bins oder Buckets) auf.\nZählen: Erfassen Sie, wie viele Werte in jedes Intervall fallen (z. B. 5% der Werte).\nIntervall-Eigenschaften: Die Intervalle der Bins sollten aufeinander folgen, sich nicht überlappen und idealerweise die gleiche Breite haben.\nDarstellung: Die Anzahl der Werte in jedem Intervall wird entlang der y-Achse aufgetragen. Für relative Häufigkeiten wird durch die Stichprobengröße geteilt.\nWenn die Intervalle gleich breit sind, wird die y-Achse als Häufigkeit interpretiert. Wenn die Intervalle unterschiedlich breit sind, wird die y-Achse als Dichte interpretiert. Dazu wird die Höhe der Balken so skaliert, dass die Fläche unter dem Histogramm \\(1\\) ergibt.",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Sets</span>"
    ]
  },
  {
    "objectID": "dataexploratory/data_sets.html#sec-dataexploratory-data_variable-measures",
    "href": "dataexploratory/data_sets.html#sec-dataexploratory-data_variable-measures",
    "title": "2  Data Sets",
    "section": "2.4 Maße für Variablen",
    "text": "2.4 Maße für Variablen\nVariablen lassen sich auf verschiedene Weisen beschreiben. Lagemaße bzw. die zentrale Tendenz gibt an, wo die Daten liegen, während die Streuung angibt, wie weit die Daten von diesem Wert entfernt sind. Die Zusammenhänge zwischen Variablen können durch Korrelationen und Kovarianzen beschrieben werden.\n\n2.4.1 Lagemaße\nVariablen können auf verschiedene Weisen beschrieben werden. Beispielweise können Lagemaße wie der Arithmetischer Mittelwert (Mean), Median oder Modus genutzt werden, um die zentrale Tendenz der Daten zu beschreiben. Welche wir einsetzen, hängt vom Messniveau der Variablen ab.\nBetrachten wir eine mindestens intervall-skalierte Variable \\(x \\in \\mathbb{R}^n\\) aus den Datensatz, so können wir die folgenden Lagemaße berechnen:\n\ndas maximale Element bzw. der Höchstwert: \\[\nx^{max} = \\max_i x_i,\n\\]\n\n\nincome_max = df[\"annual_income\"].max()\nprint(f\"{income_max=}\")\n\nincome_max=np.int64(325000)\n\n\n\nder minimale Wert bzw. das Minimum:\n\n\nincome_min = df[\"annual_income\"].min()\nprint(f\"{income_min=}\")\n\nincome_min=np.int64(28800)\n\n\n\\[\nx^{min} = \\min_i x_i,\n\\] - der arithmetische Mittelwert: \\[\n\\overline{x} = \\frac1n \\sum_{i=1}^n x_i = \\frac{x_1 + x_2 + \\cdots + x_n}{n},\n\\]\n\nincome_mean = df[\"annual_income\"].mean()\nprint(f\"{income_mean=}\")\n\nincome_mean=np.float64(86170.0)\n\n\n\nder Median ist der Wert, der die Daten in zwei gleich große Teile teilt:\n\n\\[\n\\widetilde{x} = \\begin{cases}\n                x_{(n+1)/2}& n\\quad \\text{odd}\\\\\n                \\frac{x_{n/2} + x_{n/2+1}}{2}& n\\quad \\text{even}\n                \\end{cases},\n\\]\n\nincome_median = df[\"annual_income\"].median()\nprint(f\"{income_median=}\")\n\nincome_median=np.float64(74000.0)\n\n\n\nVerallgemeinert für \\(p\\in(0,1)\\) ist das p-Quantil \\(\\overline{x}_p\\) der Wert, der die Daten in zwei Teile teilt, wobei \\(p\\) der Anteil der Daten ist, die kleiner oder gleich \\(\\overline{x}_p\\) sind.\n\n\nincome_quartiles = df[\"annual_income\"].quantile([0.25, 0.5, 0.75])\nprint(f\"{income_quartiles=}\")\n\nincome_quartiles=0.25    55750.0\n0.50    74000.0\n0.75    99500.0\nName: annual_income, dtype: float64\n\n\n\\[\n\\overline{x}_p = \\begin{cases}\n                 \\frac12\\left(x_{np} + x_{np+1}\\right) & pn \\in \\mathbb{N}\\\\\n                x_{\\lfloor np+1\\rfloor} & pn \\not\\in \\mathbb{N}\n                \\end{cases}.\n\\]\nEinige Quantile haben spezielle Namen, wie der Median für \\(p=0.5\\), das untere und obere Quartil für \\(p=0.25\\) und \\(p=0.75\\) (oder erstes, zweites (Median) und drittes Quartil), respektive.\n\n\n\n\n\n\nCaution\n\n\n\nWie gut lassen sich Arithmetischer Mittelwert, Median und Mode aus dem Histogramm ablesen?\n\n\n\n\n2.4.2 Kumulative Histogramme und Empirische Verteilungsfunktionen\nAls Alternative haben sich kumulative Histogramme, wie in ?fig-sec-dataexploratory-sets-kum-histogram, etabliert, die die kumulative Verteilungsfunktion (Cumulative Density Function / CDF) visualisieren. Diese Funktion gibt an, wie viele Werte kleiner oder gleich einem bestimmten Wert sind. Zur Konstruktion der CDF werden die Daten in aufsteigender Reihenfolge sortiert und die relative Häufigkeit der Werte berechnet.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create a histogram of the annual income\nsns.histplot(df[\"annual_income\"], bins=10, stat = 'density', cumulative=True)\n\n\n\n\n\n\n\n\n\n\n2.4.3 Steuungsmaße\nSteuungsmaße beschreiben die Streuung der Daten um den zentralen Wert. Beispiele sind die Spannweite, Varianz und die Standardabweichung.\nDie Spannweite ist die Differenz zwischen dem größten und kleinsten Wert: \\[\n\\text{Spannweite} = x^{max} - x^{min}.\n\\] Die Varianz ist ein Maß für die mittlere quadratische Abweichung der Daten vom Mittelwert. Die Einheit der Varianz ist das Quadrat der Einheit der Daten: \\[\n\\sigma = \\sqrt{\\operatorname{Var}(x)}.\n\\] \\[\n\\operatorname{Var}(x) = \\frac1n \\sum_{i=1}^n (x_i - \\mu)^2.\n\\]\nDie Standardabweichung ist die Quadratwurzel der Varianz. Damit hat sie die gleiche Einheit wie die Daten:\n\\[\n\\sigma = \\sqrt{\\operatorname{Var}(x)}.\n\\]\nIn Python können wir die Varianz und Standardabweichung mit pandas oder numpy berechnen:\n\nprint(f\"Varianz: {df['annual_income'].var()}\")\nprint(f\"Standardabweichung: {df['annual_income'].std()}\")\n\nVarianz: 3313901734.6938777\nStandardabweichung: 57566.49837096119\n\n\n\n\n\n\n\n\nKorrigerte Stichproben-Varianz\n\n\n\nDie Varianz einer Stichprobe ist kein erwartungstreuer Schätzer für die Varianz der Grundgesamtheit. Die Begriffe werden wir in Section 5.1 noch genauer betrachten. Einfach gesagt, die Varianz einer Stichprobe ist tendenziell kleiner als die Varianz der Grundgesamtheit, da wird beim zufälligen Ziehen wahrscheinlich eher aus der Mitte als von den Extremen ziehen. Die korrigierte Stichproben-Varianz wird durch \\(n-1\\) statt \\(n\\) im Nenner definiert. In pandas wird die korrigierte Stichproben-Varianz als Standard verwendet, die unkorrigierte Varianz kann mit dem Parameter ddof=0 berechnet werden.\n\n\n\n\n2.4.4 Zusammenhangsmaße\nIn der Statistik beschreiben Zusammenhangsmaße die Beziehung zwischen zwei Variablen. Beispiele sind die Kovarianz und der Korrelationskoeffizient. Diese geben einen Hinweis darauf, ob und wie stark zwei Variablen zusammenhängen.\n\n2.4.4.1 Korrrelation\nIn der Statistik beschreibt der Begriff Korrelation oder Abhängigkeit jede statistische Beziehung zwischen bivariaten Daten (gepaarte Daten) oder Zufallsvariablen.\nIn unserem Datensatz können wir beispielsweise untersuchen: - emp_length: Anzahl der Jahre im Beruf - annual_income: Jährliches Einkommen - debt_to_income: Schulden-Einkommens-Verhältnis\nUm die Daten besser zu verstehen können wir zunächst einen Scatterplot ?fig-sec-dataexploratory-sets-scatterplot erstellen:\n\nimport seaborn as sns\n\ndf_reduced = df[[\"emp_length\", \"annual_income\", \"debt_to_income\"]]\n\nsns.pairplot(df_reduced)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiskussion\n\n\n\nWie interpretieren Sie den Zusammenhang zwischen den Variablen emp_length, annual_income und debt_to_income? Was würde entsprechend Ihres Domänenwissens Sinn ergeben?\n\n\n\n\n2.4.4.2 Kovarianz\nDie Kovarianz ist ein Maß für die gemeinsame Variabilität zweier Variablen. Sie ist definiert als der Erwartungswert des Produkts der Abweichungen der Zufallsvariablen von ihren Erwartungswerten:\n\\[\n\\operatorname{cov}(x, y) = \\frac1n \\sum_{i=1}^n (x_i - \\overline{x})(y_i - \\overline{y}).\n\\]\nIn Python können wir die Kovarianz-Matrix mit pandas direkt berechnen:\n\ndf_reduced.cov()\n\n\n\n\n\n\n\n\nemp_length\nannual_income\ndebt_to_income\n\n\n\n\nemp_length\n12.393174\n1.924082e+04\n-0.051111\n\n\nannual_income\n19240.824468\n3.313902e+09\n-8584.039227\n\n\ndebt_to_income\n-0.051111\n-8.584039e+03\n0.918269\n\n\n\n\n\n\n\nDie Kovarianz kann Wertebereiche von \\(-\\infty\\) bis \\(+\\infty\\) annehmen und ist nicht normiert. Um die Stärke der Beziehung zu quantifizieren, verwenden wir den Korrelationskoeffizienten, der leichter zu interpretieren ist.\n\n\n2.4.4.3 Korrelationskoeffizient\nDer Korrelationskoeffizient nach Pearson ist ein Maß für den linearen Zusammenhang zwischen zwei Variablen. Er ist definiert als das Verhältnis der Kovarianz der beiden Variablen zur Multiplikation ihrer Standardabweichungen:\n\\[\n\\rho_{x,y} = \\operatorname{corr}(x, y) = \\frac{\\operatorname{cov}(x, y)}{\\sigma_x \\sigma_y},\n\\]\nwobei \\(\\sigma_x\\) und \\(\\sigma_y\\) die Standardabweichungen der Variablen sind.\nIn Python können wir den Korrelationskoeffizienten mit numpy berechnen:\n\ndf_reduced.corr()\n\n\n\n\n\n\n\n\nemp_length\nannual_income\ndebt_to_income\n\n\n\n\nemp_length\n1.000000\n0.093156\n-0.014857\n\n\nannual_income\n0.093156\n1.000000\n-0.155610\n\n\ndebt_to_income\n-0.014857\n-0.155610\n1.000000\n\n\n\n\n\n\n\nEin Korrelationskoeffizient von \\(1\\) bedeutet eine perfekte positive Korrelation, \\(-1\\) eine perfekte negative Korrelation und \\(0\\) keine Korrelation. In diesem fall beobachten wir eine leichte negative Korrelation zwischen emp_length und debt_to_income und eine leichte positive Korrelation zwischen emp_length und annual_income. Eine Variable kann auch mit sich selbst perfekt korreliert sein, was zu einem Korrelationskoeffizienten von \\(1\\) führt.\n\n\n\n\n\n\nVorsicht\n\n\n\nDer Korrelationskoeffizient misst nur lineare Zusammenhänge. Nicht-lineare Zusammenhänge werden nicht erfasst. Es kann auch Zusammenhänge geben, die nicht durch den Korrelationskoeffizienten erfasst werden, z.B. wenn die Daten nicht normalverteilt sind. In Figure 2.1 sehen wir einige Beispiele in denen definitiv Korrelationen bestehen, die aber nicht durch den Korrelationskoeffizienten erfasst werden.\n\n\n\n\n\n\nFigure 2.1: Beispiele Korrelationskoeffizient DATAtab (retrieved 2025)\n\n\n\n\n\n\n\n\n\n\n\nKorrelation vs. Kausalität\n\n\n\nEine hohe Korrelation bedeutet nicht notwendigerweise Kausalität. Es ist wichtig, die Daten und den Kontext zu verstehen, um sinnvolle Schlussfolgerungen zu ziehen. Ansonsten besteht die Gefahr, dass Zusammenhänge fehlinterpretiert werden. Ein bekanntes Beispiel ist die Korrelation zwischen der Anzahl der Piraten und der globalen Temperatur, die in Figure 2.2 dargestellt ist. In der Wissenschaft begegnet man diesem Problem mit kontrollierten Experimenten.\n\n\n\n\n\n\nFigure 2.2: Korrelation Piraten Klima RedAndr and Mikhail Ryazanov (2011)\n\n\n\n\n\n\n\n\n\nDATAtab. retrieved 2025. “Korrelationskoeffizient Tutorial Image.” https://datatab.de/assets/tutorial/Korrelationskoeffizient.png.\n\n\nRedAndr and Mikhail Ryazanov. 2011. “Satirical diagram illustrating the influence of pirates decreasing on global warming as per Pastafarian beliefs.” https://commons.wikimedia.org/wiki/File:PiratesVsTemp(en).svg.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59: 1–23.",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Sets</span>"
    ]
  },
  {
    "objectID": "dataexploratory/tutorial.html",
    "href": "dataexploratory/tutorial.html",
    "title": "Tutorial: Getting Data Tidy and finding Correlation",
    "section": "",
    "text": "Objective\nThe Global Energy Forecasting Competition Hong, Pinson, and Fan (2014) is a data science competition that aims to advance the field of energy forecasting. The competition is held every two years and the data is made available to the public for research purposes. Different teams from around the world participate in the competition and the best models are selected based on their performance.\nIn 2012 one of the goals was to forecast the load of a power system. The data consists of hourly load data for a period of 5 years. The data was not provided in a tidy format and we need to clean it up before we can start working with it. The main objective of the load forecasting competition was to predict an accurate system load for each hour in each of the zones.",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse",
      "Tutorial: Getting Data Tidy and finding Correlation"
    ]
  },
  {
    "objectID": "dataexploratory/tutorial.html#data",
    "href": "dataexploratory/tutorial.html#data",
    "title": "Tutorial: Getting Data Tidy and finding Correlation",
    "section": "Data",
    "text": "Data\nThe data is provided in a zip file that contains different files:\n\nHoliday_List.csv\nLoad_history.csv\ntemperature_history.csv\n\nYour task is to make sense from the data an bring it into a tidy format. Store the data in a pandas DataFrame and save it as a csv file. Also reload it, to make shure it works.",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse",
      "Tutorial: Getting Data Tidy and finding Correlation"
    ]
  },
  {
    "objectID": "dataexploratory/tutorial.html#crisp-dm",
    "href": "dataexploratory/tutorial.html#crisp-dm",
    "title": "Tutorial: Getting Data Tidy and finding Correlation",
    "section": "Crisp-DM",
    "text": "Crisp-DM\nWe start by using the first four steps of the CRISP-DM process to make sense of the data using exploratory data analysis.\n\nBusiness Understanding\n\nWhat is the system load of a power system and why is it important to forecast it?\nWhat are the benefits of accurate load forecasting?\nWhat factors should influence the load of a power system?\n\nData Understanding\nHow many systems are there in the data? What are the features of the data? What is the time period of the data?\nDatenaufbereitung\nWhat is a meaningful structure for the data? What should be colums and what should be rows? How can we bring the data into a tidy format?\nModellierung\nIs there a seasionality in the data? Plot the average load for each hour of the day, day of the week and month of the year in a Boxplot. How do the distributions between the different systems compare? Are there any outliers? Are there any correlations between the load and the temperature? Plot the load against the temperature and compute the correlation coefficient.\n\n\n\n\n\n\n\nTip\n\n\n\nMake sure to store not only the processed, but also the processed data in a csv file. This way you can always go back to the original data and start over if you make a mistake. Also store the preprocessing steps in a separate script, so you can reproduce the results later. It is not uncommon, to recieve new data that needs to be processed in the same way. This is also the time to think of a proper folder structure for your project. Do not forget all the things you learned in the software design courses. You can also use modules like cookiecutter to create a project structure like this:\n├── LICENSE            &lt;- Open-source license if one is chosen\n├── Makefile           &lt;- Makefile with convenience commands like `make data` or `make train`\n├── README.md          &lt;- The top-level README for developers using this project.\n├── data\n│   ├── external       &lt;- Data from third party sources.\n│   ├── interim        &lt;- Intermediate data that has been transformed.\n│   ├── processed      &lt;- The final, canonical data sets for modeling.\n│   └── raw            &lt;- The original, immutable data dump.\n│\n├── docs               &lt;- A default mkdocs project; see www.mkdocs.org for details\n│\n├── models             &lt;- Trained and serialized models, model predictions, or model summaries\n│\n├── notebooks          &lt;- Jupyter notebooks. Naming convention is a number (for ordering),\n│                         the creator's initials, and a short `-` delimited description, e.g.\n│                         `1.0-jqp-initial-data-exploration`.\n...\n\n\n\n\n\n\nHong, Tao, Pierre Pinson, and Shu Fan. 2014. “Global Energy Forecasting Competition 2012.” International Journal of Forecasting 30 (2): 357–63. https://doi.org/https://doi.org/10.1016/j.ijforecast.2013.07.001.",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse",
      "Tutorial: Getting Data Tidy and finding Correlation"
    ]
  },
  {
    "objectID": "statistics/index.html",
    "href": "statistics/index.html",
    "title": "Statistik",
    "section": "",
    "text": "“Ich bin Ingenieur und gewohnt, in Wahrscheinlichkeiten zu denken, nach der Mathematik der Vernunft.” - Frisch (1957)\n\nDie klassische Statistik ist ein Teilbereich der Mathematik, der sich mit der Sammlung, Analyse, Interpretation, Präsentation und Modellierung von Daten beschäftigt. Ein großen Teil der Statistik ist die Wahrscheinlichkeitstheorie, die sich mit dem Verständnis von Zufallsereignissen befasst.\nIn Kapitel 3  Stichproben und Zufallsvariablen beschäftigen wir uns zunöchst damit, wie Daten erhoben werden und wie man die Wahrscheinlichkeiten von Ereignissen berechnet. In zweiten Kapitel ?sec-statistics-distributions betrachten wir typische Verteilungen, die geeigenet sind zufällige Ereignisse zu beschreiben.\n?sec-statistics-estimates\n?sec-statistics-hypothesis\n\n\n\n\nFrisch, Max. 1957. Homo Faber. Ein Bericht. Frankfurt am Main: Suhrkamp.",
    "crumbs": [
      "Statistik"
    ]
  },
  {
    "objectID": "statistics/sampling.html",
    "href": "statistics/sampling.html",
    "title": "3  Stichproben und Zufallsvariablen",
    "section": "",
    "text": "3.1 Stichprobenziehung aus einer Grundgesamtheit\nIn diesem Abschnitt {#sec-statistics-sampling} behandeln wir Stichproben und Zufallsvariablen.\nEine Stichprobe umfasst \\(n\\) Beobachtungen aus einer Grundgesamtheit, der Menge \\(N\\) aller möglichen Beobachtungen. Sie ist eine Teilmenge der Grundgesamtheit und sollte idealerweise Rückschlüsse auf diese ermöglichen.\nDie Grundgesamtheit (population) ist die Gesamtheit aller untersuchbaren Beobachtungen, die Stichprobe (sample) eine Teilmenge davon. Eine repräsentative Stichprobe erlaubt Verallgemeinerungen. Da die vollständige Datenerhebung der Grundgesamtheit oft zu aufwendig und kostspielig ist, ziehen wir Rückschlüsse aus Stichproben (siehe Figure Figure 3.1). Dies gelingt am besten mit einer großen, zufällig ausgewählten Stichprobe.\nFigure 3.1: Visualisierung der Stichprobenziehung aus einer Grundgesamtheit.\nAllerdings kommt es hier zu einen Unterschied zwischen der Sichtweise der klassischen Statistik und dem Ansatz den den viele Data Scientists verfolgen. In der klassischen Statistik wird die Stichprobe so gewählt, dass sie repräsentativ für die Grundgesamtheit ist.\nAls Data Scientist hingegen, sind wir oft an den Daten interessiert, die uns zur Verfügung stehen. Wir haben keine Möglichkeit, die Grundgesamtheit zu beeinflussen. Wir müssen also mit den Daten arbeiten, die wir haben und uns dabei bewusst sein, dass wir einem Sampling-Bias unterliegen.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Stichproben und Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "statistics/sampling.html#sec-sampling-population",
    "href": "statistics/sampling.html#sec-sampling-population",
    "title": "3  Stichproben und Zufallsvariablen",
    "section": "",
    "text": "Note\n\n\n\nWenn wir die Leistungsfähigkeit in Mathematik unter Studierenden auswerten wollen, dann sollten wir unsere Stichprobe nicht nur im Studiengang Mechatronik nachfragen.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWenn wir die die Lebensdauer eines Werkzeugs auf einer 5-Achs-Fräsmaschinene prognostizieren wollen, können wir die Modelle nicht zwischen Betrieben vergleichen, die die Maschine regelmäßig warten und solchen, die das nicht tun.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Stichproben und Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "statistics/sampling.html#sec-classical-vs-datascience",
    "href": "statistics/sampling.html#sec-classical-vs-datascience",
    "title": "3  Stichproben und Zufallsvariablen",
    "section": "3.2 Unterschied zwischen klassischer Statistik und Data Science",
    "text": "3.2 Unterschied zwischen klassischer Statistik und Data Science\nDie klassische Statistik wählt Stichproben so, dass sie die Grundgesamtheit repräsentieren, während Data Scientists oft mit verfügbaren Daten arbeiten und keinen Einfluss auf die Grundgesamtheit haben. Dies führt zu einem möglichen Sampling-Bias.\n\n\n\n\n\n\nNote\n\n\n\nZur Bewertung der Mathematikleistung von Studierenden wäre eine Stichprobe nur aus Mechatronik nicht repräsentativ.\n\n\n\n\n\n\n\n\nNote\n\n\n\nBei der Prognose der Werkzeuglebensdauer einer 5-Achs-Fräsmaschine sind Vergleiche zwischen gewarteten und ungewarteten Maschinen verzerrt.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Stichproben und Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "statistics/sampling.html#sec-grouping-data",
    "href": "statistics/sampling.html#sec-grouping-data",
    "title": "3  Stichproben und Zufallsvariablen",
    "section": "3.3 Gruppieren von Daten",
    "text": "3.3 Gruppieren von Daten\nEin bewusster Bias kann bei der Datenauswahl entstehen. Fragen wir z. B. nur Personen mit Hypothek (mortgage) nach ihrem Einkommen und nicht Mieter (rent), ist die Stichprobe nicht repräsentativ (siehe Figure Figure 3.2).\n\nimport pandas as pd\nimport seaborn as sns\n\ndf = pd.read_csv(\"../_assets/dataexploratory/loan50.csv\")\nsns.histplot(data=df, x=\"annual_income\", bins=30, hue=\"homeownership\")\n\n\n\n\n\n\n\nFigure 3.2: Verteilung des jährlichen Einkommens nach Wohneigentum.\n\n\n\n\n\n\ndf.groupby(\"homeownership\")[\"annual_income\"].mean()\n\nhomeownership\nmortgage    99807.692308\nown         67666.666667\nrent        71928.571429\nName: annual_income, dtype: float64",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Stichproben und Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "statistics/sampling.html#sec-data-analysis",
    "href": "statistics/sampling.html#sec-data-analysis",
    "title": "3  Stichproben und Zufallsvariablen",
    "section": "3.4 Analyse der Daten",
    "text": "3.4 Analyse der Daten\n\n\n\n\n\n\nNote\n\n\n\nInwiefern entsprechen die Daten den Erwartungen?\n\n\nIm Beispiel haben wir eine ordinal skalierte Variable (homeownership) und eine metrisch skalierte Variable (annual_income). Wir untersuchen ihren Zusammenhang. Korrelation und Kausalität, wie in der letzten Einheit besprochen, sind hier nicht anwendbar, da homeownership ordinal ist.\n\n3.4.1 Boxplot\nEin Boxplot eignet sich zum Vergleich ordinaler und metrischer Variablen. Er zeigt die Verteilung der metrischen Variable (annual_income) für die Ausprägungen der ordinalen Variable (homeownership). Die Box umfasst Median sowie erstes und drittes Quartil, die Whisker die Datenreichweite (1,5-fache Interquartilsdistanz ab den Quartilen), und Punkte markieren Ausreißer (siehe Figure Figure 3.3).\n\nimport pandas as pd\nimport seaborn as sns\n\ndf = pd.read_csv(\"../_assets/dataexploratory/loan50.csv\")\nsns.boxplot(data=df, x=\"homeownership\", y=\"annual_income\")\nsns.stripplot(data=df, x=\"homeownership\", y=\"annual_income\", color=\"black\", size=3, alpha=0.5)\n\n\n\n\n\n\n\nFigure 3.3: Boxplot des jährlichen Einkommens nach Wohneigentum.\n\n\n\n\n\nDer Boxplot ist einfach zu erstellen und zu interpretieren, jedoch bei stark unterschiedlichen Verteilungen wenig aussagekräftig. Hier können Daten transformationen oder alternative Visualisierungen helfen.\n\n\n\n\n\n\nTip\n\n\n\nEine moderne Alternative zum Boxplot ist der Violinplot. Er zeigt die Verteilung als geschätzte Wahrscheinlichkeitsdichte (bisher für uns ein geglättetes Histogramm) und ist informativer, da er die Datenverteilung detaillierter darstellt (siehe Figure ?fig-violin-plot). \n\n\n\n\n3.4.2 Experimente\nIn der Statistik unterscheiden wir Beobachtungsstudien, bei denen Daten ohne Eingriff beobachtet werden, von Experimenten, bei denen Daten manipuliert werden, um Effekte zu prüfen. Experimente sind aufwendiger und teurer, ermöglichen aber die Untersuchung von Kausalzusammenhängen.\n\n\n\n\n\n\nNote\n\n\n\nBeobachtungsstudien können longitudinal sein, z. B. die Mathematikleistung von Studierenden über die Zeit, oder Querschnittsstudien, z. B. die Leistung nach Studiengängen zu einem Zeitpunkt.\n\n\n\n\n\n\n\n\nNote\n\n\n\nUm die Wirkung von Studiengängen auf die Mathematikleistung zu prüfen, könnten wir ein Experiment durchführen: Studierende zufällig Studiengängen zuweisen und ihre Leistung messen. Dies erfordert Zufallsauswahl und -zuweisung (ethische Bedenken beachten). Eine Kontrollgruppe ohne Studiengang schließt externe Einflüsse (z. B. Alter) aus. Eine Messung vor dem Studium ist bei zufälliger Zuweisung entbehrlich.\n\n\nExperimente sind der Goldstandard für Kausalität. Eine unabhängige Variable (z. B. Studiengang) wird manipuliert, andere Variablen konstant gehalten oder durch große Stichproben ausgeglichen. Findet sich eine Korrelation zur abhängigen Variable (z. B. Mathematikleistung), ist Kausalität plausibel. Im Data Science wird oft pragmatisch mit vorhandenen Daten gearbeitet – schneller, aber weniger zuverlässig.\n\n3.4.2.1 Beispiel: Ist ein Würfel gezinkt?\nStellen wir uns vor, eine Kollegin besteht auf ihrem eigenen Würfel. Ist er gezinkt? Eine Beobachtungsstudie könnte die Augenzahlen mit einem fairen Würfel (Kontrollgruppe) vergleichen. Wir werfen beide Würfel 1000-mal und prüfen die Verteilung (siehe Figure Figure 3.4). Auffällige Abweichungen machen misstrauisch.\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\nfair_dice_rolls = np.random.randint(1, 7, 1000)\nmanipulated_dice_rolls = np.random.choice([1, 2, 3, 4, 5, 6], 1000, p=[1.9/12, 1.9/12, 1.9/12, 1.9/12, 1.9/12, 2.5/12])\n\nsns.histplot(fair_dice_rolls, bins=6, discrete=True, color=\"lightblue\", alpha=0.2, label=\"Fairer Würfel\")\nsns.histplot(manipulated_dice_rolls, bins=6, discrete=True, color=\"red\", alpha=0.2, label=\"Manipulierter Würfel\")\nplt.legend()\n\n\n\n\n\n\n\nFigure 3.4: Verteilung der Würfelergebnisse: fairer vs. manipulierter Würfel.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Stichproben und Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "statistics/sampling.html#sec-variable-perspectives",
    "href": "statistics/sampling.html#sec-variable-perspectives",
    "title": "3  Stichproben und Zufallsvariablen",
    "section": "3.5 Blickpunkte auf Variablen",
    "text": "3.5 Blickpunkte auf Variablen\nWir haben Variablen (Spalten in tidy data) untersucht und betrachten sie aus verschiedenen Perspektiven.\n\n3.5.1 Skalenniveaus\nSkalenniveaus klassifizieren Variablen in nominal, ordinal, metrisch und verhältnisskaliert. Sie bestimmen, welche statistischen Methoden zur Analyse geeignet sind.\n\n\n3.5.2 Im Kontext von Experimenten\nIn Experimenten und Beobachtungsstudien unterscheiden wir unabhängige (Einflussgröße) und abhängige (gemessene Effekte) Variablen. Bezeichnungen variieren je nach Fachgebiet (siehe Table (tab-variable-terms?)). Später erkennen wir, dass mehrere unabhängige und abhängige Variablen möglich sind, doch zunächst bleiben wir bei Singular.\n\n\n\nAnwendungsfeld\nUnabhängige Variable\nAbhängige Variable\n\n\n\n\nStatistik\nExplanatory Variable\nResponse Variable\n\n\nMachine Learning\nFeatures\nTarget\n\n\nExperimente\nTreatment\nOutcome\n\n\nPsychologie\nIndependent Variable\nDependent Variable\n\n\nForecasts\nPredictor\nPredicted Variable\n\n\nÖkonometrie\nExplanatory Variable\nDependent Variable\n\n\nInformatik\nInput\nOutput\n\n\nProgramming\nArgument\nReturn Value\n\n\nProgramming\nX\ny",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Stichproben und Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "statistics/sampling.html#sec-probability",
    "href": "statistics/sampling.html#sec-probability",
    "title": "3  Stichproben und Zufallsvariablen",
    "section": "3.6 Wahrscheinlichkeitsrechnung",
    "text": "3.6 Wahrscheinlichkeitsrechnung\nEine weitere Perspektive sind Prozesse hinter Beobachtungen: deterministisch (gleiches Ergebnis bei gleichen Bedingungen) oder zufällig (unterschiedliche Ergebnisse trotz gleicher Bedingungen). Zufällige Prozesse werden durch Wahrscheinlichkeiten beschrieben.\n\n\n\n\n\n\nImportant\n\n\n\nOb das Universum deterministisch oder zufällig ist, spielt keine Rolle. Zufälligkeit bedeutet hier, dass Ergebnisse nicht a priori vorhersagbar sind – sei es durch echte Zufallsprozesse (z. B. Würfeln) oder unvollständige Modellierung (z. B. fehlende Variablen).\n\n\n\n3.6.1 Zufallsvariablen\nEine Zufallsvariable nimmt zufällig Werte an – diskret (bestimmte Werte, z. B. Würfelaugenzahl: 1–6) oder kontinuierlich (Werte in einem Intervall, z. B. Temperatur). Die Augenzahl eines Würfels ist eine diskrete Zufallsvariable; jeder Wurf ist eine Realisierung (siehe Figure Figure 3.5).\n\nimport numpy as np\nimport seaborn as sns\n\nnp.random.seed(42)\ndice_rolls = np.random.randint(1, 7, 1000)\nsns.histplot(dice_rolls, bins=6, discrete=True)\n\n\n\n\n\n\n\nFigure 3.5: Verteilung der Würfelergebnisse bei 1000 Würfen.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Stichproben und Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "statistics/sampling.html#sec-random-variables-coin",
    "href": "statistics/sampling.html#sec-random-variables-coin",
    "title": "3  Stichproben und Zufallsvariablen",
    "section": "3.7 Zufallsvariablen und Münzwurf",
    "text": "3.7 Zufallsvariablen und Münzwurf\nÄhnlich wie beim Würfel ist beim Münzwurf die Zufallsvariable die Seite, die oben liegt (Kopf oder Zahl). Für numerische Analysen wandeln wir diese kategorialen Werte in 0 (Zahl) und 1 (Kopf) um, wodurch der Ereignisraum {Kopf, Zahl} zu {0, 1} wird.\n\n3.7.1 Begriffe der Wahrscheinlichkeit\nWir definieren: - Zufallsexperiment/-prozess: Ein Prozess mit unvorhersagbarem Ergebnis, z. B. Münzwurf, Würfeln oder Kartenziehen. - Ereignisraum (sample space): Alle möglichen Ergebnisse eines Zufallsexperiments, z. B. {Kopf, Zahl} beim Münzwurf. - Zufallsvariable: Eine Funktion, die jedem Ergebnis eine Zahl zuordnet, z. B. 1 für Kopf und 0 für Zahl.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Stichproben und Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "statistics/sampling.html#sec-frequentist-probability",
    "href": "statistics/sampling.html#sec-frequentist-probability",
    "title": "3  Stichproben und Zufallsvariablen",
    "section": "3.8 Frequentistische Wahrscheinlichkeit",
    "text": "3.8 Frequentistische Wahrscheinlichkeit\n\n\n\n\n\n\nImportant\n\n\n\nDie Wahrscheinlichkeit eines Ergebnisses ist der Anteil, wie oft es bei unendlich vielen Wiederholungen eines Zufallsprozesses eintritt. Sie liegt zwischen 0 und 1 und kann als Prozentsatz (0–100 %) angegeben werden.\n\n\nIn Figure Figure 3.5 trat die Augenzahl 2 etwa 167-mal in 1000 Würfen auf, was einer Wahrscheinlichkeit von ca. 1/6 (0,167) entspricht – ebenso für die anderen Augenzahlen. Bei endlichen Beobachtungen ist dies eine Schätzung; die exakte Wahrscheinlichkeit gilt nur für unendlich viele Versuche (siehe Figure Figure 3.6).\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nnumber_of_rolls = 1000\nnp.random.seed(10)\ndice_rolls = np.random.randint(1, 7, number_of_rolls)\n\nproportion_ones = np.cumsum(dice_rolls == 1) / np.arange(1, number_of_rolls + 1)\n\nsns.lineplot(x=np.arange(1, number_of_rolls + 1), y=proportion_ones)\nsns.lineplot(x=[1, number_of_rolls + 1], y=[1/6, 1/6], color=\"red\", linestyle=\"--\")\nplt.xlabel(\"$n$ - Anzahl der Würfe\")\nplt.ylabel(\"$\\hat{p}_n$ - Anteil der 1en\")\n\n&lt;&gt;:14: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n&lt;&gt;:14: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n/tmp/ipykernel_3099/965827469.py:14: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n\n\n\n\n\n\n\nText(0, 0.5, '$\\\\hat{p}_n$ - Anteil der 1en')\n\n\n(a) Anteil der Würfelergebnisse ‘1’ über die Anzahl der Würfe.\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 3.6\n\n\n\n\n\n3.8.1 Konvergenz und Schätzungen\nFigure Figure 3.6 zeigt den Anteil \\(\\hat{p}_n\\) der Augenzahl 1 bei jedem Schritt \\(n\\) einer Simulation. Er konvergiert gegen die Wahrscheinlichkeit 1/6 (ca. 0,167). Der beobachtete Anteil \\(\\hat{p}_n\\) schätzt die Wahrscheinlichkeit \\(p\\) und wird mit mehr Beobachtungen genauer; \\(p\\) ist der Grenzwert.\n\n\n\n\n\n\nImportant\n\n\n\nIn der Statistik arbeiten wir oft mit Schätzungen, da unendlich viele Beobachtungen fehlen. Die Unsicherheit der Schätzungen muss berücksichtigt werden. Schätzungen kennzeichnen wir mit \\(\\hat{p}\\).\n\n\n\n\n\n\n\n\nImportant\n\n\n\nGesetz der großen Zahlen: Mit steigender Beobachtungszahl konvergiert der Anteil eines Ergebnisses gegen dessen Wahrscheinlichkeit.\n\n\n\n\n3.8.2 Wahrscheinlichkeitsnotation\nFür verschiedene Ergebnisse schreiben wir \\(P(X = x)\\) als Wahrscheinlichkeit, dass die Zufallsvariable \\(X\\) (z. B. Münzwurf) den Wert \\(x\\) annimmt.\n\nFür einen fairen Münzwurf:\n\\(P(X = \\text{Kopf}) = 0.5\\), \\(P(X = \\text{Zahl}) = 0.5\\)\noder: \\(P(X = 1) = 0.5\\), \\(P(X = 0) = 0.5\\).\n\nFür einen fairen Würfelwurf:\n\\(P(X = 1) = P(X = 2) = P(X = 3) = P(X = 4) = P(X = 5) = P(X = 6) = 1/6\\).\n\nDie Summe der Wahrscheinlichkeiten aller Ergebnisse eines Zufallsexperiments ist stets 1, da mindestens ein Ergebnis eintritt. Dies entspricht der Fläche unter einem normalisierten Histogramm oder einer Dichtefunktion.\n\n\n3.8.3 Disjunkte Ereignisse\nZwei Ereignisse \\(A\\) und \\(B\\) sind disjunkt (sich ausschließend), wenn sie nicht gleichzeitig eintreten können. Beispiel: Bei einem Würfelwurf sind „Augenzahl 1“ und „Augenzahl 2“ disjunkt. Die Wahrscheinlichkeit, dass eines von beiden eintritt, ist \\(P(A \\cup B)\\) (logisches „oder“, „A vereinigt B“):\n\\[P(X=1 \\cup X=2) = P(X=1) + P(X=2) = \\frac{1}{6} + \\frac{1}{6} = \\frac{1}{3}.\\]\n\n\n\n\n\n\nImportant\n\n\n\nAdditionsregel: Für sich ausschließende Ereignisse \\(A\\) und \\(B\\) gilt:\n\\[P(A \\cup B) = P(A) + P(B).\\]\nFür mehrere disjunkte Ereignisse \\(A_1, \\ldots, A_n\\):\n\\[P(A_1 \\cup A_2 \\cup \\ldots \\cup A_n) = P(A_1) + P(A_2) + \\ldots + P(A_n).\\]\n\n\n\n3.8.3.1 Beispiel: Kreditnehmer-Datensatz\nIm Datensatz aus Kapitel 2 beschreibt homeownership, ob ein Kreditnehmer mietet, eine Hypothek hat oder Eigentümer ist. Von 50 Kreditnehmern (siehe Code-Ausgabe) sind die Verteilungen: Miete (21), Hypothek (26), Eigentum (3).\n\nimport pandas as pd\n\ndf = pd.read_csv(\"../_assets/dataexploratory/loan50.csv\")\nprint(f\"Anzahl Beobachtungen: {df['homeownership'].shape[0]}\")\nprint(df[\"homeownership\"].value_counts())\n\nAnzahl Beobachtungen: 50\nhomeownership\nmortgage    26\nrent        21\nown          3\nName: count, dtype: int64\n\n\n\nSind Miete, Hypothek und Eigentum disjunkt?\nBestimmen Sie den Anteil der Kredite mit Hypothek und Eigentum separat.\nNutzen Sie die Additionsregel für disjunkte Ereignisse, um die Wahrscheinlichkeit zu berechnen, dass ein zufällig ausgewählter Kreditnehmer eine Hypothek hat oder Eigentümer ist.\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nJa, die Kategorien sind disjunkt, da ein Kreditnehmer nur eine davon haben kann.\nAnteil Hypothek: \\(\\frac{26}{50}\\), Anteil Eigentum: \\(\\frac{3}{50}\\).\nWahrscheinlichkeit (Hypothek oder Eigentum): \\(\\frac{26}{50} + \\frac{3}{50} = \\frac{29}{50}\\). Dies entspricht der Wahrscheinlichkeit, nicht zu mieten.\n\n\n\n\n\n\n\n3.8.4 Das Komplement eines Ereignisses\nDas Komplement eines Ereignisses \\(A\\), bezeichnet als \\(A^c\\) oder \\(\\bar{A}\\), ist das Nicht-Eintreten von \\(A\\). Es umfasst alle Ergebnisse außer \\(A\\) und ist zu \\(A\\) disjunkt. Die Wahrscheinlichkeit des Komplements ist:\n\\[\nP(A^c) = 1 - P(A)\n\\]\nBeispiel: Die Wahrscheinlichkeit, dass ein Würfel nicht 1 zeigt:\n\\[\n1 - P(X \\neq 1) =1-P(X=1) = 1 - \\frac{1}{6} = \\frac{5}{6}.\n\\]\nDie Summe \\(P(A) + P(A^c) = 1\\) gilt stets, da entweder \\(A\\) oder \\(A^c\\) eintritt.\n\n\n3.8.5 Nicht-disjunkte Ereignisse\nNicht-disjunkte Ereignisse können überlappen. Beispiel: Bei einem Kartenspiel (siehe Figure Figure 3.7) interessiert die Wahrscheinlichkeit, eine Bildkarte (Bube, Dame, König) oder eine Karo-Karte zu ziehen. Bild und Karo sind nicht disjunkt, da Bildkarten in Karo beide Eigenschaften haben. Einfaches Addieren überschätzt die Wahrscheinlichkeit durch doppelte Zählung.\n\n\n\n\n\n\nFigure 3.7: Deck of 52 Cards\n\n\n\nEin Venn-Diagramm (siehe Figure Figure 3.8) zeigt: Die Schnittmenge (\\(A \\cap B\\), logisches „und“) sind Karten, die beide Eigenschaften haben; die Vereinigung (\\(A \\cup B\\), logisches „oder“) umfasst alle Karten mit mindestens einer Eigenschaft.\n\n\n\n\n\n\nFigure 3.8: Venn Diagramm of Cards\n\n\n\nVon 52 Karten sind 12 Bildkarten, 13 Karo-Karten und 3 sowohl Bild- als auch Karo-Karten. Die Wahrscheinlichkeit für „Bild oder Karo“ ist:\n\\[\nP(\\text{Bild} \\cup \\text{Karo}) = P(\\text{Bild}) + P(\\text{Karo}) - P(\\text{Bild} \\cap \\text{Karo}).\n= \\frac{12}{52} + \\frac{13}{52} - \\frac{3}{52} = \\frac{22}{52}.\n\\]\nHierraus können wir die Additionsregel für nicht-disjunkte Ereignisse formulieren:\n\n\n\n\n\n\nImportant\n\n\n\nGenerelle Additionsregel: Für nicht-disjunkte Ereignisse \\(A\\) und \\(B\\) gilt:\n\\[\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B).\n\\]\n\n\n\n3.8.5.1 Beispiel: Summe zweier Würfel\nSei \\(A\\) das Ereignis, dass die Summe der Augenzahlen zweier fairer Würfel kleiner als 12 ist.\n\nWas ist das Komplement von \\(A\\)?\nWie groß ist \\(P(A)\\)?\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nDas Komplement \\(A^c\\) ist die Summe \\(\\geq 12\\), also genau 12 (da die maximale Summe 12 beträgt).\n\\(P(A)\\) ist die Summe der Wahrscheinlichkeiten für Summen 2 bis 11.\nMögliche Summen und Kombinationen (siehe Table (tab-dice-sums?)):\n\n\n\n\n\n\n\n\n\\(A = W_1 + W_2\\)\nMögliche Kombinationen\n\n\n\n\n2\n\\((W_1=1 \\cap W_2=1)\\)\n\n\n3\n\\((W_1=1 \\cap W_2=2) \\cup (W_1=2 \\cap W_2=1)\\)\n\n\n4\n\\((W_1=1 \\cap W_2=3) \\cup (W_1=2 \\cap W_2=2) \\cup (W_1=3 \\cap W_2=1)\\)\n\n\n5\n\\((W_1=1 \\cap W_2=4) \\cup (W_1=2 \\cap W_2=3) \\cup (W_1=3 \\cap W_2=2) \\cup (W_1=4 \\cap W_2=1)\\)\n\n\n6\n\\((W_1=1 \\cap W_2=5) \\cup (W_1=2 \\cap W_2=4) \\cup (W_1=3 \\cap W_2=3) \\cup (W_1=4 \\cap W_2=2) \\cup (W_1=5 \\cap W_2=1)\\)\n\n\n7\n\\((W_1=1 \\cap W_2=6) \\cup (W_1=2 \\cap W_2=5) \\cup (W_1=3 \\cap W_2=4) \\cup (W_1=4 \\cap W_2=3) \\cup (W_1=5 \\cap W_2=2) \\cup (W_1=6 \\cap W_2=1)\\)\n\n\n8\n\\((W_1=2 \\cap W_2=6) \\cup (W_1=3 \\cap W_2=5) \\cup (W_1=4 \\cap W_2=4) \\cup (W_1=5 \\cap W_2=3) \\cup (W_1=6 \\cap W_2=2)\\)\n\n\n9\n\\((W_1=3 \\cap W_2=6) \\cup (W_1=4 \\cap W_2=5) \\cup (W_1=5 \\cap W_2=4) \\cup (W_1=6 \\cap W_2=3)\\)\n\n\n10\n\\((W_1=4 \\cap W_2=6) \\cup (W_1=5 \\cap W_2=5) \\cup (W_1=6 \\cap W_2=4)\\)\n\n\n11\n\\((W_1=5 \\cap W_2=6) \\cup (W_1=6 \\cap W_2=5)\\)\n\n\n12\n\\((W_1=6 \\cap W_2=6)\\)\n\n\n\nDie Gesamtzahl der Kombinationen beträgt \\(6 \\times 6 = 36\\). Für \\(A^c\\) (Summe = 12) gibt es 1 Fall, also \\(P(A^c) = \\frac{1}{36}\\) und \\(P(A) = 1 - P(A^c) = \\frac{35}{36}\\).\n\n\n\nBei komplexeren Berechnungen hilft eine Monte-Carlo-Simulation: Das Experiment wird mehrfach simuliert, und die Wahrscheinlichkeit ergibt sich aus dem Anteil der Treffer (siehe Code-Ausgabe).\n\nimport numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(np.random.randint(1, 7, (10000, 2)), columns=[\"W1\", \"W2\"])\ndf[\"Sum\"] = df[\"W1\"] + df[\"W2\"]\np = (df[\"Sum\"] &lt; 12).mean()\n\nprint(f\"Die Wahrscheinlichkeit, dass die Summe &lt; 12 ist, beträgt {p:.3f}\")\n\nDie Wahrscheinlichkeit, dass die Summe &lt; 12 ist, beträgt 0.973\n\n\n\n\n3.8.5.2 Simulation der Würfelsumme (Fortsetzung)\nDie Verteilung der Summen zweier Würfel kann auch grafisch dargestellt werden (siehe Figure Figure 3.9). Die Simulation bestätigt, dass die Wahrscheinlichkeit für eine Summe &lt; 12 hoch ist.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(np.random.randint(1, 7, (10000, 2)), columns=[\"W1\", \"W2\"])\ndf[\"Sum\"] = df[\"W1\"] + df[\"W2\"]\n\nsns.histplot(df[\"Sum\"], bins=11, discrete=True, stat='density')\nplt.axvline(11, color=\"red\", linestyle=\"--\")\nplt.xlabel(\"Summe der Augenzahlen\")\nplt.ylabel(\"Dichte\")\nplt.show()\n\n\n\n\n\n\n\nFigure 3.9: Verteilung der Summen zweier Würfel bei 10.000 Würfen; rote Linie markiert Summe = 11.\n\n\n\n\n\n\n\n\n3.8.6 Wahrscheinlichkeit der Würfelsumme (Fortsetzung)\nFigure Figure 3.9 zeigt die Verteilung der Summen zweier Würfel. Die Wahrscheinlichkeit, dass die Summe &lt; 12 ist, beträgt ca. 0,97 (Monte-Carlo-Schätzung).\n- 3. Die Wahrscheinlichkeit für eine Summe \\(\\geq 12\\) ist:\n\\[P(A^c) = 1 - P(A) = 1 - \\frac{35}{36} = \\frac{1}{36} \\approx 0.03.\\]",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Stichproben und Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "statistics/sampling.html#sec-independent-events",
    "href": "statistics/sampling.html#sec-independent-events",
    "title": "3  Stichproben und Zufallsvariablen",
    "section": "3.9 Unabhängige Ereignisse",
    "text": "3.9 Unabhängige Ereignisse\nZwei Ereignisse \\(A\\) und \\(B\\) sind unabhängig, wenn das Eintreten des einen das andere nicht beeinflusst (vgl. Korrelation und Kausalität in Chapter 2). Die Wahrscheinlichkeit eines Ereignisses hängt nicht vom anderen ab. Beispiele: Münzwurf und Würfelwurf oder die Ergebnisse zweier Würfel – das Ergebnis des ersten Würfels beeinflusst den zweiten nicht.\nDie Wahrscheinlichkeit, dass zwei unabhängige Ereignisse gleichzeitig eintreten, ist:\n\\[P(A \\cap B) = P(A) \\cdot P(B).\\]\n\n\n\n\n\n\nImportant\n\n\n\nMultiplikationsregel für unabhängige Ereignisse: Für unabhängige Ereignisse \\(A\\) und \\(B\\) gilt:\n\\[P(A \\cap B) = P(A) \\cdot P(B).\\]\nFür mehrere unabhängige Ereignisse \\(A_1, \\ldots, A_n\\):\n\\[P(A_1 \\cap A_2 \\cap \\ldots \\cap A_n) = P(A_1) \\cdot P(A_2) \\cdot \\ldots \\cdot P(A_n).\\]\n\n\n\n3.9.1 Beispiel 1: Würfelsummen\nDa die Ergebnisse zweier Würfel unabhängig sind, können wir die Wahrscheinlichkeiten multiplizieren (siehe Table (tab-dice-sum-probabilities?)).\n\n\n\n\n\n\n\n\n\\(A = W_1 + W_2\\)\nMögliche Kombinationen\n\\(P(A)\\)\n\n\n\n\n2\n\\(W_1=1, W_2=1\\)\n\\(P(A=2) = P(W_1=1) \\cdot P(W_2=1) = \\frac{1}{6} \\cdot \\frac{1}{6} = \\frac{1}{36}\\)\n\n\n3\n\\(W_1=1, W_2=2\\), \\(W_1=2, W_2=1\\)\n\\(P(A=3) = P(W_1=1) \\cdot P(W_2=2) + P(W_1=2) \\cdot P(W_2=1) = \\frac{1}{6} \\cdot \\frac{1}{6} + \\frac{1}{6} \\cdot \\frac{1}{6} = \\frac{2}{36}\\)\n\n\n4\n\\(W_1=1, W_2=3\\), \\(W_1=2, W_2=2\\), \\(W_1=3, W_2=1\\)\n\\(P(A=4) = P(W_1=1) \\cdot P(W_2=3) + P(W_1=2) \\cdot P(W_2=2) + P(W_1=3) \\cdot P(W_2=1) = \\frac{1}{6} \\cdot \\frac{1}{6} + \\frac{1}{6} \\cdot \\frac{1}{6} + \\frac{1}{6} \\cdot \\frac{1}{6} = \\frac{3}{36}\\)\n\n\n5\n\\(W_1=1, W_2=4\\), \\(W_1=2, W_2=3\\), \\(W_1=3, W_2=2\\), \\(W_1=4, W_2=1\\)\n\\(P(A=5) = P(W_1=1) \\cdot P(W_2=4) + P(W_1=2) \\cdot P(W_2=3) + P(W_1=3) \\cdot P(W_2=2) + P(W_1=4) \\cdot P(W_2=1) =\n\\frac{4}{36}\\)\n\n\n6\n\\(W_1=1, W_2=5\\), \\(W_1=2, W_2=4\\), \\(W_1=3, W_2=3\\), \\(W_1=4, W_2=2\\), \\(W_1=5, W_2=1\\)\n\\(P(A=6) = P(W_1=1) \\cdot P(W_2=5) + P(W_1=2) \\cdot P(W_2=4) + P(W_1=3) \\cdot P(W_2=3) + P(W_1=4) \\cdot P(W_2=2) + P(W_1=5) \\cdot P(W_2=1) =\n\\frac{5}{36}\\)\n\n\n7\n\\(W_1=1, W_2=6\\), \\(W_1=2, W_2=5\\), \\(W_1=3, W_2=4\\), \\(W_1=4, W_2=3\\), \\(W_1=5, W_2=2\\), \\(W_1=6, W_2=1\\)\n\\(P(A=7) = P(W_1=1) \\cdot P(W_2=6) + P(W_1=2) \\cdot P(W_2=5) + P(W_1=3) \\cdot P(W_2=4) + P(W_1=4) \\cdot P(W_2=3) + P(W_1=5) \\cdot P(W_2=2) + P(W_1=6) \\cdot P(W_2=1) = \\frac{6}{36}\\)\n\n\n8\n\\(W_1=2, W_2=6\\), \\(W_1=3, W_2=5\\), \\(W_1=4, W_2=4\\), \\(W_1=5, W_2=3\\), \\(W_1=6, W_2=2\\)\n\\(P(A=8) = P(W_1=2) \\cdot P(W_2=6) + P(W_1=3) \\cdot P(W_2=5) + P(W_1=4) \\cdot P(W_2=4) + P(W_1=5) \\cdot P(W_2=3) + P(W_1=6) \\cdot P(W_2=2) = \\frac{5}{36}\\)\n\n\n9\n\\(W_1=3, W_2=6\\), \\(W_1=4, W_2=5\\), \\(W_1=5, W_2=4\\), \\(W_1=6, W_2=3\\)\n\\(P(A=9) = P(W_1=3) \\cdot P(W_2=6) + P(W_1=4) \\cdot P(W_2=5) + P(W_1=5) \\cdot P(W_2=4) + P(W_1=6) \\cdot P(W_2=3) = \\frac{4}{36}\\)\n\n\n10\n\\(W_1=4, W_2=6\\), \\(W_1=5, W_2=5\\), \\(W_1=6, W_2=4\\)\n\\(P(A=10) = P(W_1=4) \\cdot P(W_2=6) + P(W_1=5) \\cdot P(W_2=5) + P(W_1=6) \\cdot P(W_2=4) = \\frac{3}{36}\\)\n\n\n11\n\\(W_1=5, W_2=6\\), \\(W_1=6, W_2=5\\)\n\\(P(A=11) = P(W_1=5) \\cdot P(W_2=6) + P(W_1=6) \\cdot P(W_2=5) = \\frac{2}{36}\\)\n\n\n12\n\\(W_1=6, W_2=6\\)\n\\(P(A=12) = P(W_1=6) \\cdot P(W_2=6) = \\frac{1}{36}\\)\n\n\n\n\n\n3.9.2 Beispiel 2: Wahr oder Falsch\nBestimmen Sie, ob die folgenden Aussagen wahr oder falsch sind, und begründen Sie Ihre Antwort.\n\nWenn eine faire Münze oft geworfen wird und die letzten acht Würfe Kopf waren, ist die Wahrscheinlichkeit, dass der nächste Wurf Kopf ist, etwas weniger als 50 %.\nDas Ziehen einer Bildkarte (Bube, Dame, König) und das Ziehen einer roten Karte aus einem vollständigen Kartenspiel sind sich gegenseitig ausschließende Ereignisse.\nDas Ziehen einer Bildkarte und das Ziehen eines Asses aus einem vollständigen Kartenspiel sind sich gegenseitig ausschließende Ereignisse.\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nFalsch. Bei einer fairen Münze ist \\(P(\\text{Kopf}) = 0.5\\). Vorherige Würfe beeinflussen den nächsten nicht, da sie unabhängig sind.\nFalsch. Bildkarten können rot sein (z. B. Karo-Dame). „Bildkarte“ und „rote Karte“ sind nicht disjunkt.\nWahr. Eine Bildkarte (Bube, Dame, König) kann kein Ass sein; die Ereignisse sind disjunkt.\n\n\n\n\n\n\n3.9.3 Bedingte Wahrscheinlichkeit\nBedingte Wahrscheinlichkeit beschreibt die Wahrscheinlichkeit eines Ereignisses \\(A\\), gegeben dass ein anderes Ereignis \\(B\\) eingetreten ist, notiert als \\(P(A | B)\\) („\\(A\\) gegeben \\(B\\)“). Beispiel: Wie wahrscheinlich ist eine Bildkarte, wenn die Karte eine Karo-Karte ist? Aus Figure Figure 3.8:\n\\[P(\\text{Bild} | \\text{Karo}) = \\frac{3}{13},\\]\nda von 13 Karo-Karten 3 Bildkarten sind.\nEine Kreuztabelle (contingency table) zeigt die Häufigkeiten von Ereigniskombinationen, z. B. im loan50-Datensatz (siehe Table (tab-contingency-loan?)).\n\nimport pandas as pd\n\ndf = pd.read_csv(\"../_assets/dataexploratory/loan50.csv\")\ncontingency_table = pd.crosstab(df['homeownership'], df['has_second_income'])\nprint(contingency_table)\n\nhas_second_income  False  True \nhomeownership                  \nmortgage              20      6\nown                    3      0\nrent                  19      2\n\n\n\n\n3.9.4 Bedingte Wahrscheinlichkeit (Fortsetzung)\nAus der Kreuztabelle (Table (tab-contingency-loan?)) ergibt sich: Von 26 Kreditnehmern mit Hypothek haben 6 ein zweites Einkommen. Die bedingte Wahrscheinlichkeit ist:\n\\[P(\\text{Zweiteinkommen} | \\text{Hypothek}) = \\frac{6}{26}.\\]\n\n\n\n\n\n\nImportant\n\n\n\nDie bedingte Wahrscheinlichkeit \\(P(A|B)\\) ist die Wahrscheinlichkeit von \\(A\\), gegeben dass \\(B\\) eingetreten ist. Sie wird berechnet als:\n\\[P(A|B) = \\frac{P(A \\cap B)}{P(B)}.\\]\nIm Beispiel:\n\\[P(\\text{Zweiteinkommen} | \\text{Hypothek}) = \\frac{P(\\text{Zweiteinkommen} \\cap \\text{Hypothek})}{P(\\text{Hypothek})} = \\frac{\\frac{6}{50}}{\\frac{26}{50}} = \\frac{6}{26}.\\]\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\\(P(A \\cap B)\\) kann hier nicht via Multiplikationsregel berechnet werden, da die Ereignisse nicht unabhängig sind. Stattdessen werden beobachtete Häufigkeiten verwendet.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nSumme bedingter Wahrscheinlichkeiten: Sind \\(A_1, \\ldots, A_k\\) alle disjunkten Ergebnisse einer Variable, gilt für ein Ereignis \\(B\\):\n\\[P(A_1|B) + \\cdots + P(A_k|B) = 1.\\]\nFür ein Ereignis und sein Komplement:\n\\[P(A|B) = 1 - P(A^c|B).\\]\n\n\n\n\n3.9.5 Beispiel: AIDS-Test\n\n\n\n\n\n\nFigure 3.10: Meme AIDS Test\n\n\n\nWie hoch ist die Wahrscheinlichkeit, dass eine Person AIDS hat, wenn folgendes bekannt ist:\n\nDie Wahrscheinlichkeit, dass eine Person AIDS hat, beträgt 0,1 % (\\(P(\\text{AIDS}) = 0.001\\)).\nBei AIDS beträgt die Wahrscheinlichkeit eines positiven Tests 99 % (\\(P(\\text{Positiv} | \\text{AIDS}) = 0.99\\), Sensitivität).\nOhne AIDS beträgt die Wahrscheinlichkeit eines positiven Tests 5 % (\\(P(\\text{Positiv} | \\text{Kein AIDS}) = 0.05\\), falsch-positiv, 1 - Spezifität).\n\nEin Baumdiagramm hilft: \\(P(\\text{AIDS}) = 0.001\\), \\(P(\\text{Kein AIDS}) = 0.999\\), \\(P(\\text{Positiv} | \\text{AIDS}) = 0.99\\), \\(P(\\text{Positiv} | \\text{Kein AIDS}) = 0.05\\).\n\n\n\n\n\ngraph LR\n    U[Person] --&gt;|0.001| A[AIDS] \n    U[Person] --&gt;|0.999| D[Kein AIDS]\n    A[AIDS] --&gt;|0.99| B[Positiv]\n    A[AIDS] --&gt;|0.01| C[Negativ]\n    D[Kein AIDS] --&gt;|0.05| E[Positiv]\n    D[Kein AIDS] --&gt;|0.95| F[Negativ]\n\n\n\n\n\n\nPfade mit positivem Test (disjunkt):\n\nOberer Pfad: \\(P(\\text{AIDS} \\cap \\text{Positiv}) = P(\\text{AIDS}) \\cdot P(\\text{Positiv} | \\text{AIDS}) = 0.001 \\cdot 0.99 = 0.00099\\).\nUnterer Pfad: \\(P(\\text{Kein AIDS} \\cap \\text{Positiv}) = P(\\text{Kein AIDS}) \\cdot P(\\text{Positiv} | \\text{Kein AIDS}) = 0.999 \\cdot 0.05 = 0.04995\\).\nGesamt: \\(P(\\text{Positiv}) = 0.00099 + 0.04995 = 0.05094\\).\n\nBedingte Wahrscheinlichkeit:\n\\[\nP(\\text{AIDS} | \\text{Positiv}) = \\frac{P(\\text{AIDS} \\cap \\text{Positiv})}{P(\\text{Positiv})} = \\frac{0.00099}{0.05094} = 0.0194.\n\\]\nTrotz positivem Test ist die Wahrscheinlichkeit für AIDS gering (ca. 1,94 %). In der Praxis folgen Bestätigungstests, und die Prävalenz in Risikogruppen ist höher als 0,1 %.\n\n\n3.9.6 Satz von Bayes\nIm AIDS-Test-Beispiel kennen wir \\(P(\\text{Positiv} | \\text{AIDS})\\) – die Wahrscheinlichkeit eines positiven Tests bei AIDS – und möchten die Umkehrung, \\(P(\\text{AIDS} | \\text{Positiv})\\) – die Wahrscheinlichkeit von AIDS bei einem positiven Test. Diese Umkehrung nennt sich bedingte Wahrscheinlichkeit in umgekehrter Richtung. Doch wie gelangen wir von der einen zur anderen? Der Satz von Bayes liefert die Lösung, indem er bedingte Wahrscheinlichkeiten umkehrt.\n\n3.9.6.1 Motivation und Herleitung\nStellen wir uns vor, wir wollen \\(P(\\text{AIDS} | \\text{Positiv})\\) berechnen. Aus der Definition der bedingten Wahrscheinlichkeit wissen wir:\n\\[P(\\text{AIDS} | \\text{Positiv}) = \\frac{P(\\text{AIDS} \\cap \\text{Positiv})}{P(\\text{Positiv})}.\\]\nGleichzeitig gilt für die umgekehrte Richtung:\n\\[P(\\text{Positiv} | \\text{AIDS}) = \\frac{P(\\text{Positiv} \\cap \\text{AIDS})}{P(\\text{AIDS})}.\\]\nDa \\(P(\\text{AIDS} \\cap \\text{Positiv}) = P(\\text{Positiv} \\cap \\text{AIDS})\\) (Schnittmengen sind symmetrisch), können wir die zweite Gleichung umstellen:\n\\[P(\\text{AIDS} \\cap \\text{Positiv}) = P(\\text{Positiv} | \\text{AIDS}) \\cdot P(\\text{AIDS}).\\]\nSetzen wir dies in die erste Gleichung ein:\n\\[P(\\text{AIDS} | \\text{Positiv}) = \\frac{P(\\text{Positiv} | \\text{AIDS}) \\cdot P(\\text{AIDS})}{P(\\text{Positiv})}.\\]\nDas ist der Satz von Bayes! Er verbindet die bekannte Bedingung (\\(P(\\text{Positiv} | \\text{AIDS})\\)) mit der gesuchten (\\(P(\\text{AIDS} | \\text{Positiv})\\)), wobei \\(P(\\text{Positiv})\\) die Gesamtwahrscheinlichkeit eines positiven Tests ist.\n\n\n3.9.6.2 Formale Definition\nDer Satz von Bayes lautet allgemein:\n\\[P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)},\\]\nwobei:\n\n\\(P(A|B)\\) die Posterior-Wahrscheinlichkeit ist (z. B. AIDS bei positivem Test),\n\n\\(P(B|A)\\) die Likelihood (z. B. positiver Test bei AIDS),\n\n\\(P(A)\\) der Prior (z. B. Grundwahrscheinlichkeit für AIDS),\n\n\\(P(B)\\) die Normalisierungskonstante (z. B. Gesamtwahrscheinlichkeit eines positiven Tests).\n\nWir können also unser Vorwissen (Prior) mit neuen Daten (Likelihood) kombinieren, um die Wahrscheinlichkeit für ein Ereignis zu aktualisieren (Posterior).\n\n\n3.9.6.3 Anwendung\nIm AIDS-Beispiel:\n\n\\(P(\\text{Positiv} | \\text{AIDS}) = 0.99\\),\n\n\\(P(\\text{AIDS}) = 0.001\\),\n\n\\(P(\\text{Positiv}) = P(\\text{Positiv} | \\text{AIDS}) \\cdot P(\\text{AIDS}) + P(\\text{Positiv} | \\text{Kein AIDS}) \\cdot P(\\text{Kein AIDS}) = 0.00099 + 0.04995 = 0.05094\\).\nDamit:\n\\[P(\\text{AIDS} | \\text{Positiv}) = \\frac{0.99 \\cdot 0.001}{0.05094} \\approx 0.01943.\\]\nDer Satz von Bayes ist in Medizin, Wirtschaft und Technik essenziell, um aus bekannten Daten (z. B. Testresultaten) auf Ursachen (z. B. Krankheiten) zu schließen.\n\n\n\n\n\n\n\nTip\n\n\n\nYoutube-Videos:\n\nThree Blue One Brown: Bayes Theorem\nVeritasium: The Bayesian Trap\n\n\n\n\n\n\n\n\n\nAssoziationsanalyse mit A-Priori-Algorithmus\n\n\n\nDie Assoziationsanalyse ist ein Verfahren, um Zusammenhänge in Daten zu finden. Ein bekannter Algorithmus ist der A-Priori-Algorithmus, der z.B. in für Predictive Maintainance oder in der Warenkorbanalyse verwendet wird. Der A-Priori-Algorithmus findet heraus, welche Produkte oft zusammen gekauft werden. Ein Beispiel ist, dass Kunden, die Windeln kaufen, oft auch Bier kaufen.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Stichproben und Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "statistics/distributions.html",
    "href": "statistics/distributions.html",
    "title": "4  Verteilungen",
    "section": "",
    "text": "4.1 Diskrete Verteilungen\nIn diesem Abschnitt untersuchen wir Verteilungen einzelner Variablen. Histogramme und Balkendiagramme veranschaulichen die Häufigkeits- oder Wahrscheinlichkeitsverteilung, wie in den folgenden Abbildungen dargestellt. Verteilungen sind die Grundlage für Simulationen, z. B. in der Monte-Carlo-Methode, wie im Tutorial zur Fahrzeugausfallzeit gezeigt.\nDiskrete Verteilungen beschreiben Zufallsvariablen mit abzählbaren Werten. Wir betrachten die Bernoulli-Verteilung, Binomial-Verteilung und Poisson-Verteilung, die häufig in der Statistik und realen Anwendungen vorkommen.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Verteilungen</span>"
    ]
  },
  {
    "objectID": "statistics/distributions.html#sec-discrete-distributions",
    "href": "statistics/distributions.html#sec-discrete-distributions",
    "title": "4  Verteilungen",
    "section": "",
    "text": "4.1.1 Bernoulli-Verteilung\nDie Bernoulli-Verteilung modelliert eine Zufallsvariable mit genau zwei möglichen Ergebnissen, z. B. einen Münzwurf: „Kopf“ (\\(X = 1\\)) mit Wahrscheinlichkeit \\(p\\) oder „Zahl“ (\\(X = 0\\)) mit Wahrscheinlichkeit \\(1-p\\). Die Wahrscheinlichkeitsfunktion lautet:\n\\[P(X = x) = \\begin{cases}\np & \\text{für } x = 1, \\\\\n1-p & \\text{für } x = 0.\n\\end{cases}\\]\nWir schreiben \\(X \\sim \\text{Bernoulli}(p)\\). Für eine faire Münze gilt \\(p = 0.5\\), also \\(X \\sim \\text{Bernoulli}(0.5)\\).\nFigure 4.1 zeigt die Verteilung für eine faire Münze. Diese einfache Verteilung ist die Basis für komplexere Modelle wie die Binomial-Verteilung und findet Anwendung in Entscheidungen mit Ja/Nein-Ergebnissen.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\np = 0.5\nx = [0, 1]  # Einfache Liste statt NumPy-Array für Klarheit\nP_X = [1-p, p]\n\nplt.bar(x, P_X, color='skyblue', width=0.6)\nplt.xlabel('Ergebnis (0 = Zahl, 1 = Kopf)')\nplt.ylabel('Wahrscheinlichkeit')\nplt.title('Bernoulli-Verteilung ($p = 0.5$)')\nplt.ylim(0, 1)\nplt.xticks(x)\nplt.grid(axis='y', alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\nFigure 4.1: Wahrscheinlichkeitsverteilung der Bernoulli-Verteilung für eine faire Münze (p = 0.5).\n\n\n\n\n\nIn Figure 4.1 wird jeder möglichen Realisierung der Zufallsvariablen \\(X\\) die theoretische Wahrscheinlichkeit zugeordnet. Im Gegensatz zu Histogrammen, die empirische Häufigkeiten darstellen, zeigt diese Abbildung die Wahrscheinlichkeitsverteilung direkt.\n\n4.1.1.1 Erwartungswert und Varianz der Bernoulli-Verteilung\n\n\n\n\n\n\nImportant\n\n\n\nDer Erwartungswert \\(E(X)\\) einer Verteilung beschreibt ihre zentrale Tendenz – den durchschnittlichen Wert der Zufallsvariablen. Für eine diskrete Zufallsvariable \\(X\\) gilt:\n\\[E(X) = \\sum_{x} x \\cdot P(X = x).\\]\nBei der Bernoulli-Verteilung (\\(X \\sim \\text{Bernoulli}(p)\\)) ist:\n\\[E(X) = 0 \\cdot (1-p) + 1 \\cdot p = p.\\]\nFür eine faire Münze (\\(p = 0.5\\)) ist \\(E(X) = 0.5\\).\n\n\n\n\n\n\n\n\nImportant\n\n\n\nDie Varianz \\(\\text{Var}(X)\\) misst die Streuung der Verteilung – wie stark die Werte um den Erwartungswert schwanken. Sie wird berechnet als:\n\\[\\text{Var}(X) = \\sum_{x} (x - E(X))^2 \\cdot P(X = x).\\]\nFür die Bernoulli-Verteilung ergibt sich:\n\\[\\text{Var}(X) = (0 - p)^2 \\cdot (1-p) + (1 - p)^2 \\cdot p = p \\cdot (1-p).\\]\nBei \\(p = 0.5\\) ist \\(\\text{Var}(X) = 0.5 \\cdot 0.5 = 0.25\\).\n\n\n\n\n\n4.1.2 Binomialverteilung\nStellen wir uns vor, wir wiederholen einen Münzwurf \\(n\\)-mal unabhängig mit der Wahrscheinlichkeit \\(p\\) für „Kopf“. Die Zufallsvariable \\(Y\\) zählt die Anzahl der Kopfwürfe und folgt einer Binomialverteilung: \\(Y \\sim \\text{Bin}(n, p)\\). Die Wahrscheinlichkeitsfunktion lautet:\n\\[P(Y = k) = \\binom{n}{k} p^k (1-p)^{n-k},\\]\nwobei \\(\\binom{n}{k}\\) die Anzahl der Möglichkeiten ist, \\(k\\) Erfolge in \\(n\\) Versuchen zu erzielen.\nFigure 4.2 zeigt die Verteilung für \\(n = 10\\) und \\(p = 0.5\\). Die Binomialverteilung ist eine Erweiterung der Bernoulli-Verteilung und nützlich, um Häufigkeiten in wiederholten Experimenten zu modellieren.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import binom\n\nn = 10\np = 0.5\ny = np.arange(0, n + 1)\nP_Y = binom.pmf(y, n, p)\n\nplt.bar(y, P_Y, color='skyblue', width=0.8)\nplt.xlabel('Anzahl der Kopfwürfe ($Y$)')\nplt.ylabel('Wahrscheinlichkeit')\nplt.title(f'Binomialverteilung ($n = {n}$, $p = {p}$)')\nplt.xticks(y)\nplt.grid(axis='y', alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\nFigure 4.2: Wahrscheinlichkeitsverteilung der Binomialverteilung für eine faire Münze (n = 10, p = 0.5).\n\n\n\n\n\nStellen wir uns vor, wir wiederholen einen Münzwurf \\(n\\)-mal. Die Würfe sind unabhängig und identisch verteilt (i.i.d.), d. h., jeder Wurf hat die gleiche Wahrscheinlichkeit \\(p\\) für „Kopf“ und ist unbeeinflusst von den anderen. Die Zufallsvariable \\(Y\\) zählt die Anzahl der Kopfwürfe und folgt einer Binomialverteilung: \\(Y \\sim \\text{Bin}(n, p)\\). Die Wahrscheinlichkeitsfunktion ist:\n\\[P(Y = y) = \\binom{n}{y} \\cdot p^y \\cdot (1-p)^{n-y},\\]\nwobei \\(\\binom{n}{y}\\) der Binomialkoeffizient ist. Dieser gibt an, auf wie viele Arten \\(y\\) Erfolge in \\(n\\) Versuchen auftreten können und wird definiert als:\n\\[\\binom{n}{y} = \\frac{n!}{y! \\cdot (n - y)!}.\\]\n\n4.1.2.1 Beispiel: Münzwurf\nFür \\(n = 10\\) Würfe mit einer fairen Münze (\\(p = 0.5\\)) ist \\(Y \\sim \\text{Bin}(10, 0.5)\\). Dies wurde in Figure 4.2 gezeigt.\n\n\n4.1.2.2 Beispiel: Gewinnlose\nPassen wir die Werte an: Beim Ziehen von 10 Losen, wobei die Wahrscheinlichkeit für ein Gewinnlos \\(p = 0.1\\) beträgt, gilt \\(Y \\sim \\text{Bin}(10, 0.1)\\). Die Zufallsvariable \\(Y\\) zählt die Anzahl der Gewinnlose. Figure 4.3 zeigt diese Verteilung. Solche Modelle sind nützlich, um Erfolge in wiederholten, unabhängigen Versuchen zu analysieren – ähnlich wie im Tutorial zur Fahrzeugausfallzeit, wo Verteilungen für Komponenten genutzt werden.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import binom\n\nn = 10\np = 0.1\ny = np.arange(0, n + 1)\nP_Y = binom.pmf(y, n, p)\n\nplt.bar(y, P_Y, color='skyblue', width=0.8)\nplt.xlabel('Anzahl der Gewinnlose ($Y$)')\nplt.ylabel('Wahrscheinlichkeit')\nplt.title(f'Binomialverteilung ($n = {n}$, $p = {p}$)')\nplt.xticks(y)\nplt.grid(axis='y', alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\nFigure 4.3: Wahrscheinlichkeitsverteilung der Binomialverteilung für das Ziehen von Gewinnlosen (n = 10, p = 0.1).\n\n\n\n\n\n\n\n4.1.2.3 Erwartungswert und Varianz der Binomialverteilung\nDer Erwartungswert \\(E(Y)\\) der Binomialverteilung (\\(Y \\sim \\text{Bin}(n, p)\\)) gibt die erwartete Anzahl der Erfolge in \\(n\\) Versuchen an:\n\\[E(Y) = n \\cdot p.\\]\nFür \\(n = 10\\) und \\(p = 0.1\\) (Gewinnlose) ist \\(E(Y) = 10 \\cdot 0.1 = 1\\).\nDie Varianz \\(\\text{Var}(Y)\\) misst die Streuung um diesen Erwartungswert:\n\\[\\text{Var}(Y) = n \\cdot p \\cdot (1-p).\\]\nIm Beispiel ist \\(\\text{Var}(Y) = 10 \\cdot 0.1 \\cdot 0.9 = 0.9\\). Diese Werte helfen, die Verteilung zu charakterisieren, z. B. in Simulationen wie im Tutorial.\n\n\n\n4.1.3 Diskrete Gleichverteilung\nDie diskrete Gleichverteilung (auch Uniformverteilung) beschreibt eine Zufallsvariable, bei der alle möglichen Werte innerhalb eines Intervalls die gleiche Wahrscheinlichkeit haben. Im Gegensatz zum Würfelbeispiel, wo die Werte bei 1 beginnen, kann das Intervall beliebig gewählt werden, z. B. \\(x = a, a+1, \\ldots, b\\), wobei \\(a\\) und \\(b\\) ganze Zahlen sind und \\(a \\leq b\\). Die Anzahl der Werte ist \\(n = b - a + 1\\), und die Wahrscheinlichkeitsfunktion lautet:\n\\[P(X = x) = \\frac{1}{b - a + 1}, \\quad \\text{für } x = a, a+1, \\ldots, b.\\]\nWir notieren \\(X \\sim \\text{DU}(a, b)\\) für die diskrete Gleichverteilung auf \\([a, b]\\).\nFigure 4.4 zeigt die Verteilung für \\(a = 3\\) und \\(b = 8\\) (z. B. ein modifizierter Würfel). Dieses flexible Intervall ist nützlich, um spezifische Szenarien zu modellieren, z. B. in Simulationen mit nicht standardisierten Bereichen.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nn = 6\nx = np.arange(1, n + 1)\nP_X = np.full(n, 1/n)  # Array mit konstanter Wahrscheinlichkeit 1/n\n\nplt.bar(x, P_X, color='skyblue', width=0.8)\nplt.xlabel('Würfelzahl ($X$)')\nplt.ylabel('Wahrscheinlichkeit')\nplt.title(f'Diskrete Gleichverteilung ($n = {n}$)')\nplt.xticks(x)\nplt.grid(axis='y', alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\nFigure 4.4: Wahrscheinlichkeitsverteilung der diskreten Gleichverteilung für einen Würfel (n = 6).\n\n\n\n\n\nIn der Abbildung (fig:sec-dataexploratory-distributions-discrete-uniform-distribution?) wird die Wahrscheinlichkeitsverteilung der diskreten Gleichverteilung für das Würfeln eines Würfels dargestellt. Die Wahrscheinlichkeitsverteilung zeigt, dass alle möglichen Werte der Zufallsvariablen die gleiche Wahrscheinlichkeit haben.\n\n4.1.3.1 Erwartungswert und Varianz der diskreten Gleichverteilung\nDer Erwartungswert \\(E(X)\\) einer diskreten Gleichverteilung auf dem Intervall \\([a, b]\\) liegt in der Mitte des Intervalls:\n\\[E(X) = \\frac{a + b}{2} = \\sum_{x} x \\cdot P(X = x).$.\\]\nFür \\(a = 3\\) und \\(b = 8\\) ergibt sich:\n\\[E(X) = \\frac{3 + 8}{2} = 5.5.\\]\nDie Varianz \\(\\text{Var}(X)\\) beschreibt die Streuung der Verteilung:\n\\[\\text{Var}(X) = \\frac{(b - a + 1)^2 - 1}{12} = \\sum_{x} (x - E(X))^2 \\cdot P(X = x)..\\]\nBei \\(a = 3\\) und \\(b = 8\\) ist \\(n = b - a + 1 = 6\\), also:\n\\[\\text{Var}(X) = \\frac{6^2 - 1}{12} = \\frac{36 - 1}{12} = \\frac{35}{12} \\approx 2.9167.\\]\nDiese Formeln gelten für jedes Intervall \\([a, b]\\), wobei \\(a\\) und \\(b\\) ganze Zahlen sind und \\(a \\leq b\\).",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Verteilungen</span>"
    ]
  },
  {
    "objectID": "statistics/distributions.html#sec-continuous-distributions",
    "href": "statistics/distributions.html#sec-continuous-distributions",
    "title": "4  Verteilungen",
    "section": "4.2 Stetige Verteilungen",
    "text": "4.2 Stetige Verteilungen\nStetige Verteilungen modellieren Zufallsvariablen, die kontinuierliche Werte annehmen können, z. B. Zeit oder Länge. Im Gegensatz zu diskreten Verteilungen gibt es hier unendlich viele mögliche Werte innerhalb eines Intervalls. Wir betrachten die Normalverteilung, Exponentialverteilung und stetige Gleichverteilung, die in Simulationen wie im Tutorial zur Fahrzeugausfallzeit eine Rolle spielen.\n\n4.2.1 Stetige Gleichverteilung\nDie stetige Gleichverteilung beschreibt eine Zufallsvariable, bei der alle Werte in einem Intervall \\([a, b]\\) gleich wahrscheinlich sind. Die Wahrscheinlichkeitsdichtefunktion (Dichte) lautet:\n\\[f(x) = \\begin{cases}\n\\frac{1}{b - a} & \\text{für } a \\leq x \\leq b, \\\\\n0 & \\text{sonst}.\n\\end{cases}\\]\nWir schreiben \\(X \\sim \\text{U}(a, b)\\). Bei stetigen Verteilungen wird die Wahrscheinlichkeit als Fläche unter der Dichte berechnet, wobei die Gesamtfläche stets 1 beträgt.\nFigure 4.5 zeigt die Dichte für \\(a = 0\\) und \\(b = 1\\). Im Tutorial wird die stetige Gleichverteilung für die Steuereinheit (\\(X \\sim \\text{U}(4000, 8000)\\)) verwendet, um Ausfallzeiten zu modellieren.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\na = 0\nb = 1\nx = np.linspace(a - 0.5, b + 0.5, 1000)\nf_X = np.where((x &gt;= a) & (x &lt;= b), 1 / (b - a), 0)\n\nplt.plot(x, f_X, color='skyblue', linewidth=2)\nplt.fill_between(x, f_X, alpha=0.2, color='skyblue')  # Fläche einfärben\nplt.xlabel('Werte der Zufallsvariablen ($X$)')\nplt.ylabel('Wahrscheinlichkeitsdichte')\nplt.title(f'Stetige Gleichverteilung ($a = {a}$, $b = {b}$)')\nplt.grid(alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\nFigure 4.5: Wahrscheinlichkeitsdichte der stetigen Gleichverteilung auf dem Intervall [0, 1].\n\n\n\n\n\nBei stetigen Verteilungen ist die Wahrscheinlichkeit eines exakten Wertes 0, da es unendlich viele mögliche Werte gibt. Stattdessen berechnen wir die Wahrscheinlichkeit für einen Wertebereich als Fläche unter der Dichtefunktion:\n\\[P(a \\leq X \\leq b) = \\int_{a}^{b} f(x) \\, dx.\\]\nDer Erwartungswert \\(E(X)\\) einer gleichverteilten Zufallsvariable \\(X \\sim \\text{U}(a, b)\\) ist:\n\\[E(X) = \\frac{a + b}{2}=\\int\\limits_{ - \\infty }^\\infty {x \\cdot f\\left( x \\right)} \\,\\,dx.\\]\nDie Varianz \\(\\text{Var}(X)\\) beträgt:\n\\[\\text{Var}(X) = \\frac{(b - a)^2}{12} = \\int\\limits_{ - \\infty }^\\infty {{{\\left( {x - {\\mu _x}} \\right)}^2}} \\cdot f\\left( x \\right)\\,\\,dx.\\]\n\n4.2.1.1 Beispiel: Ausfallwahrscheinlichkeit eines Bauteils\nDer Ausfall eines elektronischen Bauteils folgt einer stetigen Gleichverteilung auf \\([0, 3650]\\) Tagen (\\(X \\sim \\text{U}(0, 3650)\\)). Wie hoch ist die Wahrscheinlichkeit, dass es innerhalb der ersten 1000 Tage ausfällt?\nDie Dichtefunktion ist:\n\\[f(x) = \\begin{cases}\n\\frac{1}{3650 - 0} = \\frac{1}{3650} & \\text{für } 0 \\leq x \\leq 3650, \\\\\n0 & \\text{sonst}.\n\\end{cases}\\]\nDie Wahrscheinlichkeit ergibt sich durch Integration:\n\\[P(0 \\leq X \\leq 1000) = \\int_{0}^{1000} \\frac{1}{3650} \\, dx = \\frac{1000}{3650} \\approx 0.274.\\]\nDas Bauteil hat also eine 27,4 % Chance, innerhalb von 1000 Tagen auszusetzen. Dieses Prinzip wird im Tutorial bei der Steuereinheit (\\(X \\sim \\text{U}(4000, 8000)\\)) angewendet.\n\n\n\n\n\n\nLösungsschritte\n\n\n\n\n\n\nDichte: \\(f(x) = \\frac{1}{3650}\\) für \\(0 \\leq x \\leq 3650\\).\n\nIntegral: \\(P(0 \\leq X \\leq 1000) = \\int_{0}^{1000} \\frac{1}{3650} \\, dx\\).\n\nErgebnis: \\(\\frac{1000}{3650} = 0.274\\).\n\n\n\n\n\n\n\n4.2.2 Normalverteilung\nDie Normalverteilung ist eine stetige Verteilung, die in Statistik und Naturwissenschaften weit verbreitet ist. Ihre Glockenform zeigt eine symmetrische Verteilung der Werte um den Erwartungswert. Die Dichtefunktion lautet:\n\\[f(x) = \\frac{1}{\\sqrt{2\\pi} \\sigma} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right),\\]\nfür \\(-\\infty &lt; x &lt; \\infty\\). Wir schreiben \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), wobei \\(\\mu\\) der Erwartungswert und \\(\\sigma^2\\) die Varianz ist.\nFalls \\(\\mu = 0\\) und \\(\\sigma = 1\\), spricht man von der Standardnormalverteilung (\\(X \\sim \\mathcal{N}(0, 1)\\)), eine spezielle Form mit Erwartungswert 0 und Varianz 1. Figure 4.6 zeigt diese Verteilung. Im Tutorial wird die Normalverteilung für den Sensor (\\(X \\sim \\mathcal{N}(6000, 100^2)\\)) genutzt, um Ausfallzeiten zu modellieren.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nmu = 0\nsigma = 1\nx = np.linspace(-5, 5, 1000)\nf_X = 1 / (np.sqrt(2 * np.pi) * sigma) * np.exp(-((x - mu) ** 2) / (2 * sigma ** 2))\n\nplt.plot(x, f_X, color='skyblue', linewidth=2)\nplt.fill_between(x, f_X, alpha=0.2, color='skyblue')  # Fläche einfärben\nplt.xlabel('Werte der Zufallsvariablen ($X$)')\nplt.ylabel('Wahrscheinlichkeitsdichte')\nplt.title(f'Standardnormalverteilung ($\\mu = {mu}$, $\\sigma = {sigma}$)')\nplt.grid(alpha=0.3)\nplt.show()\n\n&lt;&gt;:13: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:13: SyntaxWarning:\n\ninvalid escape sequence '\\s'\n\n&lt;&gt;:13: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:13: SyntaxWarning:\n\ninvalid escape sequence '\\s'\n\n/tmp/ipykernel_3134/513147542.py:13: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n/tmp/ipykernel_3134/513147542.py:13: SyntaxWarning:\n\ninvalid escape sequence '\\s'\n\n\n\n\n\n\n\n\n\nFigure 4.6: Wahrscheinlichkeitsdichte der Standardnormalverteilung (\\(\\mu = 0\\), \\(\\sigma = 1\\)).\n\n\n\n\n\nFrüher, ohne Computer, war die Berechnung der Fläche unter der Normalverteilungskurve (\\(P(X \\leq x)\\)) schwierig. Man nutzte \\(Z\\)-Wert-Tabellen, um die Wahrscheinlichkeit für eine standardnormalverteilte Zufallsvariable \\(Z \\sim \\mathcal{N}(0, 1)\\) nachzuschlagen. Beispiele:\n\n\\(P(Z \\leq 0) = 0.5\\) (50 %).\n\n\\(P(Z \\leq 1) = 0.8413\\) (84,13 %).\n\n\\(P(Z \\leq -1) = 1 - P(Z \\leq 1) = 0.1587\\) (15,87 %).\nUmgekehrt: \\(P(Z \\leq 1.645) = 0.95\\) (95 %). Heute ersetzen Computerprogramme solche Tabellen, wie im Folgenden gezeigt.\n\n\n4.2.2.1 Erwartungswert und Varianz der Normalverteilung\nDer Erwartungswert \\(E(X)\\) einer normalverteilten Zufallsvariable \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) ist:\n\\[E(X) = \\mu = \\int\\limits_{ - \\infty }^\\infty {x \\cdot f\\left( x \\right)} \\,\\,dx.\\]\nDie Varianz \\(\\text{Var}(X)\\) beträgt:\n\\[{\\sigma _x}^2 = Var\\left( X \\right) = E{\\left( {X - {\\mu _x}} \\right)^2} = \\int\\limits_{ - \\infty }^\\infty {{{\\left( {x - {\\mu _x}} \\right)}^2}} \\cdot f\\left( x \\right)\\,\\,dx.\\]\nFür die Standardnormalverteilung (\\(X \\sim \\mathcal{N}(0, 1)\\)) gilt \\(E(X) = 0\\) und \\(\\text{Var}(X) = 1\\). Im Tutorial wird dies für den Sensor (\\(X \\sim \\mathcal{N}(6000, 100^2)\\)) genutzt.\n\n4.2.2.1.1 Standardisierung der Normalverteilung\nViele Zufallsvariablen folgen keiner Standardnormalverteilung, sondern haben andere Werte für \\(\\mu\\) und \\(\\sigma\\). Um diese auf \\(Z \\sim \\mathcal{N}(0, 1)\\) zu überführen, wird standardisiert:\n\\[Z = \\frac{X - \\mu}{\\sigma}.\\]\n\\(Z\\) hat dann \\(E(Z) = 0\\) und \\(\\text{Var}(Z) = 1\\), sodass \\(Z\\)-Tabellen oder Software genutzt werden können.\n\n\n\n4.2.2.2 Beispiel: Intelligenz-Quotient (IQ)\nDer IQ ist normalverteilt mit \\(X \\sim \\mathcal{N}(100, 15^2)\\) (\\(\\mu = 100\\), \\(\\sigma = 15\\)). Wie wahrscheinlich ist ein IQ von 130 oder mehr (\\(P(X \\geq 130)\\))?\n\n4.2.2.2.1 Analytische Berechnung\nDie Dichtefunktion ist:\n\\[f(x) = \\frac{1}{\\sqrt{2\\pi} \\cdot 15} \\exp\\left(-\\frac{(x - 100)^2}{2 \\cdot 15^2}\\right).\\]\nDie Wahrscheinlichkeit \\(P(X \\geq 130)\\) ergibt sich als:\n\\[P(X \\geq 130) = 1 - P(X \\leq 130) = 1 - \\int_{-\\infty}^{130} f(x) \\, dx.\\]\nMit Standardisierung:\n\\[Z = \\frac{130 - 100}{15} = 2, \\quad P(X \\leq 130) = P(Z \\leq 2).\\]\nAus Tabellen oder Software: \\(P(Z \\leq 2) \\approx 0.9772\\), also:\n\\[P(X \\geq 130) = 1 - 0.9772 = 0.0228 \\text{ (2,28 %)}.\\]\n\n\n4.2.2.2.2 Berechnung mit Python\nMit scipy.stats.norm können wir die kumulative Verteilungsfunktion (cdf) direkt berechnen:\n\n\n\nimport numpy as np\nfrom scipy.stats import norm\n\nmu = 100\nsigma = 15\nP_X_geq_130 = 1 - norm.cdf(130, loc=mu, scale=sigma)\n\nprint(f'P(X &gt;= 130) = {P_X_geq_130:.4f}')  # Ausgabe: 0.0228\n\nP(X &gt;= 130) = 0.0228\n\n\n\nFigure 4.7\n\n\n\n\n\n\n4.2.2.3 Visualisierung der Wahrscheinlichkeiten\nOft interessiert der Anteil einer Population innerhalb von ein oder zwei Standardabweichungen vom Mittelwert. Für eine normalverteilte Zufallsvariable \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) berechnen wir:\n\\[P(\\mu - \\sigma \\leq X \\leq \\mu + \\sigma) = P(X \\leq \\mu + \\sigma) - P(X \\leq \\mu - \\sigma),\\]\nwobei \\(P(X \\leq x)\\) die kumulative Verteilungsfunktion (CDF) ist. Für die Standardnormalverteilung gilt:\n- \\(P(\\mu - \\sigma \\leq X \\leq \\mu + \\sigma) \\approx 0.6826\\) (ca. 68 %),\n- \\(P(\\mu - 2\\sigma \\leq X \\leq \\mu + 2\\sigma) \\approx 0.9544\\) (ca. 95 %).\nIm IQ-Beispiel (\\(X \\sim \\mathcal{N}(100, 15^2)\\)) prüfen wir den Bereich von zwei Standardabweichungen (\\(\\mu - 2\\sigma = 70\\), \\(\\mu + 2\\sigma = 130\\)). Figure 4.8 zeigt diese Wahrscheinlichkeit. Solche Berechnungen sind im Tutorial nützlich, z. B. um Sensor-Ausfallzeiten zu analysieren.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\nmu = 100\nsigma = 15\nx = np.linspace(mu - 4*sigma, mu + 4*sigma, 1000)\nf_X = norm.pdf(x, loc=mu, scale=sigma)\n\n# Wahrscheinlichkeit für μ ± 2σ\nP_X_2_sigma = norm.cdf(mu + 2*sigma, loc=mu, scale=sigma) - norm.cdf(mu - 2*sigma, loc=mu, scale=sigma)\nprint(f'P({mu - 2*sigma} &lt;= X &lt;= {mu + 2*sigma}) = {P_X_2_sigma:.4f}')  # Ausgabe: 0.9545\n\n# Plot\nplt.plot(x, f_X, color='skyblue', linewidth=2)\nplt.fill_between(x, f_X, where=(x &gt;= mu - 2*sigma) & (x &lt;= mu + 2*sigma), color='lightblue', alpha=0.3, label=f'P = {P_X_2_sigma:.4f}')\nplt.axvline(mu - 2*sigma, color='red', linestyle='--', label=f'{mu - 2*sigma}')\nplt.axvline(mu + 2*sigma, color='red', linestyle='--', label=f'{mu + 2*sigma}')\nplt.xlabel('IQ-Wert ($X$)')\nplt.ylabel('Wahrscheinlichkeitsdichte')\nplt.title(f'IQ-Verteilung ($\\mu = {mu}$, $\\sigma = {sigma}$)')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\n&lt;&gt;:21: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:21: SyntaxWarning:\n\ninvalid escape sequence '\\s'\n\n&lt;&gt;:21: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:21: SyntaxWarning:\n\ninvalid escape sequence '\\s'\n\n/tmp/ipykernel_3134/3074097218.py:21: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n/tmp/ipykernel_3134/3074097218.py:21: SyntaxWarning:\n\ninvalid escape sequence '\\s'\n\n\n\nP(70 &lt;= X &lt;= 130) = 0.9545\n\n\n\n\n\n\n\n\nFigure 4.8: Wahrscheinlichkeit, dass ein IQ-Wert innerhalb von zwei Standardabweichungen liegt.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nDer IQ ist so skaliert, dass \\(\\mu = 100\\) und \\(\\sigma = 15\\). Die Wahrscheinlichkeit für einen IQ \\(\\geq 130\\) beträgt etwa 2,28 %, während 95,45 % der Bevölkerung einen IQ zwischen 70 und 130 haben (innerhalb von \\(\\pm 2\\sigma\\)). Dies zeigt die praktische Relevanz der Normalverteilung.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Verteilungen</span>"
    ]
  },
  {
    "objectID": "statistics/distributions.html#sec-other-distributions",
    "href": "statistics/distributions.html#sec-other-distributions",
    "title": "4  Verteilungen",
    "section": "4.3 Weitere Verteilungen",
    "text": "4.3 Weitere Verteilungen\nNeben der Normalverteilung spielen weitere stetige und diskrete Verteilungen eine Rolle in Statistik und Simulationen, wie im Tutorial zur Fahrzeugausfallzeit genutzt:\n\nDie Exponentialverteilung modelliert die Zeit zwischen unabhängigen Ereignissen, z. B. in der Zuverlässigkeitsanalyse oder Warteschlangentheorie.\n\nDie Poissonverteilung (diskret) beschreibt die Anzahl von Ereignissen in einem festen Zeitintervall, etwa in Zufallsprozessen.\n\nDiese Verteilungen sind im Python-Modul scipy.stats verfügbar, das Funktionen für Wahrscheinlichkeiten und Zufallszahlen bietet. Im Tutorial wird z. B. die Poissonverteilung für den Motor (\\(X \\sim \\text{Poisson}(5000)\\)) eingesetzt.\n\n4.3.1 Exponentialverteilung\nDie Dichtefunktion der Exponentialverteilung ist:\n\\[f(x) = \\lambda e^{-\\lambda x}, \\quad x \\geq 0,\\]\nwobei \\(\\lambda\\) die Rate ist. Wir schreiben \\(X \\sim \\text{Exp}(\\lambda)\\), mit \\(E(X) = \\frac{1}{\\lambda}\\) und \\(\\text{Var}(X) = \\frac{1}{\\lambda^2}\\). Sie ist im Tutorial für Ausfallzeiten relevant, wenn Ereignisse exponentiell verteilt wären.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Verteilungen</span>"
    ]
  },
  {
    "objectID": "statistics/distributions.html#sec-fitting-distributions",
    "href": "statistics/distributions.html#sec-fitting-distributions",
    "title": "4  Verteilungen",
    "section": "4.4 Fitting von Verteilungen",
    "text": "4.4 Fitting von Verteilungen\nIn der Praxis müssen wir oft die Verteilung von Daten bestimmen – ein Prozess namens Fitting. Ziel ist es, die Verteilung zu finden, die die Daten am besten beschreibt. Methoden wie Maximum-Likelihood-Schätzung oder die Methode der Momente werden dafür genutzt.\nAls Beispiel simulieren wir Lotterie-Daten mit \\(X \\sim \\text{Bin}(10, 0.1)\\) und passen verschiedene Verteilungen an, um zu vergleichen, welche die Daten am besten modelliert.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import binom, norm, uniform\n\nnp.random.seed(42)\ndata = np.random.binomial(n=10, p=0.1, size=1000)  # 1000 Ziehungen\n\n# Binomialverteilung schätzen\nn_est = 10  # Bekannt aus Simulation\np_est = np.mean(data) / n_est\nbinom_x = np.arange(0, n_est + 1)\nbinom_y = binom.pmf(binom_x, n=n_est, p=p_est)\n\n# Normalverteilung schätzen\nmu, sigma = norm.fit(data)\nnorm_x = np.linspace(0, 10, 1000)\nnorm_y = norm.pdf(norm_x, mu, sigma)\n\n# Gleichverteilung schätzen\na, b = uniform.fit(data)\nuniform_x = np.linspace(0, 10, 1000)\nuniform_y = uniform.pdf(uniform_x, a, b - a)\n\n# Plot\nplt.hist(data, bins=range(11), density=True, color='skyblue', alpha=0.7, label='Daten')\nplt.plot(binom_x, binom_y, 'ro--', label=f'Binomial ($n={n_est}$, $p={p_est:.2f}$)')\nplt.plot(norm_x, norm_y, 'g-', label=f'Normal ($\\mu={mu:.2f}$, $\\sigma={sigma:.2f}$)')\nplt.plot(uniform_x, uniform_y, 'p-', label=f'Gleichverteilung ($a={a:.2f}$, $b={b:.2f}$)')\nplt.xlabel('Anzahl der Gewinnlose ($X$)')\nplt.ylabel('Dichte')\nplt.title('Fitting von Verteilungen an Binomialdaten')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\n&lt;&gt;:27: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:27: SyntaxWarning:\n\ninvalid escape sequence '\\s'\n\n&lt;&gt;:27: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:27: SyntaxWarning:\n\ninvalid escape sequence '\\s'\n\n/tmp/ipykernel_3134/1255964559.py:27: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n/tmp/ipykernel_3134/1255964559.py:27: SyntaxWarning:\n\ninvalid escape sequence '\\s'\n\n\n\n\n\n\n\n\n\nFigure 4.9: Fitting verschiedener Verteilungen an simulierte Binomialdaten (n = 10, p = 0.1).",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Verteilungen</span>"
    ]
  },
  {
    "objectID": "statistics/distributions.html#sec-computing-distributions",
    "href": "statistics/distributions.html#sec-computing-distributions",
    "title": "4  Verteilungen",
    "section": "4.5 Rechnen mit Verteilungen, Zufallsvariablen und Erwartungswert",
    "text": "4.5 Rechnen mit Verteilungen, Zufallsvariablen und Erwartungswert\nDer zentrale Grenzwertsatz besagt, dass die Summe einer großen Anzahl von unabhängigen und identisch verteilten (i.i.d.) Zufallsvariablen einer Normalverteilung folgt – unabhängig von ihrer ursprünglichen Verteilung. Dies ist ein Grundpfeiler der Statistik und erklärt, warum Normalverteilungen in Simulationen wie im Tutorial oft auftreten.\n\n4.5.1 Rechenregeln für Erwartungswert und Varianz\nFür unabhängige Zufallsvariablen \\(X_1, X_2, \\ldots, X_n\\) gelten folgende Regeln:\n\nErwartungswert: Die Summe der Erwartungswerte gilt immer, auch ohne Unabhängigkeit:\n\\[E(X_1 + X_2 + \\cdots + X_n) = E(X_1) + E(X_2) + \\cdots + E(X_n).\\]\nVarianz: Bei Unabhängigkeit (Kovarianz = 0) ist die Varianz die Summe der Varianzen:\n\\[\\text{Var}(X_1 + X_2 + \\cdots + X_n) = \\text{Var}(X_1) + \\text{Var}(X_2) + \\cdots + \\text{Var}(X_n).\\]\nAndernfalls: \\(\\text{Var}(X_1 + X_2) = \\text{Var}(X_1) + \\text{Var}(X_2) + 2 \\text{Cov}(X_1, X_2)\\).\nLinearkombinationen: Für \\(aX + bY\\):\n\\[E(aX + bY) = aE(X) + bE(Y),\\]\n\\[\\text{Var}(aX + bY) = a^2 \\text{Var}(X) + b^2 \\text{Var}(Y) + 2ab \\text{Cov}(X, Y),\\]\nwobei \\(\\text{Cov}(X, Y) = 0\\) bei Unabhängigkeit.\n\n\n\n\n\n\n\nImportant\n\n\n\nDie Kovarianz \\(\\text{Cov}(X, Y)\\) misst den linearen Zusammenhang zwischen \\(X\\) und \\(Y\\). Bei Unabhängigkeit ist sie 0, was die Varianzberechnung vereinfacht.\n\n\n\n\n4.5.2 Beispiel: Summe von Zufallsvariablen aus verschiedenen Verteilungen\nDer zentrale Grenzwertsatz gilt auch, wenn die Zufallsvariablen nicht identisch verteilt sind, solange sie unabhängig sind und die Anzahl groß ist. Figure 4.10 zeigt, wie die Summe von Uniform-, Exponential- und Binomialverteilungen einer Normalverteilung ähnelt – ein Prinzip, das in Monte-Carlo-Simulationen wie im Tutorial genutzt wird.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\nN = 10000\n\n# Zufallsvariablen definieren\nX_uni_1 = np.random.uniform(0, 10, N)      # Uniform [0, 10]\nX_uni_2 = np.random.uniform(2, 8, N)       # Uniform [2, 8]\nX_exp_1 = np.random.exponential(2, N)      # Exponential, Scale = 2\nX_exp_2 = np.random.exponential(3, N)      # Exponential, Scale = 3\nX_bin_1 = np.random.binomial(20, 0.3, N)   # Binomial (n=20, p=0.3)\nX_bin_2 = np.random.binomial(15, 0.4, N)   # Binomial (n=15, p=0.4)\n\nX_sum = X_uni_1 + X_uni_2 + X_exp_1 + X_exp_2 + X_bin_1 + X_bin_2\n\n# Plot\nplt.figure(figsize=(12, 6))\nplt.hist(X_sum, bins=50, density=True, color='skyblue', alpha=0.7, label='Summe')\nplt.hist(X_uni_1, bins=50, density=True, alpha=0.3, label='Uniform 1')\nplt.hist(X_uni_2, bins=50, density=True, alpha=0.3, label='Uniform 2')\nplt.hist(X_exp_1, bins=50, density=True, alpha=0.3, label='Exponential 1')\nplt.hist(X_exp_2, bins=50, density=True, alpha=0.3, label='Exponential 2')\nplt.hist(X_bin_1, bins=50, density=True, alpha=0.3, label='Binomial 1')\nplt.hist(X_bin_2, bins=50, density=True, alpha=0.3, label='Binomial 2')\nplt.xlabel('Wert')\nplt.ylabel('Dichte')\nplt.title('Summe von Zufallsvariablen aus verschiedenen Verteilungen')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\nFigure 4.10: Summe von Zufallsvariablen aus verschiedenen Verteilungen (N = 10.000).\n\n\n\n\n\n\n4.5.2.1 Normalverteilungs-Fit der Summe\nUm den zentralen Grenzwertsatz zu veranschaulichen, passen wir eine Normalverteilung an die Summe an. Figure 4.11 zeigt, wie gut die Summe einer Normalverteilung entspricht.\n\nfrom scipy.stats import norm\n\n# Normalverteilung an Summe anpassen\nX_sum_mu, X_sum_sigma = norm.fit(X_sum)\n\n# Plot\nplt.hist(X_sum, bins=50, density=True, color='skyblue', alpha=0.7, label='Summe')\nx = np.linspace(min(X_sum), max(X_sum), 1000)\ny = norm.pdf(x, X_sum_mu, X_sum_sigma)\nplt.plot(x, y, 'r-', label=f'Normal ($\\mu={X_sum_mu:.1f}$, $\\sigma={X_sum_sigma:.1f}$)')\nplt.xlabel('Wert')\nplt.ylabel('Dichte')\nplt.title('Fit der Summe mit einer Normalverteilung')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\n&lt;&gt;:10: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:10: SyntaxWarning:\n\ninvalid escape sequence '\\s'\n\n&lt;&gt;:10: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:10: SyntaxWarning:\n\ninvalid escape sequence '\\s'\n\n/tmp/ipykernel_3134/3238649060.py:10: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n/tmp/ipykernel_3134/3238649060.py:10: SyntaxWarning:\n\ninvalid escape sequence '\\s'\n\n\n\n\n\n\n\n\n\nFigure 4.11: Fit der Summe von Zufallsvariablen mit einer Normalverteilung (N = 10.000).",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Verteilungen</span>"
    ]
  },
  {
    "objectID": "statistics/tutorial.html",
    "href": "statistics/tutorial.html",
    "title": "Tutorial: Monte Carlo Simulation of Vehicle Component Failure",
    "section": "",
    "text": "Ziel\nIn dieser Übung erstellen Studierende eine Monte-Carlo-Simulation zur Modellierung der Ausfallzeit von Fahrzeugkomponenten. Dabei werden Wahrscheinlichkeitskonzepte wie bedingte Wahrscheinlichkeit, Additions- und Multiplikationsregeln, statistische Unabhängigkeit sowie Visualisierung durch ein Histogramm vertieft.",
    "crumbs": [
      "Statistik",
      "Tutorial: Monte Carlo Simulation of Vehicle Component Failure"
    ]
  },
  {
    "objectID": "statistics/tutorial.html#sec-monte-carlo-story",
    "href": "statistics/tutorial.html#sec-monte-carlo-story",
    "title": "Tutorial: Monte Carlo Simulation of Vehicle Component Failure",
    "section": "Szenario und Anwendungsfall",
    "text": "Szenario und Anwendungsfall\n\nSzenario\nEin Auto besteht aus kritischen Komponenten: Motor, Sensoren und Steuereinheit. Fällt der Motor oder die Steuereinheit aus, stoppt das Fahrzeug. Der Besitzer möchte die erwartete Betriebsdauer bis zum Ausfall abschätzen, um Wartungsintervalle zu planen.\n\n\nAnwendungsfall\nDie Studierenden simulieren das Ausfallverhalten eines Fahrzeugs mit drei Komponenten:\n- Motor: Ausfallzeit folgt einer Poissonverteilung mit Mittelwert 5000 Stunden.\n- Sensor: Ausfallzeit folgt einer Normalverteilung mit \\(\\mu = 6000\\) Stunden, \\(\\sigma = 100\\) Stunden.\n- Steuereinheit: Ausfallzeit folgt einer Gleichverteilung zwischen 4000 und 8000 Stunden.\n- Bedingung: Fällt der Sensor vor dem Motor aus, verkürzt sich die Motor-Ausfallzeit um 1000 Stunden.\n- Stopp: Das Fahrzeug stoppt, wenn Motor oder Steuereinheit ausfällt.\n\nAufgaben\n\nSimulation\n\nSimuliere Ausfallzeiten für jede Komponente des Fahrzeugs.\nBestimme die Zeit bis zum Ausfall des Fahrzeugs.\nWiederhole dies für 10.000 Durchläufe.\nStelle die Zeit bis zum Ausfall in einem Histogramm dar (Figure 1).\n\n\n\nAnalyse\nFür die weitere Anaylse nehmen wir an, dass wir den Zufallsprozess der Ausfälle nicht kennen. Wir können jedoch die Ausfallereignisse aufzeichnen und analysieren. Wir interessieren uns nur für Ausfälle, die zu einem Stopp des Fahrzeugs vor 5000 Betriebs-Stunden führen.\n\nBerechne für jede Komponente die Wahrscheinlichkeit, dass sie ausgefallen ist, gegeben das Fahrzeug fällt vor 5000 Stunden aus.\nGibt es eine Korrelation zwischen allen Zeiten bis zum Ausfall aller Komponenten und des Fahrzeugs? Berechne die Korrelationskoeffizienten und visualisiere die Korrelation in einer Scatterplot-Matrix.\nWie hoch ist die Bedingte Wahrscheinlichkeit, dass die Steuereinheit den ausgefallen ist, wenn das Fahrzeug vor 4000 Stunden ausgefallen ist.\nWähle eine Verteilung für die Ausfallzeit des Fahrzeugs fitte diese. Vergleiche die Verteilung mit dem Histogramm und gibt die Parameter der Verteilung an.\nNutze die gefittete Verteilung, um die Wahrscheinlichkeit zu berechnen, dass das Fahrzeug vor 4000 Stunden ausfällt.",
    "crumbs": [
      "Statistik",
      "Tutorial: Monte Carlo Simulation of Vehicle Component Failure"
    ]
  },
  {
    "objectID": "statistics/tutorial.html#sec-monte-carlo-simulation",
    "href": "statistics/tutorial.html#sec-monte-carlo-simulation",
    "title": "Tutorial: Monte Carlo Simulation of Vehicle Component Failure",
    "section": "Simulation",
    "text": "Simulation\nZuerst erstellen wir einen leeren DataFrame, um die Struktur der simulierten Ausfallzeiten zu zeigen:\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\nn_trials = 10000\n\n# Leerer DataFrame für Ausfallzeiten\ncolumns = ['motor', 'sensor', 'control', 'vehicle_failure']\ndf = pd.DataFrame(index=range(n_trials), columns=columns)\nprint(\"Leerer DataFrame (Auszug):\")\nprint(df.head())\n\nLeerer DataFrame (Auszug):\n  motor sensor control vehicle_failure\n0   NaN    NaN     NaN             NaN\n1   NaN    NaN     NaN             NaN\n2   NaN    NaN     NaN             NaN\n3   NaN    NaN     NaN             NaN\n4   NaN    NaN     NaN             NaN\n\n\nNun simulieren wir die Ausfallzeiten und berechnen die Zeit bis zum Fahrzeugausfall:\n\nfor trial in range(n_trials):\n    # Ausfallzeiten für das Fahrzeug\n    motor_time = np.random.poisson(5000)\n    sensor_time = np.random.normal(6000, 100)\n    control_time = np.random.uniform(4000, 8000)\n\n    # Bedingung: Sensor-Ausfall verkürzt Motorzeit\n    if sensor_time &lt; motor_time:\n        motor_time -= 1000\n        if motor_time &lt; sensor_time:\n            motor_time = sensor_time\n\n\n    # Ausfallzeit des Fahrzeugs (Motor oder Steuereinheit)\n    vehicle_failure = min(motor_time, control_time)\n\n    # Daten in DataFrame speichern\n    df.loc[trial, 'motor'] = motor_time\n    df.loc[trial, 'sensor'] = sensor_time\n    df.loc[trial, 'control'] = control_time\n    df.loc[trial, 'vehicle_failure'] = vehicle_failure\n\nprint(\"DataFrame mit simulierten Ausfallzeiten (Auszug):\")\nprint(df.head())\n\nplt.hist(df['vehicle_failure'], bins=50, density=True, color='skyblue', alpha=0.7)\nplt.xlabel('Ausfallzeit des Fahrzeugs (Stunden)')\nplt.ylabel('Dichte')\nplt.title('Verteilung der Ausfallzeiten (Fahrzeug)')\nplt.show()\n\nDataFrame mit simulierten Ausfallzeiten (Auszug):\n  motor       sensor      control vehicle_failure\n0  4974  6064.768854  4624.074562     4624.074562\n1  4919  6152.302986  7464.704583            4919\n2  5020  5953.052561  4727.299869     4727.299869\n3  4928  6054.256004  6099.025727            4928\n4  4986  5898.716888  5168.578594            4986\n\n\n\n\n\n\n\n\nFigure 1: Histogramm der Ausfallzeiten des Fahrzeugs (10.000 Durchläufe).\n\n\n\n\n\nMusterlösung",
    "crumbs": [
      "Statistik",
      "Tutorial: Monte Carlo Simulation of Vehicle Component Failure"
    ]
  },
  {
    "objectID": "statistics/interference_basics.html",
    "href": "statistics/interference_basics.html",
    "title": "5  Interferenz",
    "section": "",
    "text": "5.1 Punktschätzer und Konfidenzintervalle\nInferenz (statistische Schlussfolgerung) befasst sich damit, was wir mit ausreichender Sicherheit über eine Population aussagen können, wenn nur eine Stichprobe vorliegt. Beispiele sind: Weicht ein Parameter (z. B. Mittelwert) signifikant von einem Wert ab? Unterscheiden sich die Mittelwerte zweier Verteilungen signifikant? Der zentrale Grenzwertsatz ist hier ein Schlüsselwerkzeug.\nVorsicht vor P-Hacking: Daten so lange manipulieren, bis ein „signifikantes“ Ergebnis herauskommt, ist keine Inferenz, sondern Fehlinterpretation!\nEin Punktschätzer ist eine Funktion, die einen Parameter einer Verteilung (z. B. Mittelwert, Varianz) aus einer Stichprobe schätzt. Da die wahre Verteilung der Population unbekannt ist, ziehen wir Rückschlüsse aus den Daten. Der zentrale Grenzwertsatz hilft, die Verteilung dieser Schätzer zu verstehen.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Interferenz</span>"
    ]
  },
  {
    "objectID": "statistics/interference_basics.html#sec-statistics-pointestimates",
    "href": "statistics/interference_basics.html#sec-statistics-pointestimates",
    "title": "5  Interferenz",
    "section": "",
    "text": "5.1.1 Beispiel: Bernoulli-Verteilung\nBetrachten wir eine Zufallsvariable \\(X \\sim \\text{Bernoulli}(p)\\) mit unbekanntem Parameter \\(p\\). Ein naheliegender Schätzer für \\(p\\) ist der Anteil der Einsen in einer Stichprobe, \\(\\hat{p}\\), berechnet als:\n\\[\\hat{p} = \\frac{1}{n} \\sum_{i=1}^n X_i,\\]\nwobei \\(n\\) die Stichprobengröße ist und \\(X_i\\) die Beobachtungen (0 oder 1). Dies ist zudem der Mittelwert der Stichprobe, wenn wir Erfolge als 1 und Misserfolge als 0 kodieren.\n\n\n\n\n\n\nMaximum-Likelihood-Schätzung (MLE)\n\n\n\n\n\nUm \\(\\hat{p}\\) systematisch zu bestimmen, nutzen wir die Maximum-Likelihood-Schätzung:\n1. Likelihood-Funktion: Für \\(n\\) unabhängige Bernoulli-Variablen \\(X_1, \\ldots, X_n\\) mit \\(k = \\sum_{i=1}^n X_i\\) Einsen lautet die Likelihood:\n\\[L(p) = p^k (1-p)^{n-k}.\\]\n2. Log-Likelihood: Der Logarithmus vereinfacht die Maximierung:\n\\[\\log L(p) = k \\log p + (n - k) \\log (1 - p).\\]\n3. Maximierung: Ableitung nach \\(p\\) und Nullsetzen:\n\\[\\frac{d}{dp} \\log L(p) = \\frac{k}{p} - \\frac{n - k}{1 - p} = 0.\\]\nLösung: \\(k(1 - p) = p(n - k)\\), also \\(\\hat{p} = \\frac{k}{n}\\).\nFazit: Der MLE-Schätzer \\(\\hat{p}\\) ist der Stichprobenanteil, da er die beobachteten Daten am wahrscheinlichsten macht.\n\n\n\n\n\n\n\n\n\nZentraler Grenzwertsatz für Schätzer\n\n\n\nBei großer Stichprobengröße \\(n\\) und unabhängigen, identisch verteilten (i.i.d.) Ziehungen ist \\(\\hat{p}\\) annähernd normalverteilt:\n\\[\\hat{p} \\sim \\mathcal{N}\\left(p, \\frac{p(1-p)}{n}\\right),\\]\nwobei:\n- \\(\\mu_{\\hat{p}} = p\\) (erwartungstreu, d. h., im Mittel korrekt), ergibt sich aus der Maximum-Likelihood-Schätzung,\n- \\(\\sigma_{\\hat{p}} = \\sqrt{\\frac{p(1-p)}{n}}\\) (Standardfehler, SE).\nVoraussetzung: \\(np \\geq 10\\) und \\(n(1-p) \\geq 10\\), damit die Normalapproximation gilt.\n\n\n\n\n\n\n\n\nHerleitung der Varianz\n\n\n\n\n\nDie Varianz einer Zufallsvariablen \\(X\\) ist definiert als:\n\\[ \\text{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2] \\]\nDies beschreibt, wie weit die Werte der Zufallsvariablen \\(X\\) im Durchschnitt von ihrem Erwartungswert \\(\\mathbb{E}[X]\\) abweichen.\nRechenregeln für die Varianz\n\nVarianz einer konstanten Zahl:\nFür eine Konstante \\(c\\) gilt:\n\\[ \\text{Var}(c) = 0 \\]\nVarianz einer linearen Kombination von Zufallsvariablen:\nWenn \\(X\\) und \\(Y\\) zwei Zufallsvariablen sind und \\(a, b\\) Konstanten, dann gilt: \\[ \\text{Var}(aX + bY) = a^2 \\text{Var}(X) + b^2 \\text{Var}(Y) + 2ab \\, \\text{Cov}(X, Y) \\]\nWenn \\(X\\) und \\(Y\\) unabhängig sind, fällt der Kovarianzterm weg, sodass: \\[ \\text{Var}(aX + bY) = a^2 \\text{Var}(X) + b^2 \\text{Var}(Y) \\]\n\nHerleitung der Varianz für den Fall von \\(\\hat{p}\\)\nSei \\(X_1, X_2, \\dots, X_n\\) eine Stichprobe von unabhängigen Zufallsvariablen, wobei jede \\(X_i\\) der Bernoulli-Verteilung mit Erfolgswahrscheinlichkeit \\(p\\) folgt. Der Mittelwert der Stichprobe \\(\\hat{p}\\) ist:\n\\[ \\hat{p} = \\frac{1}{n} \\sum_{i=1}^{n} X_i \\]\nDa die \\(X_i\\) unabhängig sind, gilt für die Varianz von \\(\\hat{p}\\) die Rechenregel für die Varianz der Summe unabhängiger Zufallsvariablen:\n\\[ \\text{Var}(\\hat{p}) = \\text{Var}\\left( \\frac{1}{n} \\sum_{i=1}^{n} X_i \\right) \\]\nDa die \\(X_i\\) identisch verteilt sind, haben sie die gleiche Varianz \\(\\text{Var}(X_i) = p(1 - p)\\). Es folgt:\n\\[ \\text{Var}(\\hat{p}) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\text{Var}(X_i) = \\frac{1}{n^2} \\cdot n \\cdot p(1 - p) = \\frac{p(1 - p)}{n} \\]\nDamit haben wir die Formel für die Varianz von \\(\\hat{p}\\):\n\\[ \\text{Var}(\\hat{p}) = \\frac{p(1 - p)}{n} \\]\nDiese Varianz beschreibt, wie stark der geschätzte Anteil \\(\\hat{p}\\) von der tatsächlichen Erfolgswahrscheinlichkeit \\(p\\) abweichen kann.\n\n\n\n\n\n5.1.2 Simulation: Schätzer-Verteilung\nWir illustrieren dies mit einer Grundgesamtheit \\(X \\sim \\text{Bernoulli}(0.3)\\), ziehen \\(N = 1000\\) Werte, nehmen Stichproben der Größe \\(n = 100\\) und wiederholen dies \\(m = 100\\) Mal. Der Schätzer \\(\\hat{p}\\) wird als Anteil der Einsen berechnet. Figure 5.1 zeigt die Verteilung von \\(\\hat{p}\\).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\nnp.random.seed(42)\np = 0.3  # Wahrer Parameter\nN = 1000  # Grundgesamtheit\nn = 100   # Stichprobengröße\nm = 100   # Anzahl der Wiederholungen\n\nresults = np.zeros(m)\npopulation = np.random.binomial(1, p, N)  # Grundgesamtheit einmal ziehen\n\nfor i in range(m):\n    sample = np.random.choice(population, n, replace=False)  # Stichprobe ohne Zurücklegen\n    results[i] = np.mean(sample)  # Schätzer \\hat{p}\n\n# Plot\nplt.hist(results, bins=15, density=True, color='skyblue', alpha=0.7, label='$\\hat{p}$')\nmu, sigma = norm.fit(results)\nx = np.linspace(min(results), max(results), 100)\ny = norm.pdf(x, mu, sigma)\nplt.plot(x, y, 'r-', label=f'Normal ($\\mu={mu:.3f}$, $\\sigma={sigma:.3f}$)')\nplt.xlabel('Schätzer $\\hat{p}$')\nplt.ylabel('Dichte')\nplt.title(f'Verteilung von $\\hat{{p}}$ (p = {p}, n = {n})')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\nprint(f'Mittelwert von $\\hat{{p}}$: {np.mean(results):.3f}')\nprint(f'Standardfehler von $\\hat{{p}}$: {np.std(results):.3f}')\nprint(f'Theoretischer SE: {np.sqrt(p * (1 - p) / n):.3f}')\n\n&lt;&gt;:19: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n&lt;&gt;:23: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:23: SyntaxWarning:\n\ninvalid escape sequence '\\s'\n\n&lt;&gt;:24: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n&lt;&gt;:26: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n&lt;&gt;:31: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n&lt;&gt;:32: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n&lt;&gt;:19: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n&lt;&gt;:23: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:23: SyntaxWarning:\n\ninvalid escape sequence '\\s'\n\n&lt;&gt;:24: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n&lt;&gt;:26: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n&lt;&gt;:31: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n&lt;&gt;:32: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n/tmp/ipykernel_3201/4128398071.py:19: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n/tmp/ipykernel_3201/4128398071.py:23: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n/tmp/ipykernel_3201/4128398071.py:23: SyntaxWarning:\n\ninvalid escape sequence '\\s'\n\n/tmp/ipykernel_3201/4128398071.py:24: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n/tmp/ipykernel_3201/4128398071.py:26: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n/tmp/ipykernel_3201/4128398071.py:31: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n/tmp/ipykernel_3201/4128398071.py:32: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n\n\n\n\n\n\n\n\nFigure 5.1: Verteilung des Schätzers \\(\\hat{p}\\) für eine Bernoulli-Verteilung (p = 0.3, n = 100, m = 100).\n\n\n\n\n\nMittelwert von $\\hat{p}$: 0.285\nStandardfehler von $\\hat{p}$: 0.040\nTheoretischer SE: 0.046\n\n\nDie simulierten Werte für \\(\\hat{p}\\) stimmen gut mit den theoretischen Erwartungen überein:\n- \\(\\mu_{\\hat{p}} = p = 0.3\\),\n- \\(\\text{SE}_{\\hat{p}} = \\sigma_{\\hat{p}} = \\sqrt{\\frac{p(1-p)}{n}} = \\sqrt{\\frac{0.3 \\cdot 0.7}{100}} \\approx 0.046.\\)\nDies bestätigt die Normalapproximation durch den zentralen Grenzwertsatz.\n\n\n5.1.3 Konfidenzintervalle\nEin Konfidenzintervall (KI) gibt ein Intervall an, das den wahren Wert eines Parameters mit einer bestimmten Wahrscheinlichkeit (dem Konfidenzniveau \\(1-\\alpha\\)) enthält. Bei einer Normalverteilung liegen etwa 95 % der Werte innerhalb von zwei Standardabweichungen. Dies haben wir z.B. bei der Betrachung von IQ-Werten gesehen, dort liegen 95 % der Werte innerhalb von zwei Standardabweichungen (\\(2 \\sigma = 2 \\cdot 15\\)) um den Mittelwert (\\(\\mu = 100\\)).\n\n\n\n\n\n\nDefinition eines Konfidenzintervalls\n\n\n\nFür einen Schätzer \\(\\hat{\\theta}\\) eines Parameters \\(\\theta\\) ist das Konfidenzintervall \\(\\text{CI}_{1-\\alpha}\\):\n\\[\\left(\\hat{\\theta} - z_{\\alpha/2} \\cdot \\frac{\\sigma}{\\sqrt{n}}, \\hat{\\theta} + z_{\\alpha/2} \\cdot \\frac{\\sigma}{\\sqrt{n}}\\right),\\]\nwobei:\n- \\(\\hat{\\theta}\\) der Punktschätzer (z. B. Stichprobenmittelwert),\n- \\(\\sigma\\) die Standardabweichung der Grundgesamtheit,\n- \\(n\\) die Stichprobengröße,\n- \\(z_{\\alpha/2}\\) das \\(\\alpha/2\\)-Quantil der Standardnormalverteilung.\nFalls \\(\\sigma\\) unbekannt ist, wird die Stichprobenstandardabweichung \\(s\\) verwendet:\n\\[\\text{SE} = \\frac{s}{\\sqrt{n}}.\\]\n\n\n\n\n\n\n\n\nHerleitung des Intervalls\n\n\n\n\nStandardisierung: Für eine normalverteilte Zufallsvariable \\(\\hat{\\theta} \\sim \\mathcal{N}(\\theta, \\sigma^2/n)\\) wird \\(\\frac{\\hat{\\theta} - \\theta}{\\sigma/\\sqrt{n}} \\sim \\mathcal{N}(0, 1)\\).\n\nKonfidenzniveau: Für \\(1-\\alpha = 0.95\\) liegt 95 % der Fläche zwischen \\(-z_{\\alpha/2}\\) und \\(z_{\\alpha/2}\\). Bei \\(\\alpha = 0.05\\) ist \\(z_{\\alpha/2} = 1.96\\) (da \\(P(Z \\leq 1.96) \\approx 0.975\\)).\n\nIntervall: Umstellen ergibt: \\(P\\left(\\hat{\\theta} - z_{\\alpha/2} \\cdot \\frac{\\sigma}{\\sqrt{n}} \\leq \\theta \\leq \\hat{\\theta} + z_{\\alpha/2} \\cdot \\frac{\\sigma}{\\sqrt{n}}\\right) = 1-\\alpha\\).\n\n\n\n\n\n5.1.4 Beispiel: Konfidenzintervall für \\(p\\)\nFür unsere Bernoulli-Verteilung ist \\(\\hat{p}\\) normalverteilt mit \\(\\mu_{\\hat{p}} = p\\) und Standardfehler \\(\\text{SE}_{\\hat{p}} = \\sqrt{\\frac{p(1-p)}{n}}.\\) Da \\(p\\) unbekannt ist, nutzen wir \\(\\hat{p}\\) zur Schätzung:\n\\[\\text{CI}_{0.95} = \\left(\\hat{p} - z_{\\alpha/2} \\cdot \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}, \\hat{p} + z_{\\alpha/2} \\cdot \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\right).\\]\nFür ein 95 %-Konfidenzniveau (\\(\\alpha = 0.05\\)) ist \\(z_{\\alpha/2} = 1.96\\), was wir aus Tabellen oder mit norm.ppf berechnen können.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\nnp.random.seed(42)\np = 0.3\nn = 70\nsample = np.random.binomial(1, p, n)  # Eine Stichprobe\np_hat = np.mean(sample)  # Schätzer\n\n# Konfidenzintervall\nalpha = 0.05\nz = norm.ppf(1 - alpha/2)  # z-Wert für 95%\nse = np.sqrt(p_hat * (1 - p_hat) / n)  # Standardfehler mit \\hat{p}\nci_lower = p_hat - z * se\nci_upper = p_hat + z * se\n\nprint(f'z_{{alpha/2}}: {z:.2f}')\nprint(f'\\hat{{p}}: {p_hat:.3f}')\nprint(f'95%-Konfidenzintervall: [{ci_lower:.3f}, {ci_upper:.3f}]')\n\n# Visualisierung\nx = np.linspace(0, 0.6, 1000)\ny = norm.pdf(x, p_hat, se)\nplt.plot(x, y, 'r-', label=f'Normal ($\\mu={p_hat:.3f}$, SE={se:.3f})')\nplt.axvline(p_hat, color='blue', linestyle='-', label='$\\hat{p}$')\nplt.axvline(ci_lower, color='green', linestyle='--', label='Untere Grenze')\nplt.axvline(ci_upper, color='green', linestyle='--', label='Obere Grenze')\nplt.fill_between(x, y, where=(x &gt;= ci_lower) & (x &lt;= ci_upper), color='skyblue', alpha=0.3, label='95%-KI')\nplt.xlabel('Wert von $p$')\nplt.ylabel('Dichte')\nplt.title('Konfidenzintervall für $\\hat{p}$')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\nz_{alpha/2}: 1.96\n\\hat{p}: 0.271\n95%-Konfidenzintervall: [0.167, 0.376]\n\n\n&lt;&gt;:19: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n&lt;&gt;:25: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:26: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n&lt;&gt;:32: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n&lt;&gt;:19: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n&lt;&gt;:25: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:26: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n&lt;&gt;:32: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n/tmp/ipykernel_3201/1564904913.py:19: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n/tmp/ipykernel_3201/1564904913.py:25: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n/tmp/ipykernel_3201/1564904913.py:26: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n/tmp/ipykernel_3201/1564904913.py:32: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n\n\n\n\n\n\n\n\nFigure 5.2: 95%-Konfidenzintervall für \\(\\hat{p}\\) aus einer Stichprobe (p = 0.3, n = 100).\n\n\n\n\n\nFür unser Beispiel mit \\(\\hat{p} = 0.3\\), \\(n = 70\\) und einem 95 %-Konfidenzniveau (\\(\\alpha = 0.05\\), \\(z_{\\alpha/2} = 1.96\\)) ergibt sich das Konfidenzintervall:\n\\[\\left(0.271 - 1.96 \\cdot \\sqrt{\\frac{0.271 \\cdot 0.729}{70}}, 0.271 + 1.96 \\cdot \\sqrt{\\frac{0.271 \\cdot 0.729}{70}}\\right) \\approx (0.167, 0.376).\\]\nDas bedeutet, wir sind zu 95 % sicher, dass der wahre Wert von \\(p\\) zwischen 0.167 und 0.376 liegt.\n\n\n\n\n\n\nFragen zur Anpassung\n\n\n\n\nWelchen Faktor können wir ändern, um das Konfidenzintervall zu verkleinern?\n\nWie sieht das Konfidenzintervall für ein 99 %-Niveau aus?\n\n\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\nVerkleinerung des Intervalls: Die Breite des Konfidenzintervalls hängt von \\(\\frac{\\sigma}{\\sqrt{n}}\\) ab. Eine größere Stichprobengröße \\(n\\) reduziert den Standardfehler \\(\\sqrt{\\frac{p(1-p)}{n}}\\), wodurch das Intervall schmaler wird.\n\n99 %-Konfidenzniveau: Bei \\(\\alpha = 0.01\\) ist \\(z_{\\alpha/2} = 2.58\\) (da \\(P(Z \\leq 2.58) \\approx 0.995\\)). Das Intervall wird:\n\\[\\left(0.271 - 2.58 \\cdot \\sqrt{\\frac{0.271 \\cdot 0.729}{70}}, 0.271 + 2.58 \\cdot \\sqrt{\\frac{0.271 \\cdot 0.729}{70}}\\right) \\approx (0.134, 0.408).\\]\nEin höheres Konfidenzniveau verbreitert das Intervall, da mehr Sicherheit gefordert wird.\n\n\n\n\n\n\n5.1.5 Darstellung des Konfidenzintervalls\nFigure 5.3 zeigt die Konfidenzintervalle für \\(p\\) bei 95 % und 99 % Konfidenzniveau, basierend auf \\(\\hat{p} = 0.3\\) und \\(n = 100\\). Dies verdeutlicht, wie sich das Intervall mit dem Konfidenzniveau ändert – ein nützliches Konzept, um Unsicherheiten in Simulationen wie im Tutorial zu quantifizieren.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Parameter\nmu = 0.3  # \\hat{p}\nsigma = np.sqrt(mu * (1 - mu) / 100)  # SE\nalphas = [0.05, 0.01]  # 95% und 99% Konfidenzniveau\ncolors = ['skyblue', 'lightgreen']\n\n# Normalverteilung\nx = np.linspace(0.1, 0.5, 1000)\ny = norm.pdf(x, mu, sigma)\nplt.plot(x, y, 'r-', label=f'Normal ($\\mu={mu}$, SE={sigma:.3f})')\n\n# Konfidenzintervalle\nfor i, alpha in enumerate(alphas):\n    z = norm.ppf(1 - alpha/2)\n    lower = mu - z * sigma\n    upper = mu + z * sigma\n    print(f'Konfidenzintervall für {int((1-alpha)*100)}%: ({lower:.2f}, {upper:.2f})')\n    plt.fill_between(x, y, where=(x &gt;= lower) & (x &lt;= upper), color=colors[i], alpha=0.3, label=f'{int((1-alpha)*100)}%-KI')\n    plt.axvline(lower, color=colors[i], linestyle='--', alpha=0.5)\n    plt.axvline(upper, color=colors[i], linestyle='--', alpha=0.5)\n\nplt.axvline(mu, color='blue', linestyle='-', label='$\\hat{p}$')\nplt.xlabel('Wert von $p$')\nplt.ylabel('Dichte')\nplt.title('Konfidenzintervalle für $\\hat{p}$')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\nKonfidenzintervall für 95%: (0.21, 0.39)\nKonfidenzintervall für 99%: (0.18, 0.42)\n\n\n&lt;&gt;:14: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:26: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n&lt;&gt;:29: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n&lt;&gt;:14: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:26: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n&lt;&gt;:29: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n/tmp/ipykernel_3201/1622860018.py:14: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n/tmp/ipykernel_3201/1622860018.py:26: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n/tmp/ipykernel_3201/1622860018.py:29: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n\n\n\n\n\n\n\n\nFigure 5.3: Konfidenzintervalle für \\(\\hat{p}\\) bei 95% und 99% (p = 0.3, n = 100).\n\n\n\n\n\n\n\n5.1.6 Punktschätzer und Konfidenzintervalle für andere Fälle\nDie Prinzipien für Schätzer und Konfidenzintervalle lassen sich auf andere Verteilungen und Parameter (z. B. Mittelwert, Varianz) übertragen. Entscheidend ist, die Verteilung des Schätzers zu kennen und entsprechende Formeln anzuwenden.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Interferenz</span>"
    ]
  },
  {
    "objectID": "statistics/interference_basics.html#sec-statistics-hypothesistests",
    "href": "statistics/interference_basics.html#sec-statistics-hypothesistests",
    "title": "5  Interferenz",
    "section": "5.2 Hypothesentests",
    "text": "5.2 Hypothesentests\nEin Hypothesentest ist ein Verfahren, um auf Basis einer Stichprobe zu entscheiden, ob eine Hypothese über eine Population verworfen wird. Wir stellen eine Nullhypothese \\(H_0\\) auf (z. B. „ein Parameter hat einen bestimmten Wert“), die wir widerlegen wollen, und eine Alternativhypothese \\(H_1\\), die wir unterstützen möchten.\n\n\n\n\n\n\nDialektik und Falsifikationismus\n\n\n\nDieser Ansatz mag ungewohnt erscheinen: Warum etwas aufstellen, um es zu widerlegen? Er wurzelt in der wissenschaftlichen Methode:\n- Dialektik: Eine These wird durch eine Antithese geprüft, die Synthese führt näher zur Wahrheit (Wikipedia: Dialektik).\n- Falsifikationismus: Karl Popper betonte, dass Thesen nicht beweisbar, sondern nur widerlegbar sind. Eine nicht widerlegte Hypothese bleibt vorläufig gültig, ist aber nicht endgültig bewiesen (Wikipedia: Falsifikationismus).\nIn der Statistik nutzen wir diesen Ansatz, um Hypothesen systematisch zu prüfen.\n\n\n\n5.2.1 Ablauf eines Hypothesentests\n\nHypothesen formulieren: \\(H_0\\) (zu widerlegen) und \\(H_1\\) (zu unterstützen).\n\nSignifikanzniveau wählen: \\(\\alpha\\) ist die Wahrscheinlichkeit, \\(H_0\\) fälschlicherweise abzulehnen (z. B. 0.05).\n\nTeststatistik berechnen: Bestimme \\(t\\) und dessen Verteilung unter \\(H_0\\).\n\nEntscheidung treffen: Vergleiche \\(t\\) mit einem kritischen Wert oder berechne einen p-Wert.\n\n\n\n5.2.2 Entscheidungsfehler\nBetrachten wir ein Beispiel: Ein medizinischer Test soll Krebs erkennen. Die Realität (Krebs oder kein Krebs) und das Testergebnis (positiv oder negativ) können abweichen. Das Diagramm zeigt die Wahrscheinlichkeiten:\n\n\n\n\n\ngraph LR\n    U[Person] --&gt;|0.001| A[Krebs]\n    U[Person] --&gt;|0.999| D[Kein Krebs]\n    A[Krebs] --&gt;|0.99| B[Positiv]\n    A[Krebs] --&gt;|0.01| C[Negativ]\n    D[Kein Krebs] --&gt;|0.05| E[Positiv]\n    D[Kein Krebs] --&gt;|0.95| F[Negativ]\n\n\n\n\n\n\nZiel: Personen mit Krebs identifizieren. Der schlimmste Fall wäre, Krebs zu übersehen. Wir möchten daher die Wahrscheinlichkeit maximieren, Krebs korrekt zu erkennen.\nDaraus folgt:\n- Nullhypothese \\(H_0\\): „Die Person hat keinen Krebs“ (zu widerlegen),\n- Alternativhypothese \\(H_1\\): „Die Person hat Krebs“ (zu unterstützen).\nBei einem Hypothesentest können zwei Fehler auftreten:\n- Typ-I-Fehler (\\(\\alpha\\)): \\(H_0\\) wird abgelehnt, obwohl sie wahr ist (falsch positiv, z. B. „Krebs“ trotz „kein Krebs“).\n- Typ-II-Fehler (\\(\\beta\\)): \\(H_0\\) wird beibehalten, obwohl sie falsch ist (falsch negativ, z. B. „kein Krebs“ trotz „Krebs“).\n\nTabelle der Fehlertypen\n\n\n\n\n\n\n\n\nEntscheidung über die Nullhypothese\nNullhypothese (H0) ist\n\n\nWahr\nFalsch\n\n\nEntscheidung über die\nNullhypothese (H0)\nNicht verwerfen\nKorrekte Entscheidung\n(wahr negativ, wirklich gesund)\n(Wahrscheinlichkeit = 1-α)\nTyp-II-Fehler\n(falsch negativ, hat eigentlich Krebs)\n(Wahrscheinlichkeit = β)\n\n\nVerwerfen\nTyp-I-Fehler\n(falsch positiv, ist eigentlich gesund)\n(Wahrscheinlichkeit = α)\nKorrekte Entscheidung\n(wahr positiv, hat wirklich Krebs)\n(Wahrscheinlichkeit = 1-β)\n\n\n\n\n\n\n\n\n\nSignifikanzniveau und Fehlerprioritäten\n\n\n\nIm Beispiel ist der Typ-II-Fehler (\\(\\beta\\)) besonders kritisch, da Krebs übersehen schwerwiegender ist als eine falsche Diagnose. Dennoch steuern wir die Ablehnung von \\(H_0\\) mit dem Signifikanzniveau \\(\\alpha\\), der Wahrscheinlichkeit eines Typ-I-Fehlers (falsch positiv). \\(\\alpha\\) wird vorab festgelegt, typischerweise 5 % (0.05) oder 1 % (0.01). Ein kleineres \\(\\alpha\\) reduziert das Risiko, \\(H_0\\) fälschlich zu verwerfen (z. B. unnötige Behandlungen), erhöht aber die Chance eines Typ-II-Fehlers.\n\n\n\n\n5.2.3 Beispiel: Hypothesentest für den Mittelwert einer Normalverteilung\nWir prüfen, ob Mechatronik-Studierende intelligenter sind als der Bevölkerungsdurchschnitt (IQ = 100). Angenommen, ihr IQ ist normalverteilt mit \\(\\mu = 110\\) und \\(\\sigma = 15\\), also \\(X \\sim \\mathcal{N}(110, 15^2)\\). Mit einer Stichprobe von \\(n = 100\\) Studierenden testen wir, ob ihr Mittelwert signifikant von 100 abweicht – ein Ansatz, der im Tutorial z. B. für Komponenten-Lebensdauern nützlich ist.\n\n5.2.3.1 Hypothesen und Teststatistik\n\n\\(H_0\\): \\(\\mu = 100\\) (kein Unterschied zum Durchschnitt),\n\n\\(H_1\\): \\(\\mu \\neq 100\\) (zweiseitiger Test, da „abweicht“ keine Richtung vorgibt).\nDer Schätzer ist \\(\\hat{\\mu} = \\bar{X}\\), und unter \\(H_0\\) gilt:\n\\[\\bar{X} \\sim \\mathcal{N}\\left(100, \\frac{\\sigma^2}{n}\\right) = \\mathcal{N}\\left(100, \\frac{15^2}{100}\\right).\\]\nDie Teststatistik lautet:\n\\[z = \\frac{\\bar{X} - \\mu_0}{\\sigma / \\sqrt{n}},\\]\nmit \\(\\mu_0 = 100\\), \\(\\sigma = 15\\), \\(n = 100\\). Bei \\(\\alpha = 0.05\\) ist der kritische Wert \\(z_{\\alpha/2} = 1.96\\).\n\n\n\n5.2.3.2 Simulation und Visualisierung\nFigure 5.4 zeigt die Stichprobenverteilung und die Nullhypothese.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\nnp.random.seed(42)\nmu_true = 110  # Wahrer Mittelwert (Simulation)\nsigma = 15     # Bekannte Standardabweichung\nn = 100        # Stichprobengröße\nmu_0 = 100     # Nullhypothese\n\n# Stichprobe\nX = np.random.normal(mu_true, sigma, n)\nX_bar = np.mean(X)  # Schätzer für Mittelwert\n\n# Teststatistik\nse = sigma / np.sqrt(n)  # Standardfehler\nz_stat = (X_bar - mu_0) / se\np_value = 2 * (1 - norm.cdf(abs(z_stat)))  # Zweiseitiger p-Wert\n\n# Plot\nx = np.linspace(90, 130, 1000)\ny_null = norm.pdf(x, mu_0, se)  # Verteilung unter H_0\ny_sample = norm.pdf(x, X_bar, se)  # Verteilung der Stichprobe\nplt.plot(x, y_null, 'r--', label=f'$H_0$ ($\\mu={mu_0}$)')\nplt.plot(x, y_sample, 'b-', label=f'Stichprobe ($\\hat{{\\mu}}={X_bar:.1f}$)')\nplt.axvline(mu_0, color='red', linestyle='--')\nplt.axvline(X_bar, color='blue', linestyle='-')\nplt.fill_between(x, y_null, where=(x &lt;= mu_0 - 1.96*se) | (x &gt;= mu_0 + 1.96*se), color='red', alpha=0.2, label=r'Ablehnungsbereich ($\\alpha=0.05$)')\nplt.xlabel('Mittelwert (IQ)')\nplt.ylabel('Dichte')\nplt.title('Hypothesentest: IQ von Mechatronik-Studierenden')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\nprint(f'Stichprobenmittelwert: {X_bar:.2f}')\nprint(f'Teststatistik z: {z_stat:.2f}')\nprint(f'p-Wert: {p_value:.4f}')\n\n&lt;&gt;:24: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:25: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n&lt;&gt;:25: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:24: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:25: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n&lt;&gt;:25: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n/tmp/ipykernel_3201/1135806841.py:24: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n/tmp/ipykernel_3201/1135806841.py:25: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n/tmp/ipykernel_3201/1135806841.py:25: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n\n\n\n\n\n\n\n\nFigure 5.4: Hypothesentest für den Mittelwert von Mechatronik-Studierenden (n = 100).\n\n\n\n\n\nStichprobenmittelwert: 108.44\nTeststatistik z: 5.63\np-Wert: 0.0000\n\n\n\n\n5.2.3.3 Interpretation\nAn ?fig-sec-statistics-hypothesistests-2 sehen wir, dass der Stichprobenmittelwert \\(\\hat{\\mu}\\) deutlich über \\(\\mu_0 = 100\\) liegt. Der grüne Bereich (95 %-Konfidenzintervall) schließt 100 nicht ein, und der p-Wert ist &lt; 0.05. Das deutet darauf hin, dass die Wahrscheinlichkeit für \\(\\mu = 100\\) sehr gering ist. Wir lehnen \\(H_0\\) ab und schließen, dass der Mittelwert der Mechatronik-Studierenden signifikant von 100 abweicht.\n\n\n\n5.2.4 \\(t\\)-Verteilung\nBei großen Stichproben (\\(n \\geq 30\\)) approximiert der zentrale Grenzwertsatz den Stichprobenmittelwert \\(\\bar{X}\\) durch eine Normalverteilung, besonders wenn \\(\\sigma\\) bekannt ist. Für kleine Stichproben (\\(n &lt; 30\\)) oder unbekannte Varianz ist die \\(t\\)-Verteilung (Wikipedia: Studentsche t-Verteilung) besser geeignet, da sie die Unsicherheit der geschätzten Standardabweichung einbezieht.\nDie Teststatistik der \\(t\\)-Verteilung ist:\n\\[t = \\frac{\\bar{X} - \\mu_0}{\\frac{S}{\\sqrt{n}}},\\]\nwobei:\n- \\(\\bar{X}\\): Stichprobenmittelwert,\n- \\(\\mu_0\\): Wert unter \\(H_0\\),\n- \\(S\\): Stichprobenstandardabweichung (\\(S = \\sqrt{\\frac{1}{n-1} \\sum (X_i - \\bar{X})^2}\\)),\n- \\(n\\): Stichprobengröße.\nDie Form der \\(t\\)-Verteilung hängt von den Freiheitsgraden \\(df = n - 1\\) ab: Bei kleinem \\(n\\) ist sie breiter (mehr Streuung), bei großem \\(n\\) nähert sie sich der Normalverteilung. Im Tutorial könnte die \\(t\\)-Verteilung bei kleinen Stichproben von Ausfallzeiten hilfreich sein, wenn \\(\\sigma\\) unbekannt ist.\n\n\n\n\n\n\nHerkunft der \\(t\\)-Verteilung\n\n\n\nDie \\(t\\)-Verteilung stammt von William Sealy Gosset, einem Statistiker bei der Guinness-Brauerei. Er entwickelte sie, um die Bierqualität mit kleinen Stichproben zu prüfen, und veröffentlichte 1908 unter dem Pseudonym „Student“ (Original: Student’s t). Daher heißt sie „Student’s \\(t\\)-Verteilung“.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Interferenz</span>"
    ]
  },
  {
    "objectID": "statistics/interference_advanced.html",
    "href": "statistics/interference_advanced.html",
    "title": "6  Tests",
    "section": "",
    "text": "6.1 T-Test\nNach dem zuvor beschriebenen Prinzip der Hypothesentests gibt es verschiedene Tests, die auf unterschiedliche Fragestellungen zugeschnitten sind. In diesem Abschnitt werden einige dieser Tests vorgestellt.\nWir werden uns hier auf einige typische Tests konzentrieren. Es gibt noch viele weitere Tests, die auf spezielle Fragestellungen zugeschnitten sind. Die hier vorgestellten Tests gehören jedoch zu den wichtigsten und werden in der Praxis häufig verwendet.\nDer t-Test ist eine Klasse von statistischen Tests, die verwendet werden, um Hypothesen über den Mittelwert (oder die Differenz von Mittelwerten) einer oder zweier Stichproben zu prüfen, insbesondere wenn die Standardabweichung der Grundgesamtheit unbekannt ist und aus der Stichprobe geschätzt werden muss.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tests</span>"
    ]
  },
  {
    "objectID": "statistics/interference_advanced.html#t-test",
    "href": "statistics/interference_advanced.html#t-test",
    "title": "6  Tests",
    "section": "",
    "text": "6.1.1 One-Sample Student’s T-Test (Einstichproben-t-Test)\nBeim Einstichproben-t-Test wird der Mittelwert einer Stichprobe mit einem vorgegebenen, hypothetischen Wert (\\(\\mu_0\\)) verglichen. Der Test wird typischerweise verwendet, wenn die Varianz der Grundgesamtheit unbekannt ist und aus der Stichprobe geschätzt wird. Die t-Verteilung berücksichtigt die zusätzliche Unsicherheit, die durch diese Schätzung entsteht.\nWir legen zunächst den Typ-I-Fehler (Signifikanzniveau \\(\\alpha\\)) fest. Damit definieren wir die maximale Wahrscheinlichkeit, die wir zu akzeptieren bereit sind, die Nullhypothese (\\(H_0\\)) fälschlicherweise abzulehnen, obwohl sie wahr ist (\\(P(\\text{Ablehnung } H_0 | H_0 \\text{ ist wahr}) \\leq \\alpha\\)).\nUm zu beurteilen, wie wahrscheinlich die beobachtete Stichprobe unter der Annahme ist, dass die Nullhypothese wahr ist, wird die Teststatistik \\(t\\) berechnet. Diese misst den Unterschied zwischen dem Stichprobenmittelwert \\(\\bar{x}\\) und dem hypothetischen Wert \\(\\mu_0\\) in Einheiten des Standardfehlers des Mittelwerts. Wörtlich bedeutet die Teststatistik: Um wie viele Standardfehler weicht der Stichprobenmittelwert \\(\\bar{x}\\) vom vorgegebenen Wert \\(\\mu_0\\) ab?\n\\[\nt = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}},\n\\]\nwobei \\(\\bar{x}\\) der Stichprobenmittelwert, \\(s\\) die Stichprobenstandardabweichung (oft wird die korrigierte Stichprobenstandardabweichung verwendet) und \\(n\\) die Stichprobengröße ist.\nUnter der Annahme, dass die Nullhypothese wahr ist und die Daten aus einer normalverteilten Grundgesamtheit stammen, folgt die Teststatistik \\(t\\) einer t-Verteilung mit \\(n-1\\) Freiheitsgraden.\nDie Entscheidung über die Ablehnung der Nullhypothese hängt von der Alternativhypothese (\\(H_1\\)) ab:\n\nBei \\(H_1: \\mu &gt; \\mu_0\\) (rechtseitiger Test) lehnen wir \\(H_0\\) ab, wenn \\(t &gt; t_{1-\\alpha, n-1}\\).\nBei \\(H_1: \\mu &lt; \\mu_0\\) (linksseitiger Test) lehnen wir \\(H_0\\) ab, wenn \\(t &lt; t_{\\alpha, n-1}\\) (was \\(t &lt; -t_{1-\\alpha, n-1}\\) entspricht).\nBei \\(H_1: \\mu \\neq \\mu_0\\) (zweiseitiger Test) lehnen wir \\(H_0\\) ab, wenn \\(|t| &gt; t_{1-\\alpha/2, n-1}\\).\n\nDer kritische Wert \\(t_{\\text{krit}}\\) (z.B. \\(t_{1-\\alpha, n-1}\\) oder \\(t_{1-\\alpha/2, n-1}\\)) wird aus der Tabelle der t-Verteilung oder mittels Software bestimmt.\n\n6.1.1.1 Beispiel - Einseitig: Eiweißgehalt von Braugerste\nEine Brauerei bezieht eine neue Charge Gerste. Für den Brauprozess ist es wichtig, dass der mittlere Eiweißgehalt der Gerste einen bestimmten Grenzwert nicht überschreitet, da ein zu hoher Eiweißgehalt zu unerwünscht starkem Schäumen des Bieres führen kann. Der maximal akzeptable mittlere Eiweißgehalt liegt laut Spezifikation bei \\(\\mu_0 = 11.5\\%\\).\nDie Brauerei möchte prüfen, ob die neue Charge Gerste diese Spezifikation einhält oder ob der mittlere Eiweißgehalt signifikant höher ist. Dazu wird eine Stichprobe von \\(n=50\\) Körnern aus der Charge entnommen und deren Eiweißgehalt analysiert.\n\nimport numpy as np\nfrom scipy.stats import t as t_dist, norm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Annahmen für die Simulation (wären in der Realität unbekannt)\n# Nehmen wir an, die Charge ist tatsächlich leicht über dem Limit\ntrue_mu_barley = 11.8  # Tatsächlicher mittlerer Eiweißgehalt der Charge (%)\ntrue_sigma_barley = 0.5 # Tatsächliche Standardabweichung des Eiweißgehalts (%)\n\n# Nullhypothese (Grenzwert)\nmu_0_barley = 11.5\n\n# Stichprobe ziehen\nn_barley = 50\n# Setze einen Seed für Reproduzierbarkeit\nnp.random.seed(2024)\nX_barley = np.random.normal(true_mu_barley, true_sigma_barley, n_barley)\n\n# Schätzer aus der Stichprobe berechnen\nx_bar_barley = np.mean(X_barley)\ns_barley = np.std(X_barley, ddof=1) # Korrigierte Stichprobenstandardabweichung\n\nprint(f\"Stichprobenmittelwert (x̄): {x_bar_barley:.2f}%\")\nprint(f\"Stichprobenstandardabweichung (s): {s_barley:.2f}%\")\nprint(f\"Stichprobengröße (n): {n_barley}\")\n\n# Plot der Stichprobendaten und der Nullhypothese\nplt.figure(figsize=(10, 6))\nsns.histplot(X_barley, bins=8, kde=False, alpha=0.6, label='Stichprobe Eiweißgehalt', color='darkorange', edgecolor='black')\nplt.axvline(mu_0_barley, color='red', linestyle='--', linewidth=2, label=f'Grenzwert H₀: μ ≤ {mu_0_barley}%')\nplt.axvline(x_bar_barley, color='blue', linestyle='-', linewidth=2, label=f'Stichprobenmittelwert x̄ = {x_bar_barley:.2f}%')\nplt.title('Verteilung des Eiweißgehalts der Gerstenprobe und Grenzwert')\nplt.xlabel('Eiweißgehalt (%)')\nplt.ylabel('Häufigkeit')\nplt.legend()\nplt.grid(axis='y', alpha=0.5)\nplt.show()\n\nStichprobenmittelwert (x̄): 11.80%\nStichprobenstandardabweichung (s): 0.48%\nStichprobengröße (n): 50\n\n\n\n\n\n\n\n\nFigure 6.1: Verteilung des Eiweißgehalts in der Stichprobe und der maximal zulässige Mittelwert (Nullhypothese).\n\n\n\n\n\nDurchführung des Hypothesentests:\n\nDefinition der Hypothesen:\n\n\\(H_0: \\mu \\leq 11.5\\%\\) (Der mittlere Eiweißgehalt der Charge liegt bei oder unter dem Grenzwert). Wir verwenden für die Berechnung den Grenzfall \\(H_0: \\mu = 11.5\\%\\).\n\\(H_1: \\mu &gt; 11.5\\%\\) (Der mittlere Eiweißgehalt der Charge ist höher als der Grenzwert). Wir wählen einen einseitigen Test (rechtsseitig), da uns nur eine Überschreitung des Grenzwerts Sorgen bereitet.\n\nDefinition des Signifikanzniveaus:\n\nDie Brauerei legt \\(\\alpha = 0.05\\) fest.\nDas Risiko, eine Charge fälschlicherweise als zu eiweißreich abzulehnen, obwohl sie den Grenzwert einhält (\\(H_0\\) wahr ist), soll maximal 5% betragen.\n\nBerechnung der Teststatistik \\(t\\):\n\n\\(t = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}\\)\n\n\n# Berechnung der Teststatistik für Gerste\nstandard_error_barley = s_barley / np.sqrt(n_barley)\nt_statistic_barley = (x_bar_barley - mu_0_barley) / standard_error_barley\nprint(f'Standardfehler (s/√n): {standard_error_barley:.4f}')\nprint(f'Teststatistik (t): {t_statistic_barley:.3f}')\n\nStandardfehler (s/√n): 0.0685\nTeststatistik (t): 4.381\n\n\n\nDer berechnete Wert ist \\(t \\approx 4.011\\).\n\nBestimmung des kritischen Wertes \\(t_{\\text{krit}}\\):\n\nWir führen einen einseitigen Test (rechtsseitig) mit \\(\\alpha = 0.05\\) und \\(n-1 = 50-1 = 49\\) Freiheitsgraden durch. Wir suchen den Wert \\(t_{1-\\alpha, n-1} = t_{0.95, 49}\\).\nAus der t-Verteilungstabelle oder mit Software erhalten wir \\(t_{\\text{krit}} \\approx 1.677\\).\n\n\nfrom scipy.stats import t as t_dist\n\nalpha_barley = 0.05\ndf_barley = n_barley - 1 # Freiheitsgrade\nt_critical_barley = t_dist.ppf(1 - alpha_barley, df_barley)\nprint(f'Freiheitsgrade (df): {df_barley}')\nprint(f'Kritischer Wert (t_krit) für α={alpha_barley} (einseitig): {t_critical_barley:.3f}')\n\nFreiheitsgrade (df): 49\nKritischer Wert (t_krit) für α=0.05 (einseitig): 1.677\n\n\nDie Visualisierung der t-Verteilung mit dem Ablehnungsbereich hilft, die Entscheidung zu verstehen:\n\nx_t_barley = np.linspace(t_dist.ppf(0.0001, df_barley), t_dist.ppf(0.9999, df_barley), 500)\ny_t_barley = t_dist.pdf(x_t_barley, df_barley)\n\nplt.figure(figsize=(10, 6))\nplt.plot(x_t_barley, y_t_barley, label=f't-Verteilung (df={df_barley})')\nplt.fill_between(x_t_barley, 0, y_t_barley, where=(x_t_barley &gt; t_critical_barley), color='red', alpha=0.5, label=f'Ablehnungsbereich (α={alpha_barley})')\nplt.axvline(t_statistic_barley, color='black', linestyle='--', label=f'Teststatistik t = {t_statistic_barley:.3f}')\nplt.title('t-Verteilung, kritischer Wert und Teststatistik (Gersten-Eiweißgehalt)')\nplt.xlabel('t-Wert')\nplt.ylabel('Dichte')\nplt.legend()\nplt.grid(alpha=0.5)\nplt.show()\n\n\n\n\n\n\n\nFigure 6.2: t-Verteilung mit 49 Freiheitsgraden und Ablehnungsbereich für den einseitigen Gerstentest (α=0.05).\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nZur Erinnerung: Die t-Verteilung beim Testen\nDie t-Verteilung beschreibt, wie sich die Teststatistik \\(T = \\frac{\\bar{X} - \\mu_0}{S / \\sqrt{n}}\\) verteilen würde, wenn die Nullhypothese (\\(H_0: \\mu = \\mu_0\\)) wahr wäre und die Daten aus einer normalverteilten Grundgesamtheit stammen. Wir vergleichen unseren aus der Stichprobe berechneten Wert \\(t\\) mit dieser theoretischen Verteilung, um zu sehen, ob er ein plausibler Wert unter \\(H_0\\) ist oder eher ein extremer, unwahrscheinlicher Wert, der gegen \\(H_0\\) spricht.\n\n\nEntscheidung:\n\nWir vergleichen die Teststatistik \\(t\\) mit dem kritischen Wert \\(t_{\\text{krit}}\\). Regel: Lehne \\(H_0\\) ab, wenn \\(t &gt; t_{\\text{krit}}\\).\nDa \\(t \\approx 4.011 &gt; 1.677 \\approx t_{\\text{krit}}\\), lehnen wir die Nullhypothese \\(H_0\\) ab.\nInterpretation: Das Ergebnis ist statistisch signifikant auf dem 5%-Niveau. Es gibt ausreichende Evidenz aus der Stichprobe, um zu schlussfolgern, dass der mittlere Eiweißgehalt dieser Gerstencharge signifikant über dem Grenzwert von 11.5% liegt. Die Brauerei sollte diese Charge möglicherweise nicht verwenden oder entsprechend behandeln, um Probleme mit dem Schaum zu vermeiden.\np-Wert: Berechnung der Wahrscheinlichkeit, unter \\(H_0\\) eine Teststatistik zu beobachten, die mindestens so extrem ist wie \\(t \\approx 4.011\\).\n\n\n\np_value_barley = 1 - t_dist.cdf(t_statistic_barley, df_barley)\nprint(f'p-Wert: {p_value_barley:.3e}') # Formatierung in wissenschaftlicher Notation\n# Vergleich mit alpha\nif p_value_barley &lt; alpha_barley:\n    print(f\"Da p-Wert ({p_value_barley:.3e}) &lt; α ({alpha_barley}), wird H₀ abgelehnt.\")\nelse:\n    print(f\"Da p-Wert ({p_value_barley:.3e}) &gt;= α ({alpha_barley}), wird H₀ nicht abgelehnt.\")\n\np-Wert: 3.110e-05\nDa p-Wert (3.110e-05) &lt; α (0.05), wird H₀ abgelehnt.\n\n\n\nDer p-Wert ist sehr klein (\\(p \\approx 9.98 \\times 10^{-5}\\)). Das bedeutet, es ist extrem unwahrscheinlich, einen Stichprobenmittelwert wie den beobachteten (oder einen noch höheren) zu erhalten, wenn der wahre mittlere Eiweißgehalt der Charge tatsächlich nur 11.5% (oder weniger) wäre. Da \\(p &lt; \\alpha\\), wird \\(H_0\\) abgelehnt.\n\n\n\n\n\n\n\nImportant\n\n\n\nKonsequenz der Testentscheidung\nIn diesem Beispiel führt die Ablehnung der Nullhypothese zu der Schlussfolgerung, dass der Eiweißgehalt wahrscheinlich zu hoch ist. Dies hat praktische Konsequenzen für die Brauerei (z.B. Ablehnung der Charge, Anpassung des Brauprozesses). Wäre die Nullhypothese nicht abgelehnt worden (\\(t \\leq t_{krit}\\) oder \\(p &gt; \\alpha\\)), hätte die Brauerei keinen statistischen Grund gehabt, die Charge aufgrund des Eiweißgehalts abzulehnen (basierend auf dieser Stichprobe und dem gewählten Signifikanzniveau).\n\n\n\n\n\n\n\n\nImportant\n\n\n\nDer p-Wert\nDer p-Wert ist die Wahrscheinlichkeit, unter Annahme der Gültigkeit der Nullhypothese (\\(H_0\\)), ein Ergebnis zu erhalten, das mindestens so extrem ist wie das in der Stichprobe beobachtete Ergebnis (repräsentiert durch die Teststatistik).\n\nEin kleiner p-Wert (typischerweise \\(p \\leq \\alpha\\)) deutet darauf hin, dass das beobachtete Ergebnis unter \\(H_0\\) sehr unwahrscheinlich ist. Dies liefert Evidenz gegen \\(H_0\\) und führt zur Ablehnung von \\(H_0\\).\nEin großer p-Wert (\\(p &gt; \\alpha\\)) bedeutet, dass das beobachtete Ergebnis unter \\(H_0\\) durchaus plausibel ist. Es gibt keine ausreichende Evidenz, um \\(H_0\\) abzulehnen.\n\nDer p-Wert ist nicht die Wahrscheinlichkeit, dass \\(H_0\\) wahr ist.\n\n\n\n\n6.1.1.2 Aufgabe - Zweiseitig: Eiweißgehalt von Braugerste\nEine Brauerei prüft, ob der Eiweißgehalt einer neuen Gerstencharge im optimalen Bereich von 10.5% liegt. Ein zu hoher oder zu niedriger Eiweißgehalt kann die Bierqualität beeinträchtigen. Führen Sie einen zweiseitigen t-Test durch, um zu testen, ob der mittlere Eiweißgehalt der Charge signifikant von 10.5% abweicht. Verwenden Sie eine Stichprobe von \\(n=50\\) Körnern mit den folgenden Daten:\n\nStichprobenmittelwert: \\(\\bar{x} = 10.47\\%\\)\nStichprobenstandardabweichung: \\(s = 0.49\\%\\)\nSignifikanzniveau: \\(\\alpha = 0.05\\)\n\nHypothesen:\n\n\\(H_0: \\mu = 10.5\\%\\) (Der Eiweißgehalt entspricht dem Zielwert).\n\\(H_1: \\mu \\neq 10.5\\%\\) (Der Eiweißgehalt weicht ab).\n\nEntscheidungsregel:\n\nZweiseitig: \\(H_0\\) wird abgelehnt, wenn \\(|t| &gt; t_{1-\\alpha/2, n-1}\\).\nKritischer Wert \\(t_{\\text{krit}}\\) aus einer t-Verteilungstabelle oder Software.\n\nBerechnen Sie die Teststatistik \\(t\\), den kritischen Wert \\(t_{\\text{krit}}\\) und den p-Wert. Entscheiden Sie, ob \\(H_0\\) abgelehnt wird, und interpretieren Sie das Ergebnis.\n\n\n\n\n\n\nMusterlösung\n\n\n\n\n\n6.1.1.2.1 Musterlösung\nSchritt 1: Gegebene Werte\n\n\\(\\bar{x} = 10.47\\%\\), \\(s = 0.49\\%\\), \\(n = 50\\), \\(\\mu_0 = 10.5\\%\\), \\(\\alpha = 0.05\\).\n\nSchritt 2: Teststatistik berechnen Die Formel für die Teststatistik ist:\n\\[\nt = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}\n\\]\nStandardfehler: \\(s / \\sqrt{n} = 0.49 / \\sqrt{50} \\approx 0.0693\\)\nTeststatistik:\n\\[\nt = \\frac{10.47 - 10.5}{0.0693} \\approx -0.433\n\\]\n\nimport numpy as np\nx_bar = 10.47\ns = 0.49\nn = 50\nmu_0 = 10.5\nstandard_error = s / np.sqrt(n)\nt_statistic = (x_bar - mu_0) / standard_error\nprint(f\"Standardfehler: {standard_error:.4f}\")\nprint(f\"Teststatistik t: {t_statistic:.3f}\")\n\nStandardfehler: 0.0693\nTeststatistik t: -0.433\n\n\nSchritt 3: Kritischer Wert\nFreiheitsgrade: \\(df = n - 1 = 49\\). Für einen zweiseitigen Test mit \\(\\alpha = 0.05\\) ist der kritische Wert \\(t_{1-\\alpha/2, 49} = t_{0.975, 49} \\approx 2.010\\) (aus t-Tabelle oder Software).\n\nfrom scipy.stats import t as t_dist\nalpha = 0.05\ndf = n - 1\nt_critical = t_dist.ppf(1 - alpha/2, df)\nprint(f\"Kritischer Wert t_krit: ±{t_critical:.3f}\")\n\nKritischer Wert t_krit: ±2.010\n\n\nSchritt 4: p-Wert\nDer p-Wert für einen zweiseitigen Test ist: \\(p = 2 \\cdot P(T \\geq |t|)\\).\n\np_value = 2 * (1 - t_dist.cdf(abs(t_statistic), df))\nprint(f\"p-Wert: {p_value:.3f}\")\n\np-Wert: 0.667\n\n\nErgebnis: \\(p \\approx 0.667\\).\nSchritt 5: Entscheidung\n\nVergleich: \\(|t| \\approx 0.433 &lt; 2.010 \\approx t_{\\text{krit}}\\), daher wird \\(H_0\\) nicht abgelehnt.\nAlternativ: \\(p \\approx 0.667 &gt; 0.05\\), bestätigt die Nicht-Ablehnung.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import t as t_dist\n\n# Gegebene Werte\nx_bar = 10.47\ns = 0.49\nn = 50\nmu_0 = 10.5\nalpha = 0.05\n\n# Berechnung der Teststatistik\nstandard_error = s / np.sqrt(n)\nt_statistic = (x_bar - mu_0) / standard_error\ndf = n - 1  # Freiheitsgrade\n\n# Kritischer Wert für zweiseitigen Test\nt_critical = t_dist.ppf(1 - alpha/2, df)\n\n# Daten für die t-Verteilung\nx = np.linspace(-4, 4, 1000)\ny = t_dist.pdf(x, df)\n\n# Plot der t-Verteilung\nplt.figure(figsize=(10, 6))\nplt.plot(x, y, label=f't-Verteilung (df={df})', color='blue')\n\n# Schattierte Ablehnungsbereiche\nplt.fill_between(x, y, where=(x &lt;= -t_critical), color='red', alpha=0.3, label='Ablehnungsbereich')\nplt.fill_between(x, y, where=(x &gt;= t_critical), color='red', alpha=0.3)\n\n# Teststatistik markieren\nplt.axvline(t_statistic, color='green', linestyle='--', label=f'Teststatistik t = {t_statistic:.3f}')\nplt.axvline(-t_critical, color='black', linestyle=':', label=f'Kritischer Wert ±{t_critical:.3f}')\nplt.axvline(t_critical, color='black', linestyle=':')\n\n# Beschriftungen\nplt.title('t-Test: Teststatistik und Ablehnungsbereich')\nplt.xlabel('t-Wert')\nplt.ylabel('Dichte')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Plot anzeigen\nplt.show()\n\n# Ausgabe der berechneten Werte\nprint(f\"Standardfehler: {standard_error:.4f}\")\nprint(f\"Teststatistik t: {t_statistic:.3f}\")\nprint(f\"Kritischer Wert t_krit: ±{t_critical:.3f}\")\n\n\n\n\n\n\n\nFigure 6.3: t-Verteilung mit 49 Freiheitsgraden und Ablehnungsbereich für den zweiseitigen Gerstentest (α=0.05).\n\n\n\n\n\nStandardfehler: 0.0693\nTeststatistik t: -0.433\nKritischer Wert t_krit: ±2.010\n\n\nInterpretation\nEs gibt keinen statistischen Hinweis, dass der Eiweißgehalt der Gerstencharge signifikant von 10.5% abweicht. Die Charge liegt im optimalen Bereich, und die Brauerei kann sie verwenden, ohne Anpassungen vornehmen zu müssen.\n\n\n\n\n\n\n6.1.2 Zwei-Stichproben-t-Test (Independent Samples t-Test)\nHäufig ist man daran interessiert, ob sich die Mittelwerte zweier unabhängiger Stichproben signifikant voneinander unterscheiden. Zum Beispiel könnten wir vergleichen, ob sich die mittlere Zugfestigkeit von Stählen zweier verschiedener Lieferanten unterscheidet. Auch hierfür kann ein t-Test eingesetzt werden (vgl. Abb. Figure 6.4).\n\n\n\n\n\n\nFigure 6.4: Grundidee des Zwei-Stichproben-t-Tests: Vergleich der Mittelwerte zweier Verteilungen. Quelle: Inductiveload (n.d.)\n\n\n\nVoraussetzungen:\n\nDie beiden Stichproben sind unabhängig voneinander.\nDie Daten in beiden Stichproben stammen aus normalverteilten Grundgesamtheiten. (Der Test ist robust gegenüber Verletzungen dieser Annahme bei ausreichender Stichprobengröße, ca. n &gt; 30 pro Gruppe).\nDie Varianzen der beiden Grundgesamtheiten sind gleich (\\(\\sigma_1^2 = \\sigma_2^2\\)). Diese Annahme kann mit Tests wie dem Levene-Test überprüft werden. Wenn die Varianzen ungleich sind, wird eine Variante des t-Tests namens Welch-Test verwendet, der die Freiheitsgrade anpasst.\n\nHypothesen (Standardfall: Test auf Gleichheit der Mittelwerte):\n\n\\(H_0: \\mu_1 = \\mu_2\\) (oder äquivalent \\(H_0: \\mu_1 - \\mu_2 = 0\\))\n\\(H_1: \\mu_1 \\neq \\mu_2\\) (zweiseitig), oder \\(H_1: \\mu_1 &gt; \\mu_2\\) (rechtsseitig), oder \\(H_1: \\mu_1 &lt; \\mu_2\\) (linksseitig)\n\nTeststatistik (bei gleichen Varianzen):\n\nBerechne die gepoolte (kombinierte) Varianz \\(s_p^2\\) als gewichteten Durchschnitt der beiden Stichprobenvarianzen: \\[\ns_p^2 = \\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}\n\\] wobei \\(n_1, n_2\\) die Stichprobengrößen und \\(s_1^2, s_2^2\\) die (korrigierten) Stichprobenvarianzen sind. \\(s_p = \\sqrt{s_p^2}\\) ist die gepoolte Standardabweichung.\n\n\n\n\n\n\n\nHerleitung der Formel für die gepoolte Varianz\n\n\n\n\n\nDie gepoolte Varianz ist eine gewichtete Mittelung der beiden Stichprobenvarianzen, wobei die Gewichte den Freiheitsgraden der Stichproben entsprechen.\nSchritt 1: Definition der Stichprobenvarianz\nFür Stichprobe 1:\n\\[\ns_1^2 = \\frac{1}{n_1 - 1} \\sum_{i=1}^{n_1} (x_{1i} - \\bar{x}_1)^2\n\\]\nFür Stichprobe 2:\n\\[\ns_2^2 = \\frac{1}{n_2 - 1} \\sum_{i=1}^{n_2} (x_{2i} - \\bar{x}_2)^2\n\\]\nSchritt 2: Umstellen\nMultiplizieren mit den Freiheitsgraden:\n\\[\n(n_1 - 1) s_1^2 = \\sum_{i=1}^{n_1} (x_{1i} - \\bar{x}_1)^2\n\\]\n\\[\n(n_2 - 1) s_2^2 = \\sum_{i=1}^{n_2} (x_{2i} - \\bar{x}_2)^2\n\\]\nSchritt 3: Gesamtstreuung kombinieren\nDie Gesamtquadratsumme (Summe der quadrierten Abweichungen innerhalb beider Gruppen):\n\\[\nSS_{\\text{total}} = \\sum_{i=1}^{n_1} (x_{1i} - \\bar{x}_1)^2 + \\sum_{i=1}^{n_2} (x_{2i} - \\bar{x}_2)^2 = (n_1 - 1) s_1^2 + (n_2 - 1) s_2^2\n\\]\nSchritt 4: Pooled Variance als Mittelwert\nDie gepoolte Varianz ist diese Gesamtstreuung, geteilt durch die Gesamtanzahl der Freiheitsgrade:\n\\[\ns_p^2 = \\frac{SS_{\\text{total}}}{n_1 + n_2 - 2} = \\frac{(n_1 - 1) s_1^2 + (n_2 - 1) s_2^2}{n_1 + n_2 - 2}\n\\]\n\n\n\n\nBerechne die Teststatistik \\(t\\): \\[\nt = \\frac{(\\bar{x}_1 - \\bar{x}_2) - (\\mu_1 - \\mu_2)_0}{s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\n\\] Für den Standardtest auf Gleichheit ist \\((\\mu_1 - \\mu_2)_0 = 0\\), also vereinfacht sich die Formel zu: \\[\nt = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\n\\]\n\nFreiheitsgrade und Entscheidung:\n\nDie Teststatistik folgt unter \\(H_0\\) einer t-Verteilung mit \\(df = n_1 + n_2 - 2\\) Freiheitsgraden.\nBestimme den kritischen Wert \\(t_{\\text{krit}}\\) basierend auf \\(\\alpha\\), den Freiheitsgraden \\(df\\) und der Alternativhypothese (ein- oder zweiseitig, z.B. \\(t_{1-\\alpha/2, df}\\) für zweiseitig).\nVergleiche \\(t\\) mit \\(t_{\\text{krit}}\\). Bei einem zweiseitigen Test: Lehne \\(H_0\\) ab, wenn \\(|t| &gt; t_{\\text{krit}}\\).\nAlternativ: Berechne den p-Wert und vergleiche ihn mit \\(\\alpha\\). Lehne \\(H_0\\) ab, wenn \\(p \\leq \\alpha\\).\n\n\n\n\n\n\n\nNote\n\n\n\nTest auf eine spezifische Differenz der Erwartungswerte\nStatt auf Gleichheit der Mittelwerte (\\(\\mu_1 - \\mu_2 = 0\\)) zu testen, könnte man auch prüfen, ob sich die Mittelwerte um einen bestimmten Betrag \\(\\omega_0\\) unterscheiden. Die Nullhypothese wäre dann \\(H_0: \\mu_1 - \\mu_2 = \\omega_0\\). Die Teststatistik \\(t\\) wird dann wie folgt berechnet: \\[\nt = \\frac{(\\bar{x}_1 - \\bar{x}_2) - \\omega_0}{s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\n\\] Die Freiheitsgrade der t-Verteilung bleiben \\(n_1 + n_2 - 2\\) (bei Varianzgleichheit).\n\n\n\n\n6.1.3 Zwei-Stichproben-t-Test (Paired Samples t-Test)\nIn manchen Fällen sind die beiden Stichproben nicht unabhängig, sondern gepaart. Das klassische Beispiel ist eine Messung vor und nach einer Behandlung an denselben Untersuchungseinheiten (z.B. Patienten, Bauteilen). Hier interessiert uns nicht der Unterschied der Mittelwerte unabhängiger Gruppen, sondern der mittlere Unterschied innerhalb der Paare.\nBeispiel: Ein Verfahren zum Härten eines metallischen Bauteils soll untersucht werden. Im Experiment wird der Härtegrad von \\(n=10\\) Bauteilen jeweils vor und nach der Behandlung gemessen.\nTabelle: Härtegrad in HR (Rockwell) vor und nach der Behandlung\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBauteil\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nVorher\n49.1\n49.2\n49.3\n49.4\n49.5\n49.6\n49.7\n49.8\n49.0\n50.0\n\n\nNachher\n50.2\n50.3\n50.3\n50.2\n50.7\n50.7\n50.8\n50.9\n51.0\n51.1\n\n\nDifferenz (Nachher - Vorher)\n1.1\n1.1\n1.0\n0.8\n1.2\n1.1\n1.1\n1.1\n2.0\n1.1\n\n\n\nVorgehen:\n\nBerechne die Differenzen \\(d_i = \\text{Wert}_{\\text{nachher}, i} - \\text{Wert}_{\\text{vorher}, i}\\) für jedes Paar \\(i\\).\nBehandle diese Differenzen \\(d_1, d_2, ..., d_n\\) als eine einzelne Stichprobe.\nFühre einen Einstichproben-t-Test für diese Differenzen durch, um zu prüfen, ob der mittlere Unterschied \\(\\mu_d\\) signifikant von einem bestimmten Wert (meistens 0) abweicht.\n\nHypothesen (Test auf signifikante Erhöhung):\n\n\\(H_0: \\mu_d = 0\\) (Die Behandlung hat im Mittel keinen Effekt auf die Härte.)\n\\(H_1: \\mu_d &gt; 0\\) (Die Behandlung erhöht im Mittel den Härtegrad.)\n\nBerechnungen:\n\nBerechne den Mittelwert der Differenzen \\(\\bar{d}\\) und die Standardabweichung der Differenzen \\(s_d\\).\n\n\nimport numpy as np\nfrom scipy.stats import t as t_dist\n\nvorher = np.array([49.1, 49.2, 49.3, 49.4, 49.5, 49.6, 49.7, 49.8, 49.0, 50.0])\nnachher = np.array([50.2, 50.3, 50.3, 50.2, 50.7, 50.7, 50.8, 50.9, 51.0, 51.1])\nd = nachher - vorher\nprint(\"Differenzen (d):\", d)\n\nd_bar = np.mean(d)\ns_d = np.std(d, ddof=1) # Korrigierte Standardabweichung\nn = len(d)\nprint(f'Mittlere Differenz (d̄): {d_bar:.3f}')\nprint(f'Standardabweichung der Differenzen (s_d): {s_d:.3f}')\nprint(f'Anzahl Paare (n): {n}')\n\n# Teststatistik berechnen (H0: mu_d = 0)\nmu_d0 = 0\nt_stat_paired = (d_bar - mu_d0) / (s_d / np.sqrt(n))\nprint(f'Teststatistik (t): {t_stat_paired:.3f}')\n\nDifferenzen (d): [1.1 1.1 1.  0.8 1.2 1.1 1.1 1.1 2.  1.1]\nMittlere Differenz (d̄): 1.160\nStandardabweichung der Differenzen (s_d): 0.313\nAnzahl Paare (n): 10\nTeststatistik (t): 11.705\n\n\n\nDer mittlere Unterschied beträgt \\(\\bar{d} \\approx 1.16\\) und die Standardabweichung \\(s_d \\approx 0.317\\). Die Teststatistik ist \\(t \\approx 11.58\\).\n\nTestdurchführung:\n\nVoraussetzungen: Die Differenzen sollten annähernd normalverteilt sein (oder \\(n\\) groß genug). Die Messungen sind gepaart.\nHypothesen: \\(H_0: \\mu_d = 0\\), \\(H_1: \\mu_d &gt; 0\\).\nSignifikanzniveau: \\(\\alpha = 0.05\\).\nTeststatistik: \\(t = \\frac{\\bar{d} - 0}{s_d / \\sqrt{n}} \\approx 11.58\\).\nKritischer Wert: Für einen einseitigen Test mit \\(\\alpha = 0.05\\) und \\(df = n-1 = 10-1 = 9\\) Freiheitsgraden ist \\(t_{\\text{krit}} = t_{0.95, 9}\\).\n\ndf_paired = n - 1\nalpha_paired = 0.05\nt_crit_paired = t_dist.ppf(1 - alpha_paired, df_paired)\nprint(f'Freiheitsgrade (df): {df_paired}')\nprint(f'Kritischer Wert (t_krit) für α={alpha_paired} (einseitig): {t_crit_paired:.3f}')\n\nFreiheitsgrade (df): 9\nKritischer Wert (t_krit) für α=0.05 (einseitig): 1.833\n\n\nDer kritische Wert ist \\(t_{\\text{krit}} \\approx 1.833\\).\nEntscheidung: Da \\(t \\approx 11.58 &gt; 1.833 \\approx t_{\\text{krit}}\\), lehnen wir \\(H_0\\) ab.\n\nInterpretation: Es gibt starke statistische Evidenz dafür, dass die Behandlung den Härtegrad der Bauteile signifikant erhöht.\np-Wert:\n\n\n\np_value_paired = 1 - t_dist.cdf(t_stat_paired, df_paired)\nprint(f'p-Wert (einseitig): {p_value_paired:.3e}')\nif p_value_paired &lt; alpha_paired:\n    print(f\"Da p-Wert ({p_value_paired:.3e}) &lt; α ({alpha_paired}), wird H₀ abgelehnt.\")\nelse:\n      print(f\"Da p-Wert ({p_value_paired:.3e}) &gt;= α ({alpha_paired}), wird H₀ nicht abgelehnt.\")\n\np-Wert (einseitig): 4.760e-07\nDa p-Wert (4.760e-07) &lt; α (0.05), wird H₀ abgelehnt.\n\n\nDer p-Wert ist extrem klein ($p \\approx 1.35 \\times 10^{-6}$), was die Ablehnung von $H_0$ bestätigt.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tests</span>"
    ]
  },
  {
    "objectID": "statistics/interference_advanced.html#chi-quadrat-test-ein-nicht-parametrischer-test",
    "href": "statistics/interference_advanced.html#chi-quadrat-test-ein-nicht-parametrischer-test",
    "title": "6  Tests",
    "section": "6.2 Chi-Quadrat-Test: Ein nicht-parametrischer Test",
    "text": "6.2 Chi-Quadrat-Test: Ein nicht-parametrischer Test\nDer Chi-Quadrat-Test ist ein nicht-parametrischer Test, der Hypothesen über kategoriale Daten prüft, ohne Annahmen über die Verteilung der Grundgesamtheit zu machen. Er wird häufig als Anpassungstest (Goodness-of-Fit) verwendet, um zu testen, ob beobachtete Häufigkeiten in einer Stichprobe einer erwarteten Verteilung entsprechen.\nDie Teststatistik misst die Abweichung zwischen beobachteten (\\(O_i\\)) und erwarteten (\\(E_i\\)) Häufigkeiten:\n\\[    \n\\chi^2 = \\sum \\frac{(O_i - E_i)^2}{E_i}\n\\]\nUnter der Nullhypothese (\\(H_0\\)) folgt die Statistik einer Chi-Quadrat-Verteilung mit \\(k-1\\) Freiheitsgraden (\\(k\\) = Anzahl Kategorien). Ein hoher \\(\\chi^2\\)-Wert spricht gegen \\(H_0\\).\n\n6.2.1 Beispiel: Bierqualität in einer Brauerei\nEine Brauerei prüft, ob die Qualitätsbewertungen einer neuen Biercharge („Hervorragend“, „Gut“, „Mangelhaft“) der erwarteten Verteilung entsprechen, die auf langjährigen Produktionsdaten basiert (erwartete Anteile: Hervorragend: 60%, Gut: 30%, Mangelhaft: 10%). Stichprobe: \\(n=150\\) Flaschen.\nBeobachtete Häufigkeiten:\n\nHervorragend: 95\nGut: 45\nMangelhaft: 10\n\nErwartete Häufigkeiten:\n\nHervorragend: \\(150 \\times 0.60 = 90\\)\nGut: \\(150 \\times 0.30 = 45\\)\nMangelhaft: \\(150 \\times 0.10 = 15\\)\n\nHypothesen:\n\n\\(H_0\\): Die Qualitätsverteilung der neuen Biercharge entspricht der erwarteten Verteilung.\n\\(H_1\\): Die Qualitätsverteilung weicht von der erwarteten Verteilung ab.\n\nTestdurchführung:\n\nVoraussetzungen: Erwartete Häufigkeiten \\(E_i \\geq 5\\), Stichprobe zufällig.\nSignifikanzniveau: \\(\\alpha = 0.05\\).\nTeststatistik:\n\n\nimport numpy as np\nfrom scipy.stats import chi2\n\nO = np.array([95, 45, 10])\nE = np.array([90, 45, 15])\nchi2_statistic = np.sum((O - E)**2 / E)\nprint(f\"Chi-Quadrat Statistik: {chi2_statistic:.3f}\")\n\nChi-Quadrat Statistik: 1.944\n\n\nErgebnis: \\(\\chi^2 \\approx 1.667\\).\n\nKritischer Wert:\n\nFür \\(df = 3-1 = 2\\) und \\(\\alpha = 0.05\\) ist \\(\\chi^2_{\\text{krit}} \\approx 5.991\\).\n\ndf = len(O) - 1\nalpha = 0.05\nchi2_critical = chi2.ppf(1 - alpha, df)\nprint(f\"Kritischer Wert: {chi2_critical:.3f}\")\n\nKritischer Wert: 5.991\n\n\n\np-Wert:\n\n\np_value = 1 - chi2.cdf(chi2_statistic, df)\nprint(f\"p-Wert: {p_value:.3f}\")\n\np-Wert: 0.378\n\n\nErgebnis: \\(p \\approx 0.435\\).\n\nEntscheidung:\n\nDa \\(\\chi^2 \\approx 1.667 &lt; 5.991\\) und \\(p \\approx 0.435 &gt; 0.05\\), wird \\(H_0\\) nicht abgelehnt.\nInterpretation:\nEs gibt keinen statistischen Hinweis, dass die Qualitätsverteilung der neuen Biercharge signifikant von der erwarteten Verteilung abweicht. Die Brauerei kann die Charge als qualitativ gleichwertig betrachten.\n\n\n6.2.2 Visualisierung der Teststatistik\n\nimport matplotlib.pyplot as plt\n\n# Chi-Quadrat-Verteilung\nx = np.linspace(0, 10, 1000)\ny = chi2.pdf(x, df)\n\n# Plot\nplt.figure(figsize=(10, 6))\nplt.plot(x, y, label=f'Chi-Quadrat-Verteilung (df={df})', color='blue')\nplt.fill_between(x, y, where=(x &gt;= chi2_critical), color='red', alpha=0.3, label='Ablehnungsbereich')\nplt.axvline(chi2_statistic, color='green', linestyle='--', label=f'Teststatistik = {chi2_statistic:.3f}')\nplt.axvline(chi2_critical, color='black', linestyle=':', label=f'Kritischer Wert = {chi2_critical:.3f}')\n\n# Beschriftungen\nplt.title('Chi-Quadrat-Test: Teststatistik und Ablehnungsbereich')\nplt.xlabel('Chi-Quadrat-Wert')\nplt.ylabel('Dichte')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n6.2.3 Warum ist der Chi-Quadrat-Test nicht-parametrisch?\nDer Chi-Quadrat-Test macht keine Annahmen über die Verteilung der zugrunde liegenden Daten (z. B. Normalverteilung), sondern arbeitet direkt mit Häufigkeiten. Dies macht ihn besonders geeignet für kategoriale Daten, bei denen parametrische Annahmen wie Normalität oder gleiche Varianzen nicht zutreffen.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tests</span>"
    ]
  },
  {
    "objectID": "statistics/interference_advanced.html#übersicht-über-statistische-tests",
    "href": "statistics/interference_advanced.html#übersicht-über-statistische-tests",
    "title": "6  Tests",
    "section": "6.3 Übersicht über Statistische Tests",
    "text": "6.3 Übersicht über Statistische Tests\nStatistische Tests werden verwendet, um Hypothesen über eine Grundgesamtheit anhand einer Stichprobe zu prüfen. Sie bestehen aus einer Nullhypothese (\\(H_0\\)), die den Status quo repräsentiert, und einer Alternativhypothese (\\(H_1\\)), die angenommen wird, wenn \\(H_0\\) abgelehnt wird. Tests helfen, zu unterscheiden, ob die Ergebnisse einer Stichprobe auf die Grundgesamtheit übertragbar sind oder zufällig entstanden.\n\nParametrische Tests (z. B. t-Test) setzen spezifische Verteilungsannahmen voraus (z. B. Normalität).\nNicht-parametrische Tests (z. B. Chi-Quadrat-Test, Mann-Whitney-U-Test) sind flexibler und benötigen keine Verteilungsannahmen, eignen sich aber oft für ordinal oder kategoriale Daten.\n\nWeitere Details zu Testarten findest du in dieser Übersicht.\n\n\n\n\nInductiveload, based on original work by Jhguch. n.d. “Two Sample t-Test.” https://commons.wikimedia.org/wiki/File:Two_sample_ttest.svg.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tests</span>"
    ]
  },
  {
    "objectID": "statistics/tutorial_2.html",
    "href": "statistics/tutorial_2.html",
    "title": "Tutorial 2: Muster in Netzlasten verstehen",
    "section": "",
    "text": "Ziel\nWir arbeiten mit den Daten der Global Energy Forecasting Competition Hong, Pinson, and Fan (2014), die Sie zuvor bereinigt haben. Ziel ist es, Muster in den Netzlasten zu identifizieren, insbesondere in Bezug auf Wochentag-Effekte. Die Netzlast variiert je nach Wochentag, da sich das Verhalten von Haushalten und Industrien ändert. Wir wollen herausfinden, ob es Unterschiede in der Last zwischen Wochentagen und Wochenenden/Feiertagen gibt, was für spätere Prognosemodelle relevant ist.",
    "crumbs": [
      "Statistik",
      "Tutorial 2: Muster in Netzlasten verstehen"
    ]
  },
  {
    "objectID": "statistics/tutorial_2.html#ziel",
    "href": "statistics/tutorial_2.html#ziel",
    "title": "Tutorial 2: Muster in Netzlasten verstehen",
    "section": "",
    "text": "Aufgaben\n\nTrennung der Wochentage: Visualisieren Sie die Verteilung der Netzlast für jeden Wochentag (Montag bis Sonntag und Feiertag) in einem Violin-Plot, um die Verteilungen optisch zu vergleichen. Markieren Sie Wochenenden (Samstag, Sonntag) anders.\nGruppierung: Teilen Sie die Daten in zwei Gruppen: Wochentage (Montag bis Freitag) und Sonn-/Feiertage (Samstag, Sonntag sowie Feiertage). Vergleichen Sie die Mittelwerte der Netzlasten dieser Gruppen mit einem t-Test.\nOptionaler Teil: Vergleichen Sie die Verteilungen der beiden Gruppen mit dem Kolmogorov-Smirnov (KS)-Test und visualisieren Sie die kumulativen Verteilungsfunktionen (CDFs). Dieser Teil ist in einem aufklappbaren Callout beschrieben.\n\nWir verwenden ein Signifikanzniveau von \\(\\alpha = 0.05\\).",
    "crumbs": [
      "Statistik",
      "Tutorial 2: Muster in Netzlasten verstehen"
    ]
  },
  {
    "objectID": "statistics/tutorial_2.html#analyse-mit-simulierten-daten",
    "href": "statistics/tutorial_2.html#analyse-mit-simulierten-daten",
    "title": "Tutorial 2: Muster in Netzlasten verstehen",
    "section": "Analyse mit simulierten Daten",
    "text": "Analyse mit simulierten Daten\nDa keine echten Daten vorliegen, generieren wir simulierte Netzlast-Daten für jeden Wochentag sowie Feiertage. Die Daten sind normalverteilt mit unterschiedlichen Mittelwerten und Standardabweichungen, um realistische Unterschiede widerzuspiegeln.\n\nSchritt 1: Visualisierung der Wochentag-Verteilungen\nWir generieren Netzlast-Daten für jeden Wochentag und visualisieren die Verteilungen in einem Violin-Plot, wobei Wochenenden anders gefärbt sind.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n# Simulierte Daten generieren\nnp.random.seed(42)\ndays = ['Montag', 'Dienstag', 'Mittwoch', 'Donnerstag', 'Freitag', 'Samstag', 'Sonntag']\nn_samples = 100  # Pro Tag\nloads = []\nday_labels = []\n\nfor day in days:\n    if day in ['Samstag', 'Sonntag']:\n        # Wochenenden: Niedrigere Last\n        load = np.random.normal(90, 8, n_samples)\n    else:\n        # Wochentage: Höhere Last\n        load = np.random.normal(100, 10, n_samples)\n    loads.extend(load)\n    day_labels.extend([day] * n_samples)\n\n# Feiertage hinzufügen (ähnlich wie Wochenenden)\nholiday_load = np.random.normal(88, 9, 50)\nloads.extend(holiday_load)\nday_labels.extend(['Feiertag'] * 50)\n\n# DataFrame erstellen\ndata = pd.DataFrame({'Tag': day_labels, 'Last': loads})\n\n# Violin-Plot\nplt.figure(figsize=(12, 6))\nsns.violinplot(x='Tag', y='Last', data=data, palette={'Montag': 'blue', 'Dienstag': 'blue', \n                                                      'Mittwoch': 'blue', 'Donnerstag': 'blue', \n                                                      'Freitag': 'blue', 'Samstag': 'orange', \n                                                      'Sonntag': 'orange', 'Feiertag': 'orange'})\nplt.title('Verteilung der Netzlast nach Wochentag und Feiertagen')\nplt.xlabel('Tag')\nplt.ylabel('Netzlast (MW)')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n/tmp/ipykernel_3277/3092400038.py:33: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: Der Violin-Plot zeigt, dass die Netzlast an Wochentagen (Montag bis Freitag) tendenziell höher ist als an Wochenenden (Samstag, Sonntag) und Feiertagen. Die Verteilungen an Wochenenden und Feiertagen sind ähnlich und weisen eine geringere Variabilität auf.\n\n\nSchritt 2: t-Test für Mittelwerte (Wochentage vs. Sonn-/Feiertage)\nWir teilen die Daten in zwei Gruppen: - Wochentage: Montag bis Freitag. - Sonn-/Feiertage: Samstag, Sonntag und Feiertage.\nDann führen wir einen zweiseitigen t-Test durch, um die Mittelwerte der Netzlasten zu vergleichen.\n\nfrom scipy.stats import ttest_ind\n\n# Gruppen erstellen\nweekday_load = data[data['Tag'].isin(['Montag', 'Dienstag', 'Mittwoch', 'Donnerstag', 'Freitag'])]['Last']\nweekend_holiday_load = data[data['Tag'].isin(['Samstag', 'Sonntag', 'Feiertag'])]['Last']\n\n# t-Test\nt_statistic, p_value = ttest_ind(weekday_load, weekend_holiday_load, equal_var=False)  # Welch's t-Test\nprint(f\"t-Statistik: {t_statistic:.3f}\")\nprint(f\"p-Wert: {p_value:.3f}\")\n\n# Entscheidung\nalpha = 0.05\nif p_value &lt; alpha:\n    print(\"H_0 wird abgelehnt: Die Mittelwerte der Netzlast unterscheiden sich signifikant.\")\nelse:\n    print(\"H_0 wird nicht abgelehnt: Kein Hinweis auf unterschiedliche Mittelwerte.\")\n\nt-Statistik: 16.259\np-Wert: 0.000\nH_0 wird abgelehnt: Die Mittelwerte der Netzlast unterscheiden sich signifikant.\n\n\nErgebnis: Der p-Wert liegt typischerweise unter 0.05 (z. B. \\[   p \\approx 0.000   \\]), was darauf hindeutet, dass die Mittelwerte der Netzlast an Wochentagen und Sonn-/Feiertagen signifikant unterschiedlich sind. Die Netzlast ist an Wochentagen höher.\n\n\nSchritt 3: Optionaler Vergleich der Verteilungen mit dem KS-Test\n\n\n\n\n\n\nKolmogorov-Smirnov (KS)-Test: Erklärung und Anwendung\n\n\n\n\n\nDer Kolmogorov-Smirnov (KS)-Test ist ein nicht-parametrischer Test, der prüft, ob zwei Stichproben aus derselben Verteilung stammen. Er vergleicht die empirischen kumulativen Verteilungsfunktionen (CDFs) der beiden Stichproben.\n\nTeststatistik\nDie KS-Teststatistik \\(D\\) misst den maximalen Abstand zwischen den CDFs \\(F_1(x)\\) und \\(F_2(x)\\):\n\\[    \nD = \\sup_x |F_1(x) - F_2(x)|,\n\\]\nwobei \\(\\sup\\) der Supremum (größte Abstand) ist.\n\n\nHypothesen\n\n\\(H_0\\): Die Verteilungen der Netzlast an Wochentagen und Sonn-/Feiertagen sind gleich.\n\\(H_1\\): Die Verteilungen unterscheiden sich.\n\n\n\nAnwendung\nDer KS-Test ist nützlich, wenn keine Normalitätsannahmen getroffen werden können. Hier vergleichen wir die Verteilung der Netzlast an Wochentagen mit der an Sonn-/Feiertagen.\n\n\nPython-Implementierung und Visualisierung\nWir führen den KS-Test durch und visualisieren die CDFs der beiden Gruppen.\n\nfrom scipy.stats import ks_2samp\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# KS-Test\nstatistic, p_value = ks_2samp(weekday_load, weekend_holiday_load)\nprint(f\"KS-Statistik: {statistic:.3f}\")\nprint(f\"p-Wert: {p_value:.3f}\")\n\n# Entscheidung\nalpha = 0.05\nif p_value &lt; alpha:\n    print(\"H_0 wird abgelehnt: Die Verteilungen unterscheiden sich signifikant.\")\nelse:\n    print(\"H_0 wird nicht abgelehnt: Kein Hinweis auf unterschiedliche Verteilungen.\")\n\n# Kumulativer Verteilungsplot (CDF)\ndef plot_cdf(data, label, color):\n    sorted_data = np.sort(data)\n    y = np.arange(1, len(sorted_data) + 1) / len(sorted_data)\n    plt.plot(sorted_data, y, label=label, color=color)\n\nplt.figure(figsize=(10, 6))\nplot_cdf(weekday_load, 'Wochentage', 'blue')\nplot_cdf(weekend_holiday_load, 'Sonn-/Feiertage', 'orange')\nplt.title('Kumulative Verteilungsfunktionen (CDF) der Netzlast')\nplt.xlabel('Netzlast (MW)')\nplt.ylabel('Kumulative Wahrscheinlichkeit')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\nKS-Statistik: 0.476\np-Wert: 0.000\nH_0 wird abgelehnt: Die Verteilungen unterscheiden sich signifikant.\n\n\n\n\n\n\n\n\n\n\n\nInterpretation\nEin p-Wert \\(&lt; \\alpha = 0.05\\) zeigt, dass die Verteilungen der Netzlast an Wochentagen und Sonn-/Feiertagen unterschiedlich sind. Der CDF-Plot visualisiert diese Unterschiede: Die CDF für Wochentage liegt tendenziell rechts von der für Sonn-/Feiertage, was auf eine höhere Netzlast hinweist.\nErgebnis: Der p-Wert ist typischerweise klein (z. B. \\(p \\approx 0.000\\)), was auf signifikante Unterschiede in den Verteilungen hinweist. Der CDF-Plot bestätigt, dass die Netzlast an Wochentagen höher und anders verteilt ist als an Sonn-/Feiertagen.",
    "crumbs": [
      "Statistik",
      "Tutorial 2: Muster in Netzlasten verstehen"
    ]
  },
  {
    "objectID": "statistics/tutorial_2.html#fazit",
    "href": "statistics/tutorial_2.html#fazit",
    "title": "Tutorial 2: Muster in Netzlasten verstehen",
    "section": "Fazit",
    "text": "Fazit\n\nWochentag-Verteilungen: Der Violin-Plot zeigt, dass die Netzlast an Wochentagen höher ist als an Wochenenden und Feiertagen.\nt-Test: Der Mittelwert der Netzlast ist an Wochentagen signifikant höher als an Sonn-/Feiertagen (\\(p &lt; 0.05\\)).\nKS-Test (optional): Die Verteilungen der Netzlast unterscheiden sich signifikant zwischen Wochentagen und Sonn-/Feiertagen, wie der KS-Test und der CDF-Plot zeigen.\n\n\n\n\n\nHong, Tao, Pierre Pinson, and Shu Fan. 2014. “Global Energy Forecasting Competition 2012.” International Journal of Forecasting 30 (2): 357–63. https://doi.org/https://doi.org/10.1016/j.ijforecast.2013.07.001.",
    "crumbs": [
      "Statistik",
      "Tutorial 2: Muster in Netzlasten verstehen"
    ]
  },
  {
    "objectID": "statistics/tasks.html",
    "href": "statistics/tasks.html",
    "title": "Übungsaufgaben: Wahrscheinlichkeitsrechnung",
    "section": "",
    "text": "1. Berechnung von Mittelwert, Varianz und Kovarianz\nAufgabe:\nGegeben ist folgende Tabelle mit Werten für zwei Variablen \\(X\\) und \\(Y\\):\n\n\n\n\\(X\\)\n\\(Y\\)\n\n\n\n\n2\n3\n\n\n4\n7\n\n\n6\n5\n\n\n8\n9\n\n\n10\n11\n\n\n\nBerechne den Mittelwert von \\(X\\), den Mittelwert von \\(Y\\), die Varianz von \\(X\\), die Varianz von \\(Y\\) und die Kovarianz zwischen \\(X\\) und \\(Y\\). Interpretiere anschließend die Kovarianz hinsichtlich der Beziehung zwischen \\(X\\) und \\(Y\\).\n\n\n\n\n\n\nMusterlösung\n\n\n\n\n\nMusterlösung\n\nMittelwert von \\(X\\):\n\\[\n\\bar{X} = \\frac{2 + 4 + 6 + 8 + 10}{5} = \\frac{30}{5} = 6\n\\]\nMittelwert von \\(Y\\):\n\\[\n\\bar{Y} = \\frac{3 + 7 + 5 + 9 + 11}{5} = \\frac{35}{5} = 7\n\\]\nVarianz von \\(X\\):\nAbweichungen von \\(\\bar{X}\\):\n\\[\n(2-6)^2 = 16, \\quad (4-6)^2 = 4, \\quad (6-6)^2 = 0, \\quad (8-6)^2 = 4, \\quad (10-6)^2 = 16\n\\]\nDann:\n\\[\n\\text{Var}(X) = \\frac{16 + 4 + 0 + 4 + 16}{5} = \\frac{40}{5} = 8\n\\]\nVarianz von \\(Y\\):\nAbweichungen von \\(\\bar{Y}\\):\n\\[\n(3-7)^2 = 16, \\quad (7-7)^2 = 0, \\quad (5-7)^2 = 4, \\quad (9-7)^2 = 4, \\quad (11-7)^2 = 16\n\\]\nDann:\n\\[\n\\text{Var}(Y) = \\frac{16 + 0 + 4 + 4 + 16}{5} = \\frac{40}{5} = 8\n\\]\nKovarianz zwischen \\(X\\) und \\(Y\\):\nProdukte der Abweichungen:\n\\[\n(2-6)(3-7) = (-4)(-4) = 16, \\quad (4-6)(7-7) = (-2)(0) = 0, \\quad (6-6)(5-7) = (0)(-2) = 0, \\quad (8-6)(9-7) = (2)(2) = 4, \\quad (10-6)(11-7) = (4)(4) = 16\n\\]\nDann:\n\\[\n\\text{Cov}(X, Y) = \\frac{16 + 0 + 0 + 4 + 16}{5} = \\frac{36}{5} = 7.2\n\\]\nInterpretation der Kovarianz:\nDie Kovarianz ((X, Y) = 7.2) ist positiv. Das bedeutet, dass \\(X\\) und \\(Y\\) positiv korreliert sind. Wenn die Werte von \\(X\\) steigen, tendieren auch die Werte von \\(Y\\) dazu, zu steigen, und umgekehrt. Eine positive Kovarianz zeigt somit eine gemeinsame Bewegungsrichtung der beiden Variablen an.\n\n\n\n\n\n\n\n2. Beschreibung von Skalenniveaus von Variablen\nAufgabe:\nBestimme das Skalenniveau (nominal, ordinal, metrisch) der folgenden Variablen:\na) Geschlecht (männlich, weiblich, divers)\nb) Schulnoten (1, 2, 3, 4, 5, 6)\nc) Temperatur in Celsius\nd) Lieblingsfarbe (rot, blau, grün, etc.)\ne) Körpergröße in cm\n\n\n\n\n\n\nMusterlösung\n\n\n\n\n\nMusterlösung\n\nGeschlecht: Nominal (Kategorien ohne Reihenfolge)\n\nSchulnoten: Ordinal (Kategorien mit Reihenfolge, aber ungleiche Abstände)\n\nTemperatur in Celsius: Metrisch (kontinuierliche Werte mit gleichen Abständen, aber kein absoluter Nullpunkt)\n\nLieblingsfarbe: Nominal (Kategorien ohne Reihenfolge)\n\nKörpergröße in cm: Metrisch (kontinuierliche Werte mit wahrem Nullpunkt)\n\n\n\n\n\n\n\n3. Berechnung von Wahrscheinlichkeiten mit Additions- und Multiplikationsregeln\nAufgabe:\nIn einem Kartenspiel mit 52 Karten (4 Farben, 13 Werte) ziehst du zwei Karten nacheinander ohne Zurücklegen. Berechne:\na) Die Wahrscheinlichkeit, dass die erste Karte ein Ass ist.\nb) Die Wahrscheinlichkeit, dass die zweite Karte ein Ass ist, wenn die erste Karte ein Ass war.\nc) Die Wahrscheinlichkeit, dass beide Karten Asse sind.\n\n\n\n\n\n\nMusterlösung\n\n\n\n\n\nMusterlösung\n\nErste Karte ein Ass:\n\\[\nP(\\text{Ass}_1) = \\frac{4}{52} = \\frac{1}{13} \\approx 0.0769\n\\]\nZweite Karte ein Ass, wenn erste Karte ein Ass war:\nNach dem Ziehen eines Asses bleiben 3 Asse und 51 Karten:\n\\[\nP(\\text{Ass}_2 \\mid \\text{Ass}_1) = \\frac{3}{51} = \\frac{1}{17} \\approx 0.0588\n\\]\nBeide Karten sind Asse:\n\\[\nP(\\text{Ass}_1 \\cap \\text{Ass}_2) = P(\\text{Ass}_1) \\cdot P(\\text{Ass}_2 \\mid \\text{Ass}_1) = \\frac{1}{13} \\cdot \\frac{1}{17} = \\frac{1}{221} \\approx 0.0045\n\\]\n\n\n\n\n\n\n\n4. Berechnung von bedingten Wahrscheinlichkeiten\nAufgabe:\nIn einer Firma sind 60 % der Angestellten männlich. 30 % der männlichen Angestellten und 40 % der weiblichen Angestellten haben einen Hochschulabschluss. Berechne die Wahrscheinlichkeit, dass ein zufällig ausgewählter Angestellter mit Hochschulabschluss männlich ist.\n\n\n\n\n\n\nMusterlösung\n\n\n\n\n\nMusterlösung\nSei \\(M\\): männlich, \\(W\\): weiblich, \\(H\\): Hochschulabschluss.\nGegeben:\n- \\(P(M) = 0.6\\), \\(P(W) = 0.4\\)\n- \\(P(H \\mid M) = 0.3\\), \\(P(H \\mid W) = 0.4\\)\nGesucht: \\(P(M \\mid H)\\). Nach dem Satz von Bayes:\n\\[\nP(M \\mid H) = \\frac{P(H \\mid M) \\cdot P(M)}{P(H)}\n\\] Zuerst \\(P(H)\\):\n\\[\nP(H) = P(H \\mid M) \\cdot P(M) + P(H \\mid W) \\cdot P(W) = 0.3 \\cdot 0.6 + 0.4 \\cdot 0.4 = 0.18 + 0.16 = 0.34\n\\] Dann:\n\\[\nP(M \\mid H) = \\frac{0.3 \\cdot 0.6}{0.34} = \\frac{0.18}{0.34} \\approx 0.5294\n\\] Also etwa 52.94 %.\n\n\n\n\n\n\n5. Umrechnen von Werten einer Normalverteilung zur Standardnormalverteilung\nAufgabe:\nEine Zufallsvariable \\(X\\) ist normalverteilt mit \\(\\mu = 50\\) und \\(\\sigma = 10\\). Berechne die Wahrscheinlichkeit, dass \\(X &lt; 60\\), indem du den Wert in die Standardnormalverteilung umrechnest und aus einer Standardnormalverteilungstabelle abliest.\n\n\n\n\n\n\nMusterlösung\n\n\n\n\n\nMusterlösung\nStandardisiere \\(X = 60\\):\n\\[\nZ = \\frac{X - \\mu}{\\sigma} = \\frac{60 - 50}{10} = 1\n\\] In der Tabelle für \\(Z = 1.00\\) findest du \\(\\Phi(1.00) = 0.8413\\).\nAlso:\n\\[\nP(X &lt; 60) = P(Z &lt; 1) = 0.8413\n\\]\n\n\n\n\n\n\n6. Ablesen von Quantilen aus einem Boxplot\nAufgabe:\nErstelle mit Python einen Boxplot für eine Verteilung mit folgenden Eigenschaften: Die Box erstreckt sich von 20 bis 60, der Median liegt bei 40, und die Whisker reichen von 10 bis 70. Sieh dir den Boxplot in Figure 1 an und bestimme das 25 %-Quantil, das 50 %-Quantil (Median) und das 75 %-Quantil.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Daten generieren, die den Anforderungen entsprechen\nnp.random.seed(42)\ndata = np.concatenate([\n    np.random.uniform(10, 20, 25),  # Unterer Whisker bis Q1\n    np.random.uniform(20, 40, 25),  # Q1 bis Median\n    np.random.uniform(40, 60, 25),  # Median bis Q3\n    np.random.uniform(60, 70, 25)   # Q3 bis oberer Whisker\n])\n\nplt.boxplot(data, vert=True, patch_artist=True, showmeans=False)\nplt.title('Boxplot der Verteilung')\nplt.ylabel('Werte')\nplt.grid(alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\nFigure 1: Boxplot zur Verteilung mit spezifizierten Quantilen.\n\n\n\n\n\n\n\n\n\n\n\nMusterlösung\n\n\n\n\n\nMusterlösung\n\n25 %-Quantil (untere Quartil): Beginn der Box bei 20\n\n50 %-Quantil (Median): Linie in der Box bei 40\n\n75 %-Quantil (obere Quartil): Ende der Box bei 60",
    "crumbs": [
      "Statistik",
      "Übungsaufgaben: Wahrscheinlichkeitsrechnung"
    ]
  },
  {
    "objectID": "regression/index.html",
    "href": "regression/index.html",
    "title": "Regressions-Analyse",
    "section": "",
    "text": "Im Allgemeinen kann die Regressionsanalyse als eine Sammlung von Werkzeugen verstanden werden, die dazu verwendet werden, eine Beziehung zwischen einer abhängigen Variable \\(Y\\)$ (auch Zielvariable, Antwortvariable oder Label genannt) und der unabhängigen Variable \\(X\\) (auch Regressor, Prädiktoren, Kovariaten, erklärende Variable oder Feature genannt) zu schätzen oder festzustellen.\nWenn wir eine Regressionsfunktion \\(f\\) als Modell für die Beziehung zwischen \\(X\\) und \\(Y\\) annehmen, können wir die Regressionsanalyse als die Suche nach den Parametern \\(\\theta\\) verstehen, die die Funktion \\(f\\) am besten an die Daten anpassen.\nDas Problem der Regressionsanalyse kann also wie folgt formuliert werden: \\[\nY = f(X, \\theta) + \\epsilon,\n\\]\nwobei \\(\\theta\\) durch die Optimierung für eine gute Anpassung von \\(f\\) an die Daten gefunden wird. In der Regel erhalten wir dabei einen Fehlerterm \\(\\epsilon\\), der – wenn wir Glück haben – normalverteilt mit einem Erwartungswert von null und konstanter Varianz ist.\nRegessionen sind ein mächtiges Werkzeug für die Interpetation von Daten und die Vorhersage von Werten. In der Praxis gibt es viele verschiedene Arten von Regressionsmodellen, die sich in ihrer Komplexität und den Annahmen, die sie machen, unterscheiden. In diesem Kapitel werden wir uns auf die lineare Regression konzentrieren, die eine der einfachsten und am häufigsten verwendeten Formen der Regressionsanalyse ist und sich für Zusammenhänge zwischen einer abhängigen und einer unabhängigen Variablen mit intervallskalierten Daten eignet.\nZuerst führen wir eine einfache Lineare Regression mit einer unabhängigen Variable durch. Anschließend erweitern wir das Modell auf mehrere unabhängige und auch kategorische Variablen. Hierzu nutzen wird die Matrix-Schreibweise.\nErweiterte Themen Intercepts und Regularisierung\nDiskussion von Statistischem Lernen und Resampling\nLogistische Regression als Beispiel für Klassifikation",
    "crumbs": [
      "Regressions-Analyse"
    ]
  },
  {
    "objectID": "regression/basic_linear_regression.html",
    "href": "regression/basic_linear_regression.html",
    "title": "7  Lineare Regression",
    "section": "",
    "text": "7.1 Linare Regression mit einer Variablen\nDie lineare Regression ist ein statistisches Verfahren, das dazu verwendet wird, die Beziehung zwischen einer abhängigen Variable \\(Y\\) und einer oder mehreren unabhängigen Variablen \\(X\\) zu modellieren. Das Modell der linearen Regression kann als eine lineare Beziehung zwischen \\(Y\\) und \\(X\\) interpretiert werden, die durch die Gleichung\n\\[\nY = f(X, \\theta) + \\epsilon,\n\\]\nFür den Fall, dass wird nur eine Variable \\(X\\) betrachten, wird die lineare Regression durch die Gleichung: \\[\nY = \\beta_0 + \\beta_1 X + \\epsilon\n\\]\nbeschrieben wird. Hierbei sind \\(\\beta_0\\) und \\(\\beta_1\\) die Parameter \\(\\theta\\) des Modells, die die Steigung und den Achsenabschnitt der Regressionsgeraden bestimmen, und \\(\\epsilon\\) ist der Fehlerterm, der die Abweichung der beobachteten Werte von den vorhergesagten Werten beschreibt.\nIn diesem Fall haben wir einen einfachen Zusammenhang zwischen nur zwei Variablen \\(X\\) und \\(Y\\). In der Praxis kann die lineare Regression jedoch auch auf mehrere unabhängige Variablen erweitert werden, um komplexere Beziehungen zu modellieren.",
    "crumbs": [
      "Regressions-Analyse",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Lineare Regression</span>"
    ]
  },
  {
    "objectID": "regression/basic_linear_regression.html#linare-regression-mit-einer-variablen",
    "href": "regression/basic_linear_regression.html#linare-regression-mit-einer-variablen",
    "title": "7  Lineare Regression",
    "section": "",
    "text": "Überwachtes Lernen\n\n\n\nDie lineare Regression ist ein Beispiel für ein überwachtes Lernverfahren, bei dem wir ein Modell auf Basis von gelabelten Trainingsdaten erstellen. Das bedeutet, dass wir sowohl die unabhängigen Variablen \\(X\\) als auch die abhängige Variable \\(Y\\) kennen und das Modell so anpassen, dass es die Beziehung zwischen \\(X\\) und \\(Y\\) möglichst gut abbildet. Wir können dabei überwachen wie gut sich das Modell mit verschiedenen Parametern an die Daten anpasst und das Modell so optimieren, dass es die besten Vorhersagen liefert.\n\n\n\n\n\n\n\n\nParametrische Modelle\n\n\n\nBei der linearen Regression handelt es sich um ein parametrisches Modell, bei dem wir eine bestimmte Form der Beziehung zwischen \\(X\\) und \\(Y\\) annehmen und die Parameter \\(\\theta\\) des Modells so anpassen, dass es die Daten möglichst gut abbildet. Im Gegensatz dazu gibt es auch nicht-parametrische Modelle, bei denen keine spezifische Form der Beziehung angenommen wird und das Modell flexibler ist, aber auch mehr Daten benötigt, um zuverlässige Vorhersagen zu treffen (z.B. Neuronale Netze).\n\n\n\n\n\n\n\n\nInterpreation und Prognose\n\n\n\nEs gibt grundlegend zwei Zielstellungen, die mit der linearen Regression, und auch vielen anderen Modellen, verfolgt werden können:\n\nInterpretation: Die Interpretation der Beziehung zwischen den Variablen \\(X\\) und \\(Y\\) steht im Vordergrund. Hierbei geht es darum, die Auswirkung der unabhängigen Variablen auf die abhängige Variable zu verstehen und zu erklären und ggf. Hypothesen zu testen.\nPrognose: Die Vorhersage von Werten der abhängigen Variablen \\(Y\\) auf Basis der unabhängigen Variablen \\(X\\) steht im Vordergrund. Hierbei geht es darum, die Genauigkeit der Vorhersagen zu maximieren und die Modellgüte zu optimieren.\n\nWichtig ist es, sich vor der Anwendung eines Modells klar zu machen, welches Ziel verfolgt wird.",
    "crumbs": [
      "Regressions-Analyse",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Lineare Regression</span>"
    ]
  },
  {
    "objectID": "regression/basic_linear_regression.html#beispiel-vorhersage-von-zeitungenverkäufen",
    "href": "regression/basic_linear_regression.html#beispiel-vorhersage-von-zeitungenverkäufen",
    "title": "7  Lineare Regression",
    "section": "7.2 Beispiel: Vorhersage von Zeitungenverkäufen",
    "text": "7.2 Beispiel: Vorhersage von Zeitungenverkäufen\n\n::: {.callout-note} {.collapsed = “true”} ### Wenn wir diese Grafik stellen, welche Fragen können wir uns stellen? Fallen diese Fragen in die Kategorie der Interpretation oder der Prognose?\n\nInterpretation:\n\nGibt es einen Zusammenhang zwischen den Werbeausgaben und den Verkäufen?\nWie stark ist der Zusammenhang?\nWelche Ausgaben haben den größten Einfluss auf die Verkäufe?\nIst die Beziehung linear oder nicht-linear?\nGibt es Synergieeffekte zwischen den verschiedenen Werbekanälen?\n\nPrognose:\n\nWie gut können wir die Verkäufe vorhersagen, wenn wir die Werbeausgaben kennen?\nWie genau sind unsere Vorhersagen? :::\n\n\nWenn wir das lineare Modell aufstellen könnte, wir das folgendermaßen aussehen:\n\\[\nY ≈ β_0 + β_1 \\cdot X_1,\n\\]\nbzw. in unserem Beispiel:\n\\[\n\\text{sales} ≈ β_0 + β_1 \\cdot \\text{TV}\n\\]\nDie Parameter \\(β_0\\) und \\(β_1\\) haben dabei bestimmt Bedeutungen:\n\n\\(β_0\\) … intercept oder y-Achsenabschnitt\n\\(β_1\\) … slope oder Steigung\n\nAls nächstes müssen wir einen Weg finden die Parameter \\(β_0\\) und \\(β_1\\) zu schätzen. Dafür gibt es verschiedene Methoden. Die Klassische Methode ist die Methode der kleinsten Quadrate (OLS). Diese Methode minimiert die Summe der quadrierten Abweichungen zwischen den beobachteten Werten und den vorhergesagten Werten.\n\\[Y ≈ \\hat{β}_0 + \\hat{β}_1 \\cdot X_1\\]",
    "crumbs": [
      "Regressions-Analyse",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Lineare Regression</span>"
    ]
  },
  {
    "objectID": "regression/basic_linear_regression.html#methode-der-kleinsten-quadrate-ols",
    "href": "regression/basic_linear_regression.html#methode-der-kleinsten-quadrate-ols",
    "title": "7  Lineare Regression",
    "section": "7.3 Methode der kleinsten Quadrate (OLS)",
    "text": "7.3 Methode der kleinsten Quadrate (OLS)\nDie Methode der kleinsten Quadrate (OLS) ist ein Verfahren zur Schätzung der Parameter eines linearen Regressionsmodells, das die Summe der quadrierten Abweichungen zwischen den beobachteten Werten und den vorhergesagten Werten minimiert.\nAls erstes definieren wir den Fehler \\(\\epsilon_i\\) für jeden Datenpunkt \\(i\\) als die Differenz zwischen dem beobachteten Wert \\(y_i\\) und dem vorhergesagten Wert \\(\\hat{y}_i\\):\n\\[\n\\epsilon_i = y_i - \\hat{y}_i.\n\\]\nDie Methode der kleinsten Quadrate zielt darauf ab, die Summe der quadrierten Fehler zu minimieren, d.h. die Summe der quadrierten Abweichungen zwischen den beobachteten Werten und den vorhergesagten Werten:\n\\[\n\\text{RSS} = \\sum_{i=1}^{n} \\epsilon_i^2 = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2.\n\\]\nAuf Englisch wird die Summe als Residual Sum of Squares (RSS) bezeichnet. Die Methode der kleinsten Quadrate sucht nun die Parameter \\(\\hat{\\beta}_0\\) und \\(\\hat{\\beta}_1\\), die die Summe der quadrierten Fehler minimieren:\n\\[\n\\hat{\\beta}_0, \\hat{\\beta}_1 = \\text{argmin}_{\\beta_0, \\beta_1} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2.\n\\]\n\n\n\n\n\n\nKostenfunktion \\(J\\)\n\n\n\nDie Methode der kleinsten Quadrate kann auch als Optimierungsproblem formuliert werden, bei dem wir eine Kostenfunktion (Cost Function) minimieren, die die Summe der quadrierten Fehler zwischen den beobachteten und den vorhergesagten Werten beschreibt. Die Kostenfunktion ist in diesem Fall die Residual Sum of Squares (RSS), die wir minimieren, um die besten Parameter für das Modell zu finden. Kostenfunktionen hängen in der Regel von Modell, den Daten und den Parametern ab und dienen dazu, die Güte des Modells zu bewerten und zu optimieren. Da wir meist ein festes Modell und Daten haben und nur die Parameter anpassen, können wir die Kostenfunktion als Funktion der Parameter betrachten, die wir minimieren wollen. Ein eng verwandtes Konzept ist die Verlustfunktion (Loss Function) oder Accuracy Measures, die in der Regel den Fehler zwischen den beobachteten und den vorhergesagten Werten beschreiben Die Kostenfunktion ist in der Regel eine Summe der Verlustfunktionen über alle Datenpunkte.",
    "crumbs": [
      "Regressions-Analyse",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Lineare Regression</span>"
    ]
  },
  {
    "objectID": "regression/basic_linear_regression.html#schätzung-der-parameter-mittels-odinary-least-squares-ols",
    "href": "regression/basic_linear_regression.html#schätzung-der-parameter-mittels-odinary-least-squares-ols",
    "title": "7  Lineare Regression",
    "section": "7.4 Schätzung der Parameter mittels Odinary Least Squares (OLS)",
    "text": "7.4 Schätzung der Parameter mittels Odinary Least Squares (OLS)\nDie Methode der kleinsten Quadrate (Ordinary Least Squares, OLS) wird verwendet, um die Parameter einer linearen Regression zu schätzen. Ziel ist es, die Gerade\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\n\\]\nso zu bestimmen, dass die Summe der quadrierten Fehler (Residual Sum of Squares, RSS) minimiert wird:\n\\[\nRSS = \\sum_{i=1}^{n} \\left(y_i - \\beta_0 - \\beta_1 x_i\\right)^2.\n\\]\n\n7.4.1 1. Berechnung der Mittelwerte\nFür spätere Berechnungen ist es nützlich, die Mittelwerte der Variablen \\(x\\) und \\(y\\) zu bestimmen:\n\\[\n\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i, \\quad \\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_i.\n\\]\n\n\n7.4.2 2. Minimierung der Kostenfunktion nach \\(\\beta_1\\)\nUm \\(\\beta_1\\) zu bestimmen, setzen wir die erste Ableitung der Kostenfunktion \\(J = RSS\\) gleich null:\n\\[\n\\frac{\\partial RSS}{\\partial \\beta_1} = 0.\n\\]\nDie Ableitung der Kostenfunktion lautet:\n\\[\n\\frac{\\partial RSS}{\\partial \\beta_1} = \\frac{\\partial}{\\partial \\beta_1} \\sum_{i=1}^{n} \\left(y_i - \\beta_0 - \\beta_1 x_i\\right)^2.\n\\]\n::: {.callout-tip} {.collapsed = “true”} #### Anwendung der Kettenregel\nDie Kettenregel besagt:\n\\[\n[f(g(x))]' = f'(g(x)) \\cdot g'(x),\n\\]\nwobei hier \\(f(x) = x^2\\) und \\(g(x) = y_i - \\beta_0 - \\beta_1 x_i\\). :::\nAngewendet ergibt dies:\n\\[\n\\frac{\\partial RSS}{\\partial \\beta_1} = -2 \\sum_{i=1}^{n} x_i \\left(y_i - \\beta_0 - \\beta_1 x_i\\right).\n\\]\nSetzen wir die Ableitung gleich null:\n\\[\n-2 \\sum_{i=1}^{n} x_i \\left(y_i - \\beta_0 - \\beta_1 x_i\\right) = 0.\n\\]\nVereinfachen der Gleichung:\n\\[\n\\sum_{i=1}^{n} x_i y_i - \\beta_0 \\sum_{i=1}^{n} x_i - \\beta_1 \\sum_{i=1}^{n} x_i^2 = 0.\n\\]\nSetzen wir die Mittelwerte ein:\n\\[\n\\sum_{i=1}^{n} x_i y_i - \\beta_0 n \\bar{x} - \\beta_1 \\sum_{i=1}^{n} x_i^2 = 0.\n\\]\n\n\n7.4.3 3. Minimierung der Kostenfunktion nach \\(\\beta_0\\)\nAnalog leiten wir \\(J\\) nach \\(\\beta_0\\) ab:\n\\[\n\\frac{\\partial RSS}{\\partial \\beta_0} = 0.\n\\]\n\\[\n\\frac{\\partial RSS}{\\partial \\beta_0} = -2 \\sum_{i=1}^{n} \\left(y_i - \\beta_0 - \\beta_1 x_i\\right).\n\\]\nSetzen wir die Ableitung gleich null:\n\\[\n\\sum_{i=1}^{n} y_i - \\beta_0 \\sum_{i=1}^{n} 1 - \\beta_1 \\sum_{i=1}^{n} x_i = 0.\n\\]\nDa \\(\\sum_{i=1}^{n} 1 = n\\), folgt:\n\\[\nn \\beta_0 + \\beta_1 \\sum_{i=1}^{n} x_i = \\sum_{i=1}^{n} y_i.\n\\]\nUmformen ergibt:\n\\[\n\\beta_0 = \\frac{1}{n} \\sum_{i=1}^{n} y_i - \\beta_1 \\frac{1}{n} \\sum_{i=1}^{n} x_i.\n\\]\n\n\n7.4.4 4. Einsetzen von \\(\\beta_1\\) in die Gleichung für \\(\\beta_0\\)\nDie optimale Steigung \\(\\hat{\\beta_1}\\) erhält man durch:\n\\[\n\\hat{\\beta_1} = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}.\n\\]\nDies ist die Kovarianz zwischen \\(x\\) und \\(y\\) geteilt durch die Varianz von \\(x\\) (vgl. Kapitel ) Für den Achsenabschnitt \\(\\hat{\\beta_0}\\) setzen wir \\(\\hat{\\beta_1}\\) ein:\n\\[\n\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1} \\bar{x}.\n\\]\n\n\n7.4.5 5. Ergebnis\nDie geschätzte Regressionsgerade lautet:\n\\[\n\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1} x.\n\\]\nHiermit haben wir die OLS-Schätzung der Parameter \\(\\beta_0\\) und \\(\\beta_1\\) abgeschlossen.",
    "crumbs": [
      "Regressions-Analyse",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Lineare Regression</span>"
    ]
  },
  {
    "objectID": "regression/basic_linear_regression.html#beispiel-lineare-regression-mit-einer-variablen",
    "href": "regression/basic_linear_regression.html#beispiel-lineare-regression-mit-einer-variablen",
    "title": "7  Lineare Regression",
    "section": "7.5 Beispiel: Lineare Regression mit einer Variablen",
    "text": "7.5 Beispiel: Lineare Regression mit einer Variablen\nWir simulieren einen Datensatz mit einer linearen Beziehung zwischen \\(X\\) (€ ausgegeben für TV-Werbung) und \\(Y\\) (Verkäufe von Zeitungen). In diesem Fall kennen wir den Zufalls-Prozess, der die Daten generiert hat, und können daher die wahren Parameter des Modells bestimmen. Die Variable \\(X\\) ist gleichmäßig zwischen 0 und 300 verteilt, und \\(Y\\) wird durch die Gleichung \\[\nY = 7 + 0.05 \\cdot X + \\epsilon\n\\]\ngeneriert, wobei \\(\\epsilon\\) ein normalverteilter Fehler mit einer Standardabweichung von \\(5\\) ist.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulate population data\nnp.random.seed(0)\nX = 300 * np.random.rand(100, 1)\nY = 7 + 0.05 * X + 5 * np.random.randn(100, 1)\n\n# Simulate sample data\nsample_size = 100\nidx = np.random.choice(100, sample_size, replace=False)\nX_sample = X[idx]\nY_sample = Y[idx]\n\n# Store data in a tidy dataframe\ndata = pd.DataFrame({\"X (TV advertising spending (€))\": X_sample.flatten(), \"Y (Sales)\": Y_sample.flatten()})\nprint(data.head())\n\n   X (TV advertising spending (€))  Y (Sales)\n0                       246.297969  14.750787\n1                       140.595360  22.596482\n2                        62.663027   6.953921\n3                       172.783949  21.579346\n4                        19.244249  12.709316\n\n\n\n# Plot data\nplt.scatter(X, Y)\nplt.xlabel(\"TV advertising spending (€)\")\nplt.ylabel(\"Sales\")\nplt.title(\"TV advertising vs. sales\")\nplt.show()\n\n\n\n\nScatter plot of TV advertising vs. sales\n\n\n\n\nNun können wir unsere Formel für die OLS-Schätzung der Parameter \\(\\beta_0\\) und \\(\\beta_1\\) anwenden:\n\n# Calculate means\nX_mean = np.mean(X_sample)\nY_mean = np.mean(Y_sample)\n\n# Calculate slope\nnumerator = np.sum((X_sample - X_mean) * (Y_sample - Y_mean))\ndenominator = np.sum((X_sample - X_mean) ** 2)\nbeta_1 = numerator / denominator\n\n# Calculate intercept\nbeta_0 = Y_mean - beta_1 * X_mean\n\nprint(f\"Slope (beta_1): {beta_1}\")\nprint(f\"Intercept (beta_0): {beta_0}\")\n\nSlope (beta_1): 0.04894891702336733\nIntercept (beta_0): 8.110755387236143\n\n\nIn der Zukunft werden wir stattdessen eines von zwei Paketen verwenden, die die OLS-Schätzung automatisch durchführen: statsmodels oder scikit-learn.\n\n7.5.1 scikit-learn\nscikit-learn ist eine der bekanntesten Bibliotheken für maschinelles Lernen in Python. Sie bietet eine Vielzahl von Algorithmen und Werkzeugen für die Modellierung und Analyse von Daten. Wir werden es später und im kommenden Semester noch ausführlich verwenden.\n\nfrom sklearn.linear_model import LinearRegression\n\n# Defines what kind of model we want to use\nmodel = LinearRegression()\n\n# Fits the model to the data\nmodel.fit(X_sample, Y_sample)\n\n# Get slope and intercept from the models coefficients\nbeta_1_sklearn = model.coef_[0][0]\nbeta_0_sklearn = model.intercept_[0]\n\nprint(f\"Slope (beta_1): {beta_1_sklearn}\")\nprint(f\"Intercept (beta_0): {beta_0_sklearn}\")\n\nSlope (beta_1): 0.04894891702336733\nIntercept (beta_0): 8.110755387236143\n\n\n\n\n7.5.2 statsmodels\nstatsmodels ist eine Bibliothek für statistische Modellierung in Python. Sie bietet eine Vielzahl von statistischen Modellen und Tests, darunter auch die lineare Regression. Die Ausgabe von statsmodels ist oft detaillierter und enthält mehr statistische Informationen als die von scikit-learn.\n\nimport statsmodels.api as sm\n\n# Fit linear regression model\nX_sm = sm.add_constant(X_sample)\nmodel_sm = sm.OLS(Y_sample, X_sm).fit()\n\n# Get slope and intercept\nprint(model_sm.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.419\nModel:                            OLS   Adj. R-squared:                  0.413\nMethod:                 Least Squares   F-statistic:                     70.80\nDate:                Mon, 21 Apr 2025   Prob (F-statistic):           3.29e-13\nTime:                        08:16:58   Log-Likelihood:                -302.46\nNo. Observations:                 100   AIC:                             608.9\nDf Residuals:                      98   BIC:                             614.1\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          8.1108      0.966      8.392      0.000       6.193      10.029\nx1             0.0489      0.006      8.414      0.000       0.037       0.060\n==============================================================================\nOmnibus:                       11.746   Durbin-Watson:                   2.121\nProb(Omnibus):                  0.003   Jarque-Bera (JB):                4.097\nSkew:                           0.138   Prob(JB):                        0.129\nKurtosis:                       2.047   Cond. No.                         319.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\n\n\nArbeit mit DataFrames und Formula-Strings\n\n\n\nAnstelle die Daten in einem Numpy-Array zu speichern, können wir auch ein Pandas DataFrame verwenden. Dies hat den Vorteil, dass wir die Spalten mit ihren Namen ansprechen können und so den Code besser lesbar machen. Zudem läss sich die lineare Regression auch mit Hilfe von Formel-Strings durchführen, die die Beziehung zwischen den Variablen angeben.\n\nimport statsmodels.formula.api as smf\n\n# Create DataFrame\ndata = pd.DataFrame({\"TV\": X_sample.flatten(), \"Sales\": Y_sample.flatten()})\n# Fit linear regression model using formula string\nmodel_smf = smf.ols(\"Sales ~ TV\", data=data).fit()\n\n\n\nWenn wir die Ergebnisse der OLS-Schätzung betrachten, sehen wir einige Zusatzinformationen, die uns helfen, die Güte des Modells zu bewerten\n\n7.5.2.1 Koeffizienten und deren Konfidenzintervalle\nWir können die geschätzen Koeffizienten \\(\\hat{\\beta}_0\\) und \\(\\hat{\\beta}_1\\) sowie deren Konfidenzintervalle ablesen. Die Konfidenzintervalle geben an. Es wird davon ausgegangen, dass die Schätzer der Koeffizienten normalverteilt sind. Entsprechend sind die Konfidenzintervalle symmetrisch um den Schätzer und 95% der Werte liegen innerhalb von zwei Standardabweichung des Schätzers z.B:\n\\[\n[CI_{0.025}^{\\beta_0}, CI^{\\beta_0}_{0.975}] = [8.1108 - 2 0.966 , 8.1108 + 2 0.966] = [6.193, 10.029]\n\\]\n\n\n7.5.2.2 Teststatistiken \\(t\\) und \\(p\\)-Werte\nEine weitere wichtige Frage ist, ob ein Koeffizient (Parameter) tatsächlich siginifikant von \\(0\\) verschiedenen ist. Im unserem konkreten Beispiel stellt sich die Fragen: Verändert sich der Umsatz, wenn wir mehr Geld in TV-Werbung investieren? Dies kann mit einem T-Test überprüft werden.\n\n\\(H_0\\): \\(\\beta_1 = 0\\) (kein Effekt)\n\\(H_1\\): \\(\\beta_1 \\neq 0\\) (Effekt)\n\nDie Teststatistik \\(t\\) gibt an, wie viele Standardabweichungen der Schätzer \\(\\hat{\\beta}_1\\) vom Wert \\(0\\) entfernt ist. Ein hoher \\(t\\)-Wert deutet darauf hin, dass der Schätzer signifikant von \\(0\\) verschieden ist. Der \\(p\\)-Wert gibt die Wahrscheinlichkeit an, dass der beobachtete Effekt auftritt, wenn die Nullhypothese wahr ist. Ein \\(p\\)-Wert kleiner als \\(0.05\\) wird oft als Hinweis darauf interpretiert, dass der Effekt signifikant ist.\nIm Vorliegenden Fall können wir also die Nullhypothese, dass der Umsatz unabhängig von den Werbeausgaben ist mit hoher Konfidenz ablehnen.\n\n\n7.5.2.3 Koeffizient der Determination \\(R^2\\)\nDer Koeffizient der Determination \\(R^2\\) gibt an, wie gut das Modell die beobachteten Daten erklärt. Er liegt zwischen \\(0\\) und \\(1\\) und gibt den Anteil der Varianz der abhängigen Variable \\(Y\\) an, der durch das Modell erklärt wird. Ein \\(R^2\\) von \\(1\\) bedeutet, dass das Modell alle Variationen in \\(Y\\) erklärt, während ein \\(R^2\\) von \\(0\\) bedeutet, dass das Modell keine Variationen erklärt. In unserem Fall beträgt \\(R^2 = 0.41\\), was darauf hindeutet, dass das Modell etwa \\(41\\%\\) der Variationen in den Verkäufen erklärt.\nDas Bestimmtheitsmaß \\(R^2\\) ist definiert als: \\[\nR^2 = \\frac{TSS-RSS}{TSS} = 1- \\frac{RSS}{TSS},\n\\]\nwobei \\(RSS\\) die Residual Sum of Squares und \\(TSS\\) die Total Sum of Squares ist. \\(TSS\\) ist die Summe der quadrierten Abweichungen der abhängigen Variable \\(Y\\) von ihrem Mittelwert \\(\\bar{y}\\).\nDie Varianz und TSS hängen eng miteinander zusammen:\n\\[\nTSS = \\sum_{i=1}^{n} (y_i - \\bar{y})^2 = n \\text{Var}(Y).\n\\]\nIntuitiv kann man sagen, R² gibt an, wie viel Prozent der Varianz der abhängigen Variable durch die unabhängigen Variablen erklärt wird, bzw. genauer. Wie veringern sich die Fehler, wenn man ein naives Mittel-Wert-Modells (\\(f(x)=\\bar(y)\\)) durch das lineare Modell ersetzt.\n\n# Plot data\nplt.scatter(X, Y)\nplt.plot(X, beta_0 + beta_1 * X, color=\"red\", label=\"OLS regression\")\nplt.plot(X, np.mean(Y) * np.ones_like(X), color=\"green\", label=\"Mean of Y\")\nplt.legend()\nplt.xlabel(\"TV advertising spending (€)\")\nplt.ylabel(\"Sales\")\nplt.title(\"TV advertising vs. sales\")\nplt.show()\n\n\n\n\nScatter plot of TV advertising vs. sales\n\n\n\n\n\n\n\n\n\n\nStolpersteine beim Interpretieren von \\(R^2\\)\n\n\n\n\nEin hoher \\(R^2\\) bedeutet nicht unbedingt, dass das Modell gut ist. Ein Modell mit einem hohen \\(R^2\\) kann immer noch schlechte Vorhersagen machen, wenn es nicht gut generalisiert.\nEin niedriger \\(R^2\\) bedeutet nicht unbedingt, dass das Modell schlecht ist. Ein Modell mit einem niedrigen \\(R^2\\) kann immer noch nützlich sein, wenn es die Beziehung zwischen den Variablen gut erklärt.\n\\(R^2\\) hängt von der Anzahl der Variablen im Modell ab. Ein Modell mit mehr Variablen wird tendenziell ein höheres \\(R^2\\) haben, auch wenn es nicht besser ist.\n\nDie Folgede Abbildung zeigt zudem verschiende Zusammenhänge und den \\(R^2\\) Wert für die jeweiligen Modelle.\n\n\n\n\n\n\n\n\n\n\nAnnahmen für die lineare Regression\n\n\n\nDamit eine Lineare Regression sinnvoll ist und die Schätzung der Parameter gut funktioniert müssen einige Annahmen erfüllt sein:\n\nLineare Beziehung: Die Beziehung zwischen den unabhängigen und abhängigen Variablen ist linear.\nUnabhängigkeit: Die Beobachtungen sind unabhängig voneinander.\nHomoskedastizität: Die Varianz der Fehler ist konstant.\nNormalverteilung: Die Fehler sind normalverteilt.\nKeine Multikollinearität: Die unabhängigen Variablen sind nicht stark miteinander korreliert.",
    "crumbs": [
      "Regressions-Analyse",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Lineare Regression</span>"
    ]
  },
  {
    "objectID": "regression/advanced_linear_regression.html",
    "href": "regression/advanced_linear_regression.html",
    "title": "8  Lineare Regression",
    "section": "",
    "text": "8.1 Abbildung nicht-linearer Zusammenhänge mit Linearen Modellen\nZusammenhänge können auch zwischen mehr als zwei Variablen bestehen. In diesem Fall sprechen wir von einer multiplen linearen Regression. Das Modell kann dann wie folgt formuliert werden:\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_p X_p + \\epsilon\n\\]\nAls Beispiel könnten wir nicht nur die Absatzmenge eines Produkts in Abhängigkeit von der Werbeausgabe für TV-Spots, sondern auch von der Werbeausgabe für Radio-Spots modellieren. Wir simuliieren die Daten wieder mit Python, so dass wir die Zusammenhänge kennen.\nWir können nun drei unabhängige Modelle fitten und visualisieren.\nOffensichtlich sind die Modelle von mittlerer Qualität. Wir können die drei unabhängigen Variablen auch in einem Modell zusammenfassen.\nWir können das Modell visualisieren, indem wir dreidimensionale Daten plotten. Wir lassen \\(x_3\\) weg, da es den geringsten Einfluss auf die abhängige Variable zu haben scheint.\nDas Modell ist linear und jede der drei Variablen hat einen positiven Einfluss auf die abhängige Variable. Das Modell kann wie folgt geschrieben werden:\n\\[\nY =  -689.7 + 4.9 X_1 + 13.3 X_2 + 0.6 X_3,\n\\]\noder noch klarer:\n\\[\n\\text{sales} = 689.7 \\text{units} + 4.9 \\frac{\\text{units}}{Euro} \\times \\text{TV advertising spending} + 13.3 \\frac{\\text{units}}{Euro} \\times \\text{Radio advertising spending} + 0.6 \\frac{\\text{units}}{Euro} \\times \\text{Newspaper advertising spending}.\n\\]\nDieses lineare Modell eigent sich ausgezeichnet für eine Interpretation des Daten.\nWichtig ist auch, dass wir unsere Interpretation der Welt durch unser Modell nicht verabsolutieren. Es ist immer nur eine Näherung und kann nie die Realität vollständig abbilden. Wenn wir unser Model wörtlich nehmen, könnten wir uns in die Irre führen. Stellen, wir uns vor, die Firma stellt die Werbeausgaben für Zeitungswerbung ein. Dann würde unser Modell den folgenden Absatz vorhersagen:\n\\[\n\\text{sales} = - 689.7 \\text{units}\n\\]\nEin negativer Absatz ist offensichtlich in der Realität nicht zu erwarten. Entsprechend haben wir den Grenzen unseres Modells erreicht. Die reale Welt lässt sich nur in bestimmten Grenzen durch das lineare Modell abbilden.\nIn der Praxis hört man oft, dass sich lineare Modelle nur für lineare Zusammenhänge eignen. Das ist nicht ganz richtig. Lineare Modelle können auch nicht-lineare Zusammenhänge abbilden. Zunächst vergegenwärtigen wir uns nochmal was passiert, wenn wir die Vorhersage unseres Modells nur in Abhängigkeit von einer Variablen betrachten. Jeder schwarze Punkt in (fig:regression-linear-advanced-linear-regression?) zeigt die beobachtete Absatzmenge zu einem Zeitpunkt in Abhängigkeit von den Werbeausgaben für TV-Spots. Der blaue Punkt zeigt die Vorhersage unseres Modells, in das Modell geht natürlich auch die Werbeausgaben für Radio- und Zeitungswerbung ein. Deswegen liegen die blauen Punkte nicht auf einer Linie, obwohl wir ein lineares Modell verwenden.\nfrom sklearn.linear_model import LinearRegression\n\n# Prediction for TV advertising spending\nX = X_sample[0]\nplt.scatter(X, Y_sample, color='black')\nY_pred = model_sm.params[0] + model_sm.params[1] * X_1 + model_sm.params[2] * X_2 + model_sm.params[3] * X_3\nplt.scatter(X_1, Y_pred, color='blue')\nplt.xlabel(\"TV advertising spending (€)\")\nplt.ylabel(\"Sales\")\nplt.title(\"Linear Regression with TV advertising spending (€)\")\n# Add legend\nplt.legend([\"Data\", \"Prediction\"])\nplt.show()\n\n/tmp/ipykernel_3387/1829400649.py:6: FutureWarning:\n\nSeries.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n\n\n\n\n\n\nLineare Regression für die drei unabhängigen Variablen als Plot für eine\nManchmal begegnen wir Zusammenhängen, die ganz klar nicht linear sind, beispielsweise der zwischen Verbrauch in Meilen pro Gallone und der Leistung eines Autos. Offensichtlich sinkt der Verbrauch nicht linear mit der Leistung:\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\ndf = pd.read_csv(r\"../_assets/regression/Auto_Data_Set_963_49.csv\")\n\n# linear model\n\nmodel = LinearRegression()\nmodel.fit(df[[\"horsepower\"]], df[\"mpg\"])\nY_pred = model.predict(df[[\"horsepower\"]])\n\n\n# Scatter plot between mpg und horsepower\nplt.scatter(df[\"horsepower\"], df[\"mpg\"])\nplt.plot(df[\"horsepower\"], Y_pred, color='blue', linewidth=3)\nplt.xlabel(\"Horsepower\")\nplt.ylabel(\"Miles per Gallon\")\nplt.title(\"Scatter plot between Horsepower and Miles per Gallon\")\n# Legend with parameters\nplt.legend([f\"Linear Model: $Y = {model.intercept_:.1f} + {model.coef_[0]:.1f}X$\"])\n# Add RSS\n\nplt.show()\nEine Lösung ist die Transformation der unabhängigen Variablen. In unserem Beispiel könnten wir die Quadratwurzel der Werbeausgaben verwenden. Das Modell wird dann wie folgt aussehen:\n\\[\n\\text{mpg} = \\beta_0 + \\beta_1 \\sqrt{\\text{horsepower}} + \\epsilon\n\\]\n# Transformation of the independent variable\ndf[\"sqrt_horsepower\"] = np.sqrt(df[\"horsepower\"])\n\n# linear model\nmodel = LinearRegression()\nmodel.fit(df[[\"sqrt_horsepower\"]], df[\"mpg\"])\n\n# Sort before plotting\ndf = df.sort_values(by=\"horsepower\")\n\nY_pred = model.predict(df[[\"sqrt_horsepower\"]])\n\n# Scatter plot between mpg und horsepower\nplt.scatter(df[\"horsepower\"], df[\"mpg\"])\nplt.plot(df[\"horsepower\"], Y_pred, color='blue', linewidth=3)\nplt.xlabel(\"Horsepower\")\nplt.ylabel(\"Miles per Gallon\")\nplt.title(\"Scatter plot between Horsepower and Miles per Gallon\")\n# Legend with parameters\nplt.legend([f\"Linear Model: $Y = {model.intercept_:.1f} {model.coef_[0]:.1f}X^{{0.5}}$\"])\nplt.show()\n\n\n\n\nLineare Regression für mit transformierten unabhängigen Variablen",
    "crumbs": [
      "Regressions-Analyse",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lineare Regression</span>"
    ]
  },
  {
    "objectID": "regression/advanced_linear_regression.html#abbildung-nicht-linearer-zusammenhänge-mit-linearen-modellen",
    "href": "regression/advanced_linear_regression.html#abbildung-nicht-linearer-zusammenhänge-mit-linearen-modellen",
    "title": "8  Lineare Regression",
    "section": "",
    "text": "Feature Engineering\n\n\n\nDie Transformation der unabhängigen Variablen wird auch als Feature Engineering bezeichnet. Feature Engineering ist ein wichtiger Schritt in der Modellierung, um die Leistungsfähigkeit von Modellen zu verbessern.",
    "crumbs": [
      "Regressions-Analyse",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lineare Regression</span>"
    ]
  },
  {
    "objectID": "regression/advanced_linear_regression.html#umgang-mit-categorical-variablen",
    "href": "regression/advanced_linear_regression.html#umgang-mit-categorical-variablen",
    "title": "8  Lineare Regression",
    "section": "8.2 Umgang mit Categorical Variablen",
    "text": "8.2 Umgang mit Categorical Variablen\nBisher haben wir nur numerische Variablen betrachtet. In der Praxis haben wir es aber oft mit kategorischen Variablen zu tun. Kategorische Variablen sind Variablen, die eine endliche Anzahl von Kategorien haben. Beispiele sind Geschlecht, Region oder Produkttyp. Beispielsweise wird im Car-Data-Set die Herkunft der Autos erfasst.\n\ndf[\"origin\"] = df[\"origin\"].map({1: \"USA\", 2: \"Europe\", 3: \"Japan\"})\ndf[\"origin\"].value_counts().plot(kind='bar')\nplt.xlabel(\"Origin\")\nplt.ylabel(\"Count\")\n\nText(0, 0.5, 'Count')\n\n\n\n\n\n\n\n\n\nNun wollen wir untersuchen, ob wir die Herkunft der Autos verwenden können, um den Verbrauch vorherzusagemn. Wie immer starten wir mit einer explorativen Analyse.\n\nimport seaborn as sns\n\nsns.boxplot(x=\"origin\", y=\"mpg\", data=df)\nplt.xlabel(\"Origin\")\nplt.ylabel(\"Miles per Gallon\")\n\nText(0, 0.5, 'Miles per Gallon')\n\n\n\n\n\n\n\n\n\nOffensichtlich gibt es hier einen Zusammenhang. Wir wollen unser Modell von vorhin erweiten, um es weiter zu verbessern. Um kategoriale Variablen in einer Linearen Regression zu Berücksichtigen gibt es zwei Möglichkeiten.\n\n8.2.1 Dummy-Variablen\n\n\n8.2.2 One-Hot-Encoding",
    "crumbs": [
      "Regressions-Analyse",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lineare Regression</span>"
    ]
  },
  {
    "objectID": "regression/advanced_linear_regression.html#modell-nur-mit-kategorischen-variablen",
    "href": "regression/advanced_linear_regression.html#modell-nur-mit-kategorischen-variablen",
    "title": "8  Lineare Regression",
    "section": "8.3 Modell nur mit Kategorischen Variablen",
    "text": "8.3 Modell nur mit Kategorischen Variablen",
    "crumbs": [
      "Regressions-Analyse",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lineare Regression</span>"
    ]
  },
  {
    "objectID": "regression/advanced_linear_regression.html#modell-mit-allem",
    "href": "regression/advanced_linear_regression.html#modell-mit-allem",
    "title": "8  Lineare Regression",
    "section": "8.4 Modell mit allem",
    "text": "8.4 Modell mit allem",
    "crumbs": [
      "Regressions-Analyse",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lineare Regression</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "This concludes our introduction to the basis of data science with a focus on engineering topics.\nThese notes introduced the main concepts with a clear focus of accurate mathematical representation and close illustration with programmatic examples.\nIf we need to sum up the main concept that is present in large sections of these notes it is that the correct representation is key in finding good concepts of computer based processing, To this extend the second key concept is to make sure the concepts can be handled via a computer and the student, so theory and programmatic application go hand in hand.\nBoth concepts stay true if we move to more evolved data science methods in further classes.",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Carroll, Lewis. 2015. Alice’s Adventures in Wonderland\nUnfolded.\n\n\nDATAtab. retrieved 2025. “Korrelationskoeffizient Tutorial\nImage.” https://datatab.de/assets/tutorial/Korrelationskoeffizient.png.\n\n\nDiez, David M., Christopher D. Barr, and Mine Cetinkaya-Rundel. 2019.\nOpenIntro Statistics. 4th ed. Boston, MA: OpenIntro. \\url{https://www.openintro.org/stat/textbook.php?stat_book=os}.\n\n\nFrisch, Max. 1957. Homo Faber. Ein Bericht. Frankfurt am Main:\nSuhrkamp.\n\n\nHong, Tao, Pierre Pinson, and Shu Fan. 2014. “Global Energy\nForecasting Competition 2012.” International Journal of\nForecasting 30 (2): 357–63. https://doi.org/https://doi.org/10.1016/j.ijforecast.2013.07.001.\n\n\nHyndman, RJ. 2018. Forecasting: Principles and Practice.\nOTexts.\n\n\nInductiveload, based on original work by Jhguch. n.d. “Two Sample\nt-Test.” https://commons.wikimedia.org/wiki/File:Two_sample_ttest.svg.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, et al.\n2013. An Introduction to Statistical Learning. Vol. 112.\nSpringer.\n\n\nKozyrkov, Cassie. 2018. “What on Earth Is Data Science?”\nmedium.com. \\url{https://kozyrkov.medium.com/what-on-earth-is-data-science-eb1237d8cb37}.\n\n\nMatthes, Eric. 2023. Python Crash Course - a Hands-on, Project-Based\nIntroduction to Programming. 3rd ed. No Starch Press. https://ehmatthes.github.io/pcc_3e/.\n\n\nRedAndr and Mikhail Ryazanov. 2011. “Satirical diagram illustrating the influence of pirates\ndecreasing on global warming as per Pastafarian beliefs.”\nhttps://commons.wikimedia.org/wiki/File:PiratesVsTemp(en).svg.\n\n\nShearer, Colin. 2000. “The CRISP-DM Model: The New Blueprint for\nData Mining.” Journal of Data Warehousing 5 (4): 13–22.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of\nStatistical Software 59: 1–23.\n\n\nWikimedia Commons contributors. retrieved 2025. “CRISP-DM\nProcess Diagram.” https://upload.wikimedia.org/wikipedia/commons/b/b9/CRISP-DM_Process_Diagram.png.",
    "crumbs": [
      "References"
    ]
  }
]