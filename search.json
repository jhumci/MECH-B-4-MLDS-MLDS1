[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning and Data Science 1",
    "section": "",
    "text": "Preface\nDies sind die Vorlesungsnotizen f√ºr die Lehrveranstaltung Machine Learning + Data Science I (Lecture) am MCI | The Entrepreneurial School Bachelor-Studiengangs Mechatronik im Sommer-Semester 2025.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#danksagungen",
    "href": "index.html#danksagungen",
    "title": "Machine Learning and Data Science 1",
    "section": "Danksagungen",
    "text": "Danksagungen\nWir danken der Open-Source-Community f√ºr die Bereitstellung exzellenter Tutorials und Anleitungen zu Data-Science-Themen in und mit Python im Web.\nEinzelne Quellen werden an den entsprechenden Stellen im Dokument zitiert.\nBesonderer Dank gilt Peter Kandof, f√ºr das Aufsetzen eines Beispielprojekts f√ºr die Vorlesung anhand von MECH-M-DUAL-1-DBM - Grundlagen datenbasierter Methoden. Diese Notizen wurden mit Quarto erstellt.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Einleitung",
    "section": "",
    "text": "Leistungs√ºberpr√ºfung\nIn dieser Lehrveranstaltung werden wir uns mit den grundlegenden Konzepten moderner Data-Science-Techniken befassen und eine solide Basis in Statistik und maschinellem Lernen legen. Wir behandeln alle essenziellen Grundlagen, die notwendig sind, um sich in diesem Bereich sicher zu bewegen. Dabei beschr√§nken wir uns nicht nur auf die theoretischen Aspekte, sondern nutzen auch Python, um die Inhalte programmatisch zu veranschaulichen.\nDie verwendeten Referenzen wurden nach Qualit√§t, freier Verf√ºgbarkeit und der Nutzung von Python ausgew√§hlt. F√ºr die statistischen Grundlagen greifen wir auf Beispiele aus Diez, Barr, and Cetinkaya-Rundel (2019) zur√ºck, w√§hrend f√ºr die Einf√ºhrung in das statistische Lernen James et al. (2013) herangezogen wird.\nDieses Skriptum richtet sich an Studierende der Ingenieurwissenschaften, weshalb mathematische Konzepte nur selten mit strengen Beweisen versehen sind.\nDie Vorlesungsteil der Lehrveranstaltung wird mit einer Klausur bewertet.",
    "crumbs": [
      "Einleitung"
    ]
  },
  {
    "objectID": "intro.html#leistungs√ºberpr√ºfung",
    "href": "intro.html#leistungs√ºberpr√ºfung",
    "title": "Einleitung",
    "section": "",
    "text": "Diez, David M., Christopher D. Barr, and Mine Cetinkaya-Rundel. 2019. OpenIntro Statistics. 4th ed. Boston, MA: OpenIntro. \\url{https://www.openintro.org/stat/textbook.php?stat_book=os}.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, et al. 2013. An Introduction to Statistical Learning. Vol. 112. Springer.",
    "crumbs": [
      "Einleitung"
    ]
  },
  {
    "objectID": "dataexploratory/index.html",
    "href": "dataexploratory/index.html",
    "title": "Umgang mit Daten und Explorative Analyse",
    "section": "",
    "text": "Alice: Would you tell me, please, which way I ought to go from here?\nThe Cheshire Cat: That depends a good deal on where you want to get to.\n‚Äî Carroll (2015)\n\n\nIn dieser Vorlesung erarbeiten wir gemeinsam die Kompetenzen, um Daten zu analysieren, Modelle zu erstellen und Vorhersagen zu treffen. Hierbei lernen wir alle Grundlagen kennen, die notwendig sind, um sich dabei zurechtzufinden. Zun√§chst gibt, es aber die Richtung zu kl√§ren, in die wir uns bewegen wollen.\n\n\n\n\n\n\nNote\n\n\n\nDiese Notizen setzen voraus, dass Sie √ºber grundlegende Programmierkenntnisse in Python verf√ºgen und wir bauen auf diesem Wissen auf. In diesem Sinne verwenden wir Python als Werkzeug und beschreiben die inneren Abl√§ufe nur, wenn es uns hilft, die behandelten Themen besser zu verstehen.\nFalls dies nicht der Fall ist, schauen Sie sich MCI-MECH-B-3-SWD-SWD-ILV an, einen Kurs √ºber Softwaredesign im selben Bachelor-Programm und von denselben Autoren.\n\n\nZus√§tzlich k√∂nnen wir die folgenden B√ºcher √ºber Python empfehlen:\n\nMatthes (2023): Python Crash Course - A hands-on, project-based introduction to programming: Online Material.\nPython Cheat Sheet provided by Matthes (2023).\n\nF√ºr die Statistik und Machine Learning ortientieren wir uns an folgenden B√ºchern:\n\nDiez, Barr, and Cetinkaya-Rundel (2019): OpenIntro Statistics: Online Material\nJames et al. (2013): Introduction to Statistical Learning: Online Material\n\n\n\n\n\nCarroll, Lewis. 2015. Alice‚Äôs Adventures in Wonderland Unfolded.\n\n\nDiez, David M., Christopher D. Barr, and Mine Cetinkaya-Rundel. 2019. OpenIntro Statistics. 4th ed. Boston, MA: OpenIntro. \\url{https://www.openintro.org/stat/textbook.php?stat_book=os}.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, et al. 2013. An Introduction to Statistical Learning. Vol. 112. Springer.\n\n\nMatthes, Eric. 2023. Python Crash Course - a Hands-on, Project-Based Introduction to Programming. 3rd ed. No Starch Press. https://ehmatthes.github.io/pcc_3e/.",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse"
    ]
  },
  {
    "objectID": "dataexploratory/data_science.html",
    "href": "dataexploratory/data_science.html",
    "title": "1¬† Data Science, Statisik, und Machine Learning",
    "section": "",
    "text": "1.1 Data Sciene: Projektvorgehen\nData Science, Statisik, Machine Learning und K√ºstliche Intelligenz sind Begriffe, die in den letzten Jahren immer h√§ufiger in den Medien und in der Wissenschaft auftauchen.\nEs gibt unterschiedlichte Definitionen f√ºr diese Begriffe, die sich je nach Kontext und Anwendungsbereich unterscheiden. Damit wir uns verstehen, versuchen wir es so:\nUm uns in unseren folgenden Abenteuern zurrechtzufinden, besch√§ftigen wir uns zun√§chst hier in Chapter 1 mit dem gundlegenden Herangehen an Data Science-Probleme. In Chapter 2 mit den Daten, die wir analysieren wollen.\nFigure¬†1.1 zeigt einige der vielen Entscheidungen, die wir treffen m√ºssen, wenn wir uns in unserem eigenen Projekt bewegen.\nSelbst, wenn uns das Ziel klar ist, k√∂nnen wir uns immernoch verlaufen. Um dies zu verhindern, gibt es verschiedene Vorgehensweisen, die uns helfen auf dem Pfad zu bleiben und uns helfen ein tiefgehendes Verst√§ndnis f√ºr einen Datensatz zu entwickeln. Hierbei unterst√ºtzen Standardvorgehensweisen wie CRISP-DM (Cross-Industry Standard Process for Data Mining Figure¬†1.2) Shearer (2000). Es gibts aber auch neuere und spezifische Prozesse, die in bestimmten Bereichen, wie z.B. im Feld der Zeitreihenprognosen Hyndman (2018), angewendet werden. F√ºr den Anfang begn√ºgen wir uns jedoch mit dem etabliertesten und verbreitesten Prozess.",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Data Science, Statisik, und Machine Learning</span>"
    ]
  },
  {
    "objectID": "dataexploratory/data_science.html#ssec-dataexploratory-data_science-project",
    "href": "dataexploratory/data_science.html#ssec-dataexploratory-data_science-project",
    "title": "1¬† Data Science, Statisik, und Machine Learning",
    "section": "",
    "text": "Figure¬†1.2: CRISP-DM Prozessdiagramm Wikimedia Commons contributors (retrieved 2025)\n\n\n\n\n1.1.1 CRISP-DM: Ein systematischer Ansatz f√ºr datenbezogene Projekte\nDer CRISP-DM-Prozess stellt ein generisches Rahmenwerk dar, das es erm√∂glicht, datengetriebene Projekte von der Problemdefinition bis hin zur operativen Anwendung zu strukturieren. Der Prozess ist in sechs zentrale Phasen unterteilt:\n\nBusiness Understanding\nZiel ist es, die gesch√§ftlichen Anforderungen und Problemstellungen klar zu definieren. Diese Phase umfasst Fragestellungen wie: Was m√∂chten wir mit den Daten l√∂sen? oder Welche Ergebnisse und Metriken sind entscheidend f√ºr den Erfolg?\nData Understanding\nHier wird das vorliegende Datenmaterial genauer untersucht, einschlie√ülich seiner Struktur, St√∂rfaktoren und potenzieller Verzerrungen. Eine erste Erkundung der Daten kann entscheidend sein, um Hypothesen zu entwickeln.\nDatenaufbereitung\nIn dieser Phase werden die Rohdaten bereinigt und in ein Format gebracht, das f√ºr die Analyse geeignet ist (vgl. Chapter 2). Aktivit√§ten umfassen das Entfernen fehlender Werte, Transformation von Variablen und die Auswahl relevanter Features.\nModellierung\nAufbau eines Modells zur Beantwortung der Kernfrage. Hierbei kann z. B. ein Regressionsmodell f√ºr Prognosen oder ein Klassifikationsmodell bei Entscheidungsproblematiken im Vordergrund stehen.\nEvaluierung\n√úberpr√ºfung, ob das Modell tats√§chlich valide und praktisch anwendbar ist. Kernfragen sind: Passt das Modell zu unseren Zielen? und Sind die Ergebnisse sinnvoll und umsetzbar?\nDeployment (Inbetriebnahme)\nDas Modell wird implementiert, um tats√§chliche Entscheidungen oder Vorhersagen zu unterst√ºtzen. Dies k√∂nnte z. B. bedeuten, ein automatisiertes System zu schaffen, das regelm√§√üig aktualisierte Prognosen liefert.\n\nTipp: Obwohl der Prozess linear erscheint, sind R√ºckspr√ºnge oft unvermeidlich, z. B. wenn das Modell nicht ausreichend Performanz liefert oder die Anforderungen sich √§ndern. Auch werden wir mehr √ºber der Daten und die Prozesse (Schritte 2 und 3) lernen, je mehr wir uns mit den Daten besch√§ftigen.\n\n\n1.1.2 Data Science als People Business?\nEs ist wichtig zu erkennen, dass Daten allein nur einen Ausschnitt der Realit√§t darstellen und ohne Kontext wenig n√ºtzen. Ein Kernelement ist daher, sich ausreichendes Dom√§nenwissen anzueignen, um die Semantik der zugrunde liegenden Daten zu interpretieren. H√§ufig kann es dabei hilfreich sein, Daten aus angrenzenden Kontexten hinzuzuziehen und den Dialog mit Expert:innen oder Personen mit Erfahrungswissen zu suchen.\nAls Standardvorgehen f√ºr viele datenbasierte Projekte empfiehlt Hyndman (2018) in seinem einflussreichen Werk zur Zeitreihenanalyse, diese Prinzipien auch auf Prognosen anzuwenden, √§hnliches gibt aber f√ºr alle Probblem.\n\n\n\n\n\n\nBeispiel:\n\n\n\nWenn wir historische Verkaufsdaten eines Gesch√§fts analysieren, um zuk√ºnftige Trends zu prognostizieren, sollten wir sowohl mit der sp√§teren Nutzer:in des Forecasts (z.B. Produktionsplaner:in f√ºr das Business Understanding), als auch mit den Erzeugen der Daten (z.B. Sales-Abteilung f√ºr das Data Understanding) sprechen.\n\nDieses bestimmt, wie unsere Modellierung aussehen soll.\n\nWie weit in die Zukunft m√ºssen wir vorhersagen (Prognosehorizont)?\nWelche Aufl√∂sung ben√∂tigt unsere Prognose (z.B. tagescharf oder w√∂chtentlich)?\nIn welche Systeme (z.B. Dashboards) muss die Prognose sp√§ter deployed werden?\nWie soll das Modell evaluiert werden (z.B. ist es wichtiger an keinem Tag gro√üe Ausrei√üer zu haben oder die kumulativen Absatzzahlen √ºber das Jahr hinweg genau zu treffen?)\n\nDurch Gespr√§che mit weiteren Stakeholdern ergibt sich zudem Business & Data Understanding\n\nGibt es saisonale Effekte (z.B. Insekten-Schutz-Produkte)\nGibt es systematische Fehler in der Datenaufzeichntung (End-of-Year-Effecs)\nGab es Systemumstellungen in der Datenerfassung oder andere Externe Br√ºche (z.B. Markteintritte)\n\n\nDie Herausforderung ist hierbei jedoch, dass Prognosen fehleranf√§llig sind. Ein pl√∂tzlicher Markteinbruch oder ein externes Ereignis, wie ein sozio√∂konomischer Schock, k√∂nnte die Vorhersagen unbrauchbar machen. Evtl. geh√∂rt zum Business Understanding auch die Grenzen einer datanbasierenden L√∂sung zu verstehen.\n\n\n\n\n1.1.3 Ein Beispiel-Datenset: loan50\nIm Folgenden nutzen wir das loan50-Datenset, das aus dem Lehrbuch von Diez, Barr, and Cetinkaya-Rundel (2019) stammt und zur Erkundung solcher Fragestellungen dient.\nDas loan50-Datenset enth√§lt Informationen zu 50 vergebenen Krediten, die √ºber die Lending Club Plattform vermittelt wurden. Diese Plattform erm√∂glicht es Einzelpersonen, untereinander Kredite zu vergeben. Wie in vielen Finanzanwendungen sind jedoch nicht alle Kreditnehmer:innen gleich:\n\nKandidat:innen mit hoher R√ºckzahlungssicherheit werden bevorzugt behandelt und erhalten in der Regel Kredite mit niedrigeren Zinss√§tzen.\nRisikoreichere Antragsteller:innen k√∂nnten hingegen keine Angebote erhalten oder hohe Zinss√§tze ablehnen.\n\n\n\n\n\n\n\nWarning\n\n\n\nAchtung: Dieses Datenset enth√§lt nur tats√§chlich vergebene Kredite und repr√§sentiert daher nur eine Teilmenge aller m√∂glichen Kreditanfragen. Diese Einschr√§nkung kann dazu f√ºhren, dass wir relevante Informationen √ºber nicht vergebene Kreditantr√§ge nicht betrachten. Solche Probeme bezeichnet man gemeinhin als Bias (Verzerrung).\n\n\nEinige der verf√ºgbaren Variablen:\nDie unten aufgelisteten Variablen beschreiben die Eigenschaften des umfassenderen loans_full_schema-Datensatzes, wovon eine Teilmenge im loan50-Datenset enthalten ist.\n\n\n\nVariable\nBeschreibung\n\n\n\n\nemp_title\nBerufsbezeichnung\n\n\nemp_length\nAnzahl der Jahre im Beruf (aufgerundet, Werte √ºber 10 Jahre werden als 10 dargestellt)\n\n\nstate\nUS-Bundesstaat (zweistellige Abk√ºrzung)\n\n\nhome_ownership\nWohnsituation der Bewerber:innen (z. B. Eigentum, gemietet)\n\n\nannual_income\nJ√§hrliches Einkommen\n\n\nverified_income\nArt der Verifikation des Einkommens\n\n\ndebt_to_income\nSchulden-Einkommens-Verh√§ltnis\n\n\ngrade\nBewertung des Kredits, wobei A die h√∂chste Stufe ist\n\n\n‚Ä¶\nWeitere Variablen finden Sie in der vollst√§ndigen Beschreibung: loan50 - OpenIntro Dataset\n\n\n\n\n\n\n\n\n\nBusiness und Data Understanding\n\n\n\n\nWelche Fragestellungen k√∂nnten mit diesem Datensatz beantwortet werden?\n\nBeispielsweise: Gibt es einen Zusammenhang zwischen dem Schulden-Einkommens-Verh√§ltnis und der Kreditbewilligung?\n\nWas m√ºsste noch bekannt sein, um die Daten besser zu verstehen?\n\nWelche spezifischen Regeln wurden aufgestellt, um Kredite zu vergeben oder abzulehnen?\nWo bestehen potenzielle Verzerrungen (z. B. durch die fehlenden Daten zu abgelehnten Antr√§gen)?\n\n\n\n\n\n\n\n\nDiez, David M., Christopher D. Barr, and Mine Cetinkaya-Rundel. 2019. OpenIntro Statistics. 4th ed. Boston, MA: OpenIntro. \\url{https://www.openintro.org/stat/textbook.php?stat_book=os}.\n\n\nHyndman, RJ. 2018. Forecasting: Principles and Practice. OTexts.\n\n\nKozyrkov, Cassie. 2018. ‚ÄúWhat on Earth Is Data Science?‚Äù medium.com. \\url{https://kozyrkov.medium.com/what-on-earth-is-data-science-eb1237d8cb37}.\n\n\nShearer, Colin. 2000. ‚ÄúThe CRISP-DM Model: The New Blueprint for Data Mining.‚Äù Journal of Data Warehousing 5 (4): 13‚Äì22.\n\n\nWikimedia Commons contributors. retrieved 2025. ‚ÄúCRISP-DM Process Diagram.‚Äù https://upload.wikimedia.org/wikipedia/commons/b/b9/CRISP-DM_Process_Diagram.png.",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Data Science, Statisik, und Machine Learning</span>"
    ]
  },
  {
    "objectID": "dataexploratory/data_sets.html",
    "href": "dataexploratory/data_sets.html",
    "title": "2¬† Data Sets",
    "section": "",
    "text": "2.1 Tidy-Data\nEin sauber aufbereiteter Datensatz ist eine grundlegende Voraussetzung f√ºr jede datenbasierte Analyse und ist im CRISP-DM Teil der Datenaufbereitung. Wir starten zun√§chst mit dem Konzept von Tidy Data Section 2.1, welches sich mit der sauberen Strukturierung von Daten befasst. Anschlie√üend werden wir uns mit den Typen von Variablen Section 2.2 befassen, die unter anderem den Ausschlag gibt, welche verschiedenen Visualisierungen Section 2.3 sinnvoll sind, um sich ein Data Understanding zu erarbeiten. Zum Schluss werden wir uns mit den verschiedenen Ma√üen f√ºr Variablen Section 2.4 auseinandersetzen, mit welchen man Datens√§tze beschreiben kann.\nWenn wir mit Computern automatisiert arbeiten m√∂chten, ist neben der Semantik der Daten auch deren Syntax essenziell. Das bedeutet, dass die Daten in einer Struktur vorliegen m√ºssen, die ihre Semantik sinnvoll abbildet.\nEin weitverbreiteter Standard, der in diesem Zusammenhang h√§ufig genutzt wird, sind die von Wickham (2014) beschriebenen Tidy Data Conventions. Dieses Datenformat ist de facto eine Grundlage f√ºr viele Softwarepakete wie pandas, statsmodels, sklearn, tensorflow und andere Werkzeuge im Bereich Datenanalyse und des maschinelles Lernen.",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data Sets</span>"
    ]
  },
  {
    "objectID": "dataexploratory/data_sets.html#sec-dataexploratory-data_tidy-data",
    "href": "dataexploratory/data_sets.html#sec-dataexploratory-data_tidy-data",
    "title": "2¬† Data Sets",
    "section": "",
    "text": "Hinweis: Datenbanknormalisierung\n\n\n\n\n\nIm Grunde handelt es sich bei diesem Format um ein Prinzip, das auch in der Datenbanknormalisierung nach Codd verfolgt wird. Ihnen wird dieses Konzept in relationalen Datenbanken (SQL) erneut begegnen.\n\n\n\n\n2.1.1 Was bedeutet ‚ÄúTidy Data‚Äù?\nTidy Data folgt drei Grundprinzipien:\n\n\n\n\n\n\nGrundprinzipien von Tidy Data\n\n\n\n\nJede Zeile repr√§sentiert eine Beobachtung (bzw. eine Einheit).\nJede Spalte repr√§sentiert eine Variable (bzw. ein Attribut).\nJede Zelle enth√§lt genau einen pr√§zisen Wert (einen primitiven Datentyp wie int, float, str oder bool ‚Äì keine Listen, Tupel oder geschachtelten Objekte).\n\n\n\nEin Beispiel f√ºr nicht-Tidy-Daten k√∂nnte eine Spalte enthalten, in der mehrere Werte in einer Liste zusammengefasst sind. Solche Daten sind schwerer zu verarbeiten und unflexibler beim Einsatz in Analysetools.\nTidy Data hilft uns bei der Datenbereinigung und Datenanalyse. Es erleichtert die Automatisierung und Standardisierung von Prozessen und reduziert die Wahrscheinlichkeit von Fehlern.\n\nKompatibilit√§t: Viele Python-Bibliotheken wie pandas, statsmodels oder seaborn setzen voraus, dass die verwendeten Daten im Tidy-Format vorliegen.\nAutomatisierung: Tidy-Daten erleichtern Standardoperationen wie Filtern, Gruppieren und Pivotieren erheblich.\nFehlerpr√§vention: Unstrukturierte oder verschachtelte Datenstrukturen sind fehleranf√§llig und schwer zu debuggen.\n\n\n\n\n\n\n\nDaten in das Tidy-Format transformieren\n\n\n\nEs gibt viele hilfreiche Funktionen und Methoden in pandas, um Daten zu ‚Äútidy-fizieren‚Äù. Ein Beispiel ist die Verwendung der Methoden stack, unstack und melt. Diese helfen dabei, Daten umzustrukturieren und in die gew√ºnschte lange (viele Zeilen) oder weite (viele Spalten) Form zu bringen. Ein hilfreicher Artikel hierzu ist Reshape with Pandas.\nüí° Tipp: Wenn Sie unsicher sind, wie Sie Ihre Daten umorganisieren sollten, k√∂nnen Sie ein Beispiel (z.B. head() eines DataFrames) und die gew√ºnschte Struktur (also Spaltennamen) in ein Large Language Model eingeben. Oft erhalten Sie klare Vorschl√§ge zur Umstrukturierung!\n\n\n\n\n2.1.2 Positive Beispiele f√ºr Tidy Data\nDer folgende Beispielcode zeigt, wie Sie ein CSV-Datei laden und sich mit den ersten Zeilen vertraut machen k√∂nnen. Gl√ºcklicher Weise ist dieser Datensatz bereits im Tidy-Format. Jede Zeile repr√§sentiert eine Beobachtung (Kreditnehmer) und jede Spalte eine Variable (Attribut).\n\nimport pandas as pd\n\n# Lesen der CSV-Datei in einen DataFrame\ndf = pd.read_csv(r\"../_assets/dataexploratory/loan50.csv\")\n# Ausgabe der ersten Zeilen des Datensatzes\nprint(df.head())\n\n  state  emp_length  term homeownership  annual_income verified_income  \\\n0    NJ         3.0    60          rent          59000    Not Verified   \n1    CA        10.0    36          rent          60000    Not Verified   \n2    SC         NaN    36      mortgage          75000        Verified   \n3    CA         0.0    36          rent          75000    Not Verified   \n4    OH         4.0    60      mortgage         254000    Not Verified   \n\n   debt_to_income  total_credit_limit  total_credit_utilized  \\\n0        0.557525               95131                  32894   \n1        1.305683               51929                  78341   \n2        1.056280              301373                  79221   \n3        0.574347               59890                  43076   \n4        0.238150              422619                  60490   \n\n   num_cc_carrying_balance        loan_purpose  loan_amount grade  \\\n0                        8  debt_consolidation        22000     B   \n1                        2         credit_card         6000     B   \n2                       14  debt_consolidation        25000     E   \n3                       10         credit_card         6000     B   \n4                        2    home_improvement        25000     B   \n\n   interest_rate  public_record_bankrupt loan_status  has_second_income  \\\n0          10.90                       0     Current              False   \n1           9.92                       1     Current              False   \n2          26.30                       0     Current              False   \n3           9.92                       0     Current              False   \n4           9.43                       0     Current              False   \n\n   total_income  \n0         59000  \n1         60000  \n2         75000  \n3         75000  \n4        254000  \n\n\n\n\n2.1.3 Negative Beispiele f√ºr Tidy Data\nFolgendes Datenbeispiel zeigt, wie ein Datensatz nicht im Tidy-Format aussieht. Wir sehen die Strombedarfe von verschiedenen Netzgebieten zone_id zu verschiedenen Zeitpunkten. Allerdings ist es ung√ºnstig, dass nicht jede Kombination aus Zone und Zeitpunkt eine eigene Zeile hat. Stattdessen sind die Werte f√ºr alle 24 Stunden in einer eigenen Spalte.\n\nimport pandas as pd\n\ndf = pd.read_csv(r\"../_assets/dataexploratory/GEFCom2012/GEFCOM2012_Data/Load/Load_history.csv\")\nprint(df.head())\n\n   zone_id  year  month  day      h1      h2      h3      h4      h5      h6  \\\n0        1  2004      1    1  16,853  16,450  16,517  16,873  17,064  17,727   \n1        1  2004      1    2  14,155  14,038  14,019  14,489  14,920  16,072   \n2        1  2004      1    3  14,439  14,272  14,109  14,081  14,775  15,491   \n3        1  2004      1    4  11,273  10,415   9,943   9,859   9,881  10,248   \n4        1  2004      1    5  10,750  10,321  10,107  10,065  10,419  12,101   \n\n   ...     h15     h16     h17     h18     h19     h20     h21     h22  \\\n0  ...  13,518  13,138  14,130  16,809  18,150  18,235  17,925  16,904   \n1  ...  16,127  15,448  15,839  17,727  18,895  18,650  18,443  17,580   \n2  ...  13,507  13,414  13,826  15,825  16,996  16,394  15,406  14,278   \n3  ...  14,207  13,614  14,162  16,237  17,430  17,218  16,633  15,238   \n4  ...  13,845  14,350  15,501  17,307  18,786  19,089  19,192  18,416   \n\n      h23     h24  \n0  16,162  14,750  \n1  16,467  15,258  \n2  13,315  12,424  \n3  13,580  11,727  \n4  17,006  16,018  \n\n[5 rows x 28 columns]\n\n\nEine Umwandlung in das Tidy-Format w√ºrde, wie folgt aussehen, wobei wird darauf achten sollten, dass der timestamp als datetime-Objekt und die load als int gespeichert wird:\n\n\n\nzone_id\ntimestamp\nload (kW)\n\n\n\n\n1\n2012-01-01 00:00:00\n1000\n\n\n1\n2012-01-01 01:00:00\n1100",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data Sets</span>"
    ]
  },
  {
    "objectID": "dataexploratory/data_sets.html#sec-dataexploratory-data_variable-types",
    "href": "dataexploratory/data_sets.html#sec-dataexploratory-data_variable-types",
    "title": "2¬† Data Sets",
    "section": "2.2 Messniveaus von Variablen",
    "text": "2.2 Messniveaus von Variablen\nVariablen sind die Bausteine von Daten und repr√§sentieren die Merkmale, die wir messen oder beobachten. Im Kapitel Section 2.2 werden wir uns noch tiefer mit Variablen auseinandersetzen. In der Datenanalyse ist es wichtig, die Art der Variablen zu kennen, da dies beeinflusst, welche Methoden und Visualisierungen f√ºr die Daten geeignet sind. Variablen lassen sich nach ihrem Messniveau klassifizieren, was wiederum die Art der Informationen beschreibt, die sie enthalten. Die bekannteste Klassifikation von Variablen basiert auf den vier Messniveaus von Stanley Smith Stevens: Nominal, Ordinal, Intervall und Ratio (Verh√§ltnis).\n\nNutzen Sie den oben gezeigten Datensatz loan50, um die folgenden Aufgaben f√ºr unterschiedliche Variablenarten zu l√∂sen:\n\n\nSortieren: Wie k√∂nnte man die Werte der Variablen\n\nstate,\ngrade,\nein Beispiel f√ºr ein Intervallniveau (aber kein Ratio),\nannual_income\nsinnvoll in aufsteigender Reihenfolge anordnen?\n\nZentrale Werte bestimmen: Wie l√§sst sich ein zentraler Wert bestimmen, sei es durch den Modus, die Median oder den Mittelwert?\nBeziehungen beschreiben: Welche Aussage k√∂nnte man √ºber die Beziehung zwischen zwei Werten einer Variablen machen?\n\n\n2.2.1 Nominale Variablen\nDefinition: Nominale Variablen kategorisieren Daten ohne eine festgelegte Reihenfolge.\n\ndf = pd.read_csv(r\"../_assets/dataexploratory/loan50.csv\")\nprint(df[\"state\"].head())\n\n0    NJ\n1    CA\n2    SC\n3    CA\n4    OH\nName: state, dtype: object\n\n\n\nprint(df[\"state\"].value_counts().head())\n\nstate\nCA    9\nTX    5\nIL    4\nNJ    3\nMD    3\nName: count, dtype: int64\n\n\n\nprint(df[\"state\"].mode().head())\n\n0    CA\nName: state, dtype: object\n\n\n\nSortieren: Es gibt keine inh√§rente Methode, diese Werte zu sortieren. Dies liegt daran, dass nominale Daten keine Reihenfolge implizieren.\nZentraler Wert: Modus, da dieser Wert am h√§ufigsten vorkommt.\nBeziehungen: Die Beziehung zwischen zwei Werten kann nur beschreiben, ob sie in derselben Kategorie sind oder nicht.\n\n\n\n2.2.2 Ordinale Variablen\nOrdinale Variablen haben eine nat√ºrliche Ordnung, aber der Abstand zwischen den Werten ist nicht zwingend gleichm√§√üig.\n\nprint(df[\"grade\"].head())\n\n0    B\n1    B\n2    E\n3    B\n4    B\nName: grade, dtype: object\n\n\n\nprint(df[\"grade\"].sort_values().head())\n\n49    A\n18    A\n33    A\n36    A\n14    A\nName: grade, dtype: object\n\n\n\nprint(df[\"grade\"].value_counts().head())\n\ngrade\nB    19\nA    15\nD     8\nC     6\nE     2\nName: count, dtype: int64\n\n\n\nprint(df[\"grade\"].mode())\n\n0    B\nName: grade, dtype: object\n\n\n\n# Define the order for the categorical values\ngrade_order = sorted(df[\"grade\"].unique())\n\n# Convert the 'grade' column to a categorical type with the specified order\ndf['grade'] = pd.Categorical(df['grade'], categories=grade_order, ordered=True)\n\n# Convert categorical data to numerical codes\ngrade_codes = df['grade'].cat.codes\n\nprint(grade_codes.median())\n\n1.0\n\n\n\nSortieren: Mit geeigneten Regeln ist es m√∂glich, diese Werte in aufsteigender Reihenfolge zu ordnen: [\"C\", \"B\", \"A\"].\nZentraler Wert:: Der Modus ist geeignet, und der Median zeigt auf, dass 50 % der Werte gleich oder niedriger als \"B\" sind.\nBeziehungen: Zwei Werte lassen sich nach ihrer Position der Reihenfolge vergleichen: h√∂her oder niedriger.\n\n\n\n2.2.3 Intervallskalierte Variablen\nIntervallskalierte Variablen haben geordnete Werte mit gleichm√§√üigen Abst√§nden zwischen ihnen, aber sie besitzen keinen absoluten Nullpunkt. Ein Beispiel ist das j√§hrliche Einkommen.\n\nprint(df[\"annual_income\"].head())\n\nprint(df[\"annual_income\"].mean())\n\n0     59000\n1     60000\n2     75000\n3     75000\n4    254000\nName: annual_income, dtype: int64\n86170.0\n\n\n\nSortieren:: Daten k√∂nnen numerisch in aufsteigender Reihenfolge sortiert werden.\nZentraler Wert:: Der Modus und Median sind geeignete Ma√üe. Der arithmetische Mittelwert berechnet sich als: \\(\\mu = \\frac{1}{n} \\sum x_i\\)\nBeziehungen: Der Abstand (Intervall) zwischen zwei Werten kann quantifiziert werden.\n\n\n\n\n\n\n\nUnterschied zwischen Intervall- und Ratiodaten\n\n\n\nIm Gegensatz zum Ratio-Messniveau besitzen Intervall-Daten keinen absoluten Nullpunkt. Aussagen wie ‚Äúdas Doppelte‚Äù sind daher nicht sinnvoll.\n\n\n\n\n\n\n\n\nTip\n\n\n\nEs ist oft hilfreich, sich das Messniveau einer Variablen vor Beginn der Analyse klar zu machen. Das Messniveau entscheidet auch, welche Visualisierung sinnvoll sind. M√∂gliche Fehleinsch√§tzungen k√∂nnen zu falschen oder unzul√§ssigen Berechnungen f√ºhren, z. B. Mittelwerte bei Nominaldaten. Daten sollten entsprechend ihrem Typ gereinigt und transformiert werden.",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data Sets</span>"
    ]
  },
  {
    "objectID": "dataexploratory/data_sets.html#sec-dataexploratory-data_visualization",
    "href": "dataexploratory/data_sets.html#sec-dataexploratory-data_visualization",
    "title": "2¬† Data Sets",
    "section": "2.3 Visualisierungen",
    "text": "2.3 Visualisierungen\n\n\n\n\n\n\nTip\n\n\n\nEs gibt viele M√∂glichkeiten, Daten zu visualisieren, um Muster und Trends zu erkennen. Zwei weit verbreitete Pakete sind matplotlib und plotly. Im folgenden benutzen wir vorallem seaborn, welches eine Erweiterung von matplotlib ist und speziell f√ºr statistische Visualisierungen entwickelt wurde.\n\n\n\n2.3.1 Histogramme\nEin Histogramm ist eine angen√§herte Darstellung der Verteilung einer intervallskalierten Variable. Es liefert wertvolle Informationen √ºber:\n\nZentralwert: Wo liegen die Daten?\nVarianz: Wie stark streuen die Daten?\nVerteilung: Wie h√§ufig treten bestimmte Werte auf?\n\n(fig:sec-dataexploratory-sets-histogram?) zeigt ein Histogramm des j√§hrlichen Einkommens aus dem Datensatz loan50.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create a histogram of the annual income\nsns.histplot(df[\"annual_income\"], bins=10, stat = 'count')\n\n\n\n\nHistogramm des j√§hrlichen Einkommens\n\n\n\n\n\n2.3.1.1 Konstruktion eines Histogramms\nEin Histogramm wird in wenigen Schritten erstellt. Meinst wird dies bereits f√ºr uns wie in seaborn erledigt, es ist jedoch hilfreich, die Schritte zu kennen, um die Visualisierung besser zu verstehen, da sie manchmal abgewandelt wird.\n\nBinning: Teilen Sie die Werte der beobachteten Variablen \\(x_i\\) in eine Reihe von Intervallen (Bins oder Buckets) auf.\nZ√§hlen: Erfassen Sie, wie viele Werte in jedes Intervall fallen (z. B. 5% der Werte).\nIntervall-Eigenschaften: Die Intervalle der Bins sollten aufeinander folgen, sich nicht √ºberlappen und idealerweise die gleiche Breite haben.\nDarstellung: Die Anzahl der Werte in jedem Intervall wird entlang der y-Achse aufgetragen. F√ºr relative H√§ufigkeiten wird durch die Stichprobengr√∂√üe geteilt.\nWenn die Intervalle gleich breit sind, wird die y-Achse als H√§ufigkeit interpretiert. Wenn die Intervalle unterschiedlich breit sind, wird die y-Achse als Dichte interpretiert. Dazu wird die H√∂he der Balken so skaliert, dass die Fl√§che unter dem Histogramm \\(1\\) ergibt.",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data Sets</span>"
    ]
  },
  {
    "objectID": "dataexploratory/data_sets.html#sec-dataexploratory-data_variable-measures",
    "href": "dataexploratory/data_sets.html#sec-dataexploratory-data_variable-measures",
    "title": "2¬† Data Sets",
    "section": "2.4 Ma√üe f√ºr Variablen",
    "text": "2.4 Ma√üe f√ºr Variablen\nVariablen lassen sich auf verschiedene Weisen beschreiben. Lagema√üe bzw. die zentrale Tendenz gibt an, wo die Daten liegen, w√§hrend die Streuung angibt, wie weit die Daten von diesem Wert entfernt sind. Die Zusammenh√§nge zwischen Variablen k√∂nnen durch Korrelationen und Kovarianzen beschrieben werden.\n\n2.4.1 Lagema√üe\nVariablen k√∂nnen auf verschiedene Weisen beschrieben werden. Beispielweise k√∂nnen Lagema√üe wie der Arithmetischer Mittelwert (Mean), Median oder Modus genutzt werden, um die zentrale Tendenz der Daten zu beschreiben. Welche wir einsetzen, h√§ngt vom Messniveau der Variablen ab.\nBetrachten wir eine mindestens intervall-skalierte Variable \\(x \\in \\mathbb{R}^n\\) aus den Datensatz, so k√∂nnen wir die folgenden Lagema√üe berechnen:\n\ndas maximale Element bzw. der H√∂chstwert: \\[\nx^{max} = \\max_i x_i,\n\\]\n\n\nincome_max = df[\"annual_income\"].max()\nprint(f\"{income_max=}\")\n\nincome_max=np.int64(325000)\n\n\n\nder minimale Wert bzw. das Minimum:\n\n\nincome_min = df[\"annual_income\"].min()\nprint(f\"{income_min=}\")\n\nincome_min=np.int64(28800)\n\n\n\\[\nx^{min} = \\min_i x_i,\n\\] - der arithmetische Mittelwert: \\[\n\\overline{x} = \\frac1n \\sum_{i=1}^n x_i = \\frac{x_1 + x_2 + \\cdots + x_n}{n},\n\\]\n\nincome_mean = df[\"annual_income\"].mean()\nprint(f\"{income_mean=}\")\n\nincome_mean=np.float64(86170.0)\n\n\n\nder Median ist der Wert, der die Daten in zwei gleich gro√üe Teile teilt:\n\n\\[\n\\widetilde{x} = \\begin{cases}\n                x_{(n+1)/2}& n\\quad \\text{odd}\\\\\n                \\frac{x_{n/2} + x_{n/2+1}}{2}& n\\quad \\text{even}\n                \\end{cases},\n\\]\n\nincome_median = df[\"annual_income\"].median()\nprint(f\"{income_median=}\")\n\nincome_median=np.float64(74000.0)\n\n\n\nVerallgemeinert f√ºr \\(p\\in(0,1)\\) ist das p-Quantil \\(\\overline{x}_p\\) der Wert, der die Daten in zwei Teile teilt, wobei \\(p\\) der Anteil der Daten ist, die kleiner oder gleich \\(\\overline{x}_p\\) sind.\n\n\nincome_quartiles = df[\"annual_income\"].quantile([0.25, 0.5, 0.75])\nprint(f\"{income_quartiles=}\")\n\nincome_quartiles=0.25    55750.0\n0.50    74000.0\n0.75    99500.0\nName: annual_income, dtype: float64\n\n\n\\[\n\\overline{x}_p = \\begin{cases}\n                 \\frac12\\left(x_{np} + x_{np+1}\\right) & pn \\in \\mathbb{N}\\\\\n                x_{\\lfloor np+1\\rfloor} & pn \\not\\in \\mathbb{N}\n                \\end{cases}.\n\\]\nEinige Quantile haben spezielle Namen, wie der Median f√ºr \\(p=0.5\\), das untere und obere Quartil f√ºr \\(p=0.25\\) und \\(p=0.75\\) (oder erstes, zweites (Median) und drittes Quartil), respektive.\n\n\n\n\n\n\nCaution\n\n\n\nWie gut lassen sich Arithmetischer Mittelwert, Median und Mode aus dem Histogramm ablesen?\n\n\n\n\n2.4.2 Kumulative Histogramme und Empirische Verteilungsfunktionen\nAls Alternative haben sich kumulative Histogramme, wie in ?fig-sec-dataexploratory-sets-kum-histogram, etabliert, die die kumulative Verteilungsfunktion (Cumulative Density Function / CDF) visualisieren. Diese Funktion gibt an, wie viele Werte kleiner oder gleich einem bestimmten Wert sind. Zur Konstruktion der CDF werden die Daten in aufsteigender Reihenfolge sortiert und die relative H√§ufigkeit der Werte berechnet.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create a histogram of the annual income\nsns.histplot(df[\"annual_income\"], bins=10, stat = 'density', cumulative=True)\n\n\n\n\n\n\n\n\n\n\n2.4.3 Steuungsma√üe\nSteuungsma√üe beschreiben die Streuung der Daten um den zentralen Wert. Beispiele sind die Spannweite, Varianz und die Standardabweichung.\nDie Spannweite ist die Differenz zwischen dem gr√∂√üten und kleinsten Wert: \\[\n\\text{Spannweite} = x^{max} - x^{min}.\n\\] Die Varianz ist ein Ma√ü f√ºr die mittlere quadratische Abweichung der Daten vom Mittelwert. Die Einheit der Varianz ist das Quadrat der Einheit der Daten: \\[\n\\sigma = \\sqrt{\\operatorname{Var}(x)}.\n\\] \\[\n\\operatorname{Var}(x) = \\frac1n \\sum_{i=1}^n (x_i - \\mu)^2.\n\\]\nDie Standardabweichung ist die Quadratwurzel der Varianz. Damit hat sie die gleiche Einheit wie die Daten:\n\\[\n\\sigma = \\sqrt{\\operatorname{Var}(x)}.\n\\]\nIn Python k√∂nnen wir die Varianz und Standardabweichung mit pandas oder numpy berechnen:\n\nprint(f\"Varianz: {df['annual_income'].var()}\")\nprint(f\"Standardabweichung: {df['annual_income'].std()}\")\n\nVarianz: 3313901734.6938777\nStandardabweichung: 57566.49837096119\n\n\n\n\n\n\n\n\nKorrigerte Stichproben-Varianz\n\n\n\nDie Varianz einer Stichprobe ist kein erwartungstreuer Sch√§tzer f√ºr die Varianz der Grundgesamtheit. Die Begriffe werden wir in Section 5.1 noch genauer betrachten. Einfach gesagt, die Varianz einer Stichprobe ist tendenziell kleiner als die Varianz der Grundgesamtheit, da wird beim zuf√§lligen Ziehen wahrscheinlich eher aus der Mitte als von den Extremen ziehen. Die korrigierte Stichproben-Varianz wird durch \\(n-1\\) statt \\(n\\) im Nenner definiert. In pandas wird die korrigierte Stichproben-Varianz als Standard verwendet, die unkorrigierte Varianz kann mit dem Parameter ddof=0 berechnet werden.\n\n\n\n\n2.4.4 Zusammenhangsma√üe\nIn der Statistik beschreiben Zusammenhangsma√üe die Beziehung zwischen zwei Variablen. Beispiele sind die Kovarianz und der Korrelationskoeffizient. Diese geben einen Hinweis darauf, ob und wie stark zwei Variablen zusammenh√§ngen.\n\n2.4.4.1 Korrrelation\nIn der Statistik beschreibt der Begriff Korrelation oder Abh√§ngigkeit jede statistische Beziehung zwischen bivariaten Daten (gepaarte Daten) oder Zufallsvariablen.\nIn unserem Datensatz k√∂nnen wir beispielsweise untersuchen: - emp_length: Anzahl der Jahre im Beruf - annual_income: J√§hrliches Einkommen - debt_to_income: Schulden-Einkommens-Verh√§ltnis\nUm die Daten besser zu verstehen k√∂nnen wir zun√§chst einen Scatterplot ?fig-sec-dataexploratory-sets-scatterplot erstellen:\n\nimport seaborn as sns\n\ndf_reduced = df[[\"emp_length\", \"annual_income\", \"debt_to_income\"]]\n\nsns.pairplot(df_reduced)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiskussion\n\n\n\nWie interpretieren Sie den Zusammenhang zwischen den Variablen emp_length, annual_income und debt_to_income? Was w√ºrde entsprechend Ihres Dom√§nenwissens Sinn ergeben?\n\n\n\n\n2.4.4.2 Kovarianz\nDie Kovarianz ist ein Ma√ü f√ºr die gemeinsame Variabilit√§t zweier Variablen. Sie ist definiert als der Erwartungswert des Produkts der Abweichungen der Zufallsvariablen von ihren Erwartungswerten:\n\\[\n\\operatorname{cov}(x, y) = \\frac1n \\sum_{i=1}^n (x_i - \\overline{x})(y_i - \\overline{y}).\n\\]\nIn Python k√∂nnen wir die Kovarianz-Matrix mit pandas direkt berechnen:\n\ndf_reduced.cov()\n\n\n\n\n\n\n\n\nemp_length\nannual_income\ndebt_to_income\n\n\n\n\nemp_length\n12.393174\n1.924082e+04\n-0.051111\n\n\nannual_income\n19240.824468\n3.313902e+09\n-8584.039227\n\n\ndebt_to_income\n-0.051111\n-8.584039e+03\n0.918269\n\n\n\n\n\n\n\nDie Kovarianz kann Wertebereiche von \\(-\\infty\\) bis \\(+\\infty\\) annehmen und ist nicht normiert. Um die St√§rke der Beziehung zu quantifizieren, verwenden wir den Korrelationskoeffizienten, der leichter zu interpretieren ist.\n\n\n2.4.4.3 Korrelationskoeffizient\nDer Korrelationskoeffizient nach Pearson ist ein Ma√ü f√ºr den linearen Zusammenhang zwischen zwei Variablen. Er ist definiert als das Verh√§ltnis der Kovarianz der beiden Variablen zur Multiplikation ihrer Standardabweichungen:\n\\[\n\\rho_{x,y} = \\operatorname{corr}(x, y) = \\frac{\\operatorname{cov}(x, y)}{\\sigma_x \\sigma_y},\n\\]\nwobei \\(\\sigma_x\\) und \\(\\sigma_y\\) die Standardabweichungen der Variablen sind.\nIn Python k√∂nnen wir den Korrelationskoeffizienten mit numpy berechnen:\n\ndf_reduced.corr()\n\n\n\n\n\n\n\n\nemp_length\nannual_income\ndebt_to_income\n\n\n\n\nemp_length\n1.000000\n0.093156\n-0.014857\n\n\nannual_income\n0.093156\n1.000000\n-0.155610\n\n\ndebt_to_income\n-0.014857\n-0.155610\n1.000000\n\n\n\n\n\n\n\nEin Korrelationskoeffizient von \\(1\\) bedeutet eine perfekte positive Korrelation, \\(-1\\) eine perfekte negative Korrelation und \\(0\\) keine Korrelation. In diesem fall beobachten wir eine leichte negative Korrelation zwischen emp_length und debt_to_income und eine leichte positive Korrelation zwischen emp_length und annual_income. Eine Variable kann auch mit sich selbst perfekt korreliert sein, was zu einem Korrelationskoeffizienten von \\(1\\) f√ºhrt.\n\n\n\n\n\n\nVorsicht\n\n\n\nDer Korrelationskoeffizient misst nur lineare Zusammenh√§nge. Nicht-lineare Zusammenh√§nge werden nicht erfasst. Es kann auch Zusammenh√§nge geben, die nicht durch den Korrelationskoeffizienten erfasst werden, z.B. wenn die Daten nicht normalverteilt sind. In Figure¬†2.1 sehen wir einige Beispiele in denen definitiv Korrelationen bestehen, die aber nicht durch den Korrelationskoeffizienten erfasst werden.\n\n\n\n\n\n\nFigure¬†2.1: Beispiele Korrelationskoeffizient DATAtab (retrieved 2025)\n\n\n\n\n\n\n\n\n\n\n\nKorrelation vs.¬†Kausalit√§t\n\n\n\nEine hohe Korrelation bedeutet nicht notwendigerweise Kausalit√§t. Es ist wichtig, die Daten und den Kontext zu verstehen, um sinnvolle Schlussfolgerungen zu ziehen. Ansonsten besteht die Gefahr, dass Zusammenh√§nge fehlinterpretiert werden. Ein bekanntes Beispiel ist die Korrelation zwischen der Anzahl der Piraten und der globalen Temperatur, die in Figure¬†2.2 dargestellt ist. In der Wissenschaft begegnet man diesem Problem mit kontrollierten Experimenten.\n\n\n\n\n\n\nFigure¬†2.2: Korrelation Piraten Klima RedAndr and Mikhail Ryazanov (2011)\n\n\n\n\n\n\n\n\n\nDATAtab. retrieved 2025. ‚ÄúKorrelationskoeffizient Tutorial Image.‚Äù https://datatab.de/assets/tutorial/Korrelationskoeffizient.png.\n\n\nRedAndr and Mikhail Ryazanov. 2011. ‚ÄúSatirical diagram illustrating the influence of pirates decreasing on global warming as per Pastafarian beliefs.‚Äù https://commons.wikimedia.org/wiki/File:PiratesVsTemp(en).svg.\n\n\nWickham, Hadley. 2014. ‚ÄúTidy Data.‚Äù Journal of Statistical Software 59: 1‚Äì23.",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data Sets</span>"
    ]
  },
  {
    "objectID": "dataexploratory/tutorial.html",
    "href": "dataexploratory/tutorial.html",
    "title": "Tutorial: Getting Data Tidy and finding Correlation",
    "section": "",
    "text": "Objective\nThe Global Energy Forecasting Competition Hong, Pinson, and Fan (2014) is a data science competition that aims to advance the field of energy forecasting. The competition is held every two years and the data is made available to the public for research purposes. Different teams from around the world participate in the competition and the best models are selected based on their performance.\nIn 2012 one of the goals was to forecast the load of a power system. The data consists of hourly load data for a period of 5 years. The data was not provided in a tidy format and we need to clean it up before we can start working with it. The main objective of the load forecasting competition was to predict an accurate system load for each hour in each of the zones.",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse",
      "Tutorial: Getting Data Tidy and finding Correlation"
    ]
  },
  {
    "objectID": "dataexploratory/tutorial.html#data",
    "href": "dataexploratory/tutorial.html#data",
    "title": "Tutorial: Getting Data Tidy and finding Correlation",
    "section": "Data",
    "text": "Data\nThe data is provided in a zip file that contains different files:\n\nHoliday_List.csv\nLoad_history.csv\ntemperature_history.csv\n\nYour task is to make sense from the data an bring it into a tidy format. Store the data in a pandas DataFrame and save it as a csv file. Also reload it, to make shure it works.",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse",
      "Tutorial: Getting Data Tidy and finding Correlation"
    ]
  },
  {
    "objectID": "dataexploratory/tutorial.html#crisp-dm",
    "href": "dataexploratory/tutorial.html#crisp-dm",
    "title": "Tutorial: Getting Data Tidy and finding Correlation",
    "section": "Crisp-DM",
    "text": "Crisp-DM\nWe start by using the first four steps of the CRISP-DM process to make sense of the data using exploratory data analysis.\n\nBusiness Understanding\n\nWhat is the system load of a power system and why is it important to forecast it?\nWhat are the benefits of accurate load forecasting?\nWhat factors should influence the load of a power system?\n\nData Understanding\nHow many systems are there in the data? What are the features of the data? What is the time period of the data?\nDatenaufbereitung\nWhat is a meaningful structure for the data? What should be colums and what should be rows? How can we bring the data into a tidy format?\nModellierung\nIs there a seasionality in the data? Plot the average load for each hour of the day, day of the week and month of the year in a Boxplot. How do the distributions between the different systems compare? Are there any outliers? Are there any correlations between the load and the temperature? Plot the load against the temperature and compute the correlation coefficient.\n\n\n\n\n\n\n\nTip\n\n\n\nMake sure to store not only the processed, but also the processed data in a csv file. This way you can always go back to the original data and start over if you make a mistake. Also store the preprocessing steps in a separate script, so you can reproduce the results later. It is not uncommon, to recieve new data that needs to be processed in the same way. This is also the time to think of a proper folder structure for your project. Do not forget all the things you learned in the software design courses. You can also use modules like cookiecutter to create a project structure like this:\n‚îú‚îÄ‚îÄ LICENSE            &lt;- Open-source license if one is chosen\n‚îú‚îÄ‚îÄ Makefile           &lt;- Makefile with convenience commands like `make data` or `make train`\n‚îú‚îÄ‚îÄ README.md          &lt;- The top-level README for developers using this project.\n‚îú‚îÄ‚îÄ data\n‚îÇ   ‚îú‚îÄ‚îÄ external       &lt;- Data from third party sources.\n‚îÇ   ‚îú‚îÄ‚îÄ interim        &lt;- Intermediate data that has been transformed.\n‚îÇ   ‚îú‚îÄ‚îÄ processed      &lt;- The final, canonical data sets for modeling.\n‚îÇ   ‚îî‚îÄ‚îÄ raw            &lt;- The original, immutable data dump.\n‚îÇ\n‚îú‚îÄ‚îÄ docs               &lt;- A default mkdocs project; see www.mkdocs.org for details\n‚îÇ\n‚îú‚îÄ‚îÄ models             &lt;- Trained and serialized models, model predictions, or model summaries\n‚îÇ\n‚îú‚îÄ‚îÄ notebooks          &lt;- Jupyter notebooks. Naming convention is a number (for ordering),\n‚îÇ                         the creator's initials, and a short `-` delimited description, e.g.\n‚îÇ                         `1.0-jqp-initial-data-exploration`.\n...\n\n\n\n\n\n\nHong, Tao, Pierre Pinson, and Shu Fan. 2014. ‚ÄúGlobal Energy Forecasting Competition 2012.‚Äù International Journal of Forecasting 30 (2): 357‚Äì63. https://doi.org/https://doi.org/10.1016/j.ijforecast.2013.07.001.",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse",
      "Tutorial: Getting Data Tidy and finding Correlation"
    ]
  },
  {
    "objectID": "statistics/index.html",
    "href": "statistics/index.html",
    "title": "Statistik",
    "section": "",
    "text": "‚ÄúIch bin Ingenieur und gewohnt, in Wahrscheinlichkeiten zu denken, nach der Mathematik der Vernunft.‚Äù - Frisch (1957)\n\nDie klassische Statistik ist ein Teilbereich der Mathematik, der sich mit der Sammlung, Analyse, Interpretation, Pr√§sentation und Modellierung von Daten besch√§ftigt. Ein gro√üen Teil der Statistik ist die Wahrscheinlichkeitstheorie, die sich mit dem Verst√§ndnis von Zufallsereignissen befasst.\nIn Kapitel 3¬† Stichproben und Zufallsvariablen besch√§ftigen wir uns zun√∂chst damit, wie Daten erhoben werden und wie man die Wahrscheinlichkeiten von Ereignissen berechnet. In zweiten Kapitel ?sec-statistics-distributions betrachten wir typische Verteilungen, die geeigenet sind zuf√§llige Ereignisse zu beschreiben.\n?sec-statistics-estimates\n?sec-statistics-hypothesis\n\n\n\n\nFrisch, Max. 1957. Homo Faber. Ein Bericht. Frankfurt am Main: Suhrkamp.",
    "crumbs": [
      "Statistik"
    ]
  },
  {
    "objectID": "statistics/sampling.html",
    "href": "statistics/sampling.html",
    "title": "3¬† Stichproben und Zufallsvariablen",
    "section": "",
    "text": "3.1 Stichprobenziehung aus einer Grundgesamtheit\nIn diesem Abschnitt {#sec-statistics-sampling} behandeln wir Stichproben und Zufallsvariablen.\nEine Stichprobe umfasst \\(n\\) Beobachtungen aus einer Grundgesamtheit, der Menge \\(N\\) aller m√∂glichen Beobachtungen. Sie ist eine Teilmenge der Grundgesamtheit und sollte idealerweise R√ºckschl√ºsse auf diese erm√∂glichen.\nDie Grundgesamtheit (population) ist die Gesamtheit aller untersuchbaren Beobachtungen, die Stichprobe (sample) eine Teilmenge davon. Eine repr√§sentative Stichprobe erlaubt Verallgemeinerungen. Da die vollst√§ndige Datenerhebung der Grundgesamtheit oft zu aufwendig und kostspielig ist, ziehen wir R√ºckschl√ºsse aus Stichproben (siehe Figure Figure¬†3.1). Dies gelingt am besten mit einer gro√üen, zuf√§llig ausgew√§hlten Stichprobe.\nFigure¬†3.1: Visualisierung der Stichprobenziehung aus einer Grundgesamtheit.\nAllerdings kommt es hier zu einen Unterschied zwischen der Sichtweise der klassischen Statistik und dem Ansatz den den viele Data Scientists verfolgen. In der klassischen Statistik wird die Stichprobe so gew√§hlt, dass sie repr√§sentativ f√ºr die Grundgesamtheit ist.\nAls Data Scientist hingegen, sind wir oft an den Daten interessiert, die uns zur Verf√ºgung stehen. Wir haben keine M√∂glichkeit, die Grundgesamtheit zu beeinflussen. Wir m√ºssen also mit den Daten arbeiten, die wir haben und uns dabei bewusst sein, dass wir einem Sampling-Bias unterliegen.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Stichproben und Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "statistics/sampling.html#sec-sampling-population",
    "href": "statistics/sampling.html#sec-sampling-population",
    "title": "3¬† Stichproben und Zufallsvariablen",
    "section": "",
    "text": "Note\n\n\n\nWenn wir die Leistungsf√§higkeit in Mathematik unter Studierenden auswerten wollen, dann sollten wir unsere Stichprobe nicht nur im Studiengang Mechatronik nachfragen.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWenn wir die die Lebensdauer eines Werkzeugs auf einer 5-Achs-Fr√§smaschinene prognostizieren wollen, k√∂nnen wir die Modelle nicht zwischen Betrieben vergleichen, die die Maschine regelm√§√üig warten und solchen, die das nicht tun.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Stichproben und Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "statistics/sampling.html#sec-classical-vs-datascience",
    "href": "statistics/sampling.html#sec-classical-vs-datascience",
    "title": "3¬† Stichproben und Zufallsvariablen",
    "section": "3.2 Unterschied zwischen klassischer Statistik und Data Science",
    "text": "3.2 Unterschied zwischen klassischer Statistik und Data Science\nDie klassische Statistik w√§hlt Stichproben so, dass sie die Grundgesamtheit repr√§sentieren, w√§hrend Data Scientists oft mit verf√ºgbaren Daten arbeiten und keinen Einfluss auf die Grundgesamtheit haben. Dies f√ºhrt zu einem m√∂glichen Sampling-Bias.\n\n\n\n\n\n\nNote\n\n\n\nZur Bewertung der Mathematikleistung von Studierenden w√§re eine Stichprobe nur aus Mechatronik nicht repr√§sentativ.\n\n\n\n\n\n\n\n\nNote\n\n\n\nBei der Prognose der Werkzeuglebensdauer einer 5-Achs-Fr√§smaschine sind Vergleiche zwischen gewarteten und ungewarteten Maschinen verzerrt.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Stichproben und Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "statistics/sampling.html#sec-grouping-data",
    "href": "statistics/sampling.html#sec-grouping-data",
    "title": "3¬† Stichproben und Zufallsvariablen",
    "section": "3.3 Gruppieren von Daten",
    "text": "3.3 Gruppieren von Daten\nEin bewusster Bias kann bei der Datenauswahl entstehen. Fragen wir z. B. nur Personen mit Hypothek (mortgage) nach ihrem Einkommen und nicht Mieter (rent), ist die Stichprobe nicht repr√§sentativ (siehe Figure Figure¬†3.2).\n\nimport pandas as pd\nimport seaborn as sns\n\ndf = pd.read_csv(\"../_assets/dataexploratory/loan50.csv\")\nsns.histplot(data=df, x=\"annual_income\", bins=30, hue=\"homeownership\")\n\n\n\n\n\n\n\nFigure¬†3.2: Verteilung des j√§hrlichen Einkommens nach Wohneigentum.\n\n\n\n\n\n\ndf.groupby(\"homeownership\")[\"annual_income\"].mean()\n\nhomeownership\nmortgage    99807.692308\nown         67666.666667\nrent        71928.571429\nName: annual_income, dtype: float64",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Stichproben und Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "statistics/sampling.html#sec-data-analysis",
    "href": "statistics/sampling.html#sec-data-analysis",
    "title": "3¬† Stichproben und Zufallsvariablen",
    "section": "3.4 Analyse der Daten",
    "text": "3.4 Analyse der Daten\n\n\n\n\n\n\nNote\n\n\n\nInwiefern entsprechen die Daten den Erwartungen?\n\n\nIm Beispiel haben wir eine ordinal skalierte Variable (homeownership) und eine metrisch skalierte Variable (annual_income). Wir untersuchen ihren Zusammenhang. Korrelation und Kausalit√§t, wie in der letzten Einheit besprochen, sind hier nicht anwendbar, da homeownership ordinal ist.\n\n3.4.1 Boxplot\nEin Boxplot eignet sich zum Vergleich ordinaler und metrischer Variablen. Er zeigt die Verteilung der metrischen Variable (annual_income) f√ºr die Auspr√§gungen der ordinalen Variable (homeownership). Die Box umfasst Median sowie erstes und drittes Quartil, die Whisker die Datenreichweite (1,5-fache Interquartilsdistanz ab den Quartilen), und Punkte markieren Ausrei√üer (siehe Figure Figure¬†3.3).\n\nimport pandas as pd\nimport seaborn as sns\n\ndf = pd.read_csv(\"../_assets/dataexploratory/loan50.csv\")\nsns.boxplot(data=df, x=\"homeownership\", y=\"annual_income\")\nsns.stripplot(data=df, x=\"homeownership\", y=\"annual_income\", color=\"black\", size=3, alpha=0.5)\n\n\n\n\n\n\n\nFigure¬†3.3: Boxplot des j√§hrlichen Einkommens nach Wohneigentum.\n\n\n\n\n\nDer Boxplot ist einfach zu erstellen und zu interpretieren, jedoch bei stark unterschiedlichen Verteilungen wenig aussagekr√§ftig. Hier k√∂nnen Daten transformationen oder alternative Visualisierungen helfen.\n\n\n\n\n\n\nTip\n\n\n\nEine moderne Alternative zum Boxplot ist der Violinplot. Er zeigt die Verteilung als gesch√§tzte Wahrscheinlichkeitsdichte (bisher f√ºr uns ein gegl√§ttetes Histogramm) und ist informativer, da er die Datenverteilung detaillierter darstellt (siehe Figure ?fig-violin-plot). \n\n\n\n\n3.4.2 Experimente\nIn der Statistik unterscheiden wir Beobachtungsstudien, bei denen Daten ohne Eingriff beobachtet werden, von Experimenten, bei denen Daten manipuliert werden, um Effekte zu pr√ºfen. Experimente sind aufwendiger und teurer, erm√∂glichen aber die Untersuchung von Kausalzusammenh√§ngen.\n\n\n\n\n\n\nNote\n\n\n\nBeobachtungsstudien k√∂nnen longitudinal sein, z. B. die Mathematikleistung von Studierenden √ºber die Zeit, oder Querschnittsstudien, z. B. die Leistung nach Studieng√§ngen zu einem Zeitpunkt.\n\n\n\n\n\n\n\n\nNote\n\n\n\nUm die Wirkung von Studieng√§ngen auf die Mathematikleistung zu pr√ºfen, k√∂nnten wir ein Experiment durchf√ºhren: Studierende zuf√§llig Studieng√§ngen zuweisen und ihre Leistung messen. Dies erfordert Zufallsauswahl und -zuweisung (ethische Bedenken beachten). Eine Kontrollgruppe ohne Studiengang schlie√üt externe Einfl√ºsse (z. B. Alter) aus. Eine Messung vor dem Studium ist bei zuf√§lliger Zuweisung entbehrlich.\n\n\nExperimente sind der Goldstandard f√ºr Kausalit√§t. Eine unabh√§ngige Variable (z. B. Studiengang) wird manipuliert, andere Variablen konstant gehalten oder durch gro√üe Stichproben ausgeglichen. Findet sich eine Korrelation zur abh√§ngigen Variable (z. B. Mathematikleistung), ist Kausalit√§t plausibel. Im Data Science wird oft pragmatisch mit vorhandenen Daten gearbeitet ‚Äì schneller, aber weniger zuverl√§ssig.\n\n3.4.2.1 Beispiel: Ist ein W√ºrfel gezinkt?\nStellen wir uns vor, eine Kollegin besteht auf ihrem eigenen W√ºrfel. Ist er gezinkt? Eine Beobachtungsstudie k√∂nnte die Augenzahlen mit einem fairen W√ºrfel (Kontrollgruppe) vergleichen. Wir werfen beide W√ºrfel 1000-mal und pr√ºfen die Verteilung (siehe Figure Figure¬†3.4). Auff√§llige Abweichungen machen misstrauisch.\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\nfair_dice_rolls = np.random.randint(1, 7, 1000)\nmanipulated_dice_rolls = np.random.choice([1, 2, 3, 4, 5, 6], 1000, p=[1.9/12, 1.9/12, 1.9/12, 1.9/12, 1.9/12, 2.5/12])\n\nsns.histplot(fair_dice_rolls, bins=6, discrete=True, color=\"lightblue\", alpha=0.2, label=\"Fairer W√ºrfel\")\nsns.histplot(manipulated_dice_rolls, bins=6, discrete=True, color=\"red\", alpha=0.2, label=\"Manipulierter W√ºrfel\")\nplt.legend()\n\n\n\n\n\n\n\nFigure¬†3.4: Verteilung der W√ºrfelergebnisse: fairer vs.¬†manipulierter W√ºrfel.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Stichproben und Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "statistics/sampling.html#sec-variable-perspectives",
    "href": "statistics/sampling.html#sec-variable-perspectives",
    "title": "3¬† Stichproben und Zufallsvariablen",
    "section": "3.5 Blickpunkte auf Variablen",
    "text": "3.5 Blickpunkte auf Variablen\nWir haben Variablen (Spalten in tidy data) untersucht und betrachten sie aus verschiedenen Perspektiven.\n\n3.5.1 Skalenniveaus\nSkalenniveaus klassifizieren Variablen in nominal, ordinal, metrisch und verh√§ltnisskaliert. Sie bestimmen, welche statistischen Methoden zur Analyse geeignet sind.\n\n\n3.5.2 Im Kontext von Experimenten\nIn Experimenten und Beobachtungsstudien unterscheiden wir unabh√§ngige (Einflussgr√∂√üe) und abh√§ngige (gemessene Effekte) Variablen. Bezeichnungen variieren je nach Fachgebiet (siehe Table (tab-variable-terms?)). Sp√§ter erkennen wir, dass mehrere unabh√§ngige und abh√§ngige Variablen m√∂glich sind, doch zun√§chst bleiben wir bei Singular.\n\n\n\nAnwendungsfeld\nUnabh√§ngige Variable\nAbh√§ngige Variable\n\n\n\n\nStatistik\nExplanatory Variable\nResponse Variable\n\n\nMachine Learning\nFeatures\nTarget\n\n\nExperimente\nTreatment\nOutcome\n\n\nPsychologie\nIndependent Variable\nDependent Variable\n\n\nForecasts\nPredictor\nPredicted Variable\n\n\n√ñkonometrie\nExplanatory Variable\nDependent Variable\n\n\nInformatik\nInput\nOutput\n\n\nProgramming\nArgument\nReturn Value\n\n\nProgramming\nX\ny",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Stichproben und Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "statistics/sampling.html#sec-probability",
    "href": "statistics/sampling.html#sec-probability",
    "title": "3¬† Stichproben und Zufallsvariablen",
    "section": "3.6 Wahrscheinlichkeitsrechnung",
    "text": "3.6 Wahrscheinlichkeitsrechnung\nEine weitere Perspektive sind Prozesse hinter Beobachtungen: deterministisch (gleiches Ergebnis bei gleichen Bedingungen) oder zuf√§llig (unterschiedliche Ergebnisse trotz gleicher Bedingungen). Zuf√§llige Prozesse werden durch Wahrscheinlichkeiten beschrieben.\n\n\n\n\n\n\nImportant\n\n\n\nOb das Universum deterministisch oder zuf√§llig ist, spielt keine Rolle. Zuf√§lligkeit bedeutet hier, dass Ergebnisse nicht a priori vorhersagbar sind ‚Äì sei es durch echte Zufallsprozesse (z. B. W√ºrfeln) oder unvollst√§ndige Modellierung (z. B. fehlende Variablen).\n\n\n\n3.6.1 Zufallsvariablen\nEine Zufallsvariable nimmt zuf√§llig Werte an ‚Äì diskret (bestimmte Werte, z. B. W√ºrfelaugenzahl: 1‚Äì6) oder kontinuierlich (Werte in einem Intervall, z. B. Temperatur). Die Augenzahl eines W√ºrfels ist eine diskrete Zufallsvariable; jeder Wurf ist eine Realisierung (siehe Figure Figure¬†3.5).\n\nimport numpy as np\nimport seaborn as sns\n\nnp.random.seed(42)\ndice_rolls = np.random.randint(1, 7, 1000)\nsns.histplot(dice_rolls, bins=6, discrete=True)\n\n\n\n\n\n\n\nFigure¬†3.5: Verteilung der W√ºrfelergebnisse bei 1000 W√ºrfen.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Stichproben und Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "statistics/sampling.html#sec-random-variables-coin",
    "href": "statistics/sampling.html#sec-random-variables-coin",
    "title": "3¬† Stichproben und Zufallsvariablen",
    "section": "3.7 Zufallsvariablen und M√ºnzwurf",
    "text": "3.7 Zufallsvariablen und M√ºnzwurf\n√Ñhnlich wie beim W√ºrfel ist beim M√ºnzwurf die Zufallsvariable die Seite, die oben liegt (Kopf oder Zahl). F√ºr numerische Analysen wandeln wir diese kategorialen Werte in 0 (Zahl) und 1 (Kopf) um, wodurch der Ereignisraum {Kopf, Zahl} zu {0, 1} wird.\n\n3.7.1 Begriffe der Wahrscheinlichkeit\nWir definieren: - Zufallsexperiment/-prozess: Ein Prozess mit unvorhersagbarem Ergebnis, z. B. M√ºnzwurf, W√ºrfeln oder Kartenziehen. - Ereignisraum (sample space): Alle m√∂glichen Ergebnisse eines Zufallsexperiments, z. B. {Kopf, Zahl} beim M√ºnzwurf. - Zufallsvariable: Eine Funktion, die jedem Ergebnis eine Zahl zuordnet, z. B. 1 f√ºr Kopf und 0 f√ºr Zahl.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Stichproben und Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "statistics/sampling.html#sec-frequentist-probability",
    "href": "statistics/sampling.html#sec-frequentist-probability",
    "title": "3¬† Stichproben und Zufallsvariablen",
    "section": "3.8 Frequentistische Wahrscheinlichkeit",
    "text": "3.8 Frequentistische Wahrscheinlichkeit\n\n\n\n\n\n\nImportant\n\n\n\nDie Wahrscheinlichkeit eines Ergebnisses ist der Anteil, wie oft es bei unendlich vielen Wiederholungen eines Zufallsprozesses eintritt. Sie liegt zwischen 0 und 1 und kann als Prozentsatz (0‚Äì100 %) angegeben werden.\n\n\nIn Figure Figure¬†3.5 trat die Augenzahl 2 etwa 167-mal in 1000 W√ºrfen auf, was einer Wahrscheinlichkeit von ca. 1/6 (0,167) entspricht ‚Äì ebenso f√ºr die anderen Augenzahlen. Bei endlichen Beobachtungen ist dies eine Sch√§tzung; die exakte Wahrscheinlichkeit gilt nur f√ºr unendlich viele Versuche (siehe Figure Figure¬†3.6).\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nnumber_of_rolls = 1000\nnp.random.seed(10)\ndice_rolls = np.random.randint(1, 7, number_of_rolls)\n\nproportion_ones = np.cumsum(dice_rolls == 1) / np.arange(1, number_of_rolls + 1)\n\nsns.lineplot(x=np.arange(1, number_of_rolls + 1), y=proportion_ones)\nsns.lineplot(x=[1, number_of_rolls + 1], y=[1/6, 1/6], color=\"red\", linestyle=\"--\")\nplt.xlabel(\"$n$ - Anzahl der W√ºrfe\")\nplt.ylabel(\"$\\hat{p}_n$ - Anteil der 1en\")\n\n&lt;&gt;:14: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n&lt;&gt;:14: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n/tmp/ipykernel_3135/965827469.py:14: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n\n\n\n\n\n\n\nText(0, 0.5, '$\\\\hat{p}_n$ - Anteil der 1en')\n\n\n(a) Anteil der W√ºrfelergebnisse ‚Äò1‚Äô √ºber die Anzahl der W√ºrfe.\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure¬†3.6\n\n\n\n\n\n3.8.1 Konvergenz und Sch√§tzungen\nFigure Figure¬†3.6 zeigt den Anteil \\(\\hat{p}_n\\) der Augenzahl 1 bei jedem Schritt \\(n\\) einer Simulation. Er konvergiert gegen die Wahrscheinlichkeit 1/6 (ca. 0,167). Der beobachtete Anteil \\(\\hat{p}_n\\) sch√§tzt die Wahrscheinlichkeit \\(p\\) und wird mit mehr Beobachtungen genauer; \\(p\\) ist der Grenzwert.\n\n\n\n\n\n\nImportant\n\n\n\nIn der Statistik arbeiten wir oft mit Sch√§tzungen, da unendlich viele Beobachtungen fehlen. Die Unsicherheit der Sch√§tzungen muss ber√ºcksichtigt werden. Sch√§tzungen kennzeichnen wir mit \\(\\hat{p}\\).\n\n\n\n\n\n\n\n\nImportant\n\n\n\nGesetz der gro√üen Zahlen: Mit steigender Beobachtungszahl konvergiert der Anteil eines Ergebnisses gegen dessen Wahrscheinlichkeit.\n\n\n\n\n3.8.2 Wahrscheinlichkeitsnotation\nF√ºr verschiedene Ergebnisse schreiben wir \\(P(X = x)\\) als Wahrscheinlichkeit, dass die Zufallsvariable \\(X\\) (z. B. M√ºnzwurf) den Wert \\(x\\) annimmt.\n\nF√ºr einen fairen M√ºnzwurf:\n\\(P(X = \\text{Kopf}) = 0.5\\), \\(P(X = \\text{Zahl}) = 0.5\\)\noder: \\(P(X = 1) = 0.5\\), \\(P(X = 0) = 0.5\\).\n\nF√ºr einen fairen W√ºrfelwurf:\n\\(P(X = 1) = P(X = 2) = P(X = 3) = P(X = 4) = P(X = 5) = P(X = 6) = 1/6\\).\n\nDie Summe der Wahrscheinlichkeiten aller Ergebnisse eines Zufallsexperiments ist stets 1, da mindestens ein Ergebnis eintritt. Dies entspricht der Fl√§che unter einem normalisierten Histogramm oder einer Dichtefunktion.\n\n\n3.8.3 Disjunkte Ereignisse\nZwei Ereignisse \\(A\\) und \\(B\\) sind disjunkt (sich ausschlie√üend), wenn sie nicht gleichzeitig eintreten k√∂nnen. Beispiel: Bei einem W√ºrfelwurf sind ‚ÄûAugenzahl 1‚Äú und ‚ÄûAugenzahl 2‚Äú disjunkt. Die Wahrscheinlichkeit, dass eines von beiden eintritt, ist \\(P(A \\cup B)\\) (logisches ‚Äûoder‚Äú, ‚ÄûA vereinigt B‚Äú):\n\\[P(X=1 \\cup X=2) = P(X=1) + P(X=2) = \\frac{1}{6} + \\frac{1}{6} = \\frac{1}{3}.\\]\n\n\n\n\n\n\nImportant\n\n\n\nAdditionsregel: F√ºr sich ausschlie√üende Ereignisse \\(A\\) und \\(B\\) gilt:\n\\[P(A \\cup B) = P(A) + P(B).\\]\nF√ºr mehrere disjunkte Ereignisse \\(A_1, \\ldots, A_n\\):\n\\[P(A_1 \\cup A_2 \\cup \\ldots \\cup A_n) = P(A_1) + P(A_2) + \\ldots + P(A_n).\\]\n\n\n\n3.8.3.1 Beispiel: Kreditnehmer-Datensatz\nIm Datensatz aus Kapitel 2 beschreibt homeownership, ob ein Kreditnehmer mietet, eine Hypothek hat oder Eigent√ºmer ist. Von 50 Kreditnehmern (siehe Code-Ausgabe) sind die Verteilungen: Miete (21), Hypothek (26), Eigentum (3).\n\nimport pandas as pd\n\ndf = pd.read_csv(\"../_assets/dataexploratory/loan50.csv\")\nprint(f\"Anzahl Beobachtungen: {df['homeownership'].shape[0]}\")\nprint(df[\"homeownership\"].value_counts())\n\nAnzahl Beobachtungen: 50\nhomeownership\nmortgage    26\nrent        21\nown          3\nName: count, dtype: int64\n\n\n\nSind Miete, Hypothek und Eigentum disjunkt?\nBestimmen Sie den Anteil der Kredite mit Hypothek und Eigentum separat.\nNutzen Sie die Additionsregel f√ºr disjunkte Ereignisse, um die Wahrscheinlichkeit zu berechnen, dass ein zuf√§llig ausgew√§hlter Kreditnehmer eine Hypothek hat oder Eigent√ºmer ist.\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nJa, die Kategorien sind disjunkt, da ein Kreditnehmer nur eine davon haben kann.\nAnteil Hypothek: \\(\\frac{26}{50}\\), Anteil Eigentum: \\(\\frac{3}{50}\\).\nWahrscheinlichkeit (Hypothek oder Eigentum): \\(\\frac{26}{50} + \\frac{3}{50} = \\frac{29}{50}\\). Dies entspricht der Wahrscheinlichkeit, nicht zu mieten.\n\n\n\n\n\n\n\n3.8.4 Das Komplement eines Ereignisses\nDas Komplement eines Ereignisses \\(A\\), bezeichnet als \\(A^c\\) oder \\(\\bar{A}\\), ist das Nicht-Eintreten von \\(A\\). Es umfasst alle Ergebnisse au√üer \\(A\\) und ist zu \\(A\\) disjunkt. Die Wahrscheinlichkeit des Komplements ist:\n\\[\nP(A^c) = 1 - P(A)\n\\]\nBeispiel: Die Wahrscheinlichkeit, dass ein W√ºrfel nicht 1 zeigt:\n\\[\n1 - P(X \\neq 1) =1-P(X=1) = 1 - \\frac{1}{6} = \\frac{5}{6}.\n\\]\nDie Summe \\(P(A) + P(A^c) = 1\\) gilt stets, da entweder \\(A\\) oder \\(A^c\\) eintritt.\n\n\n3.8.5 Nicht-disjunkte Ereignisse\nNicht-disjunkte Ereignisse k√∂nnen √ºberlappen. Beispiel: Bei einem Kartenspiel (siehe Figure Figure¬†3.7) interessiert die Wahrscheinlichkeit, eine Bildkarte (Bube, Dame, K√∂nig) oder eine Karo-Karte zu ziehen. Bild und Karo sind nicht disjunkt, da Bildkarten in Karo beide Eigenschaften haben. Einfaches Addieren √ºbersch√§tzt die Wahrscheinlichkeit durch doppelte Z√§hlung.\n\n\n\n\n\n\nFigure¬†3.7: Deck of 52 Cards\n\n\n\nEin Venn-Diagramm (siehe Figure Figure¬†3.8) zeigt: Die Schnittmenge (\\(A \\cap B\\), logisches ‚Äûund‚Äú) sind Karten, die beide Eigenschaften haben; die Vereinigung (\\(A \\cup B\\), logisches ‚Äûoder‚Äú) umfasst alle Karten mit mindestens einer Eigenschaft.\n\n\n\n\n\n\nFigure¬†3.8: Venn Diagramm of Cards\n\n\n\nVon 52 Karten sind 12 Bildkarten, 13 Karo-Karten und 3 sowohl Bild- als auch Karo-Karten. Die Wahrscheinlichkeit f√ºr ‚ÄûBild oder Karo‚Äú ist:\n\\[\nP(\\text{Bild} \\cup \\text{Karo}) = P(\\text{Bild}) + P(\\text{Karo}) - P(\\text{Bild} \\cap \\text{Karo}).\n= \\frac{12}{52} + \\frac{13}{52} - \\frac{3}{52} = \\frac{22}{52}.\n\\]\nHierraus k√∂nnen wir die Additionsregel f√ºr nicht-disjunkte Ereignisse formulieren:\n\n\n\n\n\n\nImportant\n\n\n\nGenerelle Additionsregel: F√ºr nicht-disjunkte Ereignisse \\(A\\) und \\(B\\) gilt:\n\\[\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B).\n\\]\n\n\n\n3.8.5.1 Beispiel: Summe zweier W√ºrfel\nSei \\(A\\) das Ereignis, dass die Summe der Augenzahlen zweier fairer W√ºrfel kleiner als 12 ist.\n\nWas ist das Komplement von \\(A\\)?\nWie gro√ü ist \\(P(A)\\)?\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nDas Komplement \\(A^c\\) ist die Summe \\(\\geq 12\\), also genau 12 (da die maximale Summe 12 betr√§gt).\n\\(P(A)\\) ist die Summe der Wahrscheinlichkeiten f√ºr Summen 2 bis 11.\nM√∂gliche Summen und Kombinationen (siehe Table (tab-dice-sums?)):\n\n\n\n\n\n\n\n\n\\(A = W_1 + W_2\\)\nM√∂gliche Kombinationen\n\n\n\n\n2\n\\((W_1=1 \\cap W_2=1)\\)\n\n\n3\n\\((W_1=1 \\cap W_2=2) \\cup (W_1=2 \\cap W_2=1)\\)\n\n\n4\n\\((W_1=1 \\cap W_2=3) \\cup (W_1=2 \\cap W_2=2) \\cup (W_1=3 \\cap W_2=1)\\)\n\n\n5\n\\((W_1=1 \\cap W_2=4) \\cup (W_1=2 \\cap W_2=3) \\cup (W_1=3 \\cap W_2=2) \\cup (W_1=4 \\cap W_2=1)\\)\n\n\n6\n\\((W_1=1 \\cap W_2=5) \\cup (W_1=2 \\cap W_2=4) \\cup (W_1=3 \\cap W_2=3) \\cup (W_1=4 \\cap W_2=2) \\cup (W_1=5 \\cap W_2=1)\\)\n\n\n7\n\\((W_1=1 \\cap W_2=6) \\cup (W_1=2 \\cap W_2=5) \\cup (W_1=3 \\cap W_2=4) \\cup (W_1=4 \\cap W_2=3) \\cup (W_1=5 \\cap W_2=2) \\cup (W_1=6 \\cap W_2=1)\\)\n\n\n8\n\\((W_1=2 \\cap W_2=6) \\cup (W_1=3 \\cap W_2=5) \\cup (W_1=4 \\cap W_2=4) \\cup (W_1=5 \\cap W_2=3) \\cup (W_1=6 \\cap W_2=2)\\)\n\n\n9\n\\((W_1=3 \\cap W_2=6) \\cup (W_1=4 \\cap W_2=5) \\cup (W_1=5 \\cap W_2=4) \\cup (W_1=6 \\cap W_2=3)\\)\n\n\n10\n\\((W_1=4 \\cap W_2=6) \\cup (W_1=5 \\cap W_2=5) \\cup (W_1=6 \\cap W_2=4)\\)\n\n\n11\n\\((W_1=5 \\cap W_2=6) \\cup (W_1=6 \\cap W_2=5)\\)\n\n\n12\n\\((W_1=6 \\cap W_2=6)\\)\n\n\n\nDie Gesamtzahl der Kombinationen betr√§gt \\(6 \\times 6 = 36\\). F√ºr \\(A^c\\) (Summe = 12) gibt es 1 Fall, also \\(P(A^c) = \\frac{1}{36}\\) und \\(P(A) = 1 - P(A^c) = \\frac{35}{36}\\).\n\n\n\nBei komplexeren Berechnungen hilft eine Monte-Carlo-Simulation: Das Experiment wird mehrfach simuliert, und die Wahrscheinlichkeit ergibt sich aus dem Anteil der Treffer (siehe Code-Ausgabe).\n\nimport numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(np.random.randint(1, 7, (10000, 2)), columns=[\"W1\", \"W2\"])\ndf[\"Sum\"] = df[\"W1\"] + df[\"W2\"]\np = (df[\"Sum\"] &lt; 12).mean()\n\nprint(f\"Die Wahrscheinlichkeit, dass die Summe &lt; 12 ist, betr√§gt {p:.3f}\")\n\nDie Wahrscheinlichkeit, dass die Summe &lt; 12 ist, betr√§gt 0.973\n\n\n\n\n3.8.5.2 Simulation der W√ºrfelsumme (Fortsetzung)\nDie Verteilung der Summen zweier W√ºrfel kann auch grafisch dargestellt werden (siehe Figure Figure¬†3.9). Die Simulation best√§tigt, dass die Wahrscheinlichkeit f√ºr eine Summe &lt; 12 hoch ist.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(np.random.randint(1, 7, (10000, 2)), columns=[\"W1\", \"W2\"])\ndf[\"Sum\"] = df[\"W1\"] + df[\"W2\"]\n\nsns.histplot(df[\"Sum\"], bins=11, discrete=True, stat='density')\nplt.axvline(11, color=\"red\", linestyle=\"--\")\nplt.xlabel(\"Summe der Augenzahlen\")\nplt.ylabel(\"Dichte\")\nplt.show()\n\n\n\n\n\n\n\nFigure¬†3.9: Verteilung der Summen zweier W√ºrfel bei 10.000 W√ºrfen; rote Linie markiert Summe = 11.\n\n\n\n\n\n\n\n\n3.8.6 Wahrscheinlichkeit der W√ºrfelsumme (Fortsetzung)\nFigure Figure¬†3.9 zeigt die Verteilung der Summen zweier W√ºrfel. Die Wahrscheinlichkeit, dass die Summe &lt; 12 ist, betr√§gt ca. 0,97 (Monte-Carlo-Sch√§tzung).\n- 3. Die Wahrscheinlichkeit f√ºr eine Summe \\(\\geq 12\\) ist:\n\\[P(A^c) = 1 - P(A) = 1 - \\frac{35}{36} = \\frac{1}{36} \\approx 0.03.\\]",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Stichproben und Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "statistics/sampling.html#sec-independent-events",
    "href": "statistics/sampling.html#sec-independent-events",
    "title": "3¬† Stichproben und Zufallsvariablen",
    "section": "3.9 Unabh√§ngige Ereignisse",
    "text": "3.9 Unabh√§ngige Ereignisse\nZwei Ereignisse \\(A\\) und \\(B\\) sind unabh√§ngig, wenn das Eintreten des einen das andere nicht beeinflusst (vgl. Korrelation und Kausalit√§t in Chapter 2). Die Wahrscheinlichkeit eines Ereignisses h√§ngt nicht vom anderen ab. Beispiele: M√ºnzwurf und W√ºrfelwurf oder die Ergebnisse zweier W√ºrfel ‚Äì das Ergebnis des ersten W√ºrfels beeinflusst den zweiten nicht.\nDie Wahrscheinlichkeit, dass zwei unabh√§ngige Ereignisse gleichzeitig eintreten, ist:\n\\[P(A \\cap B) = P(A) \\cdot P(B).\\]\n\n\n\n\n\n\nImportant\n\n\n\nMultiplikationsregel f√ºr unabh√§ngige Ereignisse: F√ºr unabh√§ngige Ereignisse \\(A\\) und \\(B\\) gilt:\n\\[P(A \\cap B) = P(A) \\cdot P(B).\\]\nF√ºr mehrere unabh√§ngige Ereignisse \\(A_1, \\ldots, A_n\\):\n\\[P(A_1 \\cap A_2 \\cap \\ldots \\cap A_n) = P(A_1) \\cdot P(A_2) \\cdot \\ldots \\cdot P(A_n).\\]\n\n\n\n3.9.1 Beispiel 1: W√ºrfelsummen\nDa die Ergebnisse zweier W√ºrfel unabh√§ngig sind, k√∂nnen wir die Wahrscheinlichkeiten multiplizieren (siehe Table (tab-dice-sum-probabilities?)).\n\n\n\n\n\n\n\n\n\\(A = W_1 + W_2\\)\nM√∂gliche Kombinationen\n\\(P(A)\\)\n\n\n\n\n2\n\\(W_1=1, W_2=1\\)\n\\(P(A=2) = P(W_1=1) \\cdot P(W_2=1) = \\frac{1}{6} \\cdot \\frac{1}{6} = \\frac{1}{36}\\)\n\n\n3\n\\(W_1=1, W_2=2\\), \\(W_1=2, W_2=1\\)\n\\(P(A=3) = P(W_1=1) \\cdot P(W_2=2) + P(W_1=2) \\cdot P(W_2=1) = \\frac{1}{6} \\cdot \\frac{1}{6} + \\frac{1}{6} \\cdot \\frac{1}{6} = \\frac{2}{36}\\)\n\n\n4\n\\(W_1=1, W_2=3\\), \\(W_1=2, W_2=2\\), \\(W_1=3, W_2=1\\)\n\\(P(A=4) = P(W_1=1) \\cdot P(W_2=3) + P(W_1=2) \\cdot P(W_2=2) + P(W_1=3) \\cdot P(W_2=1) = \\frac{1}{6} \\cdot \\frac{1}{6} + \\frac{1}{6} \\cdot \\frac{1}{6} + \\frac{1}{6} \\cdot \\frac{1}{6} = \\frac{3}{36}\\)\n\n\n5\n\\(W_1=1, W_2=4\\), \\(W_1=2, W_2=3\\), \\(W_1=3, W_2=2\\), \\(W_1=4, W_2=1\\)\n\\(P(A=5) = P(W_1=1) \\cdot P(W_2=4) + P(W_1=2) \\cdot P(W_2=3) + P(W_1=3) \\cdot P(W_2=2) + P(W_1=4) \\cdot P(W_2=1) =\n\\frac{4}{36}\\)\n\n\n6\n\\(W_1=1, W_2=5\\), \\(W_1=2, W_2=4\\), \\(W_1=3, W_2=3\\), \\(W_1=4, W_2=2\\), \\(W_1=5, W_2=1\\)\n\\(P(A=6) = P(W_1=1) \\cdot P(W_2=5) + P(W_1=2) \\cdot P(W_2=4) + P(W_1=3) \\cdot P(W_2=3) + P(W_1=4) \\cdot P(W_2=2) + P(W_1=5) \\cdot P(W_2=1) =\n\\frac{5}{36}\\)\n\n\n7\n\\(W_1=1, W_2=6\\), \\(W_1=2, W_2=5\\), \\(W_1=3, W_2=4\\), \\(W_1=4, W_2=3\\), \\(W_1=5, W_2=2\\), \\(W_1=6, W_2=1\\)\n\\(P(A=7) = P(W_1=1) \\cdot P(W_2=6) + P(W_1=2) \\cdot P(W_2=5) + P(W_1=3) \\cdot P(W_2=4) + P(W_1=4) \\cdot P(W_2=3) + P(W_1=5) \\cdot P(W_2=2) + P(W_1=6) \\cdot P(W_2=1) = \\frac{6}{36}\\)\n\n\n8\n\\(W_1=2, W_2=6\\), \\(W_1=3, W_2=5\\), \\(W_1=4, W_2=4\\), \\(W_1=5, W_2=3\\), \\(W_1=6, W_2=2\\)\n\\(P(A=8) = P(W_1=2) \\cdot P(W_2=6) + P(W_1=3) \\cdot P(W_2=5) + P(W_1=4) \\cdot P(W_2=4) + P(W_1=5) \\cdot P(W_2=3) + P(W_1=6) \\cdot P(W_2=2) = \\frac{5}{36}\\)\n\n\n9\n\\(W_1=3, W_2=6\\), \\(W_1=4, W_2=5\\), \\(W_1=5, W_2=4\\), \\(W_1=6, W_2=3\\)\n\\(P(A=9) = P(W_1=3) \\cdot P(W_2=6) + P(W_1=4) \\cdot P(W_2=5) + P(W_1=5) \\cdot P(W_2=4) + P(W_1=6) \\cdot P(W_2=3) = \\frac{4}{36}\\)\n\n\n10\n\\(W_1=4, W_2=6\\), \\(W_1=5, W_2=5\\), \\(W_1=6, W_2=4\\)\n\\(P(A=10) = P(W_1=4) \\cdot P(W_2=6) + P(W_1=5) \\cdot P(W_2=5) + P(W_1=6) \\cdot P(W_2=4) = \\frac{3}{36}\\)\n\n\n11\n\\(W_1=5, W_2=6\\), \\(W_1=6, W_2=5\\)\n\\(P(A=11) = P(W_1=5) \\cdot P(W_2=6) + P(W_1=6) \\cdot P(W_2=5) = \\frac{2}{36}\\)\n\n\n12\n\\(W_1=6, W_2=6\\)\n\\(P(A=12) = P(W_1=6) \\cdot P(W_2=6) = \\frac{1}{36}\\)\n\n\n\n\n\n3.9.2 Beispiel 2: Wahr oder Falsch\nBestimmen Sie, ob die folgenden Aussagen wahr oder falsch sind, und begr√ºnden Sie Ihre Antwort.\n\nWenn eine faire M√ºnze oft geworfen wird und die letzten acht W√ºrfe Kopf waren, ist die Wahrscheinlichkeit, dass der n√§chste Wurf Kopf ist, etwas weniger als 50 %.\nDas Ziehen einer Bildkarte (Bube, Dame, K√∂nig) und das Ziehen einer roten Karte aus einem vollst√§ndigen Kartenspiel sind sich gegenseitig ausschlie√üende Ereignisse.\nDas Ziehen einer Bildkarte und das Ziehen eines Asses aus einem vollst√§ndigen Kartenspiel sind sich gegenseitig ausschlie√üende Ereignisse.\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nFalsch. Bei einer fairen M√ºnze ist \\(P(\\text{Kopf}) = 0.5\\). Vorherige W√ºrfe beeinflussen den n√§chsten nicht, da sie unabh√§ngig sind.\nFalsch. Bildkarten k√∂nnen rot sein (z. B. Karo-Dame). ‚ÄûBildkarte‚Äú und ‚Äûrote Karte‚Äú sind nicht disjunkt.\nWahr. Eine Bildkarte (Bube, Dame, K√∂nig) kann kein Ass sein; die Ereignisse sind disjunkt.\n\n\n\n\n\n\n3.9.3 Bedingte Wahrscheinlichkeit\nBedingte Wahrscheinlichkeit beschreibt die Wahrscheinlichkeit eines Ereignisses \\(A\\), gegeben dass ein anderes Ereignis \\(B\\) eingetreten ist, notiert als \\(P(A | B)\\) (‚Äû\\(A\\) gegeben \\(B\\)‚Äú). Beispiel: Wie wahrscheinlich ist eine Bildkarte, wenn die Karte eine Karo-Karte ist? Aus Figure Figure¬†3.8:\n\\[P(\\text{Bild} | \\text{Karo}) = \\frac{3}{13},\\]\nda von 13 Karo-Karten 3 Bildkarten sind.\nEine Kreuztabelle (contingency table) zeigt die H√§ufigkeiten von Ereigniskombinationen, z. B. im loan50-Datensatz (siehe Table (tab-contingency-loan?)).\n\nimport pandas as pd\n\ndf = pd.read_csv(\"../_assets/dataexploratory/loan50.csv\")\ncontingency_table = pd.crosstab(df['homeownership'], df['has_second_income'])\nprint(contingency_table)\n\nhas_second_income  False  True \nhomeownership                  \nmortgage              20      6\nown                    3      0\nrent                  19      2\n\n\n\n\n3.9.4 Bedingte Wahrscheinlichkeit (Fortsetzung)\nAus der Kreuztabelle (Table (tab-contingency-loan?)) ergibt sich: Von 26 Kreditnehmern mit Hypothek haben 6 ein zweites Einkommen. Die bedingte Wahrscheinlichkeit ist:\n\\[P(\\text{Zweiteinkommen} | \\text{Hypothek}) = \\frac{6}{26}.\\]\n\n\n\n\n\n\nImportant\n\n\n\nDie bedingte Wahrscheinlichkeit \\(P(A|B)\\) ist die Wahrscheinlichkeit von \\(A\\), gegeben dass \\(B\\) eingetreten ist. Sie wird berechnet als:\n\\[P(A|B) = \\frac{P(A \\cap B)}{P(B)}.\\]\nIm Beispiel:\n\\[P(\\text{Zweiteinkommen} | \\text{Hypothek}) = \\frac{P(\\text{Zweiteinkommen} \\cap \\text{Hypothek})}{P(\\text{Hypothek})} = \\frac{\\frac{6}{50}}{\\frac{26}{50}} = \\frac{6}{26}.\\]\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\\(P(A \\cap B)\\) kann hier nicht via Multiplikationsregel berechnet werden, da die Ereignisse nicht unabh√§ngig sind. Stattdessen werden beobachtete H√§ufigkeiten verwendet.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nSumme bedingter Wahrscheinlichkeiten: Sind \\(A_1, \\ldots, A_k\\) alle disjunkten Ergebnisse einer Variable, gilt f√ºr ein Ereignis \\(B\\):\n\\[P(A_1|B) + \\cdots + P(A_k|B) = 1.\\]\nF√ºr ein Ereignis und sein Komplement:\n\\[P(A|B) = 1 - P(A^c|B).\\]\n\n\n\n\n3.9.5 Beispiel: AIDS-Test\n\n\n\n\n\n\nFigure¬†3.10: Meme AIDS Test\n\n\n\nWie hoch ist die Wahrscheinlichkeit, dass eine Person AIDS hat, wenn folgendes bekannt ist:\n\nDie Wahrscheinlichkeit, dass eine Person AIDS hat, betr√§gt 0,1 % (\\(P(\\text{AIDS}) = 0.001\\)).\nBei AIDS betr√§gt die Wahrscheinlichkeit eines positiven Tests 99 % (\\(P(\\text{Positiv} | \\text{AIDS}) = 0.99\\), Sensitivit√§t).\nOhne AIDS betr√§gt die Wahrscheinlichkeit eines positiven Tests 5 % (\\(P(\\text{Positiv} | \\text{Kein AIDS}) = 0.05\\), falsch-positiv, 1 - Spezifit√§t).\n\nEin Baumdiagramm hilft: \\(P(\\text{AIDS}) = 0.001\\), \\(P(\\text{Kein AIDS}) = 0.999\\), \\(P(\\text{Positiv} | \\text{AIDS}) = 0.99\\), \\(P(\\text{Positiv} | \\text{Kein AIDS}) = 0.05\\).\n\n\n\n\n\ngraph LR\n    U[Person] --&gt;|0.001| A[AIDS] \n    U[Person] --&gt;|0.999| D[Kein AIDS]\n    A[AIDS] --&gt;|0.99| B[Positiv]\n    A[AIDS] --&gt;|0.01| C[Negativ]\n    D[Kein AIDS] --&gt;|0.05| E[Positiv]\n    D[Kein AIDS] --&gt;|0.95| F[Negativ]\n\n\n\n\n\n\nPfade mit positivem Test (disjunkt):\n\nOberer Pfad: \\(P(\\text{AIDS} \\cap \\text{Positiv}) = P(\\text{AIDS}) \\cdot P(\\text{Positiv} | \\text{AIDS}) = 0.001 \\cdot 0.99 = 0.00099\\).\nUnterer Pfad: \\(P(\\text{Kein AIDS} \\cap \\text{Positiv}) = P(\\text{Kein AIDS}) \\cdot P(\\text{Positiv} | \\text{Kein AIDS}) = 0.999 \\cdot 0.05 = 0.04995\\).\nGesamt: \\(P(\\text{Positiv}) = 0.00099 + 0.04995 = 0.05094\\).\n\nBedingte Wahrscheinlichkeit:\n\\[\nP(\\text{AIDS} | \\text{Positiv}) = \\frac{P(\\text{AIDS} \\cap \\text{Positiv})}{P(\\text{Positiv})} = \\frac{0.00099}{0.05094} = 0.0194.\n\\]\nTrotz positivem Test ist die Wahrscheinlichkeit f√ºr AIDS gering (ca. 1,94 %). In der Praxis folgen Best√§tigungstests, und die Pr√§valenz in Risikogruppen ist h√∂her als 0,1 %.\n\n\n3.9.6 Satz von Bayes\nIm AIDS-Test-Beispiel kennen wir \\(P(\\text{Positiv} | \\text{AIDS})\\) ‚Äì die Wahrscheinlichkeit eines positiven Tests bei AIDS ‚Äì und m√∂chten die Umkehrung, \\(P(\\text{AIDS} | \\text{Positiv})\\) ‚Äì die Wahrscheinlichkeit von AIDS bei einem positiven Test. Diese Umkehrung nennt sich bedingte Wahrscheinlichkeit in umgekehrter Richtung. Doch wie gelangen wir von der einen zur anderen? Der Satz von Bayes liefert die L√∂sung, indem er bedingte Wahrscheinlichkeiten umkehrt.\n\n3.9.6.1 Motivation und Herleitung\nStellen wir uns vor, wir wollen \\(P(\\text{AIDS} | \\text{Positiv})\\) berechnen. Aus der Definition der bedingten Wahrscheinlichkeit wissen wir:\n\\[P(\\text{AIDS} | \\text{Positiv}) = \\frac{P(\\text{AIDS} \\cap \\text{Positiv})}{P(\\text{Positiv})}.\\]\nGleichzeitig gilt f√ºr die umgekehrte Richtung:\n\\[P(\\text{Positiv} | \\text{AIDS}) = \\frac{P(\\text{Positiv} \\cap \\text{AIDS})}{P(\\text{AIDS})}.\\]\nDa \\(P(\\text{AIDS} \\cap \\text{Positiv}) = P(\\text{Positiv} \\cap \\text{AIDS})\\) (Schnittmengen sind symmetrisch), k√∂nnen wir die zweite Gleichung umstellen:\n\\[P(\\text{AIDS} \\cap \\text{Positiv}) = P(\\text{Positiv} | \\text{AIDS}) \\cdot P(\\text{AIDS}).\\]\nSetzen wir dies in die erste Gleichung ein:\n\\[P(\\text{AIDS} | \\text{Positiv}) = \\frac{P(\\text{Positiv} | \\text{AIDS}) \\cdot P(\\text{AIDS})}{P(\\text{Positiv})}.\\]\nDas ist der Satz von Bayes! Er verbindet die bekannte Bedingung (\\(P(\\text{Positiv} | \\text{AIDS})\\)) mit der gesuchten (\\(P(\\text{AIDS} | \\text{Positiv})\\)), wobei \\(P(\\text{Positiv})\\) die Gesamtwahrscheinlichkeit eines positiven Tests ist.\n\n\n3.9.6.2 Formale Definition\nDer Satz von Bayes lautet allgemein:\n\\[P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)},\\]\nwobei:\n\n\\(P(A|B)\\) die Posterior-Wahrscheinlichkeit ist (z. B. AIDS bei positivem Test),\n\n\\(P(B|A)\\) die Likelihood (z. B. positiver Test bei AIDS),\n\n\\(P(A)\\) der Prior (z. B. Grundwahrscheinlichkeit f√ºr AIDS),\n\n\\(P(B)\\) die Normalisierungskonstante (z. B. Gesamtwahrscheinlichkeit eines positiven Tests).\n\nWir k√∂nnen also unser Vorwissen (Prior) mit neuen Daten (Likelihood) kombinieren, um die Wahrscheinlichkeit f√ºr ein Ereignis zu aktualisieren (Posterior).\n\n\n3.9.6.3 Anwendung\nIm AIDS-Beispiel:\n\n\\(P(\\text{Positiv} | \\text{AIDS}) = 0.99\\),\n\n\\(P(\\text{AIDS}) = 0.001\\),\n\n\\(P(\\text{Positiv}) = P(\\text{Positiv} | \\text{AIDS}) \\cdot P(\\text{AIDS}) + P(\\text{Positiv} | \\text{Kein AIDS}) \\cdot P(\\text{Kein AIDS}) = 0.00099 + 0.04995 = 0.05094\\).\nDamit:\n\\[P(\\text{AIDS} | \\text{Positiv}) = \\frac{0.99 \\cdot 0.001}{0.05094} \\approx 0.01943.\\]\nDer Satz von Bayes ist in Medizin, Wirtschaft und Technik essenziell, um aus bekannten Daten (z. B. Testresultaten) auf Ursachen (z. B. Krankheiten) zu schlie√üen.\n\n\n\n\n\n\n\nTip\n\n\n\nYoutube-Videos:\n\nThree Blue One Brown: Bayes Theorem\nVeritasium: The Bayesian Trap\n\n\n\n\n\n\n\n\n\nAssoziationsanalyse mit A-Priori-Algorithmus\n\n\n\nDie Assoziationsanalyse ist ein Verfahren, um Zusammenh√§nge in Daten zu finden. Ein bekannter Algorithmus ist der A-Priori-Algorithmus, der z.B. in f√ºr Predictive Maintainance oder in der Warenkorbanalyse verwendet wird. Der A-Priori-Algorithmus findet heraus, welche Produkte oft zusammen gekauft werden. Ein Beispiel ist, dass Kunden, die Windeln kaufen, oft auch Bier kaufen.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Stichproben und Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "statistics/distributions.html",
    "href": "statistics/distributions.html",
    "title": "4¬† Verteilungen",
    "section": "",
    "text": "4.1 Diskrete Verteilungen\nIn diesem Abschnitt untersuchen wir Verteilungen einzelner Variablen. Histogramme und Balkendiagramme veranschaulichen die H√§ufigkeits- oder Wahrscheinlichkeitsverteilung, wie in den folgenden Abbildungen dargestellt. Verteilungen sind die Grundlage f√ºr Simulationen, z. B. in der Monte-Carlo-Methode, wie im Tutorial zur Fahrzeugausfallzeit gezeigt.\nDiskrete Verteilungen beschreiben Zufallsvariablen mit abz√§hlbaren Werten. Wir betrachten die Bernoulli-Verteilung, Binomial-Verteilung und Poisson-Verteilung, die h√§ufig in der Statistik und realen Anwendungen vorkommen.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Verteilungen</span>"
    ]
  },
  {
    "objectID": "statistics/distributions.html#sec-discrete-distributions",
    "href": "statistics/distributions.html#sec-discrete-distributions",
    "title": "4¬† Verteilungen",
    "section": "",
    "text": "4.1.1 Bernoulli-Verteilung\nDie Bernoulli-Verteilung modelliert eine Zufallsvariable mit genau zwei m√∂glichen Ergebnissen, z. B. einen M√ºnzwurf: ‚ÄûKopf‚Äú (\\(X = 1\\)) mit Wahrscheinlichkeit \\(p\\) oder ‚ÄûZahl‚Äú (\\(X = 0\\)) mit Wahrscheinlichkeit \\(1-p\\). Die Wahrscheinlichkeitsfunktion lautet:\n\\[P(X = x) = \\begin{cases}\np & \\text{f√ºr } x = 1, \\\\\n1-p & \\text{f√ºr } x = 0.\n\\end{cases}\\]\nWir schreiben \\(X \\sim \\text{Bernoulli}(p)\\). F√ºr eine faire M√ºnze gilt \\(p = 0.5\\), also \\(X \\sim \\text{Bernoulli}(0.5)\\).\nFigure¬†4.1 zeigt die Verteilung f√ºr eine faire M√ºnze. Diese einfache Verteilung ist die Basis f√ºr komplexere Modelle wie die Binomial-Verteilung und findet Anwendung in Entscheidungen mit Ja/Nein-Ergebnissen.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\np = 0.5\nx = [0, 1]  # Einfache Liste statt NumPy-Array f√ºr Klarheit\nP_X = [1-p, p]\n\nplt.bar(x, P_X, color='skyblue', width=0.6)\nplt.xlabel('Ergebnis (0 = Zahl, 1 = Kopf)')\nplt.ylabel('Wahrscheinlichkeit')\nplt.title('Bernoulli-Verteilung ($p = 0.5$)')\nplt.ylim(0, 1)\nplt.xticks(x)\nplt.grid(axis='y', alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\nFigure¬†4.1: Wahrscheinlichkeitsverteilung der Bernoulli-Verteilung f√ºr eine faire M√ºnze (p = 0.5).\n\n\n\n\n\nIn Figure¬†4.1 wird jeder m√∂glichen Realisierung der Zufallsvariablen \\(X\\) die theoretische Wahrscheinlichkeit zugeordnet. Im Gegensatz zu Histogrammen, die empirische H√§ufigkeiten darstellen, zeigt diese Abbildung die Wahrscheinlichkeitsverteilung direkt.\n\n4.1.1.1 Erwartungswert und Varianz der Bernoulli-Verteilung\n\n\n\n\n\n\nImportant\n\n\n\nDer Erwartungswert \\(E(X)\\) einer Verteilung beschreibt ihre zentrale Tendenz ‚Äì den durchschnittlichen Wert der Zufallsvariablen. F√ºr eine diskrete Zufallsvariable \\(X\\) gilt:\n\\[E(X) = \\sum_{x} x \\cdot P(X = x).\\]\nBei der Bernoulli-Verteilung (\\(X \\sim \\text{Bernoulli}(p)\\)) ist:\n\\[E(X) = 0 \\cdot (1-p) + 1 \\cdot p = p.\\]\nF√ºr eine faire M√ºnze (\\(p = 0.5\\)) ist \\(E(X) = 0.5\\).\n\n\n\n\n\n\n\n\nImportant\n\n\n\nDie Varianz \\(\\text{Var}(X)\\) misst die Streuung der Verteilung ‚Äì wie stark die Werte um den Erwartungswert schwanken. Sie wird berechnet als:\n\\[\\text{Var}(X) = \\sum_{x} (x - E(X))^2 \\cdot P(X = x).\\]\nF√ºr die Bernoulli-Verteilung ergibt sich:\n\\[\\text{Var}(X) = (0 - p)^2 \\cdot (1-p) + (1 - p)^2 \\cdot p = p \\cdot (1-p).\\]\nBei \\(p = 0.5\\) ist \\(\\text{Var}(X) = 0.5 \\cdot 0.5 = 0.25\\).\n\n\n\n\n\n4.1.2 Binomialverteilung\nStellen wir uns vor, wir wiederholen einen M√ºnzwurf \\(n\\)-mal unabh√§ngig mit der Wahrscheinlichkeit \\(p\\) f√ºr ‚ÄûKopf‚Äú. Die Zufallsvariable \\(Y\\) z√§hlt die Anzahl der Kopfw√ºrfe und folgt einer Binomialverteilung: \\(Y \\sim \\text{Bin}(n, p)\\). Die Wahrscheinlichkeitsfunktion lautet:\n\\[P(Y = k) = \\binom{n}{k} p^k (1-p)^{n-k},\\]\nwobei \\(\\binom{n}{k}\\) die Anzahl der M√∂glichkeiten ist, \\(k\\) Erfolge in \\(n\\) Versuchen zu erzielen.\nFigure¬†4.2 zeigt die Verteilung f√ºr \\(n = 10\\) und \\(p = 0.5\\). Die Binomialverteilung ist eine Erweiterung der Bernoulli-Verteilung und n√ºtzlich, um H√§ufigkeiten in wiederholten Experimenten zu modellieren.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import binom\n\nn = 10\np = 0.5\ny = np.arange(0, n + 1)\nP_Y = binom.pmf(y, n, p)\n\nplt.bar(y, P_Y, color='skyblue', width=0.8)\nplt.xlabel('Anzahl der Kopfw√ºrfe ($Y$)')\nplt.ylabel('Wahrscheinlichkeit')\nplt.title(f'Binomialverteilung ($n = {n}$, $p = {p}$)')\nplt.xticks(y)\nplt.grid(axis='y', alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\nFigure¬†4.2: Wahrscheinlichkeitsverteilung der Binomialverteilung f√ºr eine faire M√ºnze (n = 10, p = 0.5).\n\n\n\n\n\nStellen wir uns vor, wir wiederholen einen M√ºnzwurf \\(n\\)-mal. Die W√ºrfe sind unabh√§ngig und identisch verteilt (i.i.d.), d.¬†h., jeder Wurf hat die gleiche Wahrscheinlichkeit \\(p\\) f√ºr ‚ÄûKopf‚Äú und ist unbeeinflusst von den anderen. Die Zufallsvariable \\(Y\\) z√§hlt die Anzahl der Kopfw√ºrfe und folgt einer Binomialverteilung: \\(Y \\sim \\text{Bin}(n, p)\\). Die Wahrscheinlichkeitsfunktion ist:\n\\[P(Y = y) = \\binom{n}{y} \\cdot p^y \\cdot (1-p)^{n-y},\\]\nwobei \\(\\binom{n}{y}\\) der Binomialkoeffizient ist. Dieser gibt an, auf wie viele Arten \\(y\\) Erfolge in \\(n\\) Versuchen auftreten k√∂nnen und wird definiert als:\n\\[\\binom{n}{y} = \\frac{n!}{y! \\cdot (n - y)!}.\\]\n\n4.1.2.1 Beispiel: M√ºnzwurf\nF√ºr \\(n = 10\\) W√ºrfe mit einer fairen M√ºnze (\\(p = 0.5\\)) ist \\(Y \\sim \\text{Bin}(10, 0.5)\\). Dies wurde in Figure¬†4.2 gezeigt.\n\n\n4.1.2.2 Beispiel: Gewinnlose\nPassen wir die Werte an: Beim Ziehen von 10 Losen, wobei die Wahrscheinlichkeit f√ºr ein Gewinnlos \\(p = 0.1\\) betr√§gt, gilt \\(Y \\sim \\text{Bin}(10, 0.1)\\). Die Zufallsvariable \\(Y\\) z√§hlt die Anzahl der Gewinnlose. Figure¬†4.3 zeigt diese Verteilung. Solche Modelle sind n√ºtzlich, um Erfolge in wiederholten, unabh√§ngigen Versuchen zu analysieren ‚Äì √§hnlich wie im Tutorial zur Fahrzeugausfallzeit, wo Verteilungen f√ºr Komponenten genutzt werden.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import binom\n\nn = 10\np = 0.1\ny = np.arange(0, n + 1)\nP_Y = binom.pmf(y, n, p)\n\nplt.bar(y, P_Y, color='skyblue', width=0.8)\nplt.xlabel('Anzahl der Gewinnlose ($Y$)')\nplt.ylabel('Wahrscheinlichkeit')\nplt.title(f'Binomialverteilung ($n = {n}$, $p = {p}$)')\nplt.xticks(y)\nplt.grid(axis='y', alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\nFigure¬†4.3: Wahrscheinlichkeitsverteilung der Binomialverteilung f√ºr das Ziehen von Gewinnlosen (n = 10, p = 0.1).\n\n\n\n\n\n\n\n4.1.2.3 Erwartungswert und Varianz der Binomialverteilung\nDer Erwartungswert \\(E(Y)\\) der Binomialverteilung (\\(Y \\sim \\text{Bin}(n, p)\\)) gibt die erwartete Anzahl der Erfolge in \\(n\\) Versuchen an:\n\\[E(Y) = n \\cdot p.\\]\nF√ºr \\(n = 10\\) und \\(p = 0.1\\) (Gewinnlose) ist \\(E(Y) = 10 \\cdot 0.1 = 1\\).\nDie Varianz \\(\\text{Var}(Y)\\) misst die Streuung um diesen Erwartungswert:\n\\[\\text{Var}(Y) = n \\cdot p \\cdot (1-p).\\]\nIm Beispiel ist \\(\\text{Var}(Y) = 10 \\cdot 0.1 \\cdot 0.9 = 0.9\\). Diese Werte helfen, die Verteilung zu charakterisieren, z. B. in Simulationen wie im Tutorial.\n\n\n\n4.1.3 Diskrete Gleichverteilung\nDie diskrete Gleichverteilung (auch Uniformverteilung) beschreibt eine Zufallsvariable, bei der alle m√∂glichen Werte innerhalb eines Intervalls die gleiche Wahrscheinlichkeit haben. Im Gegensatz zum W√ºrfelbeispiel, wo die Werte bei 1 beginnen, kann das Intervall beliebig gew√§hlt werden, z. B. \\(x = a, a+1, \\ldots, b\\), wobei \\(a\\) und \\(b\\) ganze Zahlen sind und \\(a \\leq b\\). Die Anzahl der Werte ist \\(n = b - a + 1\\), und die Wahrscheinlichkeitsfunktion lautet:\n\\[P(X = x) = \\frac{1}{b - a + 1}, \\quad \\text{f√ºr } x = a, a+1, \\ldots, b.\\]\nWir notieren \\(X \\sim \\text{DU}(a, b)\\) f√ºr die diskrete Gleichverteilung auf \\([a, b]\\).\nFigure¬†4.4 zeigt die Verteilung f√ºr \\(a = 3\\) und \\(b = 8\\) (z. B. ein modifizierter W√ºrfel). Dieses flexible Intervall ist n√ºtzlich, um spezifische Szenarien zu modellieren, z. B. in Simulationen mit nicht standardisierten Bereichen.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nn = 6\nx = np.arange(1, n + 1)\nP_X = np.full(n, 1/n)  # Array mit konstanter Wahrscheinlichkeit 1/n\n\nplt.bar(x, P_X, color='skyblue', width=0.8)\nplt.xlabel('W√ºrfelzahl ($X$)')\nplt.ylabel('Wahrscheinlichkeit')\nplt.title(f'Diskrete Gleichverteilung ($n = {n}$)')\nplt.xticks(x)\nplt.grid(axis='y', alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\nFigure¬†4.4: Wahrscheinlichkeitsverteilung der diskreten Gleichverteilung f√ºr einen W√ºrfel (n = 6).\n\n\n\n\n\nIn der Abbildung (fig:sec-dataexploratory-distributions-discrete-uniform-distribution?) wird die Wahrscheinlichkeitsverteilung der diskreten Gleichverteilung f√ºr das W√ºrfeln eines W√ºrfels dargestellt. Die Wahrscheinlichkeitsverteilung zeigt, dass alle m√∂glichen Werte der Zufallsvariablen die gleiche Wahrscheinlichkeit haben.\n\n4.1.3.1 Erwartungswert und Varianz der diskreten Gleichverteilung\nDer Erwartungswert \\(E(X)\\) einer diskreten Gleichverteilung auf dem Intervall \\([a, b]\\) liegt in der Mitte des Intervalls:\n\\[E(X) = \\frac{a + b}{2} = \\sum_{x} x \\cdot P(X = x).$.\\]\nF√ºr \\(a = 3\\) und \\(b = 8\\) ergibt sich:\n\\[E(X) = \\frac{3 + 8}{2} = 5.5.\\]\nDie Varianz \\(\\text{Var}(X)\\) beschreibt die Streuung der Verteilung:\n\\[\\text{Var}(X) = \\frac{(b - a + 1)^2 - 1}{12} = \\sum_{x} (x - E(X))^2 \\cdot P(X = x)..\\]\nBei \\(a = 3\\) und \\(b = 8\\) ist \\(n = b - a + 1 = 6\\), also:\n\\[\\text{Var}(X) = \\frac{6^2 - 1}{12} = \\frac{36 - 1}{12} = \\frac{35}{12} \\approx 2.9167.\\]\nDiese Formeln gelten f√ºr jedes Intervall \\([a, b]\\), wobei \\(a\\) und \\(b\\) ganze Zahlen sind und \\(a \\leq b\\).",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Verteilungen</span>"
    ]
  },
  {
    "objectID": "statistics/distributions.html#sec-continuous-distributions",
    "href": "statistics/distributions.html#sec-continuous-distributions",
    "title": "4¬† Verteilungen",
    "section": "4.2 Stetige Verteilungen",
    "text": "4.2 Stetige Verteilungen\nStetige Verteilungen modellieren Zufallsvariablen, die kontinuierliche Werte annehmen k√∂nnen, z. B. Zeit oder L√§nge. Im Gegensatz zu diskreten Verteilungen gibt es hier unendlich viele m√∂gliche Werte innerhalb eines Intervalls. Wir betrachten die Normalverteilung, Exponentialverteilung und stetige Gleichverteilung, die in Simulationen wie im Tutorial zur Fahrzeugausfallzeit eine Rolle spielen.\n\n4.2.1 Stetige Gleichverteilung\nDie stetige Gleichverteilung beschreibt eine Zufallsvariable, bei der alle Werte in einem Intervall \\([a, b]\\) gleich wahrscheinlich sind. Die Wahrscheinlichkeitsdichtefunktion (Dichte) lautet:\n\\[f(x) = \\begin{cases}\n\\frac{1}{b - a} & \\text{f√ºr } a \\leq x \\leq b, \\\\\n0 & \\text{sonst}.\n\\end{cases}\\]\nWir schreiben \\(X \\sim \\text{U}(a, b)\\). Bei stetigen Verteilungen wird die Wahrscheinlichkeit als Fl√§che unter der Dichte berechnet, wobei die Gesamtfl√§che stets 1 betr√§gt.\nFigure¬†4.5 zeigt die Dichte f√ºr \\(a = 0\\) und \\(b = 1\\). Im Tutorial wird die stetige Gleichverteilung f√ºr die Steuereinheit (\\(X \\sim \\text{U}(4000, 8000)\\)) verwendet, um Ausfallzeiten zu modellieren.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\na = 0\nb = 1\nx = np.linspace(a - 0.5, b + 0.5, 1000)\nf_X = np.where((x &gt;= a) & (x &lt;= b), 1 / (b - a), 0)\n\nplt.plot(x, f_X, color='skyblue', linewidth=2)\nplt.fill_between(x, f_X, alpha=0.2, color='skyblue')  # Fl√§che einf√§rben\nplt.xlabel('Werte der Zufallsvariablen ($X$)')\nplt.ylabel('Wahrscheinlichkeitsdichte')\nplt.title(f'Stetige Gleichverteilung ($a = {a}$, $b = {b}$)')\nplt.grid(alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\nFigure¬†4.5: Wahrscheinlichkeitsdichte der stetigen Gleichverteilung auf dem Intervall [0, 1].\n\n\n\n\n\nBei stetigen Verteilungen ist die Wahrscheinlichkeit eines exakten Wertes 0, da es unendlich viele m√∂gliche Werte gibt. Stattdessen berechnen wir die Wahrscheinlichkeit f√ºr einen Wertebereich als Fl√§che unter der Dichtefunktion:\n\\[P(a \\leq X \\leq b) = \\int_{a}^{b} f(x) \\, dx.\\]\nDer Erwartungswert \\(E(X)\\) einer gleichverteilten Zufallsvariable \\(X \\sim \\text{U}(a, b)\\) ist:\n\\[E(X) = \\frac{a + b}{2}=\\int\\limits_{ - \\infty }^\\infty {x \\cdot f\\left( x \\right)} \\,\\,dx.\\]\nDie Varianz \\(\\text{Var}(X)\\) betr√§gt:\n\\[\\text{Var}(X) = \\frac{(b - a)^2}{12} = \\int\\limits_{ - \\infty }^\\infty {{{\\left( {x - {\\mu _x}} \\right)}^2}} \\cdot f\\left( x \\right)\\,\\,dx.\\]\n\n4.2.1.1 Beispiel: Ausfallwahrscheinlichkeit eines Bauteils\nDer Ausfall eines elektronischen Bauteils folgt einer stetigen Gleichverteilung auf \\([0, 3650]\\) Tagen (\\(X \\sim \\text{U}(0, 3650)\\)). Wie hoch ist die Wahrscheinlichkeit, dass es innerhalb der ersten 1000 Tage ausf√§llt?\nDie Dichtefunktion ist:\n\\[f(x) = \\begin{cases}\n\\frac{1}{3650 - 0} = \\frac{1}{3650} & \\text{f√ºr } 0 \\leq x \\leq 3650, \\\\\n0 & \\text{sonst}.\n\\end{cases}\\]\nDie Wahrscheinlichkeit ergibt sich durch Integration:\n\\[P(0 \\leq X \\leq 1000) = \\int_{0}^{1000} \\frac{1}{3650} \\, dx = \\frac{1000}{3650} \\approx 0.274.\\]\nDas Bauteil hat also eine 27,4 % Chance, innerhalb von 1000 Tagen auszusetzen. Dieses Prinzip wird im Tutorial bei der Steuereinheit (\\(X \\sim \\text{U}(4000, 8000)\\)) angewendet.\n\n\n\n\n\n\nL√∂sungsschritte\n\n\n\n\n\n\nDichte: \\(f(x) = \\frac{1}{3650}\\) f√ºr \\(0 \\leq x \\leq 3650\\).\n\nIntegral: \\(P(0 \\leq X \\leq 1000) = \\int_{0}^{1000} \\frac{1}{3650} \\, dx\\).\n\nErgebnis: \\(\\frac{1000}{3650} = 0.274\\).\n\n\n\n\n\n\n\n4.2.2 Normalverteilung\nDie Normalverteilung ist eine stetige Verteilung, die in Statistik und Naturwissenschaften weit verbreitet ist. Ihre Glockenform zeigt eine symmetrische Verteilung der Werte um den Erwartungswert. Die Dichtefunktion lautet:\n\\[f(x) = \\frac{1}{\\sqrt{2\\pi} \\sigma} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right),\\]\nf√ºr \\(-\\infty &lt; x &lt; \\infty\\). Wir schreiben \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), wobei \\(\\mu\\) der Erwartungswert und \\(\\sigma^2\\) die Varianz ist.\nFalls \\(\\mu = 0\\) und \\(\\sigma = 1\\), spricht man von der Standardnormalverteilung (\\(X \\sim \\mathcal{N}(0, 1)\\)), eine spezielle Form mit Erwartungswert 0 und Varianz 1. Figure¬†4.6 zeigt diese Verteilung. Im Tutorial wird die Normalverteilung f√ºr den Sensor (\\(X \\sim \\mathcal{N}(6000, 100^2)\\)) genutzt, um Ausfallzeiten zu modellieren.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nmu = 0\nsigma = 1\nx = np.linspace(-5, 5, 1000)\nf_X = 1 / (np.sqrt(2 * np.pi) * sigma) * np.exp(-((x - mu) ** 2) / (2 * sigma ** 2))\n\nplt.plot(x, f_X, color='skyblue', linewidth=2)\nplt.fill_between(x, f_X, alpha=0.2, color='skyblue')  # Fl√§che einf√§rben\nplt.xlabel('Werte der Zufallsvariablen ($X$)')\nplt.ylabel('Wahrscheinlichkeitsdichte')\nplt.title(f'Standardnormalverteilung ($\\mu = {mu}$, $\\sigma = {sigma}$)')\nplt.grid(alpha=0.3)\nplt.show()\n\n&lt;&gt;:13: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:13: SyntaxWarning:\n\ninvalid escape sequence '\\s'\n\n&lt;&gt;:13: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:13: SyntaxWarning:\n\ninvalid escape sequence '\\s'\n\n/tmp/ipykernel_3171/513147542.py:13: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n/tmp/ipykernel_3171/513147542.py:13: SyntaxWarning:\n\ninvalid escape sequence '\\s'\n\n\n\n\n\n\n\n\n\nFigure¬†4.6: Wahrscheinlichkeitsdichte der Standardnormalverteilung (\\(\\mu = 0\\), \\(\\sigma = 1\\)).\n\n\n\n\n\nFr√ºher, ohne Computer, war die Berechnung der Fl√§che unter der Normalverteilungskurve (\\(P(X \\leq x)\\)) schwierig. Man nutzte \\(Z\\)-Wert-Tabellen, um die Wahrscheinlichkeit f√ºr eine standardnormalverteilte Zufallsvariable \\(Z \\sim \\mathcal{N}(0, 1)\\) nachzuschlagen. Beispiele:\n\n\\(P(Z \\leq 0) = 0.5\\) (50 %).\n\n\\(P(Z \\leq 1) = 0.8413\\) (84,13 %).\n\n\\(P(Z \\leq -1) = 1 - P(Z \\leq 1) = 0.1587\\) (15,87 %).\nUmgekehrt: \\(P(Z \\leq 1.645) = 0.95\\) (95 %). Heute ersetzen Computerprogramme solche Tabellen, wie im Folgenden gezeigt.\n\n\n4.2.2.1 Erwartungswert und Varianz der Normalverteilung\nDer Erwartungswert \\(E(X)\\) einer normalverteilten Zufallsvariable \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) ist:\n\\[E(X) = \\mu = \\int\\limits_{ - \\infty }^\\infty {x \\cdot f\\left( x \\right)} \\,\\,dx.\\]\nDie Varianz \\(\\text{Var}(X)\\) betr√§gt:\n\\[{\\sigma _x}^2 = Var\\left( X \\right) = E{\\left( {X - {\\mu _x}} \\right)^2} = \\int\\limits_{ - \\infty }^\\infty {{{\\left( {x - {\\mu _x}} \\right)}^2}} \\cdot f\\left( x \\right)\\,\\,dx.\\]\nF√ºr die Standardnormalverteilung (\\(X \\sim \\mathcal{N}(0, 1)\\)) gilt \\(E(X) = 0\\) und \\(\\text{Var}(X) = 1\\). Im Tutorial wird dies f√ºr den Sensor (\\(X \\sim \\mathcal{N}(6000, 100^2)\\)) genutzt.\n\n4.2.2.1.1 Standardisierung der Normalverteilung\nViele Zufallsvariablen folgen keiner Standardnormalverteilung, sondern haben andere Werte f√ºr \\(\\mu\\) und \\(\\sigma\\). Um diese auf \\(Z \\sim \\mathcal{N}(0, 1)\\) zu √ºberf√ºhren, wird standardisiert:\n\\[Z = \\frac{X - \\mu}{\\sigma}.\\]\n\\(Z\\) hat dann \\(E(Z) = 0\\) und \\(\\text{Var}(Z) = 1\\), sodass \\(Z\\)-Tabellen oder Software genutzt werden k√∂nnen.\n\n\n\n4.2.2.2 Beispiel: Intelligenz-Quotient (IQ)\nDer IQ ist normalverteilt mit \\(X \\sim \\mathcal{N}(100, 15^2)\\) (\\(\\mu = 100\\), \\(\\sigma = 15\\)). Wie wahrscheinlich ist ein IQ von 130 oder mehr (\\(P(X \\geq 130)\\))?\n\n4.2.2.2.1 Analytische Berechnung\nDie Dichtefunktion ist:\n\\[f(x) = \\frac{1}{\\sqrt{2\\pi} \\cdot 15} \\exp\\left(-\\frac{(x - 100)^2}{2 \\cdot 15^2}\\right).\\]\nDie Wahrscheinlichkeit \\(P(X \\geq 130)\\) ergibt sich als:\n\\[P(X \\geq 130) = 1 - P(X \\leq 130) = 1 - \\int_{-\\infty}^{130} f(x) \\, dx.\\]\nMit Standardisierung:\n\\[Z = \\frac{130 - 100}{15} = 2, \\quad P(X \\leq 130) = P(Z \\leq 2).\\]\nAus Tabellen oder Software: \\(P(Z \\leq 2) \\approx 0.9772\\), also:\n\\[P(X \\geq 130) = 1 - 0.9772 = 0.0228 \\text{ (2,28 %)}.\\]\n\n\n4.2.2.2.2 Berechnung mit Python\nMit scipy.stats.norm k√∂nnen wir die kumulative Verteilungsfunktion (cdf) direkt berechnen:\n\n\n\nimport numpy as np\nfrom scipy.stats import norm\n\nmu = 100\nsigma = 15\nP_X_geq_130 = 1 - norm.cdf(130, loc=mu, scale=sigma)\n\nprint(f'P(X &gt;= 130) = {P_X_geq_130:.4f}')  # Ausgabe: 0.0228\n\nP(X &gt;= 130) = 0.0228\n\n\n\nFigure¬†4.7\n\n\n\n\n\n\n4.2.2.3 Visualisierung der Wahrscheinlichkeiten\nOft interessiert der Anteil einer Population innerhalb von ein oder zwei Standardabweichungen vom Mittelwert. F√ºr eine normalverteilte Zufallsvariable \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) berechnen wir:\n\\[P(\\mu - \\sigma \\leq X \\leq \\mu + \\sigma) = P(X \\leq \\mu + \\sigma) - P(X \\leq \\mu - \\sigma),\\]\nwobei \\(P(X \\leq x)\\) die kumulative Verteilungsfunktion (CDF) ist. F√ºr die Standardnormalverteilung gilt:\n- \\(P(\\mu - \\sigma \\leq X \\leq \\mu + \\sigma) \\approx 0.6826\\) (ca. 68 %),\n- \\(P(\\mu - 2\\sigma \\leq X \\leq \\mu + 2\\sigma) \\approx 0.9544\\) (ca. 95 %).\nIm IQ-Beispiel (\\(X \\sim \\mathcal{N}(100, 15^2)\\)) pr√ºfen wir den Bereich von zwei Standardabweichungen (\\(\\mu - 2\\sigma = 70\\), \\(\\mu + 2\\sigma = 130\\)). Figure¬†4.8 zeigt diese Wahrscheinlichkeit. Solche Berechnungen sind im Tutorial n√ºtzlich, z. B. um Sensor-Ausfallzeiten zu analysieren.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\nmu = 100\nsigma = 15\nx = np.linspace(mu - 4*sigma, mu + 4*sigma, 1000)\nf_X = norm.pdf(x, loc=mu, scale=sigma)\n\n# Wahrscheinlichkeit f√ºr Œº ¬± 2œÉ\nP_X_2_sigma = norm.cdf(mu + 2*sigma, loc=mu, scale=sigma) - norm.cdf(mu - 2*sigma, loc=mu, scale=sigma)\nprint(f'P({mu - 2*sigma} &lt;= X &lt;= {mu + 2*sigma}) = {P_X_2_sigma:.4f}')  # Ausgabe: 0.9545\n\n# Plot\nplt.plot(x, f_X, color='skyblue', linewidth=2)\nplt.fill_between(x, f_X, where=(x &gt;= mu - 2*sigma) & (x &lt;= mu + 2*sigma), color='lightblue', alpha=0.3, label=f'P = {P_X_2_sigma:.4f}')\nplt.axvline(mu - 2*sigma, color='red', linestyle='--', label=f'{mu - 2*sigma}')\nplt.axvline(mu + 2*sigma, color='red', linestyle='--', label=f'{mu + 2*sigma}')\nplt.xlabel('IQ-Wert ($X$)')\nplt.ylabel('Wahrscheinlichkeitsdichte')\nplt.title(f'IQ-Verteilung ($\\mu = {mu}$, $\\sigma = {sigma}$)')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\n&lt;&gt;:21: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:21: SyntaxWarning:\n\ninvalid escape sequence '\\s'\n\n&lt;&gt;:21: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:21: SyntaxWarning:\n\ninvalid escape sequence '\\s'\n\n/tmp/ipykernel_3171/3074097218.py:21: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n/tmp/ipykernel_3171/3074097218.py:21: SyntaxWarning:\n\ninvalid escape sequence '\\s'\n\n\n\nP(70 &lt;= X &lt;= 130) = 0.9545\n\n\n\n\n\n\n\n\nFigure¬†4.8: Wahrscheinlichkeit, dass ein IQ-Wert innerhalb von zwei Standardabweichungen liegt.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nDer IQ ist so skaliert, dass \\(\\mu = 100\\) und \\(\\sigma = 15\\). Die Wahrscheinlichkeit f√ºr einen IQ \\(\\geq 130\\) betr√§gt etwa 2,28 %, w√§hrend 95,45 % der Bev√∂lkerung einen IQ zwischen 70 und 130 haben (innerhalb von \\(\\pm 2\\sigma\\)). Dies zeigt die praktische Relevanz der Normalverteilung.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Verteilungen</span>"
    ]
  },
  {
    "objectID": "statistics/distributions.html#sec-other-distributions",
    "href": "statistics/distributions.html#sec-other-distributions",
    "title": "4¬† Verteilungen",
    "section": "4.3 Weitere Verteilungen",
    "text": "4.3 Weitere Verteilungen\nNeben der Normalverteilung spielen weitere stetige und diskrete Verteilungen eine Rolle in Statistik und Simulationen, wie im Tutorial zur Fahrzeugausfallzeit genutzt:\n\nDie Exponentialverteilung modelliert die Zeit zwischen unabh√§ngigen Ereignissen, z. B. in der Zuverl√§ssigkeitsanalyse oder Warteschlangentheorie.\n\nDie Poissonverteilung (diskret) beschreibt die Anzahl von Ereignissen in einem festen Zeitintervall, etwa in Zufallsprozessen.\n\nDiese Verteilungen sind im Python-Modul scipy.stats verf√ºgbar, das Funktionen f√ºr Wahrscheinlichkeiten und Zufallszahlen bietet. Im Tutorial wird z. B. die Poissonverteilung f√ºr den Motor (\\(X \\sim \\text{Poisson}(5000)\\)) eingesetzt.\n\n4.3.1 Exponentialverteilung\nDie Dichtefunktion der Exponentialverteilung ist:\n\\[f(x) = \\lambda e^{-\\lambda x}, \\quad x \\geq 0,\\]\nwobei \\(\\lambda\\) die Rate ist. Wir schreiben \\(X \\sim \\text{Exp}(\\lambda)\\), mit \\(E(X) = \\frac{1}{\\lambda}\\) und \\(\\text{Var}(X) = \\frac{1}{\\lambda^2}\\). Sie ist im Tutorial f√ºr Ausfallzeiten relevant, wenn Ereignisse exponentiell verteilt w√§ren.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Verteilungen</span>"
    ]
  },
  {
    "objectID": "statistics/distributions.html#sec-fitting-distributions",
    "href": "statistics/distributions.html#sec-fitting-distributions",
    "title": "4¬† Verteilungen",
    "section": "4.4 Fitting von Verteilungen",
    "text": "4.4 Fitting von Verteilungen\nIn der Praxis m√ºssen wir oft die Verteilung von Daten bestimmen ‚Äì ein Prozess namens Fitting. Ziel ist es, die Verteilung zu finden, die die Daten am besten beschreibt. Methoden wie Maximum-Likelihood-Sch√§tzung oder die Methode der Momente werden daf√ºr genutzt.\nAls Beispiel simulieren wir Lotterie-Daten mit \\(X \\sim \\text{Bin}(10, 0.1)\\) und passen verschiedene Verteilungen an, um zu vergleichen, welche die Daten am besten modelliert.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import binom, norm, uniform\n\nnp.random.seed(42)\ndata = np.random.binomial(n=10, p=0.1, size=1000)  # 1000 Ziehungen\n\n# Binomialverteilung sch√§tzen\nn_est = 10  # Bekannt aus Simulation\np_est = np.mean(data) / n_est\nbinom_x = np.arange(0, n_est + 1)\nbinom_y = binom.pmf(binom_x, n=n_est, p=p_est)\n\n# Normalverteilung sch√§tzen\nmu, sigma = norm.fit(data)\nnorm_x = np.linspace(0, 10, 1000)\nnorm_y = norm.pdf(norm_x, mu, sigma)\n\n# Gleichverteilung sch√§tzen\na, b = uniform.fit(data)\nuniform_x = np.linspace(0, 10, 1000)\nuniform_y = uniform.pdf(uniform_x, a, b - a)\n\n# Plot\nplt.hist(data, bins=range(11), density=True, color='skyblue', alpha=0.7, label='Daten')\nplt.plot(binom_x, binom_y, 'ro--', label=f'Binomial ($n={n_est}$, $p={p_est:.2f}$)')\nplt.plot(norm_x, norm_y, 'g-', label=f'Normal ($\\mu={mu:.2f}$, $\\sigma={sigma:.2f}$)')\nplt.plot(uniform_x, uniform_y, 'p-', label=f'Gleichverteilung ($a={a:.2f}$, $b={b:.2f}$)')\nplt.xlabel('Anzahl der Gewinnlose ($X$)')\nplt.ylabel('Dichte')\nplt.title('Fitting von Verteilungen an Binomialdaten')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\n&lt;&gt;:27: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:27: SyntaxWarning:\n\ninvalid escape sequence '\\s'\n\n&lt;&gt;:27: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:27: SyntaxWarning:\n\ninvalid escape sequence '\\s'\n\n/tmp/ipykernel_3171/1255964559.py:27: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n/tmp/ipykernel_3171/1255964559.py:27: SyntaxWarning:\n\ninvalid escape sequence '\\s'\n\n\n\n\n\n\n\n\n\nFigure¬†4.9: Fitting verschiedener Verteilungen an simulierte Binomialdaten (n = 10, p = 0.1).",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Verteilungen</span>"
    ]
  },
  {
    "objectID": "statistics/distributions.html#sec-computing-distributions",
    "href": "statistics/distributions.html#sec-computing-distributions",
    "title": "4¬† Verteilungen",
    "section": "4.5 Rechnen mit Verteilungen, Zufallsvariablen und Erwartungswert",
    "text": "4.5 Rechnen mit Verteilungen, Zufallsvariablen und Erwartungswert\nDer zentrale Grenzwertsatz besagt, dass die Summe einer gro√üen Anzahl von unabh√§ngigen und identisch verteilten (i.i.d.) Zufallsvariablen einer Normalverteilung folgt ‚Äì unabh√§ngig von ihrer urspr√ºnglichen Verteilung. Dies ist ein Grundpfeiler der Statistik und erkl√§rt, warum Normalverteilungen in Simulationen wie im Tutorial oft auftreten.\n\n4.5.1 Rechenregeln f√ºr Erwartungswert und Varianz\nF√ºr unabh√§ngige Zufallsvariablen \\(X_1, X_2, \\ldots, X_n\\) gelten folgende Regeln:\n\nErwartungswert: Die Summe der Erwartungswerte gilt immer, auch ohne Unabh√§ngigkeit:\n\\[E(X_1 + X_2 + \\cdots + X_n) = E(X_1) + E(X_2) + \\cdots + E(X_n).\\]\nVarianz: Bei Unabh√§ngigkeit (Kovarianz = 0) ist die Varianz die Summe der Varianzen:\n\\[\\text{Var}(X_1 + X_2 + \\cdots + X_n) = \\text{Var}(X_1) + \\text{Var}(X_2) + \\cdots + \\text{Var}(X_n).\\]\nAndernfalls: \\(\\text{Var}(X_1 + X_2) = \\text{Var}(X_1) + \\text{Var}(X_2) + 2 \\text{Cov}(X_1, X_2)\\).\nLinearkombinationen: F√ºr \\(aX + bY\\):\n\\[E(aX + bY) = aE(X) + bE(Y),\\]\n\\[\\text{Var}(aX + bY) = a^2 \\text{Var}(X) + b^2 \\text{Var}(Y) + 2ab \\text{Cov}(X, Y),\\]\nwobei \\(\\text{Cov}(X, Y) = 0\\) bei Unabh√§ngigkeit.\n\n\n\n\n\n\n\nImportant\n\n\n\nDie Kovarianz \\(\\text{Cov}(X, Y)\\) misst den linearen Zusammenhang zwischen \\(X\\) und \\(Y\\). Bei Unabh√§ngigkeit ist sie 0, was die Varianzberechnung vereinfacht.\n\n\n\n\n4.5.2 Beispiel: Summe von Zufallsvariablen aus verschiedenen Verteilungen\nDer zentrale Grenzwertsatz gilt auch, wenn die Zufallsvariablen nicht identisch verteilt sind, solange sie unabh√§ngig sind und die Anzahl gro√ü ist. Figure¬†4.10 zeigt, wie die Summe von Uniform-, Exponential- und Binomialverteilungen einer Normalverteilung √§hnelt ‚Äì ein Prinzip, das in Monte-Carlo-Simulationen wie im Tutorial genutzt wird.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\nN = 10000\n\n# Zufallsvariablen definieren\nX_uni_1 = np.random.uniform(0, 10, N)      # Uniform [0, 10]\nX_uni_2 = np.random.uniform(2, 8, N)       # Uniform [2, 8]\nX_exp_1 = np.random.exponential(2, N)      # Exponential, Scale = 2\nX_exp_2 = np.random.exponential(3, N)      # Exponential, Scale = 3\nX_bin_1 = np.random.binomial(20, 0.3, N)   # Binomial (n=20, p=0.3)\nX_bin_2 = np.random.binomial(15, 0.4, N)   # Binomial (n=15, p=0.4)\n\nX_sum = X_uni_1 + X_uni_2 + X_exp_1 + X_exp_2 + X_bin_1 + X_bin_2\n\n# Plot\nplt.figure(figsize=(12, 6))\nplt.hist(X_sum, bins=50, density=True, color='skyblue', alpha=0.7, label='Summe')\nplt.hist(X_uni_1, bins=50, density=True, alpha=0.3, label='Uniform 1')\nplt.hist(X_uni_2, bins=50, density=True, alpha=0.3, label='Uniform 2')\nplt.hist(X_exp_1, bins=50, density=True, alpha=0.3, label='Exponential 1')\nplt.hist(X_exp_2, bins=50, density=True, alpha=0.3, label='Exponential 2')\nplt.hist(X_bin_1, bins=50, density=True, alpha=0.3, label='Binomial 1')\nplt.hist(X_bin_2, bins=50, density=True, alpha=0.3, label='Binomial 2')\nplt.xlabel('Wert')\nplt.ylabel('Dichte')\nplt.title('Summe von Zufallsvariablen aus verschiedenen Verteilungen')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\nFigure¬†4.10: Summe von Zufallsvariablen aus verschiedenen Verteilungen (N = 10.000).\n\n\n\n\n\n\n4.5.2.1 Normalverteilungs-Fit der Summe\nUm den zentralen Grenzwertsatz zu veranschaulichen, passen wir eine Normalverteilung an die Summe an. Figure¬†4.11 zeigt, wie gut die Summe einer Normalverteilung entspricht.\n\nfrom scipy.stats import norm\n\n# Normalverteilung an Summe anpassen\nX_sum_mu, X_sum_sigma = norm.fit(X_sum)\n\n# Plot\nplt.hist(X_sum, bins=50, density=True, color='skyblue', alpha=0.7, label='Summe')\nx = np.linspace(min(X_sum), max(X_sum), 1000)\ny = norm.pdf(x, X_sum_mu, X_sum_sigma)\nplt.plot(x, y, 'r-', label=f'Normal ($\\mu={X_sum_mu:.1f}$, $\\sigma={X_sum_sigma:.1f}$)')\nplt.xlabel('Wert')\nplt.ylabel('Dichte')\nplt.title('Fit der Summe mit einer Normalverteilung')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\n&lt;&gt;:10: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:10: SyntaxWarning:\n\ninvalid escape sequence '\\s'\n\n&lt;&gt;:10: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:10: SyntaxWarning:\n\ninvalid escape sequence '\\s'\n\n/tmp/ipykernel_3171/3238649060.py:10: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n/tmp/ipykernel_3171/3238649060.py:10: SyntaxWarning:\n\ninvalid escape sequence '\\s'\n\n\n\n\n\n\n\n\n\nFigure¬†4.11: Fit der Summe von Zufallsvariablen mit einer Normalverteilung (N = 10.000).",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Verteilungen</span>"
    ]
  },
  {
    "objectID": "statistics/tutorial.html",
    "href": "statistics/tutorial.html",
    "title": "Tutorial: Monte Carlo Simulation of Vehicle Component Failure",
    "section": "",
    "text": "Ziel\nIn dieser √úbung erstellen Studierende eine Monte-Carlo-Simulation zur Modellierung der Ausfallzeit von Fahrzeugkomponenten. Dabei werden Wahrscheinlichkeitskonzepte wie bedingte Wahrscheinlichkeit, Additions- und Multiplikationsregeln, statistische Unabh√§ngigkeit sowie Visualisierung durch ein Histogramm vertieft.",
    "crumbs": [
      "Statistik",
      "Tutorial: Monte Carlo Simulation of Vehicle Component Failure"
    ]
  },
  {
    "objectID": "statistics/tutorial.html#sec-monte-carlo-story",
    "href": "statistics/tutorial.html#sec-monte-carlo-story",
    "title": "Tutorial: Monte Carlo Simulation of Vehicle Component Failure",
    "section": "Szenario und Anwendungsfall",
    "text": "Szenario und Anwendungsfall\n\nSzenario\nEin Auto besteht aus kritischen Komponenten: Motor, Sensoren und Steuereinheit. F√§llt der Motor oder die Steuereinheit aus, stoppt das Fahrzeug. Der Besitzer m√∂chte die erwartete Betriebsdauer bis zum Ausfall absch√§tzen, um Wartungsintervalle zu planen.\n\n\nAnwendungsfall\nDie Studierenden simulieren das Ausfallverhalten eines Fahrzeugs mit drei Komponenten:\n- Motor: Ausfallzeit folgt einer Poissonverteilung mit Mittelwert 5000 Stunden.\n- Sensor: Ausfallzeit folgt einer Normalverteilung mit \\(\\mu = 6000\\) Stunden, \\(\\sigma = 100\\) Stunden.\n- Steuereinheit: Ausfallzeit folgt einer Gleichverteilung zwischen 4000 und 8000 Stunden.\n- Bedingung: F√§llt der Sensor vor dem Motor aus, verk√ºrzt sich die Motor-Ausfallzeit um 1000 Stunden.\n- Stopp: Das Fahrzeug stoppt, wenn Motor oder Steuereinheit ausf√§llt.\n\nAufgaben\n\nSimulation\n\nSimuliere Ausfallzeiten f√ºr jede Komponente des Fahrzeugs.\nBestimme die Zeit bis zum Ausfall des Fahrzeugs.\nWiederhole dies f√ºr 10.000 Durchl√§ufe.\nStelle die Zeit bis zum Ausfall in einem Histogramm dar (Figure¬†1).\n\n\n\nAnalyse\nF√ºr die weitere Anaylse nehmen wir an, dass wir den Zufallsprozess der Ausf√§lle nicht kennen. Wir k√∂nnen jedoch die Ausfallereignisse aufzeichnen und analysieren. Wir interessieren uns nur f√ºr Ausf√§lle, die zu einem Stopp des Fahrzeugs vor 5000 Betriebs-Stunden f√ºhren.\n\nBerechne f√ºr jede Komponente die Wahrscheinlichkeit, dass sie ausgefallen ist, gegeben das Fahrzeug f√§llt vor 5000 Stunden aus.\nGibt es eine Korrelation zwischen allen Zeiten bis zum Ausfall aller Komponenten und des Fahrzeugs? Berechne die Korrelationskoeffizienten und visualisiere die Korrelation in einer Scatterplot-Matrix.\nWie hoch ist die Bedingte Wahrscheinlichkeit, dass die Steuereinheit den ausgefallen ist, wenn das Fahrzeug vor 4000 Stunden ausgefallen ist.\nW√§hle eine Verteilung f√ºr die Ausfallzeit des Fahrzeugs fitte diese. Vergleiche die Verteilung mit dem Histogramm und gibt die Parameter der Verteilung an.\nNutze die gefittete Verteilung, um die Wahrscheinlichkeit zu berechnen, dass das Fahrzeug vor 4000 Stunden ausf√§llt.",
    "crumbs": [
      "Statistik",
      "Tutorial: Monte Carlo Simulation of Vehicle Component Failure"
    ]
  },
  {
    "objectID": "statistics/tutorial.html#sec-monte-carlo-simulation",
    "href": "statistics/tutorial.html#sec-monte-carlo-simulation",
    "title": "Tutorial: Monte Carlo Simulation of Vehicle Component Failure",
    "section": "Simulation",
    "text": "Simulation\nZuerst erstellen wir einen leeren DataFrame, um die Struktur der simulierten Ausfallzeiten zu zeigen:\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\nn_trials = 10000\n\n# Leerer DataFrame f√ºr Ausfallzeiten\ncolumns = ['motor', 'sensor', 'control', 'vehicle_failure']\ndf = pd.DataFrame(index=range(n_trials), columns=columns)\nprint(\"Leerer DataFrame (Auszug):\")\nprint(df.head())\n\nLeerer DataFrame (Auszug):\n  motor sensor control vehicle_failure\n0   NaN    NaN     NaN             NaN\n1   NaN    NaN     NaN             NaN\n2   NaN    NaN     NaN             NaN\n3   NaN    NaN     NaN             NaN\n4   NaN    NaN     NaN             NaN\n\n\nNun simulieren wir die Ausfallzeiten und berechnen die Zeit bis zum Fahrzeugausfall:\n\nfor trial in range(n_trials):\n    # Ausfallzeiten f√ºr das Fahrzeug\n    motor_time = np.random.poisson(5000)\n    sensor_time = np.random.normal(6000, 100)\n    control_time = np.random.uniform(4000, 8000)\n\n    # Bedingung: Sensor-Ausfall verk√ºrzt Motorzeit\n    if sensor_time &lt; motor_time:\n        motor_time -= 1000\n        if motor_time &lt; sensor_time:\n            motor_time = sensor_time\n\n\n    # Ausfallzeit des Fahrzeugs (Motor oder Steuereinheit)\n    vehicle_failure = min(motor_time, control_time)\n\n    # Daten in DataFrame speichern\n    df.loc[trial, 'motor'] = motor_time\n    df.loc[trial, 'sensor'] = sensor_time\n    df.loc[trial, 'control'] = control_time\n    df.loc[trial, 'vehicle_failure'] = vehicle_failure\n\nprint(\"DataFrame mit simulierten Ausfallzeiten (Auszug):\")\nprint(df.head())\n\nplt.hist(df['vehicle_failure'], bins=50, density=True, color='skyblue', alpha=0.7)\nplt.xlabel('Ausfallzeit des Fahrzeugs (Stunden)')\nplt.ylabel('Dichte')\nplt.title('Verteilung der Ausfallzeiten (Fahrzeug)')\nplt.show()\n\nDataFrame mit simulierten Ausfallzeiten (Auszug):\n  motor       sensor      control vehicle_failure\n0  4974  6064.768854  4624.074562     4624.074562\n1  4919  6152.302986  7464.704583            4919\n2  5020  5953.052561  4727.299869     4727.299869\n3  4928  6054.256004  6099.025727            4928\n4  4986  5898.716888  5168.578594            4986\n\n\n\n\n\n\n\n\nFigure¬†1: Histogramm der Ausfallzeiten des Fahrzeugs (10.000 Durchl√§ufe).\n\n\n\n\n\nMusterl√∂sung",
    "crumbs": [
      "Statistik",
      "Tutorial: Monte Carlo Simulation of Vehicle Component Failure"
    ]
  },
  {
    "objectID": "statistics/interference_basics.html",
    "href": "statistics/interference_basics.html",
    "title": "5¬† Interferenz",
    "section": "",
    "text": "5.1 Punktsch√§tzer und Konfidenzintervalle\nInferenz (statistische Schlussfolgerung) befasst sich damit, was wir mit ausreichender Sicherheit √ºber eine Population aussagen k√∂nnen, wenn nur eine Stichprobe vorliegt. Beispiele sind: Weicht ein Parameter (z. B. Mittelwert) signifikant von einem Wert ab? Unterscheiden sich die Mittelwerte zweier Verteilungen signifikant? Der zentrale Grenzwertsatz ist hier ein Schl√ºsselwerkzeug.\nVorsicht vor P-Hacking: Daten so lange manipulieren, bis ein ‚Äûsignifikantes‚Äú Ergebnis herauskommt, ist keine Inferenz, sondern Fehlinterpretation!\nEin Punktsch√§tzer ist eine Funktion, die einen Parameter einer Verteilung (z. B. Mittelwert, Varianz) aus einer Stichprobe sch√§tzt. Da die wahre Verteilung der Population unbekannt ist, ziehen wir R√ºckschl√ºsse aus den Daten. Der zentrale Grenzwertsatz hilft, die Verteilung dieser Sch√§tzer zu verstehen.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Interferenz</span>"
    ]
  },
  {
    "objectID": "statistics/interference_basics.html#sec-statistics-pointestimates",
    "href": "statistics/interference_basics.html#sec-statistics-pointestimates",
    "title": "5¬† Interferenz",
    "section": "",
    "text": "5.1.1 Beispiel: Bernoulli-Verteilung\nBetrachten wir eine Zufallsvariable \\(X \\sim \\text{Bernoulli}(p)\\) mit unbekanntem Parameter \\(p\\). Ein naheliegender Sch√§tzer f√ºr \\(p\\) ist der Anteil der Einsen in einer Stichprobe, \\(\\hat{p}\\), berechnet als:\n\\[\\hat{p} = \\frac{1}{n} \\sum_{i=1}^n X_i,\\]\nwobei \\(n\\) die Stichprobengr√∂√üe ist und \\(X_i\\) die Beobachtungen (0 oder 1). Dies ist zudem der Mittelwert der Stichprobe, wenn wir Erfolge als 1 und Misserfolge als 0 kodieren.\n\n\n\n\n\n\nMaximum-Likelihood-Sch√§tzung (MLE)\n\n\n\n\n\nUm \\(\\hat{p}\\) systematisch zu bestimmen, nutzen wir die Maximum-Likelihood-Sch√§tzung:\n1. Likelihood-Funktion: F√ºr \\(n\\) unabh√§ngige Bernoulli-Variablen \\(X_1, \\ldots, X_n\\) mit \\(k = \\sum_{i=1}^n X_i\\) Einsen lautet die Likelihood:\n\\[L(p) = p^k (1-p)^{n-k}.\\]\n2. Log-Likelihood: Der Logarithmus vereinfacht die Maximierung:\n\\[\\log L(p) = k \\log p + (n - k) \\log (1 - p).\\]\n3. Maximierung: Ableitung nach \\(p\\) und Nullsetzen:\n\\[\\frac{d}{dp} \\log L(p) = \\frac{k}{p} - \\frac{n - k}{1 - p} = 0.\\]\nL√∂sung: \\(k(1 - p) = p(n - k)\\), also \\(\\hat{p} = \\frac{k}{n}\\).\nFazit: Der MLE-Sch√§tzer \\(\\hat{p}\\) ist der Stichprobenanteil, da er die beobachteten Daten am wahrscheinlichsten macht.\n\n\n\n\n\n\n\n\n\nZentraler Grenzwertsatz f√ºr Sch√§tzer\n\n\n\nBei gro√üer Stichprobengr√∂√üe \\(n\\) und unabh√§ngigen, identisch verteilten (i.i.d.) Ziehungen ist \\(\\hat{p}\\) ann√§hernd normalverteilt:\n\\[\\hat{p} \\sim \\mathcal{N}\\left(p, \\frac{p(1-p)}{n}\\right),\\]\nwobei:\n- \\(\\mu_{\\hat{p}} = p\\) (erwartungstreu, d.¬†h., im Mittel korrekt), ergibt sich aus der Maximum-Likelihood-Sch√§tzung,\n- \\(\\sigma_{\\hat{p}} = \\sqrt{\\frac{p(1-p)}{n}}\\) (Standardfehler, SE).\nVoraussetzung: \\(np \\geq 10\\) und \\(n(1-p) \\geq 10\\), damit die Normalapproximation gilt.\n\n\n\n\n\n\n\n\nHerleitung der Varianz\n\n\n\n\n\nDie Varianz einer Zufallsvariablen \\(X\\) ist definiert als:\n\\[ \\text{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2] \\]\nDies beschreibt, wie weit die Werte der Zufallsvariablen \\(X\\) im Durchschnitt von ihrem Erwartungswert \\(\\mathbb{E}[X]\\) abweichen.\nRechenregeln f√ºr die Varianz\n\nVarianz einer konstanten Zahl:\nF√ºr eine Konstante \\(c\\) gilt:\n\\[ \\text{Var}(c) = 0 \\]\nVarianz einer linearen Kombination von Zufallsvariablen:\nWenn \\(X\\) und \\(Y\\) zwei Zufallsvariablen sind und \\(a, b\\) Konstanten, dann gilt: \\[ \\text{Var}(aX + bY) = a^2 \\text{Var}(X) + b^2 \\text{Var}(Y) + 2ab \\, \\text{Cov}(X, Y) \\]\nWenn \\(X\\) und \\(Y\\) unabh√§ngig sind, f√§llt der Kovarianzterm weg, sodass: \\[ \\text{Var}(aX + bY) = a^2 \\text{Var}(X) + b^2 \\text{Var}(Y) \\]\n\nHerleitung der Varianz f√ºr den Fall von \\(\\hat{p}\\)\nSei \\(X_1, X_2, \\dots, X_n\\) eine Stichprobe von unabh√§ngigen Zufallsvariablen, wobei jede \\(X_i\\) der Bernoulli-Verteilung mit Erfolgswahrscheinlichkeit \\(p\\) folgt. Der Mittelwert der Stichprobe \\(\\hat{p}\\) ist:\n\\[ \\hat{p} = \\frac{1}{n} \\sum_{i=1}^{n} X_i \\]\nDa die \\(X_i\\) unabh√§ngig sind, gilt f√ºr die Varianz von \\(\\hat{p}\\) die Rechenregel f√ºr die Varianz der Summe unabh√§ngiger Zufallsvariablen:\n\\[ \\text{Var}(\\hat{p}) = \\text{Var}\\left( \\frac{1}{n} \\sum_{i=1}^{n} X_i \\right) \\]\nDa die \\(X_i\\) identisch verteilt sind, haben sie die gleiche Varianz \\(\\text{Var}(X_i) = p(1 - p)\\). Es folgt:\n\\[ \\text{Var}(\\hat{p}) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\text{Var}(X_i) = \\frac{1}{n^2} \\cdot n \\cdot p(1 - p) = \\frac{p(1 - p)}{n} \\]\nDamit haben wir die Formel f√ºr die Varianz von \\(\\hat{p}\\):\n\\[ \\text{Var}(\\hat{p}) = \\frac{p(1 - p)}{n} \\]\nDiese Varianz beschreibt, wie stark der gesch√§tzte Anteil \\(\\hat{p}\\) von der tats√§chlichen Erfolgswahrscheinlichkeit \\(p\\) abweichen kann.\n\n\n\n\n\n5.1.2 Simulation: Sch√§tzer-Verteilung\nWir illustrieren dies mit einer Grundgesamtheit \\(X \\sim \\text{Bernoulli}(0.3)\\), ziehen \\(N = 1000\\) Werte, nehmen Stichproben der Gr√∂√üe \\(n = 100\\) und wiederholen dies \\(m = 100\\) Mal. Der Sch√§tzer \\(\\hat{p}\\) wird als Anteil der Einsen berechnet. Figure¬†5.1 zeigt die Verteilung von \\(\\hat{p}\\).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\nnp.random.seed(42)\np = 0.3  # Wahrer Parameter\nN = 1000  # Grundgesamtheit\nn = 100   # Stichprobengr√∂√üe\nm = 100   # Anzahl der Wiederholungen\n\nresults = np.zeros(m)\npopulation = np.random.binomial(1, p, N)  # Grundgesamtheit einmal ziehen\n\nfor i in range(m):\n    sample = np.random.choice(population, n, replace=False)  # Stichprobe ohne Zur√ºcklegen\n    results[i] = np.mean(sample)  # Sch√§tzer \\hat{p}\n\n# Plot\nplt.hist(results, bins=15, density=True, color='skyblue', alpha=0.7, label='$\\hat{p}$')\nmu, sigma = norm.fit(results)\nx = np.linspace(min(results), max(results), 100)\ny = norm.pdf(x, mu, sigma)\nplt.plot(x, y, 'r-', label=f'Normal ($\\mu={mu:.3f}$, $\\sigma={sigma:.3f}$)')\nplt.xlabel('Sch√§tzer $\\hat{p}$')\nplt.ylabel('Dichte')\nplt.title(f'Verteilung von $\\hat{{p}}$ (p = {p}, n = {n})')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\nprint(f'Mittelwert von $\\hat{{p}}$: {np.mean(results):.3f}')\nprint(f'Standardfehler von $\\hat{{p}}$: {np.std(results):.3f}')\nprint(f'Theoretischer SE: {np.sqrt(p * (1 - p) / n):.3f}')\n\n&lt;&gt;:19: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n&lt;&gt;:23: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:23: SyntaxWarning:\n\ninvalid escape sequence '\\s'\n\n&lt;&gt;:24: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n&lt;&gt;:26: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n&lt;&gt;:31: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n&lt;&gt;:32: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n&lt;&gt;:19: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n&lt;&gt;:23: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:23: SyntaxWarning:\n\ninvalid escape sequence '\\s'\n\n&lt;&gt;:24: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n&lt;&gt;:26: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n&lt;&gt;:31: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n&lt;&gt;:32: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n/tmp/ipykernel_3242/4128398071.py:19: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n/tmp/ipykernel_3242/4128398071.py:23: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n/tmp/ipykernel_3242/4128398071.py:23: SyntaxWarning:\n\ninvalid escape sequence '\\s'\n\n/tmp/ipykernel_3242/4128398071.py:24: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n/tmp/ipykernel_3242/4128398071.py:26: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n/tmp/ipykernel_3242/4128398071.py:31: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n/tmp/ipykernel_3242/4128398071.py:32: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n\n\n\n\n\n\n\n\nFigure¬†5.1: Verteilung des Sch√§tzers \\(\\hat{p}\\) f√ºr eine Bernoulli-Verteilung (p = 0.3, n = 100, m = 100).\n\n\n\n\n\nMittelwert von $\\hat{p}$: 0.285\nStandardfehler von $\\hat{p}$: 0.040\nTheoretischer SE: 0.046\n\n\nDie simulierten Werte f√ºr \\(\\hat{p}\\) stimmen gut mit den theoretischen Erwartungen √ºberein:\n- \\(\\mu_{\\hat{p}} = p = 0.3\\),\n- \\(\\text{SE}_{\\hat{p}} = \\sigma_{\\hat{p}} = \\sqrt{\\frac{p(1-p)}{n}} = \\sqrt{\\frac{0.3 \\cdot 0.7}{100}} \\approx 0.046.\\)\nDies best√§tigt die Normalapproximation durch den zentralen Grenzwertsatz.\n\n\n5.1.3 Konfidenzintervalle\nEin Konfidenzintervall (KI) gibt ein Intervall an, das den wahren Wert eines Parameters mit einer bestimmten Wahrscheinlichkeit (dem Konfidenzniveau \\(1-\\alpha\\)) enth√§lt. Bei einer Normalverteilung liegen etwa 95 % der Werte innerhalb von zwei Standardabweichungen. Dies haben wir z.B. bei der Betrachung von IQ-Werten gesehen, dort liegen 95 % der Werte innerhalb von zwei Standardabweichungen (\\(2 \\sigma = 2 \\cdot 15\\)) um den Mittelwert (\\(\\mu = 100\\)).\n\n\n\n\n\n\nDefinition eines Konfidenzintervalls\n\n\n\nF√ºr einen Sch√§tzer \\(\\hat{\\theta}\\) eines Parameters \\(\\theta\\) ist das Konfidenzintervall \\(\\text{CI}_{1-\\alpha}\\):\n\\[\\left(\\hat{\\theta} - z_{\\alpha/2} \\cdot \\frac{\\sigma}{\\sqrt{n}}, \\hat{\\theta} + z_{\\alpha/2} \\cdot \\frac{\\sigma}{\\sqrt{n}}\\right),\\]\nwobei:\n- \\(\\hat{\\theta}\\) der Punktsch√§tzer (z. B. Stichprobenmittelwert),\n- \\(\\sigma\\) die Standardabweichung der Grundgesamtheit,\n- \\(n\\) die Stichprobengr√∂√üe,\n- \\(z_{\\alpha/2}\\) das \\(\\alpha/2\\)-Quantil der Standardnormalverteilung.\nFalls \\(\\sigma\\) unbekannt ist, wird die Stichprobenstandardabweichung \\(s\\) verwendet:\n\\[\\text{SE} = \\frac{s}{\\sqrt{n}}.\\]\n\n\n\n\n\n\n\n\nHerleitung des Intervalls\n\n\n\n\nStandardisierung: F√ºr eine normalverteilte Zufallsvariable \\(\\hat{\\theta} \\sim \\mathcal{N}(\\theta, \\sigma^2/n)\\) wird \\(\\frac{\\hat{\\theta} - \\theta}{\\sigma/\\sqrt{n}} \\sim \\mathcal{N}(0, 1)\\).\n\nKonfidenzniveau: F√ºr \\(1-\\alpha = 0.95\\) liegt 95 % der Fl√§che zwischen \\(-z_{\\alpha/2}\\) und \\(z_{\\alpha/2}\\). Bei \\(\\alpha = 0.05\\) ist \\(z_{\\alpha/2} = 1.96\\) (da \\(P(Z \\leq 1.96) \\approx 0.975\\)).\n\nIntervall: Umstellen ergibt: \\(P\\left(\\hat{\\theta} - z_{\\alpha/2} \\cdot \\frac{\\sigma}{\\sqrt{n}} \\leq \\theta \\leq \\hat{\\theta} + z_{\\alpha/2} \\cdot \\frac{\\sigma}{\\sqrt{n}}\\right) = 1-\\alpha\\).\n\n\n\n\n\n5.1.4 Beispiel: Konfidenzintervall f√ºr \\(p\\)\nF√ºr unsere Bernoulli-Verteilung ist \\(\\hat{p}\\) normalverteilt mit \\(\\mu_{\\hat{p}} = p\\) und Standardfehler \\(\\text{SE}_{\\hat{p}} = \\sqrt{\\frac{p(1-p)}{n}}.\\) Da \\(p\\) unbekannt ist, nutzen wir \\(\\hat{p}\\) zur Sch√§tzung:\n\\[\\text{CI}_{0.95} = \\left(\\hat{p} - z_{\\alpha/2} \\cdot \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}, \\hat{p} + z_{\\alpha/2} \\cdot \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\right).\\]\nF√ºr ein 95 %-Konfidenzniveau (\\(\\alpha = 0.05\\)) ist \\(z_{\\alpha/2} = 1.96\\), was wir aus Tabellen oder mit norm.ppf berechnen k√∂nnen.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\nnp.random.seed(42)\np = 0.3\nn = 70\nsample = np.random.binomial(1, p, n)  # Eine Stichprobe\np_hat = np.mean(sample)  # Sch√§tzer\n\n# Konfidenzintervall\nalpha = 0.05\nz = norm.ppf(1 - alpha/2)  # z-Wert f√ºr 95%\nse = np.sqrt(p_hat * (1 - p_hat) / n)  # Standardfehler mit \\hat{p}\nci_lower = p_hat - z * se\nci_upper = p_hat + z * se\n\nprint(f'z_{{alpha/2}}: {z:.2f}')\nprint(f'\\hat{{p}}: {p_hat:.3f}')\nprint(f'95%-Konfidenzintervall: [{ci_lower:.3f}, {ci_upper:.3f}]')\n\n# Visualisierung\nx = np.linspace(0, 0.6, 1000)\ny = norm.pdf(x, p_hat, se)\nplt.plot(x, y, 'r-', label=f'Normal ($\\mu={p_hat:.3f}$, SE={se:.3f})')\nplt.axvline(p_hat, color='blue', linestyle='-', label='$\\hat{p}$')\nplt.axvline(ci_lower, color='green', linestyle='--', label='Untere Grenze')\nplt.axvline(ci_upper, color='green', linestyle='--', label='Obere Grenze')\nplt.fill_between(x, y, where=(x &gt;= ci_lower) & (x &lt;= ci_upper), color='skyblue', alpha=0.3, label='95%-KI')\nplt.xlabel('Wert von $p$')\nplt.ylabel('Dichte')\nplt.title('Konfidenzintervall f√ºr $\\hat{p}$')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\nz_{alpha/2}: 1.96\n\\hat{p}: 0.271\n95%-Konfidenzintervall: [0.167, 0.376]\n\n\n&lt;&gt;:19: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n&lt;&gt;:25: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:26: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n&lt;&gt;:32: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n&lt;&gt;:19: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n&lt;&gt;:25: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:26: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n&lt;&gt;:32: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n/tmp/ipykernel_3242/1564904913.py:19: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n/tmp/ipykernel_3242/1564904913.py:25: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n/tmp/ipykernel_3242/1564904913.py:26: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n/tmp/ipykernel_3242/1564904913.py:32: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n\n\n\n\n\n\n\n\nFigure¬†5.2: 95%-Konfidenzintervall f√ºr \\(\\hat{p}\\) aus einer Stichprobe (p = 0.3, n = 100).\n\n\n\n\n\nF√ºr unser Beispiel mit \\(\\hat{p} = 0.3\\), \\(n = 70\\) und einem 95 %-Konfidenzniveau (\\(\\alpha = 0.05\\), \\(z_{\\alpha/2} = 1.96\\)) ergibt sich das Konfidenzintervall:\n\\[\\left(0.271 - 1.96 \\cdot \\sqrt{\\frac{0.271 \\cdot 0.729}{70}}, 0.271 + 1.96 \\cdot \\sqrt{\\frac{0.271 \\cdot 0.729}{70}}\\right) \\approx (0.167, 0.376).\\]\nDas bedeutet, wir sind zu 95 % sicher, dass der wahre Wert von \\(p\\) zwischen 0.167 und 0.376 liegt.\n\n\n\n\n\n\nFragen zur Anpassung\n\n\n\n\nWelchen Faktor k√∂nnen wir √§ndern, um das Konfidenzintervall zu verkleinern?\n\nWie sieht das Konfidenzintervall f√ºr ein 99 %-Niveau aus?\n\n\n\n\n\n\n\n\n\nL√∂sung\n\n\n\n\n\n\nVerkleinerung des Intervalls: Die Breite des Konfidenzintervalls h√§ngt von \\(\\frac{\\sigma}{\\sqrt{n}}\\) ab. Eine gr√∂√üere Stichprobengr√∂√üe \\(n\\) reduziert den Standardfehler \\(\\sqrt{\\frac{p(1-p)}{n}}\\), wodurch das Intervall schmaler wird.\n\n99 %-Konfidenzniveau: Bei \\(\\alpha = 0.01\\) ist \\(z_{\\alpha/2} = 2.58\\) (da \\(P(Z \\leq 2.58) \\approx 0.995\\)). Das Intervall wird:\n\\[\\left(0.271 - 2.58 \\cdot \\sqrt{\\frac{0.271 \\cdot 0.729}{70}}, 0.271 + 2.58 \\cdot \\sqrt{\\frac{0.271 \\cdot 0.729}{70}}\\right) \\approx (0.134, 0.408).\\]\nEin h√∂heres Konfidenzniveau verbreitert das Intervall, da mehr Sicherheit gefordert wird.\n\n\n\n\n\n\n5.1.5 Darstellung des Konfidenzintervalls\nFigure¬†5.3 zeigt die Konfidenzintervalle f√ºr \\(p\\) bei 95 % und 99 % Konfidenzniveau, basierend auf \\(\\hat{p} = 0.3\\) und \\(n = 100\\). Dies verdeutlicht, wie sich das Intervall mit dem Konfidenzniveau √§ndert ‚Äì ein n√ºtzliches Konzept, um Unsicherheiten in Simulationen wie im Tutorial zu quantifizieren.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Parameter\nmu = 0.3  # \\hat{p}\nsigma = np.sqrt(mu * (1 - mu) / 100)  # SE\nalphas = [0.05, 0.01]  # 95% und 99% Konfidenzniveau\ncolors = ['skyblue', 'lightgreen']\n\n# Normalverteilung\nx = np.linspace(0.1, 0.5, 1000)\ny = norm.pdf(x, mu, sigma)\nplt.plot(x, y, 'r-', label=f'Normal ($\\mu={mu}$, SE={sigma:.3f})')\n\n# Konfidenzintervalle\nfor i, alpha in enumerate(alphas):\n    z = norm.ppf(1 - alpha/2)\n    lower = mu - z * sigma\n    upper = mu + z * sigma\n    print(f'Konfidenzintervall f√ºr {int((1-alpha)*100)}%: ({lower:.2f}, {upper:.2f})')\n    plt.fill_between(x, y, where=(x &gt;= lower) & (x &lt;= upper), color=colors[i], alpha=0.3, label=f'{int((1-alpha)*100)}%-KI')\n    plt.axvline(lower, color=colors[i], linestyle='--', alpha=0.5)\n    plt.axvline(upper, color=colors[i], linestyle='--', alpha=0.5)\n\nplt.axvline(mu, color='blue', linestyle='-', label='$\\hat{p}$')\nplt.xlabel('Wert von $p$')\nplt.ylabel('Dichte')\nplt.title('Konfidenzintervalle f√ºr $\\hat{p}$')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\nKonfidenzintervall f√ºr 95%: (0.21, 0.39)\nKonfidenzintervall f√ºr 99%: (0.18, 0.42)\n\n\n&lt;&gt;:14: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:26: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n&lt;&gt;:29: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n&lt;&gt;:14: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:26: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n&lt;&gt;:29: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n/tmp/ipykernel_3242/1622860018.py:14: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n/tmp/ipykernel_3242/1622860018.py:26: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n/tmp/ipykernel_3242/1622860018.py:29: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n\n\n\n\n\n\n\n\nFigure¬†5.3: Konfidenzintervalle f√ºr \\(\\hat{p}\\) bei 95% und 99% (p = 0.3, n = 100).\n\n\n\n\n\n\n\n5.1.6 Punktsch√§tzer und Konfidenzintervalle f√ºr andere F√§lle\nDie Prinzipien f√ºr Sch√§tzer und Konfidenzintervalle lassen sich auf andere Verteilungen und Parameter (z. B. Mittelwert, Varianz) √ºbertragen. Entscheidend ist, die Verteilung des Sch√§tzers zu kennen und entsprechende Formeln anzuwenden.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Interferenz</span>"
    ]
  },
  {
    "objectID": "statistics/interference_basics.html#sec-statistics-hypothesistests",
    "href": "statistics/interference_basics.html#sec-statistics-hypothesistests",
    "title": "5¬† Interferenz",
    "section": "5.2 Hypothesentests",
    "text": "5.2 Hypothesentests\nEin Hypothesentest ist ein Verfahren, um auf Basis einer Stichprobe zu entscheiden, ob eine Hypothese √ºber eine Population verworfen wird. Wir stellen eine Nullhypothese \\(H_0\\) auf (z. B. ‚Äûein Parameter hat einen bestimmten Wert‚Äú), die wir widerlegen wollen, und eine Alternativhypothese \\(H_1\\), die wir unterst√ºtzen m√∂chten.\n\n\n\n\n\n\nDialektik und Falsifikationismus\n\n\n\nDieser Ansatz mag ungewohnt erscheinen: Warum etwas aufstellen, um es zu widerlegen? Er wurzelt in der wissenschaftlichen Methode:\n- Dialektik: Eine These wird durch eine Antithese gepr√ºft, die Synthese f√ºhrt n√§her zur Wahrheit (Wikipedia: Dialektik).\n- Falsifikationismus: Karl Popper betonte, dass Thesen nicht beweisbar, sondern nur widerlegbar sind. Eine nicht widerlegte Hypothese bleibt vorl√§ufig g√ºltig, ist aber nicht endg√ºltig bewiesen (Wikipedia: Falsifikationismus).\nIn der Statistik nutzen wir diesen Ansatz, um Hypothesen systematisch zu pr√ºfen.\n\n\n\n5.2.1 Ablauf eines Hypothesentests\n\nHypothesen formulieren: \\(H_0\\) (zu widerlegen) und \\(H_1\\) (zu unterst√ºtzen).\n\nSignifikanzniveau w√§hlen: \\(\\alpha\\) ist die Wahrscheinlichkeit, \\(H_0\\) f√§lschlicherweise abzulehnen (z. B. 0.05).\n\nTeststatistik berechnen: Bestimme \\(t\\) und dessen Verteilung unter \\(H_0\\).\n\nEntscheidung treffen: Vergleiche \\(t\\) mit einem kritischen Wert oder berechne einen p-Wert.\n\n\n\n5.2.2 Entscheidungsfehler\nBetrachten wir ein Beispiel: Ein medizinischer Test soll Krebs erkennen. Die Realit√§t (Krebs oder kein Krebs) und das Testergebnis (positiv oder negativ) k√∂nnen abweichen. Das Diagramm zeigt die Wahrscheinlichkeiten:\n\n\n\n\n\ngraph LR\n    U[Person] --&gt;|0.001| A[Krebs]\n    U[Person] --&gt;|0.999| D[Kein Krebs]\n    A[Krebs] --&gt;|0.99| B[Positiv]\n    A[Krebs] --&gt;|0.01| C[Negativ]\n    D[Kein Krebs] --&gt;|0.05| E[Positiv]\n    D[Kein Krebs] --&gt;|0.95| F[Negativ]\n\n\n\n\n\n\nZiel: Personen mit Krebs identifizieren. Der schlimmste Fall w√§re, Krebs zu √ºbersehen. Wir m√∂chten daher die Wahrscheinlichkeit maximieren, Krebs korrekt zu erkennen.\nDaraus folgt:\n- Nullhypothese \\(H_0\\): ‚ÄûDie Person hat keinen Krebs‚Äú (zu widerlegen),\n- Alternativhypothese \\(H_1\\): ‚ÄûDie Person hat Krebs‚Äú (zu unterst√ºtzen).\nBei einem Hypothesentest k√∂nnen zwei Fehler auftreten:\n- Typ-I-Fehler (\\(\\alpha\\)): \\(H_0\\) wird abgelehnt, obwohl sie wahr ist (falsch positiv, z. B. ‚ÄûKrebs‚Äú trotz ‚Äûkein Krebs‚Äú).\n- Typ-II-Fehler (\\(\\beta\\)): \\(H_0\\) wird beibehalten, obwohl sie falsch ist (falsch negativ, z. B. ‚Äûkein Krebs‚Äú trotz ‚ÄûKrebs‚Äú).\n\nTabelle der Fehlertypen\n\n\n\n\n\n\n\n\nEntscheidung √ºber die Nullhypothese\nNullhypothese (H0) ist\n\n\nWahr\nFalsch\n\n\nEntscheidung √ºber die\nNullhypothese (H0)\nNicht verwerfen\nKorrekte Entscheidung\n(wahr negativ, wirklich gesund)\n(Wahrscheinlichkeit = 1-Œ±)\nTyp-II-Fehler\n(falsch negativ, hat eigentlich Krebs)\n(Wahrscheinlichkeit = Œ≤)\n\n\nVerwerfen\nTyp-I-Fehler\n(falsch positiv, ist eigentlich gesund)\n(Wahrscheinlichkeit = Œ±)\nKorrekte Entscheidung\n(wahr positiv, hat wirklich Krebs)\n(Wahrscheinlichkeit = 1-Œ≤)\n\n\n\n\n\n\n\n\n\nSignifikanzniveau und Fehlerpriorit√§ten\n\n\n\nIm Beispiel ist der Typ-II-Fehler (\\(\\beta\\)) besonders kritisch, da Krebs √ºbersehen schwerwiegender ist als eine falsche Diagnose. Dennoch steuern wir die Ablehnung von \\(H_0\\) mit dem Signifikanzniveau \\(\\alpha\\), der Wahrscheinlichkeit eines Typ-I-Fehlers (falsch positiv). \\(\\alpha\\) wird vorab festgelegt, typischerweise 5 % (0.05) oder 1 % (0.01). Ein kleineres \\(\\alpha\\) reduziert das Risiko, \\(H_0\\) f√§lschlich zu verwerfen (z. B. unn√∂tige Behandlungen), erh√∂ht aber die Chance eines Typ-II-Fehlers.\n\n\n\n\n5.2.3 Beispiel: Hypothesentest f√ºr den Mittelwert einer Normalverteilung\nWir pr√ºfen, ob Mechatronik-Studierende intelligenter sind als der Bev√∂lkerungsdurchschnitt (IQ = 100). Angenommen, ihr IQ ist normalverteilt mit \\(\\mu = 110\\) und \\(\\sigma = 15\\), also \\(X \\sim \\mathcal{N}(110, 15^2)\\). Mit einer Stichprobe von \\(n = 100\\) Studierenden testen wir, ob ihr Mittelwert signifikant von 100 abweicht ‚Äì ein Ansatz, der im Tutorial z. B. f√ºr Komponenten-Lebensdauern n√ºtzlich ist.\n\n5.2.3.1 Hypothesen und Teststatistik\n\n\\(H_0\\): \\(\\mu = 100\\) (kein Unterschied zum Durchschnitt),\n\n\\(H_1\\): \\(\\mu \\neq 100\\) (zweiseitiger Test, da ‚Äûabweicht‚Äú keine Richtung vorgibt).\nDer Sch√§tzer ist \\(\\hat{\\mu} = \\bar{X}\\), und unter \\(H_0\\) gilt:\n\\[\\bar{X} \\sim \\mathcal{N}\\left(100, \\frac{\\sigma^2}{n}\\right) = \\mathcal{N}\\left(100, \\frac{15^2}{100}\\right).\\]\nDie Teststatistik lautet:\n\\[z = \\frac{\\bar{X} - \\mu_0}{\\sigma / \\sqrt{n}},\\]\nmit \\(\\mu_0 = 100\\), \\(\\sigma = 15\\), \\(n = 100\\). Bei \\(\\alpha = 0.05\\) ist der kritische Wert \\(z_{\\alpha/2} = 1.96\\).\n\n\n\n5.2.3.2 Simulation und Visualisierung\nFigure¬†5.4 zeigt die Stichprobenverteilung und die Nullhypothese.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\nnp.random.seed(42)\nmu_true = 110  # Wahrer Mittelwert (Simulation)\nsigma = 15     # Bekannte Standardabweichung\nn = 100        # Stichprobengr√∂√üe\nmu_0 = 100     # Nullhypothese\n\n# Stichprobe\nX = np.random.normal(mu_true, sigma, n)\nX_bar = np.mean(X)  # Sch√§tzer f√ºr Mittelwert\n\n# Teststatistik\nse = sigma / np.sqrt(n)  # Standardfehler\nz_stat = (X_bar - mu_0) / se\np_value = 2 * (1 - norm.cdf(abs(z_stat)))  # Zweiseitiger p-Wert\n\n# Plot\nx = np.linspace(90, 130, 1000)\ny_null = norm.pdf(x, mu_0, se)  # Verteilung unter H_0\ny_sample = norm.pdf(x, X_bar, se)  # Verteilung der Stichprobe\nplt.plot(x, y_null, 'r--', label=f'$H_0$ ($\\mu={mu_0}$)')\nplt.plot(x, y_sample, 'b-', label=f'Stichprobe ($\\hat{{\\mu}}={X_bar:.1f}$)')\nplt.axvline(mu_0, color='red', linestyle='--')\nplt.axvline(X_bar, color='blue', linestyle='-')\nplt.fill_between(x, y_null, where=(x &lt;= mu_0 - 1.96*se) | (x &gt;= mu_0 + 1.96*se), color='red', alpha=0.2, label=r'Ablehnungsbereich ($\\alpha=0.05$)')\nplt.xlabel('Mittelwert (IQ)')\nplt.ylabel('Dichte')\nplt.title('Hypothesentest: IQ von Mechatronik-Studierenden')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\nprint(f'Stichprobenmittelwert: {X_bar:.2f}')\nprint(f'Teststatistik z: {z_stat:.2f}')\nprint(f'p-Wert: {p_value:.4f}')\n\n&lt;&gt;:24: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:25: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n&lt;&gt;:25: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:24: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:25: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n&lt;&gt;:25: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n/tmp/ipykernel_3242/1135806841.py:24: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n/tmp/ipykernel_3242/1135806841.py:25: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n/tmp/ipykernel_3242/1135806841.py:25: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n\n\n\n\n\n\n\n\nFigure¬†5.4: Hypothesentest f√ºr den Mittelwert von Mechatronik-Studierenden (n = 100).\n\n\n\n\n\nStichprobenmittelwert: 108.44\nTeststatistik z: 5.63\np-Wert: 0.0000\n\n\n\n\n5.2.3.3 Interpretation\nAn ?fig-sec-statistics-hypothesistests-2 sehen wir, dass der Stichprobenmittelwert \\(\\hat{\\mu}\\) deutlich √ºber \\(\\mu_0 = 100\\) liegt. Der gr√ºne Bereich (95 %-Konfidenzintervall) schlie√üt 100 nicht ein, und der p-Wert ist &lt; 0.05. Das deutet darauf hin, dass die Wahrscheinlichkeit f√ºr \\(\\mu = 100\\) sehr gering ist. Wir lehnen \\(H_0\\) ab und schlie√üen, dass der Mittelwert der Mechatronik-Studierenden signifikant von 100 abweicht.\n\n\n\n5.2.4 \\(t\\)-Verteilung\nBei gro√üen Stichproben (\\(n \\geq 30\\)) approximiert der zentrale Grenzwertsatz den Stichprobenmittelwert \\(\\bar{X}\\) durch eine Normalverteilung, besonders wenn \\(\\sigma\\) bekannt ist. F√ºr kleine Stichproben (\\(n &lt; 30\\)) oder unbekannte Varianz ist die \\(t\\)-Verteilung (Wikipedia: Studentsche t-Verteilung) besser geeignet, da sie die Unsicherheit der gesch√§tzten Standardabweichung einbezieht.\nDie Teststatistik der \\(t\\)-Verteilung ist:\n\\[t = \\frac{\\bar{X} - \\mu_0}{\\frac{S}{\\sqrt{n}}},\\]\nwobei:\n- \\(\\bar{X}\\): Stichprobenmittelwert,\n- \\(\\mu_0\\): Wert unter \\(H_0\\),\n- \\(S\\): Stichprobenstandardabweichung (\\(S = \\sqrt{\\frac{1}{n-1} \\sum (X_i - \\bar{X})^2}\\)),\n- \\(n\\): Stichprobengr√∂√üe.\nDie Form der \\(t\\)-Verteilung h√§ngt von den Freiheitsgraden \\(df = n - 1\\) ab: Bei kleinem \\(n\\) ist sie breiter (mehr Streuung), bei gro√üem \\(n\\) n√§hert sie sich der Normalverteilung. Im Tutorial k√∂nnte die \\(t\\)-Verteilung bei kleinen Stichproben von Ausfallzeiten hilfreich sein, wenn \\(\\sigma\\) unbekannt ist.\n\n\n\n\n\n\nHerkunft der \\(t\\)-Verteilung\n\n\n\nDie \\(t\\)-Verteilung stammt von William Sealy Gosset, einem Statistiker bei der Guinness-Brauerei. Er entwickelte sie, um die Bierqualit√§t mit kleinen Stichproben zu pr√ºfen, und ver√∂ffentlichte 1908 unter dem Pseudonym ‚ÄûStudent‚Äú (Original: Student‚Äôs t). Daher hei√üt sie ‚ÄûStudent‚Äôs \\(t\\)-Verteilung‚Äú.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Interferenz</span>"
    ]
  },
  {
    "objectID": "statistics/interference_advanced.html",
    "href": "statistics/interference_advanced.html",
    "title": "6¬† Tests",
    "section": "",
    "text": "6.1 T-Test\nNach dem zuvor beschriebenen Prinzip der Hypothesentests gibt es verschiedene Tests, die auf unterschiedliche Fragestellungen zugeschnitten sind. In diesem Abschnitt werden einige dieser Tests vorgestellt.\nWir werden uns hier auf einige typische Tests konzentrieren. Es gibt noch viele weitere Tests, die auf spezielle Fragestellungen zugeschnitten sind. Die hier vorgestellten Tests geh√∂ren jedoch zu den wichtigsten und werden in der Praxis h√§ufig verwendet.\nDer t-Test ist eine Klasse von statistischen Tests, die verwendet werden, um Hypothesen √ºber den Mittelwert (oder die Differenz von Mittelwerten) einer oder zweier Stichproben zu pr√ºfen, insbesondere wenn die Standardabweichung der Grundgesamtheit unbekannt ist und aus der Stichprobe gesch√§tzt werden muss.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Tests</span>"
    ]
  },
  {
    "objectID": "statistics/interference_advanced.html#t-test",
    "href": "statistics/interference_advanced.html#t-test",
    "title": "6¬† Tests",
    "section": "",
    "text": "6.1.1 One-Sample Student‚Äôs T-Test (Einstichproben-t-Test)\nBeim Einstichproben-t-Test wird der Mittelwert einer Stichprobe mit einem vorgegebenen, hypothetischen Wert (\\(\\mu_0\\)) verglichen. Der Test wird typischerweise verwendet, wenn die Varianz der Grundgesamtheit unbekannt ist und aus der Stichprobe gesch√§tzt wird. Die t-Verteilung ber√ºcksichtigt die zus√§tzliche Unsicherheit, die durch diese Sch√§tzung entsteht.\nWir legen zun√§chst den Typ-I-Fehler (Signifikanzniveau \\(\\alpha\\)) fest. Damit definieren wir die maximale Wahrscheinlichkeit, die wir zu akzeptieren bereit sind, die Nullhypothese (\\(H_0\\)) f√§lschlicherweise abzulehnen, obwohl sie wahr ist (\\(P(\\text{Ablehnung } H_0 | H_0 \\text{ ist wahr}) \\leq \\alpha\\)).\nUm zu beurteilen, wie wahrscheinlich die beobachtete Stichprobe unter der Annahme ist, dass die Nullhypothese wahr ist, wird die Teststatistik \\(t\\) berechnet. Diese misst den Unterschied zwischen dem Stichprobenmittelwert \\(\\bar{x}\\) und dem hypothetischen Wert \\(\\mu_0\\) in Einheiten des Standardfehlers des Mittelwerts. W√∂rtlich bedeutet die Teststatistik: Um wie viele Standardfehler weicht der Stichprobenmittelwert \\(\\bar{x}\\) vom vorgegebenen Wert \\(\\mu_0\\) ab?\n\\[\nt = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}},\n\\]\nwobei \\(\\bar{x}\\) der Stichprobenmittelwert, \\(s\\) die Stichprobenstandardabweichung (oft wird die korrigierte Stichprobenstandardabweichung verwendet) und \\(n\\) die Stichprobengr√∂√üe ist.\nUnter der Annahme, dass die Nullhypothese wahr ist und die Daten aus einer normalverteilten Grundgesamtheit stammen, folgt die Teststatistik \\(t\\) einer t-Verteilung mit \\(n-1\\) Freiheitsgraden.\nDie Entscheidung √ºber die Ablehnung der Nullhypothese h√§ngt von der Alternativhypothese (\\(H_1\\)) ab:\n\nBei \\(H_1: \\mu &gt; \\mu_0\\) (rechtseitiger Test) lehnen wir \\(H_0\\) ab, wenn \\(t &gt; t_{1-\\alpha, n-1}\\).\nBei \\(H_1: \\mu &lt; \\mu_0\\) (linksseitiger Test) lehnen wir \\(H_0\\) ab, wenn \\(t &lt; t_{\\alpha, n-1}\\) (was \\(t &lt; -t_{1-\\alpha, n-1}\\) entspricht).\nBei \\(H_1: \\mu \\neq \\mu_0\\) (zweiseitiger Test) lehnen wir \\(H_0\\) ab, wenn \\(|t| &gt; t_{1-\\alpha/2, n-1}\\).\n\nDer kritische Wert \\(t_{\\text{krit}}\\) (z.B. \\(t_{1-\\alpha, n-1}\\) oder \\(t_{1-\\alpha/2, n-1}\\)) wird aus der Tabelle der t-Verteilung oder mittels Software bestimmt.\n\n6.1.1.1 Beispiel - Einseitig: Eiwei√ügehalt von Braugerste\nEine Brauerei bezieht eine neue Charge Gerste. F√ºr den Brauprozess ist es wichtig, dass der mittlere Eiwei√ügehalt der Gerste einen bestimmten Grenzwert nicht √ºberschreitet, da ein zu hoher Eiwei√ügehalt zu unerw√ºnscht starkem Sch√§umen des Bieres f√ºhren kann. Der maximal akzeptable mittlere Eiwei√ügehalt liegt laut Spezifikation bei \\(\\mu_0 = 11.5\\%\\).\nDie Brauerei m√∂chte pr√ºfen, ob die neue Charge Gerste diese Spezifikation einh√§lt oder ob der mittlere Eiwei√ügehalt signifikant h√∂her ist. Dazu wird eine Stichprobe von \\(n=50\\) K√∂rnern aus der Charge entnommen und deren Eiwei√ügehalt analysiert.\n\nimport numpy as np\nfrom scipy.stats import t as t_dist, norm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Annahmen f√ºr die Simulation (w√§ren in der Realit√§t unbekannt)\n# Nehmen wir an, die Charge ist tats√§chlich leicht √ºber dem Limit\ntrue_mu_barley = 11.8  # Tats√§chlicher mittlerer Eiwei√ügehalt der Charge (%)\ntrue_sigma_barley = 0.5 # Tats√§chliche Standardabweichung des Eiwei√ügehalts (%)\n\n# Nullhypothese (Grenzwert)\nmu_0_barley = 11.5\n\n# Stichprobe ziehen\nn_barley = 50\n# Setze einen Seed f√ºr Reproduzierbarkeit\nnp.random.seed(2024)\nX_barley = np.random.normal(true_mu_barley, true_sigma_barley, n_barley)\n\n# Sch√§tzer aus der Stichprobe berechnen\nx_bar_barley = np.mean(X_barley)\ns_barley = np.std(X_barley, ddof=1) # Korrigierte Stichprobenstandardabweichung\n\nprint(f\"Stichprobenmittelwert (xÃÑ): {x_bar_barley:.2f}%\")\nprint(f\"Stichprobenstandardabweichung (s): {s_barley:.2f}%\")\nprint(f\"Stichprobengr√∂√üe (n): {n_barley}\")\n\n# Plot der Stichprobendaten und der Nullhypothese\nplt.figure(figsize=(10, 6))\nsns.histplot(X_barley, bins=8, kde=False, alpha=0.6, label='Stichprobe Eiwei√ügehalt', color='darkorange', edgecolor='black')\nplt.axvline(mu_0_barley, color='red', linestyle='--', linewidth=2, label=f'Grenzwert H‚ÇÄ: Œº ‚â§ {mu_0_barley}%')\nplt.axvline(x_bar_barley, color='blue', linestyle='-', linewidth=2, label=f'Stichprobenmittelwert xÃÑ = {x_bar_barley:.2f}%')\nplt.title('Verteilung des Eiwei√ügehalts der Gerstenprobe und Grenzwert')\nplt.xlabel('Eiwei√ügehalt (%)')\nplt.ylabel('H√§ufigkeit')\nplt.legend()\nplt.grid(axis='y', alpha=0.5)\nplt.show()\n\nStichprobenmittelwert (xÃÑ): 11.80%\nStichprobenstandardabweichung (s): 0.48%\nStichprobengr√∂√üe (n): 50\n\n\n\n\n\n\n\n\nFigure¬†6.1: Verteilung des Eiwei√ügehalts in der Stichprobe und der maximal zul√§ssige Mittelwert (Nullhypothese).\n\n\n\n\n\nDurchf√ºhrung des Hypothesentests:\n\nDefinition der Hypothesen:\n\n\\(H_0: \\mu \\leq 11.5\\%\\) (Der mittlere Eiwei√ügehalt der Charge liegt bei oder unter dem Grenzwert). Wir verwenden f√ºr die Berechnung den Grenzfall \\(H_0: \\mu = 11.5\\%\\).\n\\(H_1: \\mu &gt; 11.5\\%\\) (Der mittlere Eiwei√ügehalt der Charge ist h√∂her als der Grenzwert). Wir w√§hlen einen einseitigen Test (rechtsseitig), da uns nur eine √úberschreitung des Grenzwerts Sorgen bereitet.\n\nDefinition des Signifikanzniveaus:\n\nDie Brauerei legt \\(\\alpha = 0.05\\) fest.\nDas Risiko, eine Charge f√§lschlicherweise als zu eiwei√üreich abzulehnen, obwohl sie den Grenzwert einh√§lt (\\(H_0\\) wahr ist), soll maximal 5% betragen.\n\nBerechnung der Teststatistik \\(t\\):\n\n\\(t = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}\\)\n\n\n# Berechnung der Teststatistik f√ºr Gerste\nstandard_error_barley = s_barley / np.sqrt(n_barley)\nt_statistic_barley = (x_bar_barley - mu_0_barley) / standard_error_barley\nprint(f'Standardfehler (s/‚àön): {standard_error_barley:.4f}')\nprint(f'Teststatistik (t): {t_statistic_barley:.3f}')\n\nStandardfehler (s/‚àön): 0.0685\nTeststatistik (t): 4.381\n\n\n\nDer berechnete Wert ist \\(t \\approx 4.011\\).\n\nBestimmung des kritischen Wertes \\(t_{\\text{krit}}\\):\n\nWir f√ºhren einen einseitigen Test (rechtsseitig) mit \\(\\alpha = 0.05\\) und \\(n-1 = 50-1 = 49\\) Freiheitsgraden durch. Wir suchen den Wert \\(t_{1-\\alpha, n-1} = t_{0.95, 49}\\).\nAus der t-Verteilungstabelle oder mit Software erhalten wir \\(t_{\\text{krit}} \\approx 1.677\\).\n\n\nfrom scipy.stats import t as t_dist\n\nalpha_barley = 0.05\ndf_barley = n_barley - 1 # Freiheitsgrade\nt_critical_barley = t_dist.ppf(1 - alpha_barley, df_barley)\nprint(f'Freiheitsgrade (df): {df_barley}')\nprint(f'Kritischer Wert (t_krit) f√ºr Œ±={alpha_barley} (einseitig): {t_critical_barley:.3f}')\n\nFreiheitsgrade (df): 49\nKritischer Wert (t_krit) f√ºr Œ±=0.05 (einseitig): 1.677\n\n\nDie Visualisierung der t-Verteilung mit dem Ablehnungsbereich hilft, die Entscheidung zu verstehen:\n\nx_t_barley = np.linspace(t_dist.ppf(0.0001, df_barley), t_dist.ppf(0.9999, df_barley), 500)\ny_t_barley = t_dist.pdf(x_t_barley, df_barley)\n\nplt.figure(figsize=(10, 6))\nplt.plot(x_t_barley, y_t_barley, label=f't-Verteilung (df={df_barley})')\nplt.fill_between(x_t_barley, 0, y_t_barley, where=(x_t_barley &gt; t_critical_barley), color='red', alpha=0.5, label=f'Ablehnungsbereich (Œ±={alpha_barley})')\nplt.axvline(t_statistic_barley, color='black', linestyle='--', label=f'Teststatistik t = {t_statistic_barley:.3f}')\nplt.title('t-Verteilung, kritischer Wert und Teststatistik (Gersten-Eiwei√ügehalt)')\nplt.xlabel('t-Wert')\nplt.ylabel('Dichte')\nplt.legend()\nplt.grid(alpha=0.5)\nplt.show()\n\n\n\n\n\n\n\nFigure¬†6.2: t-Verteilung mit 49 Freiheitsgraden und Ablehnungsbereich f√ºr den einseitigen Gerstentest (Œ±=0.05).\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nZur Erinnerung: Die t-Verteilung beim Testen\nDie t-Verteilung beschreibt, wie sich die Teststatistik \\(T = \\frac{\\bar{X} - \\mu_0}{S / \\sqrt{n}}\\) verteilen w√ºrde, wenn die Nullhypothese (\\(H_0: \\mu = \\mu_0\\)) wahr w√§re und die Daten aus einer normalverteilten Grundgesamtheit stammen. Wir vergleichen unseren aus der Stichprobe berechneten Wert \\(t\\) mit dieser theoretischen Verteilung, um zu sehen, ob er ein plausibler Wert unter \\(H_0\\) ist oder eher ein extremer, unwahrscheinlicher Wert, der gegen \\(H_0\\) spricht.\n\n\nEntscheidung:\n\nWir vergleichen die Teststatistik \\(t\\) mit dem kritischen Wert \\(t_{\\text{krit}}\\). Regel: Lehne \\(H_0\\) ab, wenn \\(t &gt; t_{\\text{krit}}\\).\nDa \\(t \\approx 4.011 &gt; 1.677 \\approx t_{\\text{krit}}\\), lehnen wir die Nullhypothese \\(H_0\\) ab.\nInterpretation: Das Ergebnis ist statistisch signifikant auf dem 5%-Niveau. Es gibt ausreichende Evidenz aus der Stichprobe, um zu schlussfolgern, dass der mittlere Eiwei√ügehalt dieser Gerstencharge signifikant √ºber dem Grenzwert von 11.5% liegt. Die Brauerei sollte diese Charge m√∂glicherweise nicht verwenden oder entsprechend behandeln, um Probleme mit dem Schaum zu vermeiden.\np-Wert: Berechnung der Wahrscheinlichkeit, unter \\(H_0\\) eine Teststatistik zu beobachten, die mindestens so extrem ist wie \\(t \\approx 4.011\\).\n\n\n\np_value_barley = 1 - t_dist.cdf(t_statistic_barley, df_barley)\nprint(f'p-Wert: {p_value_barley:.3e}') # Formatierung in wissenschaftlicher Notation\n# Vergleich mit alpha\nif p_value_barley &lt; alpha_barley:\n    print(f\"Da p-Wert ({p_value_barley:.3e}) &lt; Œ± ({alpha_barley}), wird H‚ÇÄ abgelehnt.\")\nelse:\n    print(f\"Da p-Wert ({p_value_barley:.3e}) &gt;= Œ± ({alpha_barley}), wird H‚ÇÄ nicht abgelehnt.\")\n\np-Wert: 3.110e-05\nDa p-Wert (3.110e-05) &lt; Œ± (0.05), wird H‚ÇÄ abgelehnt.\n\n\n\nDer p-Wert ist sehr klein (\\(p \\approx 9.98 \\times 10^{-5}\\)). Das bedeutet, es ist extrem unwahrscheinlich, einen Stichprobenmittelwert wie den beobachteten (oder einen noch h√∂heren) zu erhalten, wenn der wahre mittlere Eiwei√ügehalt der Charge tats√§chlich nur 11.5% (oder weniger) w√§re. Da \\(p &lt; \\alpha\\), wird \\(H_0\\) abgelehnt.\n\n\n\n\n\n\n\nImportant\n\n\n\nKonsequenz der Testentscheidung\nIn diesem Beispiel f√ºhrt die Ablehnung der Nullhypothese zu der Schlussfolgerung, dass der Eiwei√ügehalt wahrscheinlich zu hoch ist. Dies hat praktische Konsequenzen f√ºr die Brauerei (z.B. Ablehnung der Charge, Anpassung des Brauprozesses). W√§re die Nullhypothese nicht abgelehnt worden (\\(t \\leq t_{krit}\\) oder \\(p &gt; \\alpha\\)), h√§tte die Brauerei keinen statistischen Grund gehabt, die Charge aufgrund des Eiwei√ügehalts abzulehnen (basierend auf dieser Stichprobe und dem gew√§hlten Signifikanzniveau).\n\n\n\n\n\n\n\n\nImportant\n\n\n\nDer p-Wert\nDer p-Wert ist die Wahrscheinlichkeit, unter Annahme der G√ºltigkeit der Nullhypothese (\\(H_0\\)), ein Ergebnis zu erhalten, das mindestens so extrem ist wie das in der Stichprobe beobachtete Ergebnis (repr√§sentiert durch die Teststatistik).\n\nEin kleiner p-Wert (typischerweise \\(p \\leq \\alpha\\)) deutet darauf hin, dass das beobachtete Ergebnis unter \\(H_0\\) sehr unwahrscheinlich ist. Dies liefert Evidenz gegen \\(H_0\\) und f√ºhrt zur Ablehnung von \\(H_0\\).\nEin gro√üer p-Wert (\\(p &gt; \\alpha\\)) bedeutet, dass das beobachtete Ergebnis unter \\(H_0\\) durchaus plausibel ist. Es gibt keine ausreichende Evidenz, um \\(H_0\\) abzulehnen.\n\nDer p-Wert ist nicht die Wahrscheinlichkeit, dass \\(H_0\\) wahr ist.\n\n\n\n\n6.1.1.2 Aufgabe - Zweiseitig: Eiwei√ügehalt von Braugerste\nEine Brauerei pr√ºft, ob der Eiwei√ügehalt einer neuen Gerstencharge im optimalen Bereich von 10.5% liegt. Ein zu hoher oder zu niedriger Eiwei√ügehalt kann die Bierqualit√§t beeintr√§chtigen. F√ºhren Sie einen zweiseitigen t-Test durch, um zu testen, ob der mittlere Eiwei√ügehalt der Charge signifikant von 10.5% abweicht. Verwenden Sie eine Stichprobe von \\(n=50\\) K√∂rnern mit den folgenden Daten:\n\nStichprobenmittelwert: \\(\\bar{x} = 10.47\\%\\)\nStichprobenstandardabweichung: \\(s = 0.49\\%\\)\nSignifikanzniveau: \\(\\alpha = 0.05\\)\n\nHypothesen:\n\n\\(H_0: \\mu = 10.5\\%\\) (Der Eiwei√ügehalt entspricht dem Zielwert).\n\\(H_1: \\mu \\neq 10.5\\%\\) (Der Eiwei√ügehalt weicht ab).\n\nEntscheidungsregel:\n\nZweiseitig: \\(H_0\\) wird abgelehnt, wenn \\(|t| &gt; t_{1-\\alpha/2, n-1}\\).\nKritischer Wert \\(t_{\\text{krit}}\\) aus einer t-Verteilungstabelle oder Software.\n\nBerechnen Sie die Teststatistik \\(t\\), den kritischen Wert \\(t_{\\text{krit}}\\) und den p-Wert. Entscheiden Sie, ob \\(H_0\\) abgelehnt wird, und interpretieren Sie das Ergebnis.\n\n\n\n\n\n\nMusterl√∂sung\n\n\n\n\n\n6.1.1.2.1 Musterl√∂sung\nSchritt 1: Gegebene Werte\n\n\\(\\bar{x} = 10.47\\%\\), \\(s = 0.49\\%\\), \\(n = 50\\), \\(\\mu_0 = 10.5\\%\\), \\(\\alpha = 0.05\\).\n\nSchritt 2: Teststatistik berechnen Die Formel f√ºr die Teststatistik ist:\n\\[\nt = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}\n\\]\nStandardfehler: \\(s / \\sqrt{n} = 0.49 / \\sqrt{50} \\approx 0.0693\\)\nTeststatistik:\n\\[\nt = \\frac{10.47 - 10.5}{0.0693} \\approx -0.433\n\\]\n\nimport numpy as np\nx_bar = 10.47\ns = 0.49\nn = 50\nmu_0 = 10.5\nstandard_error = s / np.sqrt(n)\nt_statistic = (x_bar - mu_0) / standard_error\nprint(f\"Standardfehler: {standard_error:.4f}\")\nprint(f\"Teststatistik t: {t_statistic:.3f}\")\n\nStandardfehler: 0.0693\nTeststatistik t: -0.433\n\n\nSchritt 3: Kritischer Wert\nFreiheitsgrade: \\(df = n - 1 = 49\\). F√ºr einen zweiseitigen Test mit \\(\\alpha = 0.05\\) ist der kritische Wert \\(t_{1-\\alpha/2, 49} = t_{0.975, 49} \\approx 2.010\\) (aus t-Tabelle oder Software).\n\nfrom scipy.stats import t as t_dist\nalpha = 0.05\ndf = n - 1\nt_critical = t_dist.ppf(1 - alpha/2, df)\nprint(f\"Kritischer Wert t_krit: ¬±{t_critical:.3f}\")\n\nKritischer Wert t_krit: ¬±2.010\n\n\nSchritt 4: p-Wert\nDer p-Wert f√ºr einen zweiseitigen Test ist: \\(p = 2 \\cdot P(T \\geq |t|)\\).\n\np_value = 2 * (1 - t_dist.cdf(abs(t_statistic), df))\nprint(f\"p-Wert: {p_value:.3f}\")\n\np-Wert: 0.667\n\n\nErgebnis: \\(p \\approx 0.667\\).\nSchritt 5: Entscheidung\n\nVergleich: \\(|t| \\approx 0.433 &lt; 2.010 \\approx t_{\\text{krit}}\\), daher wird \\(H_0\\) nicht abgelehnt.\nAlternativ: \\(p \\approx 0.667 &gt; 0.05\\), best√§tigt die Nicht-Ablehnung.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import t as t_dist\n\n# Gegebene Werte\nx_bar = 10.47\ns = 0.49\nn = 50\nmu_0 = 10.5\nalpha = 0.05\n\n# Berechnung der Teststatistik\nstandard_error = s / np.sqrt(n)\nt_statistic = (x_bar - mu_0) / standard_error\ndf = n - 1  # Freiheitsgrade\n\n# Kritischer Wert f√ºr zweiseitigen Test\nt_critical = t_dist.ppf(1 - alpha/2, df)\n\n# Daten f√ºr die t-Verteilung\nx = np.linspace(-4, 4, 1000)\ny = t_dist.pdf(x, df)\n\n# Plot der t-Verteilung\nplt.figure(figsize=(10, 6))\nplt.plot(x, y, label=f't-Verteilung (df={df})', color='blue')\n\n# Schattierte Ablehnungsbereiche\nplt.fill_between(x, y, where=(x &lt;= -t_critical), color='red', alpha=0.3, label='Ablehnungsbereich')\nplt.fill_between(x, y, where=(x &gt;= t_critical), color='red', alpha=0.3)\n\n# Teststatistik markieren\nplt.axvline(t_statistic, color='green', linestyle='--', label=f'Teststatistik t = {t_statistic:.3f}')\nplt.axvline(-t_critical, color='black', linestyle=':', label=f'Kritischer Wert ¬±{t_critical:.3f}')\nplt.axvline(t_critical, color='black', linestyle=':')\n\n# Beschriftungen\nplt.title('t-Test: Teststatistik und Ablehnungsbereich')\nplt.xlabel('t-Wert')\nplt.ylabel('Dichte')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Plot anzeigen\nplt.show()\n\n# Ausgabe der berechneten Werte\nprint(f\"Standardfehler: {standard_error:.4f}\")\nprint(f\"Teststatistik t: {t_statistic:.3f}\")\nprint(f\"Kritischer Wert t_krit: ¬±{t_critical:.3f}\")\n\n\n\n\n\n\n\nFigure¬†6.3: t-Verteilung mit 49 Freiheitsgraden und Ablehnungsbereich f√ºr den zweiseitigen Gerstentest (Œ±=0.05).\n\n\n\n\n\nStandardfehler: 0.0693\nTeststatistik t: -0.433\nKritischer Wert t_krit: ¬±2.010\n\n\nInterpretation\nEs gibt keinen statistischen Hinweis, dass der Eiwei√ügehalt der Gerstencharge signifikant von 10.5% abweicht. Die Charge liegt im optimalen Bereich, und die Brauerei kann sie verwenden, ohne Anpassungen vornehmen zu m√ºssen.\n\n\n\n\n\n\n6.1.2 Zwei-Stichproben-t-Test (Independent Samples t-Test)\nH√§ufig ist man daran interessiert, ob sich die Mittelwerte zweier unabh√§ngiger Stichproben signifikant voneinander unterscheiden. Zum Beispiel k√∂nnten wir vergleichen, ob sich die mittlere Zugfestigkeit von St√§hlen zweier verschiedener Lieferanten unterscheidet. Auch hierf√ºr kann ein t-Test eingesetzt werden (vgl. Abb. Figure¬†6.4).\n\n\n\n\n\n\nFigure¬†6.4: Grundidee des Zwei-Stichproben-t-Tests: Vergleich der Mittelwerte zweier Verteilungen. Quelle: Inductiveload (n.d.)\n\n\n\nVoraussetzungen:\n\nDie beiden Stichproben sind unabh√§ngig voneinander.\nDie Daten in beiden Stichproben stammen aus normalverteilten Grundgesamtheiten. (Der Test ist robust gegen√ºber Verletzungen dieser Annahme bei ausreichender Stichprobengr√∂√üe, ca. n &gt; 30 pro Gruppe).\nDie Varianzen der beiden Grundgesamtheiten sind gleich (\\(\\sigma_1^2 = \\sigma_2^2\\)). Diese Annahme kann mit Tests wie dem Levene-Test √ºberpr√ºft werden. Wenn die Varianzen ungleich sind, wird eine Variante des t-Tests namens Welch-Test verwendet, der die Freiheitsgrade anpasst.\n\nHypothesen (Standardfall: Test auf Gleichheit der Mittelwerte):\n\n\\(H_0: \\mu_1 = \\mu_2\\) (oder √§quivalent \\(H_0: \\mu_1 - \\mu_2 = 0\\))\n\\(H_1: \\mu_1 \\neq \\mu_2\\) (zweiseitig), oder \\(H_1: \\mu_1 &gt; \\mu_2\\) (rechtsseitig), oder \\(H_1: \\mu_1 &lt; \\mu_2\\) (linksseitig)\n\nTeststatistik (bei gleichen Varianzen):\n\nBerechne die gepoolte (kombinierte) Varianz \\(s_p^2\\) als gewichteten Durchschnitt der beiden Stichprobenvarianzen: \\[\ns_p^2 = \\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}\n\\] wobei \\(n_1, n_2\\) die Stichprobengr√∂√üen und \\(s_1^2, s_2^2\\) die (korrigierten) Stichprobenvarianzen sind. \\(s_p = \\sqrt{s_p^2}\\) ist die gepoolte Standardabweichung.\n\n\n\n\n\n\n\nHerleitung der Formel f√ºr die gepoolte Varianz\n\n\n\n\n\nDie gepoolte Varianz ist eine gewichtete Mittelung der beiden Stichprobenvarianzen, wobei die Gewichte den Freiheitsgraden der Stichproben entsprechen.\nSchritt 1: Definition der Stichprobenvarianz\nF√ºr Stichprobe 1:\n\\[\ns_1^2 = \\frac{1}{n_1 - 1} \\sum_{i=1}^{n_1} (x_{1i} - \\bar{x}_1)^2\n\\]\nF√ºr Stichprobe 2:\n\\[\ns_2^2 = \\frac{1}{n_2 - 1} \\sum_{i=1}^{n_2} (x_{2i} - \\bar{x}_2)^2\n\\]\nSchritt 2: Umstellen\nMultiplizieren mit den Freiheitsgraden:\n\\[\n(n_1 - 1) s_1^2 = \\sum_{i=1}^{n_1} (x_{1i} - \\bar{x}_1)^2\n\\]\n\\[\n(n_2 - 1) s_2^2 = \\sum_{i=1}^{n_2} (x_{2i} - \\bar{x}_2)^2\n\\]\nSchritt 3: Gesamtstreuung kombinieren\nDie Gesamtquadratsumme (Summe der quadrierten Abweichungen innerhalb beider Gruppen):\n\\[\nSS_{\\text{total}} = \\sum_{i=1}^{n_1} (x_{1i} - \\bar{x}_1)^2 + \\sum_{i=1}^{n_2} (x_{2i} - \\bar{x}_2)^2 = (n_1 - 1) s_1^2 + (n_2 - 1) s_2^2\n\\]\nSchritt 4: Pooled Variance als Mittelwert\nDie gepoolte Varianz ist diese Gesamtstreuung, geteilt durch die Gesamtanzahl der Freiheitsgrade:\n\\[\ns_p^2 = \\frac{SS_{\\text{total}}}{n_1 + n_2 - 2} = \\frac{(n_1 - 1) s_1^2 + (n_2 - 1) s_2^2}{n_1 + n_2 - 2}\n\\]\n\n\n\n\nBerechne die Teststatistik \\(t\\): \\[\nt = \\frac{(\\bar{x}_1 - \\bar{x}_2) - (\\mu_1 - \\mu_2)_0}{s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\n\\] F√ºr den Standardtest auf Gleichheit ist \\((\\mu_1 - \\mu_2)_0 = 0\\), also vereinfacht sich die Formel zu: \\[\nt = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\n\\]\n\nFreiheitsgrade und Entscheidung:\n\nDie Teststatistik folgt unter \\(H_0\\) einer t-Verteilung mit \\(df = n_1 + n_2 - 2\\) Freiheitsgraden.\nBestimme den kritischen Wert \\(t_{\\text{krit}}\\) basierend auf \\(\\alpha\\), den Freiheitsgraden \\(df\\) und der Alternativhypothese (ein- oder zweiseitig, z.B. \\(t_{1-\\alpha/2, df}\\) f√ºr zweiseitig).\nVergleiche \\(t\\) mit \\(t_{\\text{krit}}\\). Bei einem zweiseitigen Test: Lehne \\(H_0\\) ab, wenn \\(|t| &gt; t_{\\text{krit}}\\).\nAlternativ: Berechne den p-Wert und vergleiche ihn mit \\(\\alpha\\). Lehne \\(H_0\\) ab, wenn \\(p \\leq \\alpha\\).\n\n\n\n\n\n\n\nNote\n\n\n\nTest auf eine spezifische Differenz der Erwartungswerte\nStatt auf Gleichheit der Mittelwerte (\\(\\mu_1 - \\mu_2 = 0\\)) zu testen, k√∂nnte man auch pr√ºfen, ob sich die Mittelwerte um einen bestimmten Betrag \\(\\omega_0\\) unterscheiden. Die Nullhypothese w√§re dann \\(H_0: \\mu_1 - \\mu_2 = \\omega_0\\). Die Teststatistik \\(t\\) wird dann wie folgt berechnet: \\[\nt = \\frac{(\\bar{x}_1 - \\bar{x}_2) - \\omega_0}{s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\n\\] Die Freiheitsgrade der t-Verteilung bleiben \\(n_1 + n_2 - 2\\) (bei Varianzgleichheit).\n\n\n\n\n6.1.3 Zwei-Stichproben-t-Test (Paired Samples t-Test)\nIn manchen F√§llen sind die beiden Stichproben nicht unabh√§ngig, sondern gepaart. Das klassische Beispiel ist eine Messung vor und nach einer Behandlung an denselben Untersuchungseinheiten (z.B. Patienten, Bauteilen). Hier interessiert uns nicht der Unterschied der Mittelwerte unabh√§ngiger Gruppen, sondern der mittlere Unterschied innerhalb der Paare.\nBeispiel: Ein Verfahren zum H√§rten eines metallischen Bauteils soll untersucht werden. Im Experiment wird der H√§rtegrad von \\(n=10\\) Bauteilen jeweils vor und nach der Behandlung gemessen.\nTabelle: H√§rtegrad in HR (Rockwell) vor und nach der Behandlung\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBauteil\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nVorher\n49.1\n49.2\n49.3\n49.4\n49.5\n49.6\n49.7\n49.8\n49.0\n50.0\n\n\nNachher\n50.2\n50.3\n50.3\n50.2\n50.7\n50.7\n50.8\n50.9\n51.0\n51.1\n\n\nDifferenz (Nachher - Vorher)\n1.1\n1.1\n1.0\n0.8\n1.2\n1.1\n1.1\n1.1\n2.0\n1.1\n\n\n\nVorgehen:\n\nBerechne die Differenzen \\(d_i = \\text{Wert}_{\\text{nachher}, i} - \\text{Wert}_{\\text{vorher}, i}\\) f√ºr jedes Paar \\(i\\).\nBehandle diese Differenzen \\(d_1, d_2, ..., d_n\\) als eine einzelne Stichprobe.\nF√ºhre einen Einstichproben-t-Test f√ºr diese Differenzen durch, um zu pr√ºfen, ob der mittlere Unterschied \\(\\mu_d\\) signifikant von einem bestimmten Wert (meistens 0) abweicht.\n\nHypothesen (Test auf signifikante Erh√∂hung):\n\n\\(H_0: \\mu_d = 0\\) (Die Behandlung hat im Mittel keinen Effekt auf die H√§rte.)\n\\(H_1: \\mu_d &gt; 0\\) (Die Behandlung erh√∂ht im Mittel den H√§rtegrad.)\n\nBerechnungen:\n\nBerechne den Mittelwert der Differenzen \\(\\bar{d}\\) und die Standardabweichung der Differenzen \\(s_d\\).\n\n\nimport numpy as np\nfrom scipy.stats import t as t_dist\n\nvorher = np.array([49.1, 49.2, 49.3, 49.4, 49.5, 49.6, 49.7, 49.8, 49.0, 50.0])\nnachher = np.array([50.2, 50.3, 50.3, 50.2, 50.7, 50.7, 50.8, 50.9, 51.0, 51.1])\nd = nachher - vorher\nprint(\"Differenzen (d):\", d)\n\nd_bar = np.mean(d)\ns_d = np.std(d, ddof=1) # Korrigierte Standardabweichung\nn = len(d)\nprint(f'Mittlere Differenz (dÃÑ): {d_bar:.3f}')\nprint(f'Standardabweichung der Differenzen (s_d): {s_d:.3f}')\nprint(f'Anzahl Paare (n): {n}')\n\n# Teststatistik berechnen (H0: mu_d = 0)\nmu_d0 = 0\nt_stat_paired = (d_bar - mu_d0) / (s_d / np.sqrt(n))\nprint(f'Teststatistik (t): {t_stat_paired:.3f}')\n\nDifferenzen (d): [1.1 1.1 1.  0.8 1.2 1.1 1.1 1.1 2.  1.1]\nMittlere Differenz (dÃÑ): 1.160\nStandardabweichung der Differenzen (s_d): 0.313\nAnzahl Paare (n): 10\nTeststatistik (t): 11.705\n\n\n\nDer mittlere Unterschied betr√§gt \\(\\bar{d} \\approx 1.16\\) und die Standardabweichung \\(s_d \\approx 0.317\\). Die Teststatistik ist \\(t \\approx 11.58\\).\n\nTestdurchf√ºhrung:\n\nVoraussetzungen: Die Differenzen sollten ann√§hernd normalverteilt sein (oder \\(n\\) gro√ü genug). Die Messungen sind gepaart.\nHypothesen: \\(H_0: \\mu_d = 0\\), \\(H_1: \\mu_d &gt; 0\\).\nSignifikanzniveau: \\(\\alpha = 0.05\\).\nTeststatistik: \\(t = \\frac{\\bar{d} - 0}{s_d / \\sqrt{n}} \\approx 11.58\\).\nKritischer Wert: F√ºr einen einseitigen Test mit \\(\\alpha = 0.05\\) und \\(df = n-1 = 10-1 = 9\\) Freiheitsgraden ist \\(t_{\\text{krit}} = t_{0.95, 9}\\).\n\ndf_paired = n - 1\nalpha_paired = 0.05\nt_crit_paired = t_dist.ppf(1 - alpha_paired, df_paired)\nprint(f'Freiheitsgrade (df): {df_paired}')\nprint(f'Kritischer Wert (t_krit) f√ºr Œ±={alpha_paired} (einseitig): {t_crit_paired:.3f}')\n\nFreiheitsgrade (df): 9\nKritischer Wert (t_krit) f√ºr Œ±=0.05 (einseitig): 1.833\n\n\nDer kritische Wert ist \\(t_{\\text{krit}} \\approx 1.833\\).\nEntscheidung: Da \\(t \\approx 11.58 &gt; 1.833 \\approx t_{\\text{krit}}\\), lehnen wir \\(H_0\\) ab.\n\nInterpretation: Es gibt starke statistische Evidenz daf√ºr, dass die Behandlung den H√§rtegrad der Bauteile signifikant erh√∂ht.\np-Wert:\n\n\n\np_value_paired = 1 - t_dist.cdf(t_stat_paired, df_paired)\nprint(f'p-Wert (einseitig): {p_value_paired:.3e}')\nif p_value_paired &lt; alpha_paired:\n    print(f\"Da p-Wert ({p_value_paired:.3e}) &lt; Œ± ({alpha_paired}), wird H‚ÇÄ abgelehnt.\")\nelse:\n      print(f\"Da p-Wert ({p_value_paired:.3e}) &gt;= Œ± ({alpha_paired}), wird H‚ÇÄ nicht abgelehnt.\")\n\np-Wert (einseitig): 4.760e-07\nDa p-Wert (4.760e-07) &lt; Œ± (0.05), wird H‚ÇÄ abgelehnt.\n\n\nDer p-Wert ist extrem klein ($p \\approx 1.35 \\times 10^{-6}$), was die Ablehnung von $H_0$ best√§tigt.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Tests</span>"
    ]
  },
  {
    "objectID": "statistics/interference_advanced.html#chi-quadrat-test-ein-nicht-parametrischer-test",
    "href": "statistics/interference_advanced.html#chi-quadrat-test-ein-nicht-parametrischer-test",
    "title": "6¬† Tests",
    "section": "6.2 Chi-Quadrat-Test: Ein nicht-parametrischer Test",
    "text": "6.2 Chi-Quadrat-Test: Ein nicht-parametrischer Test\nDer Chi-Quadrat-Test ist ein nicht-parametrischer Test, der Hypothesen √ºber kategoriale Daten pr√ºft, ohne Annahmen √ºber die Verteilung der Grundgesamtheit zu machen. Er wird h√§ufig als Anpassungstest (Goodness-of-Fit) verwendet, um zu testen, ob beobachtete H√§ufigkeiten in einer Stichprobe einer erwarteten Verteilung entsprechen.\nDie Teststatistik misst die Abweichung zwischen beobachteten (\\(O_i\\)) und erwarteten (\\(E_i\\)) H√§ufigkeiten:\n\\[    \n\\chi^2 = \\sum \\frac{(O_i - E_i)^2}{E_i}\n\\]\nUnter der Nullhypothese (\\(H_0\\)) folgt die Statistik einer Chi-Quadrat-Verteilung mit \\(k-1\\) Freiheitsgraden (\\(k\\) = Anzahl Kategorien). Ein hoher \\(\\chi^2\\)-Wert spricht gegen \\(H_0\\).\n\n6.2.1 Beispiel: Bierqualit√§t in einer Brauerei\nEine Brauerei pr√ºft, ob die Qualit√§tsbewertungen einer neuen Biercharge (‚ÄûHervorragend‚Äú, ‚ÄûGut‚Äú, ‚ÄûMangelhaft‚Äú) der erwarteten Verteilung entsprechen, die auf langj√§hrigen Produktionsdaten basiert (erwartete Anteile: Hervorragend: 60%, Gut: 30%, Mangelhaft: 10%). Stichprobe: \\(n=150\\) Flaschen.\nBeobachtete H√§ufigkeiten:\n\nHervorragend: 95\nGut: 45\nMangelhaft: 10\n\nErwartete H√§ufigkeiten:\n\nHervorragend: \\(150 \\times 0.60 = 90\\)\nGut: \\(150 \\times 0.30 = 45\\)\nMangelhaft: \\(150 \\times 0.10 = 15\\)\n\nHypothesen:\n\n\\(H_0\\): Die Qualit√§tsverteilung der neuen Biercharge entspricht der erwarteten Verteilung.\n\\(H_1\\): Die Qualit√§tsverteilung weicht von der erwarteten Verteilung ab.\n\nTestdurchf√ºhrung:\n\nVoraussetzungen: Erwartete H√§ufigkeiten \\(E_i \\geq 5\\), Stichprobe zuf√§llig.\nSignifikanzniveau: \\(\\alpha = 0.05\\).\nTeststatistik:\n\n\nimport numpy as np\nfrom scipy.stats import chi2\n\nO = np.array([95, 45, 10])\nE = np.array([90, 45, 15])\nchi2_statistic = np.sum((O - E)**2 / E)\nprint(f\"Chi-Quadrat Statistik: {chi2_statistic:.3f}\")\n\nChi-Quadrat Statistik: 1.944\n\n\nErgebnis: \\(\\chi^2 \\approx 1.667\\).\n\nKritischer Wert:\n\nF√ºr \\(df = 3-1 = 2\\) und \\(\\alpha = 0.05\\) ist \\(\\chi^2_{\\text{krit}} \\approx 5.991\\).\n\ndf = len(O) - 1\nalpha = 0.05\nchi2_critical = chi2.ppf(1 - alpha, df)\nprint(f\"Kritischer Wert: {chi2_critical:.3f}\")\n\nKritischer Wert: 5.991\n\n\n\np-Wert:\n\n\np_value = 1 - chi2.cdf(chi2_statistic, df)\nprint(f\"p-Wert: {p_value:.3f}\")\n\np-Wert: 0.378\n\n\nErgebnis: \\(p \\approx 0.435\\).\n\nEntscheidung:\n\nDa \\(\\chi^2 \\approx 1.667 &lt; 5.991\\) und \\(p \\approx 0.435 &gt; 0.05\\), wird \\(H_0\\) nicht abgelehnt.\nInterpretation:\nEs gibt keinen statistischen Hinweis, dass die Qualit√§tsverteilung der neuen Biercharge signifikant von der erwarteten Verteilung abweicht. Die Brauerei kann die Charge als qualitativ gleichwertig betrachten.\n\n\n6.2.2 Visualisierung der Teststatistik\n\nimport matplotlib.pyplot as plt\n\n# Chi-Quadrat-Verteilung\nx = np.linspace(0, 10, 1000)\ny = chi2.pdf(x, df)\n\n# Plot\nplt.figure(figsize=(10, 6))\nplt.plot(x, y, label=f'Chi-Quadrat-Verteilung (df={df})', color='blue')\nplt.fill_between(x, y, where=(x &gt;= chi2_critical), color='red', alpha=0.3, label='Ablehnungsbereich')\nplt.axvline(chi2_statistic, color='green', linestyle='--', label=f'Teststatistik = {chi2_statistic:.3f}')\nplt.axvline(chi2_critical, color='black', linestyle=':', label=f'Kritischer Wert = {chi2_critical:.3f}')\n\n# Beschriftungen\nplt.title('Chi-Quadrat-Test: Teststatistik und Ablehnungsbereich')\nplt.xlabel('Chi-Quadrat-Wert')\nplt.ylabel('Dichte')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n6.2.3 Warum ist der Chi-Quadrat-Test nicht-parametrisch?\nDer Chi-Quadrat-Test macht keine Annahmen √ºber die Verteilung der zugrunde liegenden Daten (z. B. Normalverteilung), sondern arbeitet direkt mit H√§ufigkeiten. Dies macht ihn besonders geeignet f√ºr kategoriale Daten, bei denen parametrische Annahmen wie Normalit√§t oder gleiche Varianzen nicht zutreffen.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Tests</span>"
    ]
  },
  {
    "objectID": "statistics/interference_advanced.html#√ºbersicht-√ºber-statistische-tests",
    "href": "statistics/interference_advanced.html#√ºbersicht-√ºber-statistische-tests",
    "title": "6¬† Tests",
    "section": "6.3 √úbersicht √ºber Statistische Tests",
    "text": "6.3 √úbersicht √ºber Statistische Tests\nStatistische Tests werden verwendet, um Hypothesen √ºber eine Grundgesamtheit anhand einer Stichprobe zu pr√ºfen. Sie bestehen aus einer Nullhypothese (\\(H_0\\)), die den Status quo repr√§sentiert, und einer Alternativhypothese (\\(H_1\\)), die angenommen wird, wenn \\(H_0\\) abgelehnt wird. Tests helfen, zu unterscheiden, ob die Ergebnisse einer Stichprobe auf die Grundgesamtheit √ºbertragbar sind oder zuf√§llig entstanden.\n\nParametrische Tests (z. B. t-Test) setzen spezifische Verteilungsannahmen voraus (z.B. Normalit√§t).\nNicht-parametrische Tests (z. B. Chi-Quadrat-Test, Mann-Whitney-U-Test) sind flexibler und ben√∂tigen keine Verteilungsannahmen, eignen sich aber oft f√ºr ordinal oder kategoriale Daten.\n\nWeitere Details zu Testarten findest du in dieser √úbersicht.\n\n\n\n\nInductiveload, based on original work by Jhguch. n.d. ‚ÄúTwo Sample t-Test.‚Äù https://commons.wikimedia.org/wiki/File:Two_sample_ttest.svg.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Tests</span>"
    ]
  },
  {
    "objectID": "statistics/tutorial_2.html",
    "href": "statistics/tutorial_2.html",
    "title": "Tutorial 2: Muster in Netzlasten verstehen",
    "section": "",
    "text": "Ziel\nWir arbeiten mit den Daten der Global Energy Forecasting Competition Hong, Pinson, and Fan (2014), die Sie zuvor bereinigt haben. Ziel ist es, Muster in den Netzlasten zu identifizieren, insbesondere in Bezug auf Wochentag-Effekte. Die Netzlast variiert je nach Wochentag, da sich das Verhalten von Haushalten und Industrien √§ndert. Wir wollen herausfinden, ob es Unterschiede in der Last zwischen Wochentagen und Wochenenden/Feiertagen gibt, was f√ºr sp√§tere Prognosemodelle relevant ist.",
    "crumbs": [
      "Statistik",
      "Tutorial 2: Muster in Netzlasten verstehen"
    ]
  },
  {
    "objectID": "statistics/tutorial_2.html#ziel",
    "href": "statistics/tutorial_2.html#ziel",
    "title": "Tutorial 2: Muster in Netzlasten verstehen",
    "section": "",
    "text": "Aufgaben\n\nTrennung der Wochentage: Visualisieren Sie die Verteilung der Netzlast f√ºr jeden Wochentag (Montag bis Sonntag und Feiertag) in einem Violin-Plot, um die Verteilungen optisch zu vergleichen. Markieren Sie Wochenenden (Samstag, Sonntag) anders.\nGruppierung: Teilen Sie die Daten in zwei Gruppen: Wochentage (Montag bis Freitag) und Sonn-/Feiertage (Samstag, Sonntag sowie Feiertage). Vergleichen Sie die Mittelwerte der Netzlasten dieser Gruppen mit einem t-Test.\nOptionaler Teil: Vergleichen Sie die Verteilungen der beiden Gruppen mit dem Kolmogorov-Smirnov (KS)-Test und visualisieren Sie die kumulativen Verteilungsfunktionen (CDFs). Dieser Teil ist in einem aufklappbaren Callout beschrieben.\n\nWir verwenden ein Signifikanzniveau von \\(\\alpha = 0.05\\).",
    "crumbs": [
      "Statistik",
      "Tutorial 2: Muster in Netzlasten verstehen"
    ]
  },
  {
    "objectID": "statistics/tutorial_2.html#analyse-mit-simulierten-daten",
    "href": "statistics/tutorial_2.html#analyse-mit-simulierten-daten",
    "title": "Tutorial 2: Muster in Netzlasten verstehen",
    "section": "Analyse mit simulierten Daten",
    "text": "Analyse mit simulierten Daten\nDa keine echten Daten vorliegen, generieren wir simulierte Netzlast-Daten f√ºr jeden Wochentag sowie Feiertage. Die Daten sind normalverteilt mit unterschiedlichen Mittelwerten und Standardabweichungen, um realistische Unterschiede widerzuspiegeln.\n\nSchritt 1: Visualisierung der Wochentag-Verteilungen\nWir generieren Netzlast-Daten f√ºr jeden Wochentag und visualisieren die Verteilungen in einem Violin-Plot, wobei Wochenenden anders gef√§rbt sind.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n# Simulierte Daten generieren\nnp.random.seed(42)\ndays = ['Montag', 'Dienstag', 'Mittwoch', 'Donnerstag', 'Freitag', 'Samstag', 'Sonntag']\nn_samples = 100  # Pro Tag\nloads = []\nday_labels = []\n\nfor day in days:\n    if day in ['Samstag', 'Sonntag']:\n        # Wochenenden: Niedrigere Last\n        load = np.random.normal(90, 8, n_samples)\n    else:\n        # Wochentage: H√∂here Last\n        load = np.random.normal(100, 10, n_samples)\n    loads.extend(load)\n    day_labels.extend([day] * n_samples)\n\n# Feiertage hinzuf√ºgen (√§hnlich wie Wochenenden)\nholiday_load = np.random.normal(88, 9, 50)\nloads.extend(holiday_load)\nday_labels.extend(['Feiertag'] * 50)\n\n# DataFrame erstellen\ndata = pd.DataFrame({'Tag': day_labels, 'Last': loads})\n\n# Violin-Plot\nplt.figure(figsize=(12, 6))\nsns.violinplot(x='Tag', y='Last', data=data, palette={'Montag': 'blue', 'Dienstag': 'blue', \n                                                      'Mittwoch': 'blue', 'Donnerstag': 'blue', \n                                                      'Freitag': 'blue', 'Samstag': 'orange', \n                                                      'Sonntag': 'orange', 'Feiertag': 'orange'})\nplt.title('Verteilung der Netzlast nach Wochentag und Feiertagen')\nplt.xlabel('Tag')\nplt.ylabel('Netzlast (MW)')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n/tmp/ipykernel_3377/3092400038.py:33: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: Der Violin-Plot zeigt, dass die Netzlast an Wochentagen (Montag bis Freitag) tendenziell h√∂her ist als an Wochenenden (Samstag, Sonntag) und Feiertagen. Die Verteilungen an Wochenenden und Feiertagen sind √§hnlich und weisen eine geringere Variabilit√§t auf.\n\n\nSchritt 2: t-Test f√ºr Mittelwerte (Wochentage vs.¬†Sonn-/Feiertage)\nWir teilen die Daten in zwei Gruppen: - Wochentage: Montag bis Freitag. - Sonn-/Feiertage: Samstag, Sonntag und Feiertage.\nDann f√ºhren wir einen zweiseitigen t-Test durch, um die Mittelwerte der Netzlasten zu vergleichen.\n\nfrom scipy.stats import ttest_ind\n\n# Gruppen erstellen\nweekday_load = data[data['Tag'].isin(['Montag', 'Dienstag', 'Mittwoch', 'Donnerstag', 'Freitag'])]['Last']\nweekend_holiday_load = data[data['Tag'].isin(['Samstag', 'Sonntag', 'Feiertag'])]['Last']\n\n# t-Test\nt_statistic, p_value = ttest_ind(weekday_load, weekend_holiday_load, equal_var=False)  # Welch's t-Test\nprint(f\"t-Statistik: {t_statistic:.3f}\")\nprint(f\"p-Wert: {p_value:.3f}\")\n\n# Entscheidung\nalpha = 0.05\nif p_value &lt; alpha:\n    print(\"H_0 wird abgelehnt: Die Mittelwerte der Netzlast unterscheiden sich signifikant.\")\nelse:\n    print(\"H_0 wird nicht abgelehnt: Kein Hinweis auf unterschiedliche Mittelwerte.\")\n\nt-Statistik: 16.259\np-Wert: 0.000\nH_0 wird abgelehnt: Die Mittelwerte der Netzlast unterscheiden sich signifikant.\n\n\nErgebnis: Der p-Wert liegt typischerweise unter 0.05 (z. B. \\[   p \\approx 0.000   \\]), was darauf hindeutet, dass die Mittelwerte der Netzlast an Wochentagen und Sonn-/Feiertagen signifikant unterschiedlich sind. Die Netzlast ist an Wochentagen h√∂her.\n\n\nSchritt 3: Optionaler Vergleich der Verteilungen mit dem KS-Test\n\n\n\n\n\n\nKolmogorov-Smirnov (KS)-Test: Erkl√§rung und Anwendung\n\n\n\n\n\nDer Kolmogorov-Smirnov (KS)-Test ist ein nicht-parametrischer Test, der pr√ºft, ob zwei Stichproben aus derselben Verteilung stammen. Er vergleicht die empirischen kumulativen Verteilungsfunktionen (CDFs) der beiden Stichproben.\n\nTeststatistik\nDie KS-Teststatistik \\(D\\) misst den maximalen Abstand zwischen den CDFs \\(F_1(x)\\) und \\(F_2(x)\\):\n\\[    \nD = \\sup_x |F_1(x) - F_2(x)|,\n\\]\nwobei \\(\\sup\\) der Supremum (gr√∂√üte Abstand) ist.\n\n\nHypothesen\n\n\\(H_0\\): Die Verteilungen der Netzlast an Wochentagen und Sonn-/Feiertagen sind gleich.\n\\(H_1\\): Die Verteilungen unterscheiden sich.\n\n\n\nAnwendung\nDer KS-Test ist n√ºtzlich, wenn keine Normalit√§tsannahmen getroffen werden k√∂nnen. Hier vergleichen wir die Verteilung der Netzlast an Wochentagen mit der an Sonn-/Feiertagen.\n\n\nPython-Implementierung und Visualisierung\nWir f√ºhren den KS-Test durch und visualisieren die CDFs der beiden Gruppen.\n\nfrom scipy.stats import ks_2samp\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# KS-Test\nstatistic, p_value = ks_2samp(weekday_load, weekend_holiday_load)\nprint(f\"KS-Statistik: {statistic:.3f}\")\nprint(f\"p-Wert: {p_value:.3f}\")\n\n# Entscheidung\nalpha = 0.05\nif p_value &lt; alpha:\n    print(\"H_0 wird abgelehnt: Die Verteilungen unterscheiden sich signifikant.\")\nelse:\n    print(\"H_0 wird nicht abgelehnt: Kein Hinweis auf unterschiedliche Verteilungen.\")\n\n# Kumulativer Verteilungsplot (CDF)\ndef plot_cdf(data, label, color):\n    sorted_data = np.sort(data)\n    y = np.arange(1, len(sorted_data) + 1) / len(sorted_data)\n    plt.plot(sorted_data, y, label=label, color=color)\n\nplt.figure(figsize=(10, 6))\nplot_cdf(weekday_load, 'Wochentage', 'blue')\nplot_cdf(weekend_holiday_load, 'Sonn-/Feiertage', 'orange')\nplt.title('Kumulative Verteilungsfunktionen (CDF) der Netzlast')\nplt.xlabel('Netzlast (MW)')\nplt.ylabel('Kumulative Wahrscheinlichkeit')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\nKS-Statistik: 0.476\np-Wert: 0.000\nH_0 wird abgelehnt: Die Verteilungen unterscheiden sich signifikant.\n\n\n\n\n\n\n\n\n\n\n\nInterpretation\nEin p-Wert \\(&lt; \\alpha = 0.05\\) zeigt, dass die Verteilungen der Netzlast an Wochentagen und Sonn-/Feiertagen unterschiedlich sind. Der CDF-Plot visualisiert diese Unterschiede: Die CDF f√ºr Wochentage liegt tendenziell rechts von der f√ºr Sonn-/Feiertage, was auf eine h√∂here Netzlast hinweist.\nErgebnis: Der p-Wert ist typischerweise klein (z. B. \\(p \\approx 0.000\\)), was auf signifikante Unterschiede in den Verteilungen hinweist. Der CDF-Plot best√§tigt, dass die Netzlast an Wochentagen h√∂her und anders verteilt ist als an Sonn-/Feiertagen.",
    "crumbs": [
      "Statistik",
      "Tutorial 2: Muster in Netzlasten verstehen"
    ]
  },
  {
    "objectID": "statistics/tutorial_2.html#fazit",
    "href": "statistics/tutorial_2.html#fazit",
    "title": "Tutorial 2: Muster in Netzlasten verstehen",
    "section": "Fazit",
    "text": "Fazit\n\nWochentag-Verteilungen: Der Violin-Plot zeigt, dass die Netzlast an Wochentagen h√∂her ist als an Wochenenden und Feiertagen.\nt-Test: Der Mittelwert der Netzlast ist an Wochentagen signifikant h√∂her als an Sonn-/Feiertagen (\\(p &lt; 0.05\\)).\nKS-Test (optional): Die Verteilungen der Netzlast unterscheiden sich signifikant zwischen Wochentagen und Sonn-/Feiertagen, wie der KS-Test und der CDF-Plot zeigen.\n\n\n\n\n\nHong, Tao, Pierre Pinson, and Shu Fan. 2014. ‚ÄúGlobal Energy Forecasting Competition 2012.‚Äù International Journal of Forecasting 30 (2): 357‚Äì63. https://doi.org/https://doi.org/10.1016/j.ijforecast.2013.07.001.",
    "crumbs": [
      "Statistik",
      "Tutorial 2: Muster in Netzlasten verstehen"
    ]
  },
  {
    "objectID": "statistics/tasks.html",
    "href": "statistics/tasks.html",
    "title": "√úbungsaufgaben: Wahrscheinlichkeitsrechnung",
    "section": "",
    "text": "1. Berechnung von Mittelwert, Varianz und Kovarianz\nAufgabe:\nGegeben ist folgende Tabelle mit Werten f√ºr zwei Variablen \\(X\\) und \\(Y\\):\n\n\n\n\\(X\\)\n\\(Y\\)\n\n\n\n\n2\n3\n\n\n4\n7\n\n\n6\n5\n\n\n8\n9\n\n\n10\n11\n\n\n\nBerechne den Mittelwert von \\(X\\), den Mittelwert von \\(Y\\), die Varianz von \\(X\\), die Varianz von \\(Y\\) und die Kovarianz zwischen \\(X\\) und \\(Y\\). Interpretiere anschlie√üend die Kovarianz hinsichtlich der Beziehung zwischen \\(X\\) und \\(Y\\).\n\n\n\n\n\n\nMusterl√∂sung\n\n\n\n\n\nMusterl√∂sung\n\nMittelwert von \\(X\\):\n\\[\n\\bar{X} = \\frac{2 + 4 + 6 + 8 + 10}{5} = \\frac{30}{5} = 6\n\\]\nMittelwert von \\(Y\\):\n\\[\n\\bar{Y} = \\frac{3 + 7 + 5 + 9 + 11}{5} = \\frac{35}{5} = 7\n\\]\nVarianz von \\(X\\):\nAbweichungen von \\(\\bar{X}\\):\n\\[\n(2-6)^2 = 16, \\quad (4-6)^2 = 4, \\quad (6-6)^2 = 0, \\quad (8-6)^2 = 4, \\quad (10-6)^2 = 16\n\\]\nDann:\n\\[\n\\text{Var}(X) = \\frac{16 + 4 + 0 + 4 + 16}{5} = \\frac{40}{5} = 8\n\\]\nVarianz von \\(Y\\):\nAbweichungen von \\(\\bar{Y}\\):\n\\[\n(3-7)^2 = 16, \\quad (7-7)^2 = 0, \\quad (5-7)^2 = 4, \\quad (9-7)^2 = 4, \\quad (11-7)^2 = 16\n\\]\nDann:\n\\[\n\\text{Var}(Y) = \\frac{16 + 0 + 4 + 4 + 16}{5} = \\frac{40}{5} = 8\n\\]\nKovarianz zwischen \\(X\\) und \\(Y\\):\nProdukte der Abweichungen:\n\\[\n(2-6)(3-7) = (-4)(-4) = 16, \\quad (4-6)(7-7) = (-2)(0) = 0, \\quad (6-6)(5-7) = (0)(-2) = 0, \\quad (8-6)(9-7) = (2)(2) = 4, \\quad (10-6)(11-7) = (4)(4) = 16\n\\]\nDann:\n\\[\n\\text{Cov}(X, Y) = \\frac{16 + 0 + 0 + 4 + 16}{5} = \\frac{36}{5} = 7.2\n\\]\nInterpretation der Kovarianz:\nDie Kovarianz ((X, Y) = 7.2) ist positiv. Das bedeutet, dass \\(X\\) und \\(Y\\) positiv korreliert sind. Wenn die Werte von \\(X\\) steigen, tendieren auch die Werte von \\(Y\\) dazu, zu steigen, und umgekehrt. Eine positive Kovarianz zeigt somit eine gemeinsame Bewegungsrichtung der beiden Variablen an.\n\n\n\n\n\n\n\n2. Beschreibung von Skalenniveaus von Variablen\nAufgabe:\nBestimme das Skalenniveau (nominal, ordinal, metrisch) der folgenden Variablen:\na) Geschlecht (m√§nnlich, weiblich, divers)\nb) Schulnoten (1, 2, 3, 4, 5, 6)\nc) Temperatur in Celsius\nd) Lieblingsfarbe (rot, blau, gr√ºn, etc.)\ne) K√∂rpergr√∂√üe in cm\n\n\n\n\n\n\nMusterl√∂sung\n\n\n\n\n\nMusterl√∂sung\n\nGeschlecht: Nominal (Kategorien ohne Reihenfolge)\n\nSchulnoten: Ordinal (Kategorien mit Reihenfolge, aber ungleiche Abst√§nde)\n\nTemperatur in Celsius: Metrisch (kontinuierliche Werte mit gleichen Abst√§nden, aber kein absoluter Nullpunkt)\n\nLieblingsfarbe: Nominal (Kategorien ohne Reihenfolge)\n\nK√∂rpergr√∂√üe in cm: Metrisch (kontinuierliche Werte mit wahrem Nullpunkt)\n\n\n\n\n\n\n\n3. Berechnung von Wahrscheinlichkeiten mit Additions- und Multiplikationsregeln\nAufgabe:\nIn einem Kartenspiel mit 52 Karten (4 Farben, 13 Werte) ziehst du zwei Karten nacheinander ohne Zur√ºcklegen. Berechne:\na) Die Wahrscheinlichkeit, dass die erste Karte ein Ass ist.\nb) Die Wahrscheinlichkeit, dass die zweite Karte ein Ass ist, wenn die erste Karte ein Ass war.\nc) Die Wahrscheinlichkeit, dass beide Karten Asse sind.\n\n\n\n\n\n\nMusterl√∂sung\n\n\n\n\n\nMusterl√∂sung\n\nErste Karte ein Ass:\n\nEs gibt 4 Asse in einem Deck mit 52 Karten. Das Ereignis \\(A_1\\) (erste Karte ist ein Ass) hat die Wahrscheinlichkeit:\n\\[\nP(A_1) = \\frac{\\text{Anzahl der Asse}}{\\text{Gesamtanzahl der Karten}} = \\frac{4}{52} = \\frac{1}{13} \\approx 0.0769\n\\]\n\nZweite Karte ein Ass, wenn erste Karte ein Ass war:\nNachdem die erste Karte ein Ass war, bleiben 3 Asse und 51 Karten im Deck. Das Ereignis \\(A_2\\) (zweite Karte ist ein Ass) unter der Bedingung \\(A_1\\) hat die bedingte Wahrscheinlichkeit:\n\\[\nP(A_2 \\mid A_1) = \\frac{\\text{Anzahl der verbleibenden Asse}}{\\text{Anzahl der verbleibenden Karten}} = \\frac{3}{51} = \\frac{1}{17} \\approx 0.0588\n\\]\n\nFormalisierung: Die bedingte Wahrscheinlichkeit ist definiert als:\n\\[\nP(A_2 \\mid A_1) = \\frac{P(A_1 \\cap A_2)}{P(A_1)}\n\\]\nDabei ist \\(P(A_1 \\cap A_2)\\) die Wahrscheinlichkeit, dass beide Karten Asse sind (siehe c)), und \\(P(A_1) = \\frac{4}{52}\\).\n\nBeide Karten sind Asse:\nDas Ereignis, dass beide Karten Asse sind, ist die Schnittmenge \\(A_1 \\cap A_2\\). Nach der Multiplikationsregel f√ºr abh√§ngige Ereignisse:\n\\[\nP(A_1 \\cap A_2) = P(A_1) \\cdot P(A_2 \\mid A_1) = \\frac{1}{13} \\cdot \\frac{1}{17} = \\frac{1}{221} \\approx 0.0045\n\\]\n\nAlternative Berechnung:\n- Erste Karte ein Ass: \\(\\frac{4}{52}\\).\n- Zweite Karte ein Ass (nach Ziehen eines Asses): \\(\\frac{3}{51}\\).\n- Gesamtwahrscheinlichkeit:\n\\[\n\\frac{4}{52} \\cdot \\frac{3}{51} = \\frac{12}{2652} = \\frac{1}{221}\n\\]\n\n\n\n\n\n\n4. Berechnung von bedingten Wahrscheinlichkeiten\nAufgabe:\nIn einer Firma sind 60 % der Angestellten m√§nnlich. 30 % der m√§nnlichen Angestellten und 40 % der weiblichen Angestellten haben einen Hochschulabschluss. Berechne die Wahrscheinlichkeit, dass ein zuf√§llig ausgew√§hlter Angestellter mit Hochschulabschluss m√§nnlich ist.\n\n\n\n\n\n\nMusterl√∂sung\n\n\n\n\n\nMusterl√∂sung\nGegeben:\n- \\(P(M) = 0.6\\): 60 % der Angestellten sind m√§nnlich.\n- \\(P(W) = 0.4\\): 40 % der Angestellten sind weiblich (da \\(P(M) + P(W) = 1\\)).\n- \\(P(H \\mid M) = 0.3\\): 30 % der m√§nnlichen Angestellten haben einen Hochschulabschluss.\n- \\(P(H \\mid W) = 0.4\\): 40 % der weiblichen Angestellten haben einen Hochschulabschluss.\nGesucht:\n- \\(P(M \\mid H)\\): Wahrscheinlichkeit, dass ein Angestellter m√§nnlich ist, unter der Bedingung, dass er einen Hochschulabschluss hat.\n\nSchritt-f√ºr-Schritt-Berechnung\n\n\n1. Definition der Ereignisse\n\n\\(M\\): Der Angestellte ist m√§nnlich.\n\n\\(W\\): Der Angestellte ist weiblich.\n\n\\(H\\): Der Angestellte hat einen Hochschulabschluss.\n\n\n\n2. Anwendung des Satzes von Bayes\nDie gesuchte Wahrscheinlichkeit \\(P(M \\mid H)\\) wird mit dem Satz von Bayes berechnet:\n\\[\nP(M \\mid H) = \\frac{P(H \\mid M) \\cdot P(M)}{P(H)}\n\\]\nHierbei ist \\(P(H)\\) die Gesamtwahrscheinlichkeit, dass ein Angestellter einen Hochschulabschluss hat, unabh√§ngig vom Geschlecht.\n\n\n3. Berechnung von \\(P(H)\\)\nDa ein Angestellter entweder m√§nnlich oder weiblich ist (disjunkte und vollst√§ndige Partition), verwenden wir die Formel der totalen Wahrscheinlichkeit:\n\\[\nP(H) = P(H \\mid M) \\cdot P(M) + P(H \\mid W) \\cdot P(W)\n\\]\nEinsetzen der gegebenen Werte:\n\\[\nP(H) = (0.3 \\cdot 0.6) + (0.4 \\cdot 0.4)\n\\]\n- M√§nnlicher Beitrag: \\(0.3 \\cdot 0.6 = 0.18\\)\n- Weiblicher Beitrag: \\(0.4 \\cdot 0.4 = 0.16\\)\n- Summe:\n\\[\nP(H) = 0.18 + 0.16 = 0.34\n\\]\nAlso: 34 % aller Angestellten haben einen Hochschulabschluss.\n\n\n4. Berechnung von \\(P(M \\mid H)\\)\nNun setzen wir die Werte in die Bayes-Formel ein:\n\\[\nP(M \\mid H) = \\frac{P(H \\mid M) \\cdot P(M)}{P(H)} = \\frac{0.3 \\cdot 0.6}{0.34} = \\frac{0.18}{0.34}\n\\]\nBerechnung:\n\\[\n\\frac{0.18}{0.34} = \\frac{18}{34} = \\frac{9}{17} \\approx 0.52941176\n\\]\nGerundet:\n\\[\nP(M \\mid H) \\approx 0.5294 \\text{ oder } 52.94 \\, \\%\n\\]\n\n\nAlternative Sichtweise: Intuitives Beispiel\nUm die L√∂sung zu veranschaulichen, nehmen wir an, die Firma hat 100 Angestellte:\n- 60 Angestellte sind m√§nnlich (\\(0.6 \\cdot 100\\)).\n- 40 Angestellte sind weiblich (\\(0.4 \\cdot 100\\)).\n- Von den 60 m√§nnlichen Angestellten haben 30 % einen Hochschulabschluss:\n\\[\n  0.3 \\cdot 60 = 18 \\text{ m√§nnliche Angestellte mit Hochschulabschluss}\n  \\]\n- Von den 40 weiblichen Angestellten haben 40 % einen Hochschulabschluss:\n\\[\n  0.4 \\cdot 40 = 16 \\text{ weibliche Angestellte mit Hochschulabschluss}\n  \\]\n- Insgesamt haben \\(18 + 16 = 34\\) Angestellte einen Hochschulabschluss.\n- Die Wahrscheinlichkeit, dass ein Angestellter mit Hochschulabschluss m√§nnlich ist:\n\\[\n  \\frac{\\text{M√§nnliche Angestellte mit Hochschulabschluss}}{\\text{Gesamtanzahl Angestellte mit Hochschulabschluss}} = \\frac{18}{34} = \\frac{9}{17} \\approx 0.5294\n  \\]\nDies best√§tigt die formale Berechnung.\n\n\nBaumdiagramm zur Visualisierung\nEin Baumdiagramm verdeutlicht die Wahrscheinlichkeiten:\n1. Erster Ast: Geschlecht\n- M√§nnlich: \\(P(M) = 0.6\\)\n- Weiblich: \\(P(W) = 0.4\\)\n2. Zweiter Ast: Hochschulabschluss\n- F√ºr m√§nnlich:\n- Hochschulabschluss: \\(P(H \\mid M) = 0.3\\)\n- Kein Hochschulabschluss: \\(P(\\neg H \\mid M) = 0.7\\)\n- F√ºr weiblich:\n- Hochschulabschluss: \\(P(H \\mid W) = 0.4\\)\n- Kein Hochschulabschluss: \\(P(\\neg H \\mid W) = 0.6\\)\n3. Pfadwahrscheinlichkeiten:\n- M√§nnlich und Hochschulabschluss: \\(P(M \\cap H) = 0.3 \\cdot 0.6 = 0.18\\)\n- Weiblich und Hochschulabschluss: \\(P(W \\cap H) = 0.4 \\cdot 0.4 = 0.16\\)\n4. Gesamtwahrscheinlichkeit f√ºr Hochschulabschluss:\n\\[\n   P(H) = 0.18 + 0.16 = 0.34\n   \\]\n5. Bedingte Wahrscheinlichkeit:\n\\[\n   P(M \\mid H) = \\frac{P(M \\cap H)}{P(H)} = \\frac{0.18}{0.34} \\approx 0.5294\n   \\]\n\n\nInterpretation\nDie Wahrscheinlichkeit, dass ein zuf√§llig ausgew√§hlter Angestellter mit Hochschulabschluss m√§nnlich ist, betr√§gt etwa 52.94 %. Obwohl 60 % der Angestellten m√§nnlich sind, ist der Anteil der m√§nnlichen Angestellten mit Hochschulabschluss (30 %) geringer als der der weiblichen (40 %), was die Wahrscheinlichkeit nahe an 50 % bringt.\n\n\n\n\n\n\n\n5. Umrechnen von Werten einer Normalverteilung zur Standardnormalverteilung\nAufgabe:\nEine Zufallsvariable \\(X\\) ist normalverteilt mit \\(\\mu = 50\\) und \\(\\sigma = 10\\). Berechne die Wahrscheinlichkeit, dass \\(X &lt; 60\\), indem du den Wert in die Standardnormalverteilung umrechnest und aus einer Standardnormalverteilungstabelle abliest.\n\n\n\n\n\n\nMusterl√∂sung\n\n\n\n\n\nMusterl√∂sung\nStandardisiere \\(X = 60\\):\n\\[\nZ = \\frac{X - \\mu}{\\sigma} = \\frac{60 - 50}{10} = 1\n\\] In der Tabelle f√ºr \\(Z = 1.00\\) findest du \\(\\Phi(1.00) = 0.8413\\).\nAlso:\n\\[\nP(X &lt; 60) = P(Z &lt; 1) = 0.8413\n\\]\n\n\n\n\n\n\n6. Ablesen von Quantilen aus einem Boxplot\nAufgabe:\nErstelle mit Python einen Boxplot f√ºr eine Verteilung mit folgenden Eigenschaften: Die Box erstreckt sich von 20 bis 60, der Median liegt bei 40, und die Whisker reichen von 10 bis 70. Sieh dir den Boxplot in Figure¬†1 an und bestimme das 25 %-Quantil, das 50 %-Quantil (Median) und das 75 %-Quantil.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Daten generieren, die den Anforderungen entsprechen\nnp.random.seed(42)\ndata = np.concatenate([\n    np.random.uniform(10, 20, 25),  # Unterer Whisker bis Q1\n    np.random.uniform(20, 40, 25),  # Q1 bis Median\n    np.random.uniform(40, 60, 25),  # Median bis Q3\n    np.random.uniform(60, 70, 25)   # Q3 bis oberer Whisker\n])\n\nplt.boxplot(data, vert=True, patch_artist=True, showmeans=False)\nplt.title('Boxplot der Verteilung')\nplt.ylabel('Werte')\nplt.grid(alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\nFigure¬†1: Boxplot zur Verteilung mit spezifizierten Quantilen.\n\n\n\n\n\n\n\n\n\n\n\nMusterl√∂sung\n\n\n\n\n\nMusterl√∂sung\n\n25 %-Quantil (untere Quartil): Beginn der Box bei 20\n\n50 %-Quantil (Median): Linie in der Box bei 40\n\n75 %-Quantil (obere Quartil): Ende der Box bei 60",
    "crumbs": [
      "Statistik",
      "√úbungsaufgaben: Wahrscheinlichkeitsrechnung"
    ]
  },
  {
    "objectID": "regression/index.html",
    "href": "regression/index.html",
    "title": "Regressions-Analyse",
    "section": "",
    "text": "Im Allgemeinen kann die Regressionsanalyse als eine Sammlung von Werkzeugen verstanden werden, die dazu verwendet werden, eine Beziehung zwischen einer abh√§ngigen Variable \\(Y\\) (auch Zielvariable, Antwortvariable oder Label genannt) und der unabh√§ngigen Variable \\(X\\) (auch Regressor, Pr√§diktoren, Kovariaten, erkl√§rende Variable oder Feature genannt) zu sch√§tzen oder festzustellen.\nWenn wir eine Regressionsfunktion \\(f\\) als Modell f√ºr die Beziehung zwischen \\(X\\) und \\(Y\\) annehmen, k√∂nnen wir die Regressionsanalyse als die Suche nach den Parametern \\(\\theta\\) verstehen, die die Funktion \\(f\\) am besten an die Daten anpassen.\nDas Problem der Regressionsanalyse kann also wie folgt formuliert werden: \\[\nY = f(X, \\theta) + \\epsilon,\n\\]\nwobei \\(\\theta\\) durch die Optimierung f√ºr eine gute Anpassung von \\(f\\) an die Daten gefunden wird. In der Regel erhalten wir dabei einen Fehlerterm \\(\\epsilon\\), der ‚Äì wenn wir Gl√ºck haben ‚Äì normalverteilt mit einem Erwartungswert von null und konstanter Varianz ist.\nRegessionen sind ein m√§chtiges Werkzeug f√ºr die Interpetation von Daten und die Vorhersage von Werten. In der Praxis gibt es viele verschiedene Arten von Regressionsmodellen, die sich in ihrer Komplexit√§t und den Annahmen, die sie machen, unterscheiden. In diesem Kapitel werden wir uns zun√§chst auf die lineare Regression konzentrieren, die eine der einfachsten und am h√§ufigsten verwendeten Formen der Regressionsanalyse ist und sich f√ºr Zusammenh√§nge zwischen einer abh√§ngigen und einer unabh√§ngigen Variablen mit intervallskalierten Daten eignet.\nZun√§chst werden wir eine einfache lineare Regression mit einer einzigen unabh√§ngigen Variable durchf√ºhren. Dieses Modell beschreibt eine lineare Beziehung zwischen der abh√§ngigen Variable \\(Y\\) und einer unabh√§ngigen Variable \\(X\\). Anschlie√üend erweitern wir das Modell, um mehrere unabh√§ngige Variablen sowie kategorische Variablen zu ber√ºcksichtigen. Um diese komplexeren Modelle mathematisch effizient zu formulieren, werden wir die Matrix-Schreibweise verwenden, die eine kompakte Darstellung der Beziehungen zwischen mehreren Variablen erm√∂glicht.\nIm n√§chsten Abschnitt behandeln wir fortgeschrittene Konzepte wie den Intercept (Achsenabschnitt) und die Regularisierung. Der Intercept repr√§sentiert den Wert der abh√§ngigen Variable, wenn alle unabh√§ngigen Variablen gleich null sind, und spielt eine zentrale Rolle in der Interpretation des Modells. Regularisierungstechniken, wie z. B. Ridge- oder Lasso-Regression, werden eingef√ºhrt, um √úberanpassung (Overfitting) zu vermeiden und die Stabilit√§t sowie die Vorhersagegenauigkeit des Modells zu verbessern, insbesondere bei hochdimensionalen Daten.\nEin weiterer Schwerpunkt liegt auf der Diskussion statistischer Lernverfahren und Resampling-Techniken. Statistische Lernverfahren umfassen Ans√§tze, wie Modelle aus Daten lernen k√∂nnen, und deren Evaluation. Resampling-Methoden, wie z. B. Kreuzvalidierung oder Bootstrapping, werden vorgestellt, um die Robustheit und Verallgemeinerungsf√§higkeit von Regressionsmodellen zu √ºberpr√ºfen und die Modellleistung auf neuen Daten zu sch√§tzen.\nLogistische Regression f√ºr Klassifikationsaufgaben Abschlie√üend betrachten wir die logistische Regression als ein Beispiel f√ºr ein Regressionsmodell, das f√ºr Klassifikationsaufgaben verwendet wird. Im Gegensatz zur linearen Regression, die kontinuierliche Zielvariablen vorhersagt, modelliert die logistische Regression die Wahrscheinlichkeit, dass eine Beobachtung zu einer bestimmten Kategorie geh√∂rt. Dies macht sie besonders n√ºtzlich f√ºr bin√§re Klassifikationsprobleme, wie z. B. die Vorhersage, ob ein Ereignis eintritt oder nicht.",
    "crumbs": [
      "Regressions-Analyse"
    ]
  },
  {
    "objectID": "regression/basic_linear_regression.html",
    "href": "regression/basic_linear_regression.html",
    "title": "7¬† Lineare Regression",
    "section": "",
    "text": "7.1 Linare Regression mit einer Variablen\nDie lineare Regression ist ein statistisches Verfahren, das dazu verwendet wird, die Beziehung zwischen einer abh√§ngigen Variable \\(Y\\) und einer oder mehreren unabh√§ngigen Variablen \\(X\\) zu modellieren. Das Modell der linearen Regression kann als eine lineare Beziehung zwischen \\(Y\\) und \\(X\\) interpretiert werden, die durch die Gleichung\n\\[\nY = f(X, \\theta) + \\epsilon,\n\\]\nF√ºr den Fall, dass wird nur eine Variable \\(X\\) betrachten, wird die lineare Regression durch die Gleichung: \\[\nY = \\beta_0 + \\beta_1 X + \\epsilon\n\\]\nbeschrieben wird. Hierbei sind \\(\\beta_0\\) und \\(\\beta_1\\) die Parameter \\(\\theta\\) des Modells, die die Steigung und den Achsenabschnitt der Regressionsgeraden bestimmen, und \\(\\epsilon\\) ist der Fehlerterm, der die Abweichung der beobachteten Werte von den vorhergesagten Werten beschreibt.\nIn diesem Fall haben wir einen einfachen Zusammenhang zwischen nur zwei Variablen \\(X\\) und \\(Y\\). In der Praxis kann die lineare Regression jedoch auch auf mehrere unabh√§ngige Variablen erweitert werden, um komplexere Beziehungen zu modellieren.",
    "crumbs": [
      "Regressions-Analyse",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Lineare Regression</span>"
    ]
  },
  {
    "objectID": "regression/basic_linear_regression.html#linare-regression-mit-einer-variablen",
    "href": "regression/basic_linear_regression.html#linare-regression-mit-einer-variablen",
    "title": "7¬† Lineare Regression",
    "section": "",
    "text": "√úberwachtes Lernen\n\n\n\nDie lineare Regression ist ein Beispiel f√ºr ein √ºberwachtes Lernverfahren, bei dem wir ein Modell auf Basis von gelabelten Trainingsdaten erstellen. Das bedeutet, dass wir sowohl die unabh√§ngigen Variablen \\(X\\) als auch die abh√§ngige Variable \\(Y\\) kennen und das Modell so anpassen, dass es die Beziehung zwischen \\(X\\) und \\(Y\\) m√∂glichst gut abbildet. Wir k√∂nnen dabei √ºberwachen wie gut sich das Modell mit verschiedenen Parametern an die Daten anpasst und das Modell so optimieren, dass es die besten Vorhersagen liefert.\n\n\n\n\n\n\n\n\nParametrische Modelle\n\n\n\nBei der linearen Regression handelt es sich um ein parametrisches Modell, bei dem wir eine bestimmte Form der Beziehung zwischen \\(X\\) und \\(Y\\) annehmen und die Parameter \\(\\theta\\) des Modells so anpassen, dass es die Daten m√∂glichst gut abbildet. Im Gegensatz dazu gibt es auch nicht-parametrische Modelle, bei denen keine spezifische Form der Beziehung angenommen wird und das Modell flexibler ist, aber auch mehr Daten ben√∂tigt, um zuverl√§ssige Vorhersagen zu treffen (z.B. Neuronale Netze).\n\n\n\n\n\n\n\n\nInterpreation und Prognose\n\n\n\nEs gibt grundlegend zwei Zielstellungen, die mit der linearen Regression, und auch vielen anderen Modellen, verfolgt werden k√∂nnen:\n\nInterpretation: Die Interpretation der Beziehung zwischen den Variablen \\(X\\) und \\(Y\\) steht im Vordergrund. Hierbei geht es darum, die Auswirkung der unabh√§ngigen Variablen auf die abh√§ngige Variable zu verstehen und zu erkl√§ren und ggf. Hypothesen zu testen.\nPrognose: Die Vorhersage von Werten der abh√§ngigen Variablen \\(Y\\) auf Basis der unabh√§ngigen Variablen \\(X\\) steht im Vordergrund. Hierbei geht es darum, die Genauigkeit der Vorhersagen zu maximieren und die Modellg√ºte zu optimieren.\n\nWichtig ist es, sich vor der Anwendung eines Modells klar zu machen, welches Ziel verfolgt wird.",
    "crumbs": [
      "Regressions-Analyse",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Lineare Regression</span>"
    ]
  },
  {
    "objectID": "regression/basic_linear_regression.html#beispiel-vorhersage-von-zeitungenverk√§ufen",
    "href": "regression/basic_linear_regression.html#beispiel-vorhersage-von-zeitungenverk√§ufen",
    "title": "7¬† Lineare Regression",
    "section": "7.2 Beispiel: Vorhersage von Zeitungenverk√§ufen",
    "text": "7.2 Beispiel: Vorhersage von Zeitungenverk√§ufen\n\n\n\n\n\nLineare Regression f√ºr die drei unabh√§ngigen Variablen\n\n\n\n\n\n\n\n\n\n\nWenn wir diese Grafik stellen, welche Fragen k√∂nnen wir uns stellen? Fallen diese Fragen in die Kategorie der Interpretation oder der Prognose?\n\n\n\n\nInterpretation:\n\nGibt es einen Zusammenhang zwischen den Werbeausgaben und den Verk√§ufen?\nWie stark ist der Zusammenhang?\nWelche Ausgaben haben den gr√∂√üten Einfluss auf die Verk√§ufe?\nIst die Beziehung linear oder nicht-linear?\nGibt es Synergieeffekte zwischen den verschiedenen Werbekan√§len?\n\nPrognose:\n\nWie gut k√∂nnen wir die Verk√§ufe vorhersagen, wenn wir die Werbeausgaben kennen?\nWie genau sind unsere Vorhersagen?\n\n\n\n\nWenn wir das lineare Modell aufstellen k√∂nnte, wir das folgenderma√üen aussehen:\n\\[\nY ‚âà Œ≤_0 + Œ≤_1 \\cdot X_1,\n\\]\nbzw. in unserem Beispiel:\n\\[\n\\text{sales} ‚âà Œ≤_0 + Œ≤_1 \\cdot \\text{TV}\n\\]\nDie Parameter \\(Œ≤_0\\) und \\(Œ≤_1\\) haben dabei bestimmt Bedeutungen:\n\n\\(Œ≤_0\\) ‚Ä¶ intercept oder y-Achsenabschnitt\n\\(Œ≤_1\\) ‚Ä¶ slope oder Steigung\n\nAls n√§chstes m√ºssen wir einen Weg finden die Parameter \\(Œ≤_0\\) und \\(Œ≤_1\\) zu sch√§tzen. Daf√ºr gibt es verschiedene Methoden. Die Klassische Methode ist die Methode der kleinsten Quadrate (OLS). Diese Methode minimiert die Summe der quadrierten Abweichungen zwischen den beobachteten Werten und den vorhergesagten Werten.\n\\[Y ‚âà \\hat{Œ≤}_0 + \\hat{Œ≤}_1 \\cdot X_1\\]",
    "crumbs": [
      "Regressions-Analyse",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Lineare Regression</span>"
    ]
  },
  {
    "objectID": "regression/basic_linear_regression.html#methode-der-kleinsten-quadrate-ols",
    "href": "regression/basic_linear_regression.html#methode-der-kleinsten-quadrate-ols",
    "title": "7¬† Lineare Regression",
    "section": "7.3 Methode der kleinsten Quadrate (OLS)",
    "text": "7.3 Methode der kleinsten Quadrate (OLS)\nDie Methode der kleinsten Quadrate (OLS) ist ein Verfahren zur Sch√§tzung der Parameter eines linearen Regressionsmodells, das die Summe der quadrierten Abweichungen zwischen den beobachteten Werten und den vorhergesagten Werten minimiert.\nAls erstes definieren wir den Fehler \\(\\epsilon_i\\) f√ºr jeden Datenpunkt \\(i\\) als die Differenz zwischen dem beobachteten Wert \\(y_i\\) und dem vorhergesagten Wert \\(\\hat{y}_i\\):\n\\[\n\\epsilon_i = y_i - \\hat{y}_i.\n\\]\nDie Methode der kleinsten Quadrate zielt darauf ab, die Summe der quadrierten Fehler zu minimieren, d.h. die Summe der quadrierten Abweichungen zwischen den beobachteten Werten und den vorhergesagten Werten:\n\\[\n\\text{RSS} = \\sum_{i=1}^{n} \\epsilon_i^2 = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2.\n\\]\nAuf Englisch wird die Summe als Residual Sum of Squares (RSS) bezeichnet. Die Methode der kleinsten Quadrate sucht nun die Parameter \\(\\hat{\\beta}_0\\) und \\(\\hat{\\beta}_1\\), die die Summe der quadrierten Fehler minimieren:\n\\[\n\\hat{\\beta}_0, \\hat{\\beta}_1 = \\text{argmin}_{\\beta_0, \\beta_1} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2.\n\\]\n\n\n\n\n\n\nKostenfunktion \\(J\\)\n\n\n\nDie Methode der kleinsten Quadrate kann auch als Optimierungsproblem formuliert werden, bei dem wir eine Kostenfunktion (Cost Function) minimieren, die die Summe der quadrierten Fehler zwischen den beobachteten und den vorhergesagten Werten beschreibt. Die Kostenfunktion ist in diesem Fall die Residual Sum of Squares (RSS), die wir minimieren, um die besten Parameter f√ºr das Modell zu finden. Kostenfunktionen h√§ngen in der Regel von Modell, den Daten und den Parametern ab und dienen dazu, die G√ºte des Modells zu bewerten und zu optimieren. Da wir meist ein festes Modell und Daten haben und nur die Parameter anpassen, k√∂nnen wir die Kostenfunktion als Funktion der Parameter betrachten, die wir minimieren wollen. Ein eng verwandtes Konzept ist die Verlustfunktion (Loss Function) oder Accuracy Measures, die in der Regel den Fehler zwischen den beobachteten und den vorhergesagten Werten beschreiben Die Kostenfunktion ist in der Regel eine Summe der Verlustfunktionen √ºber alle Datenpunkte.\n\n\n\n\n\n\n\n\nSch√§tzung der Parameter mittels Odinary Least Squares (OLS)\n\n\n\nDie Methode der kleinsten Quadrate (Ordinary Least Squares, OLS) wird verwendet, um die Parameter einer linearen Regression zu sch√§tzen. Ziel ist es, die Gerade\n\\[ y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i, \\]\nwobei \\(y_i\\) die beobachtete abh√§ngige Variable, \\(\\beta_0\\) der Achsenabschnitt, \\(\\beta_1\\) die Steigung und \\(\\varepsilon_i\\) der Fehlerterm ist, so zu bestimmen, dass die Summe der quadrierten Fehler (Residual Sum of Squares, RSS) minimiert wird:\n\\[ RSS = \\sum_{i=1}^{n} \\left(y_i - \\hat{y}i\\right)^2 = \\sum{i=1}^{n} \\left(y_i - \\beta_0 - \\beta_1 x_i\\right)^2. \\]\nHierbei ist \\(\\hat{y}_i = \\beta_0 + \\beta_1 x_i\\) die gesch√§tzte abh√§ngige Variable. Im Folgenden leiten wir die optimalen Parameter \\(\\hat{\\beta}_0\\) und \\(\\hat{\\beta}_1\\) Schritt f√ºr Schritt her.\n\nBerechnung der Mittelwerte\n\nF√ºr die Herleitung der Parameter ist es hilfreich, zun√§chst die Mittelwerte der unabh√§ngigen Variable \\(x\\) und der abh√§ngigen Variable \\(y\\) zu berechnen:\n\\[ \\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i, \\quad \\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_i. \\]\nDiese Mittelwerte werden sp√§ter verwendet, um die Formeln f√ºr \\(\\hat{\\beta}_0\\) und \\(\\hat{\\beta}_1\\) zu vereinfachen.\n\nMinimierung der Kostenfunktion nach \\(\\hat{\\beta}_1\\)\n\nUm den optimalen Wert f√ºr \\(\\hat{\\beta}_1\\) zu finden, minimieren wir die Kostenfunktion \\(RSS\\) durch Ableiten nach \\(\\beta_1\\) und Setzen der Ableitung gleich null:\n\\[ \\frac{\\partial RSS}{\\partial \\beta_1} = 0. \\]\nDie Kostenfunktion lautet:\n\\[ RSS = \\sum_{i=1}^{n} \\left(y_i - \\beta_0 - \\beta_1 x_i\\right)^2. \\]\nDie partielle Ableitung nach \\(\\beta_1\\) erfordert die Anwendung der Kettenregel.\n\n\n\n\n\n\nAnwendung der Kettenregel\n\n\n\nDie Kettenregel ist ein zentrales Werkzeug in der Optimierung, da die Kostenfunktion \\(RSS\\) eine zusammengesetzte Funktion ist. F√ºr eine Funktion \\(f(g(x)) = [g(x)]^2\\), wobei \\(g(x) = y_i - \\beta_0 - \\beta_1 x_i\\), lautet die Ableitung:\n\\[ \\frac{d}{dx} [g(x)]^2 = 2 \\cdot g(x) \\cdot g'(x). \\]\nIn unserem Fall ist \\(g(\\beta_1) = y_i - \\beta_0 - \\beta_1 x_i\\), und die Ableitung von \\(g\\) nach \\(\\beta_1\\) ergibt:\n\\[ \\frac{\\partial}{\\partial \\beta_1} (y_i - \\beta_0 - \\beta_1 x_i) = -x_i. \\]\nSomit wird die partielle Ableitung der Kostenfunktion:\n\\[ \\frac{\\partial RSS}{\\partial \\beta_1} = \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\beta_1} \\left(y_i - \\beta_0 - \\beta_1 x_i\\right)^2 = \\sum_{i=1}^{n} 2 \\left(y_i - \\beta_0 - \\beta_1 x_i\\right) \\cdot (-x_i). \\]\nDies vereinfacht sich zu:\n\\[ \\frac{\\partial RSS}{\\partial \\beta_1} = -2 \\sum_{i=1}^{n} x_i \\left(y_i - \\beta_0 - \\beta_1 x_i\\right). \\]\n\n\nSetzen wir die Ableitung gleich null:\n\\[ -2 \\sum_{i=1}^{n} x_i \\left(y_i - \\beta_0 - \\beta_1 x_i\\right) = 0. \\]\nDividieren durch \\(-2\\) ergibt:\n\\[ \\sum_{i=1}^{n} x_i \\left(y_i - \\beta_0 - \\beta_1 x_i\\right) = 0. \\]\nAusklammern und Umformen f√ºhrt zu:\n\\[ \\sum_{i=1}^{n} x_i y_i - \\beta_0 \\sum_{i=1}^{n} x_i - \\beta_1 \\sum_{i=1}^{n} x_i^2 = 0. \\]\nMit \\(\\sum_{i=1}^{n} x_i = n \\bar{x}\\) wird dies:\n\\[ \\sum_{i=1}^{n} x_i y_i - \\beta_0 n \\bar{x} - \\beta_1 \\sum_{i=1}^{n} x_i^2 = 0. \\]\n\nMinimierung der Kostenfunktion nach \\(\\hat{\\beta}_0\\)\n\nAnalog leiten wir die Kostenfunktion nach \\(\\beta_0\\) ab:\n\\[ \\frac{\\partial RSS}{\\partial \\beta_0} = 0. \\]\nDie partielle Ableitung lautet:\n\\[ \\frac{\\partial RSS}{\\partial \\beta_0} = \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\beta_0} \\left(y_i - \\beta_0 - \\beta_1 x_i\\right)^2 = -2 \\sum_{i=1}^{n} \\left(y_i - \\beta_0 - \\beta_1 x_i\\right). \\]\nSetzen wir gleich null:\n\\[ -2 \\sum_{i=1}^{n} \\left(y_i - \\beta_0 - \\beta_1 x_i\\right) = 0. \\]\nDividieren durch \\(-2\\) ergibt:\n\\[ \\sum_{i=1}^{n} \\left(y_i - \\beta_0 - \\beta_1 x_i\\right) = 0. \\]\nDies f√ºhrt zu:\n\\[ \\sum_{i=1}^{n} y_i - \\beta_0 \\sum_{i=1}^{n} 1 - \\beta_1 \\sum_{i=1}^{n} x_i = 0. \\]\nDa \\(\\sum_{i=1}^{n} 1 = n\\) und \\(\\sum_{i=1}^{n} x_i = n \\bar{x}\\), folgt:\n\\[ \\sum_{i=1}^{n} y_i - n \\beta_0 - \\beta_1 n \\bar{x} = 0. \\]\nUmformen nach \\(\\beta_0\\) ergibt:\n\\[ n \\beta_0 = \\sum_{i=1}^{n} y_i - \\beta_1 n \\bar{x}. \\]\nTeilen durch \\(n\\) liefert:\n\\[ \\beta_0 = \\frac{1}{n} \\sum_{i=1}^{n} y_i - \\beta_1 \\frac{1}{n} \\sum_{i=1}^{n} x_i = \\bar{y} - \\beta_1 \\bar{x}. \\]\n\nBerechnung von \\(\\hat{\\beta}_1\\)\n\nUm \\(\\hat{\\beta}_1\\) zu bestimmen, l√∂sen wir die Gleichung aus Schritt 2 weiter auf. Wir verwenden die Formel f√ºr \\(\\hat{\\beta}_1\\), die sich aus der Minimierung ergibt. Eine alternative und intuitivere Darstellung ist:\n\\[ \\hat{\\beta}1 = \\frac{\\sum{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}. \\]\nDiese Formel entspricht der Kovarianz von \\(x\\) und \\(y\\) geteilt durch die Varianz von \\(x\\). Die Kovarianz misst, wie stark \\(x\\) und \\(y\\) gemeinsam variieren, w√§hrend die Varianz die Streuung von \\(x\\) beschreibt.\n\nBerechnung von \\(\\hat{\\beta}_0\\)\n\nSetzen wir \\(\\hat{\\beta}_1\\) in die Gleichung f√ºr \\(\\beta_0\\) ein:\n\\[ \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}. \\]\nDies stellt sicher, dass die Regressionsgerade durch den Mittelpunkt der Daten \\((\\bar{x}, \\bar{y})\\) verl√§uft.\n\nErgebnis\n\nDie gesch√§tzte Regressionsgerade lautet:\n\\[ \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x. \\]\nMit den berechneten Werten f√ºr \\(\\hat{\\beta}_0\\) und \\(\\hat{\\beta}_1\\) ist die OLS-Sch√§tzung der Parameter abgeschlossen. Diese Parameter minimieren die Summe der quadrierten Fehler und liefern das beste lineare Modell f√ºr die gegebenen Daten.",
    "crumbs": [
      "Regressions-Analyse",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Lineare Regression</span>"
    ]
  },
  {
    "objectID": "regression/basic_linear_regression.html#beispiel-lineare-regression-mit-einer-variablen",
    "href": "regression/basic_linear_regression.html#beispiel-lineare-regression-mit-einer-variablen",
    "title": "7¬† Lineare Regression",
    "section": "7.4 Beispiel: Lineare Regression mit einer Variablen",
    "text": "7.4 Beispiel: Lineare Regression mit einer Variablen\nWir simulieren einen Datensatz mit einer linearen Beziehung zwischen \\(X\\) (‚Ç¨ ausgegeben f√ºr TV-Werbung) und \\(Y\\) (Verk√§ufe von Zeitungen). In diesem Fall kennen wir den Zufalls-Prozess, der die Daten generiert hat, und k√∂nnen daher die wahren Parameter des Modells bestimmen. Die Variable \\(X\\) ist gleichm√§√üig zwischen 0 und 300 verteilt, und \\(Y\\) wird durch die Gleichung \\[\nY = 7 + 0.05 \\cdot X + \\epsilon\n\\]\ngeneriert, wobei \\(\\epsilon\\) ein normalverteilter Fehler mit einer Standardabweichung von \\(5\\) ist.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulate population data\nnp.random.seed(0)\nX = 300 * np.random.rand(100, 1)\nY = 7 + 0.05 * X + 5 * np.random.randn(100, 1)\n\n# Simulate sample data\nsample_size = 100\nidx = np.random.choice(100, sample_size, replace=False)\nX_sample = X[idx]\nY_sample = Y[idx]\n\n# Store data in a tidy dataframe\ndata = pd.DataFrame({\"X (TV advertising spending (‚Ç¨))\": X_sample.flatten(), \"Y (Sales)\": Y_sample.flatten()})\nprint(data.head())\n\n   X (TV advertising spending (‚Ç¨))  Y (Sales)\n0                       246.297969  14.750787\n1                       140.595360  22.596482\n2                        62.663027   6.953921\n3                       172.783949  21.579346\n4                        19.244249  12.709316\n\n\n\n# Plot data\nplt.scatter(X, Y)\nplt.xlabel(\"TV advertising spending (‚Ç¨)\")\nplt.ylabel(\"Sales\")\nplt.title(\"TV advertising vs. sales\")\nplt.show()\n\n\n\n\nScatter plot of TV advertising vs.¬†sales\n\n\n\n\nNun k√∂nnen wir unsere Formel f√ºr die OLS-Sch√§tzung der Parameter \\(\\beta_0\\) und \\(\\beta_1\\) anwenden:\n\n# Calculate means\nX_mean = np.mean(X_sample)\nY_mean = np.mean(Y_sample)\n\n# Calculate slope\nnumerator = np.sum((X_sample - X_mean) * (Y_sample - Y_mean))\ndenominator = np.sum((X_sample - X_mean) ** 2)\nbeta_1 = numerator / denominator\n\n# Calculate intercept\nbeta_0 = Y_mean - beta_1 * X_mean\n\nprint(f\"Slope (beta_1): {beta_1}\")\nprint(f\"Intercept (beta_0): {beta_0}\")\n\nSlope (beta_1): 0.04894891702336733\nIntercept (beta_0): 8.110755387236143\n\n\nIn der Zukunft werden wir stattdessen eines von zwei Paketen verwenden, die die OLS-Sch√§tzung automatisch durchf√ºhren: statsmodels oder scikit-learn.\n\n7.4.1 scikit-learn\nscikit-learn ist eine der bekanntesten Bibliotheken f√ºr maschinelles Lernen in Python. Sie bietet eine Vielzahl von Algorithmen und Werkzeugen f√ºr die Modellierung und Analyse von Daten. Wir werden es sp√§ter und im kommenden Semester noch ausf√ºhrlich verwenden.\n\nfrom sklearn.linear_model import LinearRegression\n\n# Defines what kind of model we want to use\nmodel = LinearRegression()\n\n# Fits the model to the data\nmodel.fit(X_sample, Y_sample)\n\n# Get slope and intercept from the models coefficients\nbeta_1_sklearn = model.coef_[0][0]\nbeta_0_sklearn = model.intercept_[0]\n\nprint(f\"Slope (beta_1): {beta_1_sklearn}\")\nprint(f\"Intercept (beta_0): {beta_0_sklearn}\")\n\nSlope (beta_1): 0.04894891702336733\nIntercept (beta_0): 8.110755387236143\n\n\n\n\n7.4.2 statsmodels\nstatsmodels ist eine Bibliothek f√ºr statistische Modellierung in Python. Sie bietet eine Vielzahl von statistischen Modellen und Tests, darunter auch die lineare Regression. Die Ausgabe von statsmodels ist oft detaillierter und enth√§lt mehr statistische Informationen als die von scikit-learn.\n\nimport statsmodels.api as sm\n\n# Fit linear regression model\nX_sm = sm.add_constant(X_sample)\nmodel_sm = sm.OLS(Y_sample, X_sm).fit()\n\n# Get slope and intercept\nprint(model_sm.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.419\nModel:                            OLS   Adj. R-squared:                  0.413\nMethod:                 Least Squares   F-statistic:                     70.80\nDate:                Tue, 03 Jun 2025   Prob (F-statistic):           3.29e-13\nTime:                        09:39:21   Log-Likelihood:                -302.46\nNo. Observations:                 100   AIC:                             608.9\nDf Residuals:                      98   BIC:                             614.1\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          8.1108      0.966      8.392      0.000       6.193      10.029\nx1             0.0489      0.006      8.414      0.000       0.037       0.060\n==============================================================================\nOmnibus:                       11.746   Durbin-Watson:                   2.121\nProb(Omnibus):                  0.003   Jarque-Bera (JB):                4.097\nSkew:                           0.138   Prob(JB):                        0.129\nKurtosis:                       2.047   Cond. No.                         319.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\n\n\nArbeit mit DataFrames und Formula-Strings\n\n\n\nAnstelle die Daten in einem Numpy-Array zu speichern, k√∂nnen wir auch ein Pandas DataFrame verwenden. Dies hat den Vorteil, dass wir die Spalten mit ihren Namen ansprechen k√∂nnen und so den Code besser lesbar machen. Zudem l√§ss sich die lineare Regression auch mit Hilfe von Formel-Strings durchf√ºhren, die die Beziehung zwischen den Variablen angeben.\n\nimport statsmodels.formula.api as smf\n\n# Create DataFrame\ndata = pd.DataFrame({\"TV\": X_sample.flatten(), \"Sales\": Y_sample.flatten()})\n# Fit linear regression model using formula string\nmodel_smf = smf.ols(\"Sales ~ TV\", data=data).fit()\n\n\n\nWenn wir die Ergebnisse der OLS-Sch√§tzung betrachten, sehen wir einige Zusatzinformationen, die uns helfen, die G√ºte des Modells zu bewerten\n\n7.4.2.1 Koeffizienten und deren Konfidenzintervalle\nWir k√∂nnen die gesch√§tzen Koeffizienten (coef) \\(\\hat{\\beta}_0\\) und \\(\\hat{\\beta}_1\\) sowie deren Konfidenzintervalle [0.025      0.975] ablesen. Es wird davon ausgegangen, dass die Sch√§tzer der Koeffizienten normalverteilt sind. Entsprechend sind die Konfidenzintervalle symmetrisch um den Sch√§tzer und 95% der Werte liegen innerhalb von zwei Standardabweichung des Sch√§tzers z.B:\n\\[\n[CI_{0.025}^{\\beta_0}, CI^{\\beta_0}_{0.975}] = [8.1108 - 2 0.966 , 8.1108 + 2 0.966] = [6.193, 10.029]\n\\]\n\n\n7.4.2.2 Teststatistiken \\(t\\) und \\(p\\)-Werte\nEine weitere wichtige Frage ist, ob ein Koeffizient (Parameter) tats√§chlich siginifikant von \\(0\\) verschiedenen ist. Im unserem konkreten Beispiel stellt sich die Fragen: Ver√§ndert sich der Umsatz, wenn wir mehr Geld in TV-Werbung investieren? Dies kann mit einem T-Test √ºberpr√ºft werden.\n\n\\(H_0\\): \\(\\beta_1 = 0\\) (kein Effekt)\n\\(H_1\\): \\(\\beta_1 \\neq 0\\) (Effekt)\n\nDie Teststatistik \\(t\\) (t) gibt an, wie viele Standardabweichungen der Sch√§tzer \\(\\hat{\\beta}_1\\) vom Wert \\(0\\) entfernt ist. Ein hoher \\(t\\)-Wert deutet darauf hin, dass der Sch√§tzer signifikant von \\(0\\) verschieden ist. Der \\(p\\)-Wert (P&gt;|t|) gibt die Wahrscheinlichkeit an, dass der beobachtete Effekt auftritt, wenn die Nullhypothese wahr ist. Ein \\(p\\)-Wert kleiner als \\(0.05\\) wird oft als Hinweis darauf interpretiert, dass der Effekt signifikant ist.\nIm Vorliegenden Fall k√∂nnen wir also die Nullhypothese, dass der Umsatz unabh√§ngig von den Werbeausgaben ist mit hoher Konfidenz ablehnen.\n\n\n7.4.2.3 Bestimmtheitsma√ü \\(R^2\\)\nDer das Bestimmtheitsma√ü \\(R^2\\) gibt an, wie gut das Modell die beobachteten Daten erkl√§rt. Er liegt zwischen \\(0\\) und \\(1\\) und gibt den Anteil der Varianz der abh√§ngigen Variable \\(Y\\) an, der durch das Modell erkl√§rt wird. Ein \\(R^2\\) von \\(1\\) bedeutet, dass das Modell alle Variationen in \\(Y\\) erkl√§rt, w√§hrend ein \\(R^2\\) von \\(0\\) bedeutet, dass das Modell keine Variationen erkl√§rt. In unserem Fall betr√§gt \\(R^2 = 0.41\\), was darauf hindeutet, dass das Modell etwa \\(41\\%\\) der Variationen in den Verk√§ufen erkl√§rt.\nDas Bestimmtheitsma√ü \\(R^2\\) ist definiert als: \\[\nR^2 = \\frac{TSS-RSS}{TSS} = 1- \\frac{RSS}{TSS},\n\\]\nwobei \\(RSS\\) die Residual Sum of Squares (siehe oben) und \\(TSS\\) die Total Sum of Squares ist. \\(TSS\\) ist die Summe der quadrierten Abweichungen der abh√§ngigen Variable \\(Y\\) von ihrem Mittelwert \\(\\bar{y}\\).\nDie Varianz und TSS h√§ngen eng miteinander zusammen:\n\\[\nTSS = \\sum_{i=1}^{n} (y_i - \\bar{y})^2 = n \\text{Var}(Y).\n\\]\nIntuitiv kann man sagen, R¬≤ gibt an, wie viel Prozent der Varianz der abh√§ngigen Variable durch die unabh√§ngigen Variablen erkl√§rt wird, bzw. genauer: Wie veringern sich die Fehler, wenn man ein naives Mittel-Wert-Modells (\\(f(x)=\\bar{y}\\)) durch das lineare Modell ersetzt.\n\n# Plot data\nplt.scatter(X, Y)\nplt.plot(X, beta_0 + beta_1 * X, color=\"red\", label=\"OLS regression\")\nplt.plot(X, np.mean(Y) * np.ones_like(X), color=\"green\", label=\"Mean of Y\")\nplt.legend()\nplt.xlabel(\"TV advertising spending (‚Ç¨)\")\nplt.ylabel(\"Sales\")\nplt.title(\"TV advertising vs. sales\")\nplt.show()\n\n\n\n\nScatter plot of TV advertising vs.¬†sales\n\n\n\n\n\n\n\n\n\n\nStolpersteine beim Interpretieren von \\(R^2\\)\n\n\n\n\nEin hoher \\(R^2\\) bedeutet nicht unbedingt, dass das Modell gut ist. Ein Modell mit einem hohen \\(R^2\\) kann immer noch schlechte Vorhersagen machen, wenn es nicht gut generalisiert.\nEin niedriger \\(R^2\\) bedeutet nicht unbedingt, dass das Modell schlecht ist. Ein Modell mit einem niedrigen \\(R^2\\) kann immer noch n√ºtzlich sein, wenn es die Beziehung zwischen den Variablen gut erkl√§rt.\n\\(R^2\\) h√§ngt von der Anzahl der Variablen im Modell ab. Ein Modell mit mehr Variablen wird tendenziell ein h√∂heres \\(R^2\\) haben, auch wenn es nicht besser ist.\n\nDie folgede Abbildung zeigt zudem verschiende Zusammenh√§nge und den \\(R^2\\) Wert f√ºr die jeweiligen Modelle.\n\n\n\n\n\n\n\n\n\n\nAnnahmen f√ºr die lineare Regression\n\n\n\nDamit eine Lineare Regression sinnvoll ist und die Sch√§tzung der Parameter gut funktioniert m√ºssen einige Annahmen erf√ºllt sein:\n\nLineare Beziehung: Die Beziehung zwischen den unabh√§ngigen und abh√§ngigen Variablen ist linear.\nUnabh√§ngigkeit: Die Beobachtungen sind unabh√§ngig voneinander.\nHomoskedastizit√§t: Die Varianz der Fehler ist konstant.\nNormalverteilung: Die Fehler sind normalverteilt.\nKeine Multikollinearit√§t: Die unabh√§ngigen Variablen sind nicht stark miteinander korreliert\n\nNach Lehrbuch sind die Annahmen f√ºr die lineare Regression immer zu testen, wenn wir Sie f√ºr eine statische Analyse und Interpretation verwenden wollen. In der Praxis, vorallem im Fall von Vorhersagen, sind die Annahmen jedoch oft nicht erf√ºllt, und es ist wichtig, die Robustheit der Ergebnisse zu √ºberpr√ºfen und gegebenenfalls alternative Modelle oder Transformationen in Betracht zu ziehen.",
    "crumbs": [
      "Regressions-Analyse",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Lineare Regression</span>"
    ]
  },
  {
    "objectID": "regression/advanced_linear_regression.html",
    "href": "regression/advanced_linear_regression.html",
    "title": "8¬† Erweiterungen der Linearen Regression",
    "section": "",
    "text": "8.1 Abbildung nicht-linearer Zusammenh√§nge mit Linearen Modellen\nZusammenh√§nge k√∂nnen auch zwischen mehr als zwei Variablen bestehen. In diesem Fall sprechen wir von einer multiplen linearen Regression. Das Modell kann dann wie folgt formuliert werden:\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_p X_p + \\epsilon\n\\]\nAls Beispiel k√∂nnten wir nicht nur die Absatzmenge eines Produkts in Abh√§ngigkeit von der Werbeausgabe f√ºr TV-Spots, sondern auch von der Werbeausgabe f√ºr Radio-Spots modellieren. Wir simuliieren die Daten wieder mit Python, so dass wir die Zusammenh√§nge kennen.\nWir k√∂nnen nun drei unabh√§ngige Modelle fitten und visualisieren.\nOffensichtlich sind die Modelle von mittlerer Qualit√§t. Wir k√∂nnen die drei unabh√§ngigen Variablen auch in einem Modell zusammenfassen.\nWir k√∂nnen das Modell visualisieren, indem wir dreidimensionale Daten plotten. Wir lassen \\(x_3\\) weg, da es den geringsten Einfluss auf die abh√§ngige Variable zu haben scheint.\nDas Modell ist linear und jede der drei Variablen hat einen positiven Einfluss auf die abh√§ngige Variable. Das Modell kann wie folgt geschrieben werden:\n\\[\nY =  -689.7 + 4.9 X_1 + 13.3 X_2 + 0.6 X_3,\n\\]\noder noch klarer:\n\\[\n\\text{sales} = 689.7 \\text{units} + 4.9 \\frac{\\text{units}}{Euro} \\times \\text{TV advertising spending} + 13.3 \\frac{\\text{units}}{Euro} \\times \\text{Radio advertising spending} + 0.6 \\frac{\\text{units}}{Euro} \\times \\text{Newspaper advertising spending}.\n\\]\nDieses lineare Modell eigent sich ausgezeichnet f√ºr eine Interpretation des Daten.\nWichtig ist auch, dass wir unsere Interpretation der Welt durch unser Modell nicht verabsolutieren. Es ist immer nur eine N√§herung und kann nie die Realit√§t vollst√§ndig abbilden. Wenn wir unser Model w√∂rtlich nehmen, k√∂nnten wir uns in die Irre f√ºhren. Stellen, wir uns vor, die Firma stellt die Werbeausgaben f√ºr Zeitungswerbung ein. Dann w√ºrde unser Modell den folgenden Absatz vorhersagen:\n\\[\n\\text{sales} = - 689.7 \\text{units}\n\\]\nEin negativer Absatz ist offensichtlich in der Realit√§t nicht zu erwarten. Entsprechend haben wir den Grenzen unseres Modells erreicht. Die reale Welt l√§sst sich nur in bestimmten Grenzen durch das lineare Modell abbilden.\nIn der Praxis h√∂rt man oft, dass sich lineare Modelle nur f√ºr lineare Zusammenh√§nge eignen. Das ist nicht ganz richtig. Lineare Modelle k√∂nnen auch nicht-lineare Zusammenh√§nge abbilden. Zun√§chst vergegenw√§rtigen wir uns nochmal was passiert, wenn wir die Vorhersage unseres Modells nur in Abh√§ngigkeit von einer Variablen betrachten. Jeder schwarze Punkt in (fig:regression-linear-advanced-linear-regression?) zeigt die beobachtete Absatzmenge zu einem Zeitpunkt in Abh√§ngigkeit von den Werbeausgaben f√ºr TV-Spots. Der blaue Punkt zeigt die Vorhersage unseres Modells, in das Modell geht nat√ºrlich auch die Werbeausgaben f√ºr Radio- und Zeitungswerbung ein. Deswegen liegen die blauen Punkte nicht auf einer Linie, obwohl wir ein lineares Modell verwenden.\nfrom sklearn.linear_model import LinearRegression\n\n# Prediction for TV advertising spending\nX = X_sample[0]\nplt.scatter(X, Y_sample, color='black')\nY_pred = model_sm.params[0] + model_sm.params[1] * X_1 + model_sm.params[2] * X_2 + model_sm.params[3] * X_3\nplt.scatter(X_1, Y_pred, color='blue')\nplt.xlabel(\"TV advertising spending (‚Ç¨)\")\nplt.ylabel(\"Sales\")\nplt.title(\"Linear Regression with TV advertising spending (‚Ç¨)\")\n# Add legend\nplt.legend([\"Data\", \"Prediction\"])\nplt.show()\n\n/tmp/ipykernel_3489/1829400649.py:6: FutureWarning:\n\nSeries.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n\n\n\n\n\n\nLineare Regression f√ºr die drei unabh√§ngigen Variablen als Plot f√ºr eine\nManchmal begegnen wir Zusammenh√§ngen, die ganz klar nicht linear sind, beispielsweise der zwischen Verbrauch in Meilen pro Gallone und der Leistung eines Autos. Offensichtlich sinkt der Verbrauch nicht linear mit der Leistung:\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\ndf = pd.read_csv(r\"../_assets/regression/Auto_Data_Set_963_49.csv\")\n\n# linear model\n\nmodel = LinearRegression()\nmodel.fit(df[[\"horsepower\"]], df[\"mpg\"])\nY_pred = model.predict(df[[\"horsepower\"]])\n\n\n# Scatter plot between mpg und horsepower\nplt.scatter(df[\"horsepower\"], df[\"mpg\"])\nplt.plot(df[\"horsepower\"], Y_pred, color='blue', linewidth=3)\nplt.xlabel(\"Horsepower\")\nplt.ylabel(\"Miles per Gallon\")\nplt.title(\"Scatter plot between Horsepower and Miles per Gallon\")\n# Legend with parameters\nplt.legend([f\"Linear Model: $Y = {model.intercept_:.1f} + {model.coef_[0]:.1f}X$\"])\n# Add RSS\n\nplt.show()\nEine L√∂sung ist die Transformation der unabh√§ngigen Variablen. In unserem Beispiel k√∂nnten wir die Quadratwurzel der Leistung verwenden. Das Modell wird dann wie folgt aussehen:\n\\[\n\\text{mpg} = \\beta_0 + \\beta_1 \\sqrt{\\text{horsepower}} + \\epsilon\n\\]\n# Transformation of the independent variable\ndf[\"sqrt_horsepower\"] = np.sqrt(df[\"horsepower\"])\n\n# linear model\nmodel = LinearRegression()\nmodel.fit(df[[\"sqrt_horsepower\"]], df[\"mpg\"])\n\n# Sort before plotting\ndf = df.sort_values(by=\"horsepower\")\n\nY_pred = model.predict(df[[\"sqrt_horsepower\"]])\n\n# Scatter plot between mpg und horsepower\nplt.scatter(df[\"horsepower\"], df[\"mpg\"])\nplt.plot(df[\"horsepower\"], Y_pred, color='blue', linewidth=3)\nplt.xlabel(\"Horsepower\")\nplt.ylabel(\"Miles per Gallon\")\nplt.title(\"Scatter plot between Horsepower and Miles per Gallon\")\n# Legend with parameters\nplt.legend([f\"Linear Model: $Y = {model.intercept_:.1f} {model.coef_[0]:.1f}X^{{0.5}}$\"])\nplt.show()\n\n\n\n\nLineare Regression f√ºr mit transformierten unabh√§ngigen Variablen",
    "crumbs": [
      "Regressions-Analyse",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Erweiterungen der Linearen Regression</span>"
    ]
  },
  {
    "objectID": "regression/advanced_linear_regression.html#abbildung-nicht-linearer-zusammenh√§nge-mit-linearen-modellen",
    "href": "regression/advanced_linear_regression.html#abbildung-nicht-linearer-zusammenh√§nge-mit-linearen-modellen",
    "title": "8¬† Erweiterungen der Linearen Regression",
    "section": "",
    "text": "Feature Engineering\n\n\n\nDie Transformation der unabh√§ngigen Variablen wird auch als Feature Engineering bezeichnet. Feature Engineering ist ein wichtiger Schritt in der Modellierung, um die Leistungsf√§higkeit von Modellen zu verbessern.",
    "crumbs": [
      "Regressions-Analyse",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Erweiterungen der Linearen Regression</span>"
    ]
  },
  {
    "objectID": "regression/advanced_linear_regression.html#umgang-mit-kategorischen-variablen",
    "href": "regression/advanced_linear_regression.html#umgang-mit-kategorischen-variablen",
    "title": "8¬† Erweiterungen der Linearen Regression",
    "section": "8.2 Umgang mit Kategorischen Variablen",
    "text": "8.2 Umgang mit Kategorischen Variablen\nBisher haben wir nur numerische Variablen betrachtet. In der Praxis haben wir es aber oft mit kategorischen Variablen zu tun. Kategorische Variablen sind Variablen, die eine endliche Anzahl von Kategorien haben. Beispiele sind Geschlecht, Region oder Produkttyp. Beispielsweise wird im Car-Data-Set die Herkunft der Autos erfasst.\n\ndf[\"origin\"] = df[\"origin\"].map({1: \"USA\", 2: \"Europe\", 3: \"Japan\"})\ndf[\"origin\"].value_counts().plot(kind='bar')\nplt.xlabel(\"Origin\")\nplt.ylabel(\"Count\")\n\nText(0, 0.5, 'Count')\n\n\n\n\n\n\n\n\n\nNun wollen wir untersuchen, ob wir die Herkunft der Autos verwenden k√∂nnen, um den Verbrauch vorherzusagemn. Wie immer starten wir mit einer explorativen Analyse.\n\nimport seaborn as sns\n\nsns.boxplot(x=\"origin\", y=\"mpg\", data=df)\nplt.xlabel(\"Origin\")\nplt.ylabel(\"Miles per Gallon\")\n\nText(0, 0.5, 'Miles per Gallon')\n\n\n\n\n\n\n\n\n\nOffensichtlich gibt es hier einen Zusammenhang. Wir wollen unser Modell von vorhin erweiten, um es weiter zu verbessern. Um kategoriale Variablen in einer Linearen Regression zu Ber√ºcksichtigen gibt es zwei M√∂glichkeiten.\n\n8.2.1 Dummy-Variablen\nDummy-Variablen erm√∂glichen die Einbindung kategorischer Variablen in lineare Regressionsmodelle, indem sie jede Kategorie (au√üer einer Referenzkategorie) durch eine bin√§re Variable (0 oder 1) darstellen. F√ºr eine Variable mit \\(k\\) Kategorien ben√∂tigt man \\(k-1\\) Dummy-Variablen, da die Referenzkategorie durch das Fehlen aller Dummy-Variablen kodiert wird. Dies vermeidet Mehrkollinearit√§t, da die Kategorien sonst linear abh√§ngig w√§ren.\nBeispiel: Wir verwenden die Spalte ‚Äûorigin‚Äú (USA, Europe, Japan) aus dem Auto-Datensatz und erstellen Dummy-Variablen mit USA als Referenzkategorie.\n\nimport pandas as pd\n\n# Lade den Datensatz\ndf = pd.read_csv(r\"../_assets/regression/Auto_Data_Set_963_49.csv\")\ndf[\"origin\"] = df[\"origin\"].map({1: \"USA\", 2: \"Europe\", 3: \"Japan\"})\n\n# Erstelle Dummy-Variablen\ndummy_df = pd.get_dummies(df[\"origin\"], prefix=\"origin\", drop_first=True)\n\n# F√ºge die Dummy-Variablen zum DataFrame hinzu\ndf_with_dummies = pd.concat([df[[\"mpg\", \"horsepower\"]], dummy_df], axis=1)\n\n# Zeige die ersten Zeilen des DataFrames\nprint(df_with_dummies.head())\n\n    mpg  horsepower  origin_Japan  origin_USA\n0  18.0         130         False        True\n1  15.0         165         False        True\n2  18.0         150         False        True\n3  16.0         150         False        True\n4  17.0         140         False        True\n\n\nErgebnis: Der DataFrame enth√§lt nun zwei neue Spalten: origin_Japan und origin_USA. Wenn origin_Japan = 1, ist das Auto aus Japan; wenn origin_USA = 1, aus den USA; wenn beide 0 sind, aus Europa (Referenzkategorie).\nInterpretation: Diese Dummy-Variablen k√∂nnen in eine lineare Regression eingebunden werden, um den Einfluss der Herkunft auf den Verbrauch (mpg) zu modellieren. Der Koeffizient von origin_Japan gibt den Unterschied im durchschnittlichen Verbrauch zwischen europ√§ischen und japanischen Autos an, bei konstanten anderen Variablen.\n\n\n8.2.2 One-Hot-Encoding\nOne-Hot-Encoding kodiert jede Kategorie einer kategorischen Variable durch eine eigene bin√§re Spalte, was \\(k\\) Spalten f√ºr \\(k\\) Kategorien ergibt. Im Gegensatz zu Dummy-Variablen wird keine Referenzkategorie weggelassen, was in linearen Regressionen zu Mehrkollinearit√§t f√ºhren kann, wenn der Intercept im Modell enthalten ist. In solchen F√§llen wird oft eine Spalte entfernt oder der Intercept ausgeschlossen.\nBeispiel: Wir erstellen ein One-Hot-Encoding f√ºr die Spalte ‚Äûorigin‚Äú (USA, Europe, Japan) im Auto-Datensatz.\n\nimport pandas as pd\n\n# Lade den Datensatz\ndf = pd.read_csv(r\"../_assets/regression/Auto_Data_Set_963_49.csv\")\ndf[\"origin\"] = df[\"origin\"].map({1: \"USA\", 2: \"Europe\", 3: \"Japan\"})\n\n# Erstelle One-Hot-Encoding\none_hot_df  =  pd.get_dummies(df[\"origin\"], prefix=\"origin\", drop_first=False)\n# F√ºge die One-Hot-Encoding-Spalten zum DataFrame hinzu\ndf_with_one_hot = pd.concat([df[[\"mpg\", \"horsepower\"]], one_hot_df], axis=1)\n\n# Zeige die ersten Zeilen des DataFrames\nprint(df_with_one_hot.head())\n\n    mpg  horsepower  origin_Europe  origin_Japan  origin_USA\n0  18.0         130          False         False        True\n1  15.0         165          False         False        True\n2  18.0         150          False         False        True\n3  16.0         150          False         False        True\n4  17.0         140          False         False        True\n\n\nErgebnis: Der DataFrame enth√§lt drei neue Spalten: origin_USA, origin_Europe und origin_Japan. Jede Spalte zeigt an, ob ein Auto der jeweiligen Kategorie angeh√∂rt (1) oder nicht (0).\nInterpretation: In einer linearen Regression mit diesen One-Hot-kodierten Variablen und einem Intercept w√ºrde eine der Spalten (z. B. origin_USA) entfernt werden m√ºssen, um Mehrkollinearit√§t zu vermeiden. Alternativ kann das Modell ohne Intercept gesch√§tzt werden, wobei jede Spalte den durchschnittlichen Verbrauch der jeweiligen Kategorie repr√§sentiert.\n\n\n8.2.3 Modell nur mit Kategorischen Variablen\nEin Regressionsmodell, das ausschlie√ülich kategorische Variablen verwendet, sch√§tzt die Mittelwerte der abh√§ngigen Variable f√ºr jede Kategorie. F√ºr die Herkunft (USA, Europe, Japan) modelliert das Modell den durchschnittlichen Kraftstoffverbrauch (mpg) pro Land. Der Intercept entspricht dem durchschnittlichen Verbrauch der Referenzkategorie, w√§hrend die Koeffizienten der Dummy-Variablen die Differenz im Verbrauch der anderen Kategorien im Vergleich zur Referenzkategorie angeben.\nBeispiel: Wir erstellen ein Modell, das den Verbrauch (mpg) basierend auf der Herkunft vorhersagt, mit USA als Referenzkategorie.\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Lade den Datensatz\ndf = pd.read_csv(r\"../_assets/regression/Auto_Data_Set_963_49.csv\")\ndf[\"origin\"] = df[\"origin\"].map({1: \"USA\", 2: \"Europe\", 3: \"Japan\"})\n\n# Erstelle Dummy-Variablen (USA als Referenzkategorie)\nX = pd.get_dummies(df[\"origin\"], prefix=\"origin\", drop_first=True)\nX = sm.add_constant(X)  # F√ºge Intercept hinzu\n\n# Wandle Booleans in numerische Werte um\nX = X.astype(int)\n\nprint(X.head())\n\n# Fitte das Modell\nmodel = sm.OLS(df[\"mpg\"], X).fit()\n\n# Zeige die Zusammenfassung\nprint(model.summary())\n\n   const  origin_Japan  origin_USA\n0      1             0           1\n1      1             0           1\n2      1             0           1\n3      1             0           1\n4      1             0           1\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    mpg   R-squared:                       0.332\nModel:                            OLS   Adj. R-squared:                  0.328\nMethod:                 Least Squares   F-statistic:                     96.60\nDate:                Tue, 03 Jun 2025   Prob (F-statistic):           8.67e-35\nTime:                        09:39:27   Log-Likelihood:                -1282.2\nNo. Observations:                 392   AIC:                             2570.\nDf Residuals:                     389   BIC:                             2582.\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n================================================================================\n                   coef    std err          t      P&gt;|t|      [0.025      0.975]\n--------------------------------------------------------------------------------\nconst           27.6029      0.776     35.587      0.000      26.078      29.128\norigin_Japan     2.8477      1.058      2.691      0.007       0.767       4.928\norigin_USA      -7.5695      0.877     -8.634      0.000      -9.293      -5.846\n==============================================================================\nOmnibus:                       26.330   Durbin-Watson:                   0.763\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               30.217\nSkew:                           0.679   Prob(JB):                     2.74e-07\nKurtosis:                       3.066   Cond. No.                         5.42\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nErgebnis und Interpretation:\n\nIntercept: Der Intercept repr√§sentiert den durchschnittlichen Verbrauch (mpg) von Autos aus den USA (Referenzkategorie). Zum Beispiel, wenn der Intercept 27.6 ist, bedeutet dies, dass europ√§ische Autos im Durchschnitt 27.6 Meilen pro Gallone erreichen.\nKoeffizienten:\n\nDer Koeffizient f√ºr origin_Japan gibt die Differenz im durchschnittlichen Verbrauch zwischen europ√§ischen und japanischen Autos an. Ein positiver Koeffizient (z. B. 2.8) bedeutet, dass japanische Autos im Durchschnitt 2.8 Meilen pro Gallone sparsamer sind als europ√§ische Autos.\nDer Koeffizient f√ºr origin_USA interpretiert sich analog.\n\nModellqualit√§t: Der \\(R^2\\)-Wert gibt an, wie viel der Varianz im Verbrauch durch die Herkunft erkl√§rt wird. Ein niedriger \\(R^2\\) deutet darauf hin, dass andere Faktoren (z. B. Leistung) ebenfalls wichtig sind.\n\nDieses Modell ist einfach zu interpretieren, da es nur die Mittelwerte pro Kategorie vergleicht, √§hnlich einer ANOVA oder einem T-Test.\n\n\n8.2.4 Modell mit allen Variablen\nEin Modell, das numerische und kategorische Variablen kombiniert, nutzt alle verf√ºgbaren Pr√§diktoren, um die abh√§ngige Variable zu modellieren. Dies f√ºhrt oft zu einer h√∂heren Erkl√§rungskraft (z. B. h√∂herem \\(R^2\\)), da mehr Informationen ber√ºcksichtigt werden. Die Interpretation wird jedoch komplexer, da die Koeffizienten unter der Annahme interpretiert werden, dass alle anderen Variablen konstant sind.\nBeispiel: Wir modellieren den Verbrauch (mpg) basierend auf der Leistung (horsepower), deren Quadratwurzel (f√ºr nicht-lineare Effekte) und der Herkunft (als Dummy-Variablen, USA als Referenzkategorie).\n\nimport pandas as pd\nimport statsmodels.api as sm\nimport numpy as np\n\n# Lade den Datensatz\ndf = pd.read_csv(r\"../_assets/regression/Auto_Data_Set_963_49.csv\")\ndf[\"origin\"] = df[\"origin\"].map({1: \"USA\", 2: \"Europe\", 3: \"Japan\"})\n\n# Erstelle transformierte Variable\ndf[\"sqrt_horsepower\"] = np.sqrt(df[\"horsepower\"])\n\n# Erstelle Dummy-Variablen\nX = pd.get_dummies(df[\"origin\"], prefix=\"origin\", drop_first=True)\n\n# F√ºge numerische Variablen hinzu\nX[\"horsepower\"] = df[\"horsepower\"]\nX[\"sqrt_horsepower\"] = df[\"sqrt_horsepower\"]\n\n# F√ºge Intercept hinzu\nX = sm.add_constant(X)\n\n# Wandle Booleans in numerische Werte um\nX = X.astype(float)\n\n# Fitte das Modell\nmodel = sm.OLS(df[\"mpg\"], X).fit()\n\n# Zeige die Zusammenfassung\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    mpg   R-squared:                       0.719\nModel:                            OLS   Adj. R-squared:                  0.716\nMethod:                 Least Squares   F-statistic:                     247.8\nDate:                Tue, 03 Jun 2025   Prob (F-statistic):          2.51e-105\nTime:                        09:39:27   Log-Likelihood:                -1112.2\nNo. Observations:                 392   AIC:                             2234.\nDf Residuals:                     387   BIC:                             2254.\nDf Model:                           4                                         \nCovariance Type:            nonrobust                                         \n===================================================================================\n                      coef    std err          t      P&gt;|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------\nconst              95.6047      6.489     14.734      0.000      82.847     108.362\norigin_Japan        2.8432      0.688      4.134      0.000       1.491       4.195\norigin_USA         -1.2807      0.632     -2.026      0.043      -2.523      -0.038\nhorsepower          0.3650      0.057      6.456      0.000       0.254       0.476\nsqrt_horsepower   -10.9359      1.232     -8.875      0.000     -13.359      -8.513\n==============================================================================\nOmnibus:                       17.102   Durbin-Watson:                   1.198\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               26.045\nSkew:                           0.324   Prob(JB):                     2.21e-06\nKurtosis:                       4.084   Cond. No.                     3.52e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 3.52e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\nErgebnis und Interpretation:\n\nModellg√ºte: Der \\(R^2\\)-Wert ist h√∂her als in Modellen mit nur numerischen oder nur kategorischen Variablen, da das Modell mehr Informationen nutzt. Zum Beispiel k√∂nnte \\(R^2 = 0.75\\) bedeuten, dass 75 % der Varianz im Verbrauch erkl√§rt werden.\nKoeffizienten:\n\nDer Koeffizient f√ºr horsepower und sqrt_horsepower gibt den Einfluss der Leistung auf den Verbrauch an, unter Ber√ºcksichtigung der nicht-linearen Beziehung.\n\nInterpretationsschwierigkeit: Die Koeffizienten sind schwerer zu interpretieren, da sie unter der Annahme konstanter anderer Variablen gelten. Zum Beispiel k√∂nnte der Einfluss von horsepower durch die Quadratwurzel-Transformation komplexer sein.\nVorsicht: Ein komplexeres Modell kann zu √úberanpassung f√ºhren, insbesondere bei kleinen Datens√§tzen, und die Interpretation kann f√ºr praktische Entscheidungen unpraktisch werden.\n\nDieses Modell ist leistungsf√§higer, aber die Interpretation erfordert ein tieferes Verst√§ndnis der Zusammenh√§nge und der Modellannahmen.",
    "crumbs": [
      "Regressions-Analyse",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Erweiterungen der Linearen Regression</span>"
    ]
  },
  {
    "objectID": "regression/advanced_linear_regression.html#matrixschreibweise-der-linearen-regression",
    "href": "regression/advanced_linear_regression.html#matrixschreibweise-der-linearen-regression",
    "title": "8¬† Erweiterungen der Linearen Regression",
    "section": "8.3 Matrixschreibweise der linearen Regression",
    "text": "8.3 Matrixschreibweise der linearen Regression\nDie lineare Regression sch√§tzt die abh√§ngige Variable \\(y\\) als lineare Kombination der Pr√§diktoren \\(x_1, x_2, \\dots, x_p\\) plus eines Intercepts. Mathematisch wird dies als Matrixmultiplikation dargestellt, was die Berechnung effizient macht, insbesondere bei vielen Beobachtungen und Pr√§diktoren. Formel Die lineare Regression f√ºr \\(n\\) Beobachtungen und \\(p\\) Pr√§diktoren (inkl. Intercept) lautet: \\[\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\]\n\n\\(\\mathbf{y}\\): Vektor der abh√§ngigen Variable (\\(n \\times 1\\)).\n\\(\\mathbf{X}\\): Designmatrix (\\(n \\times (p+1)\\)), enth√§lt eine Spalte f√ºr den Intercept (konstante 1) und die Pr√§diktoren.\n\\(\\boldsymbol{\\beta}\\): Koeffizientenvektor (\\(((p+1) \\times 1)\\)), enth√§lt Intercept und Koeffizienten der Pr√§diktoren.\n\\(\\boldsymbol{\\epsilon}\\): Fehlervektor (\\(n \\times 1\\)).\n\nDie vorhergesagten Werte sind: \\[\\hat{\\mathbf{y}} = \\mathbf{X} \\boldsymbol{\\beta}\\] Matrixdarstellung mit Variablen F√ºr die OLS-Regression mit mpg als abh√§ngige Variable und Pr√§diktoren origin_Japan, origin_USA, horsepower, sqrt_horsepower lautet die Matrixform:\n\\[\n\\begin{bmatrix}\n\\hat{y}_1 \\\\\n\\hat{y}_2 \\\\\n\\vdots \\\\\n\\hat{y}_n\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & x_{1,\\text{Japan}} & x_{1,\\text{USA}} & x_{1,\\text{hp}} & x_{1,\\text{sqrt\\_hp}} \\\\\n1 & x_{2,\\text{Japan}} & x_{2,\\text{USA}} & x_{2,\\text{hp}} & x_{2,\\text{sqrt\\_hp}} \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n1 & x_{n,\\text{Japan}} & x_{n,\\text{USA}} & x_{n,\\text{hp}} & x_{n,\\text{sqrt\\_hp}}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\beta_2 \\\\\n\\beta_3 \\\\\n\\beta_4\n\\end{bmatrix}\n\\]\n\n\\(\\beta_0\\): Intercept (const).\n\\(\\beta_1, \\beta_2\\): Koeffizienten f√ºr origin_Japan, origin_USA.\n\\(\\beta_3, \\beta_4\\): Koeffizienten f√ºr horsepower, sqrt_horsepower.\n\\(x_{i,\\text{Japan}}, x_{i,\\text{USA}}\\): Dummy-Variablen (0 oder 1).\n\\(x_{i,\\text{hp}}, x_{i,\\text{sqrt\\_hp}}\\): Werte f√ºr horsepower und dessen Quadratwurzel.\n\n\n8.3.1 Konkretes Beispiel\nNehmen wir die Koeffizienten aus der OLS-Ausgabe:\n\n\\(\\beta_0 = 95.6047\\) (const)\n\\(\\beta_1 = 2.8432\\) (origin_Japan)\n\\(\\beta_2 = -1.2807\\) (origin_USA)\n\\(\\beta_3 = 0.3650\\) (horsepower)\n\\(\\beta_4 = -10.9359\\) (sqrt_horsepower)\n\nF√ºr drei Beobachtungen aus dem Datensatz (angenommen):\n\n\n\nBeobachtung\norigin\nhorsepower\nsqrt_horsepower\n\n\n\n\n1\nUSA\n130\n\\(\\sqrt{130} \\approx 11.40\\)\n\n\n2\nJapan\n88\n\\(\\sqrt{88} \\approx 9.38\\)\n\n\n3\nEurope\n100\n\\(\\sqrt{100} = 10.00\\)\n\n\n\nDie Designmatrix \\(\\mathbf{X}\\) f√ºr diese Beobachtungen ist:\n\\[\n\\mathbf{X} =\n\\begin{bmatrix}\n1 & 0 & 1 & 130 & 11.40 \\\\\n1 & 1 & 0 & 88 & 9.38 \\\\\n1 & 0 & 0 & 100 & 10.00 \\\\\n\\end{bmatrix}\n\\]\nDer Koeffizientenvektor ist:\n\\[\n\\boldsymbol{\\beta} =\n\\begin{bmatrix}\n95.6047 \\\\\n2.8432 \\\\\n-1.2807 \\\\\n0.3650 \\\\\n-10.9359 \\\\\n\\end{bmatrix}\n\\]\nDie vorhergesagten Werte \\(\\hat{\\mathbf{y}}\\) berechnen sich durch \\(\\hat{\\mathbf{y}} = \\mathbf{X} \\boldsymbol{\\beta}\\). F√ºr Beobachtung 1 (USA):\n\\[\n\\begin{align*}\n\\hat{y}_1 &= 95.6047 + 0 \\cdot 2.8432 + 1 \\cdot (-1.2807) + 130 \\cdot 0.3650 + 11.40 \\cdot (-10.9359) \\\\\n          &= 95.6047 - 1.2807 + 47.45 - 124.66 \\\\\n          &= 17.11\n\\end{align*}\n\\]",
    "crumbs": [
      "Regressions-Analyse",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Erweiterungen der Linearen Regression</span>"
    ]
  },
  {
    "objectID": "regression/predictions_linear_regression.html",
    "href": "regression/predictions_linear_regression.html",
    "title": "9¬† Vorhersagen mittels Linearen Regression",
    "section": "",
    "text": "9.1 Anwendungsfall: Vorhersage des Verbrauchs eines einzelnen Autos\nLineare Regression ist eine leistungsf√§hige Methode, um Vorhersagen √ºber eine abh√§ngige Variable basierend auf einer oder mehreren unabh√§ngigen Variablen zu treffen. In diesem Abschnitt zeigen wir, wie man den Kraftstoffverbrauch eines Autos vorhersagt, und erl√§utern wichtige Konzepte wie Fehlerma√üe, den Bias-Varianz-Tradeoff und die Aufteilung der Daten in Trainings-, Validierungs- und Testsets. Der Prozess wird im Kontext des CRISP-DM-Modells eingeordnet.\nZiel: Wir m√∂chten den Kraftstoffverbrauch (mpg, Meilen pro Gallone) eines einzelnen Autos basierend auf Pr√§diktoren wie Leistung (horsepower) und Herkunft (origin) vorhersagen. Dies ist ein typisches Problem des √ºberwachten Lernens, bei dem wir ein Modell trainieren, um eine kontinuierliche abh√§ngige Variable (\\(Y\\)) aus unabh√§ngigen Variablen (\\(\\vec{X}\\)) zu sch√§tzen.\nBeispiel: Gegeben ein Auto mit 120 PS und Herkunft ‚ÄûJapan‚Äú, wie hoch ist der zu erwartende Verbrauch? Wir verwenden den Auto-Datensatz, um ein lineares Regressionsmodell zu trainieren und Vorhersagen zu treffen.\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Lade den Datensatz\ndf = pd.read_csv(r\"../_assets/regression/Auto_Data_Set_963_49.csv\")\ndf[\"origin\"] = df[\"origin\"].map({1: \"USA\", 2: \"Europe\", 3: \"Japan\"})\n\n# Erstelle Dummy-Variablen (USA als Referenzkategorie)\nX = pd.get_dummies(df[\"origin\"], prefix=\"origin\", drop_first=True)\nX[\"horsepower\"] = df[\"horsepower\"]\nX = sm.add_constant(X)\nX = X.astype(float)\n\n# Fitte das Modell\nmodel = sm.OLS(df[\"mpg\"], X).fit()\n\n# Vorhersage f√ºr ein Auto mit 120 PS, Herkunft Japan\nnew_car = pd.DataFrame({\n    \"const\": [1],\n    \"origin_Europe\": [0],\n    \"origin_Japan\": [1],\n    \"horsepower\": [120]\n})\nprediction = model.predict(new_car)\nprint(f\"Vorhergesagter Verbrauch: {prediction[0]:.2f} mpg\")\n\nVorhergesagter Verbrauch: 19.91 mpg\nErgebnis: Das Modell liefert eine Vorhersage f√ºr den Verbrauch.",
    "crumbs": [
      "Regressions-Analyse",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Vorhersagen mittels Linearen Regression</span>"
    ]
  },
  {
    "objectID": "regression/predictions_linear_regression.html#anwendungsfall-vorhersage-des-verbrauchs-eines-einzelnen-autos",
    "href": "regression/predictions_linear_regression.html#anwendungsfall-vorhersage-des-verbrauchs-eines-einzelnen-autos",
    "title": "9¬† Vorhersagen mittels Linearen Regression",
    "section": "",
    "text": "9.1.1 Bezug zu Schritten des CRISP-DM\nCRISP-DM (Cross Industry Standard Process for Data Mining) bietet einen strukturierten Rahmen f√ºr Datenanalyseprojekte. Die Vorhersage des Kraftstoffverbrauchs l√§sst sich wie folgt in CRISP-DM einordnen:\n\nBusiness Understanding: Ziel ist es, den Verbrauch eines Autos zu prognostizieren, um z. B. Kaufentscheidungen zu unterst√ºtzen.\nData Understanding: Analyse des Auto-Datensatzes, z. B. durch Boxplots von mpg nach origin oder Scatterplots von mpg gegen horsepower.\nData Preparation: Bereinigung der Daten (z. B. Behandlung fehlender Werte) und Erstellung von Dummy-Variablen f√ºr origin.\nModeling: Entwicklung eines linearen Regressionsmodells mit Pr√§diktoren wie horsepower und origin.\nEvaluation: Bewertung der Modellgenauigkeit anhand von Fehlerma√üen wie MSE auf einem Testset.\nDeployment: Anwendung des Modells, um Vorhersagen f√ºr neue Autos zu treffen, z. B. in einer Entscheidungsunterst√ºtzungssoftware.\n\nBeispiel: Die Datenaufbereitung und Modellierungsschritte sind im obigen Code enthalten, w√§hrend die Evaluation weiter unten durch Fehlerma√üe erfolgt.\n\n\n9.1.2 Fehlerma√üe: Von RSS zu MSE\nFehlerma√üe quantifizieren, wie gut ein Modell die Daten vorhersagt. Die Summe der quadrierten Residuen (Residual Sum of Squares, RSS) misst die Gesamtabweichung der vorhergesagten Werte \\(\\hat{y}_j\\) von den tats√§chlichen Werten \\(y_j\\):\n\\[RSS = \\sum_{j=1}^n (y_j - \\hat{y}_j)^2\\]\nDer \\[RSS\\] ist nur schwer interpretierbar, da er von der Anzahl der Beobachtungen abh√§ngt. Daher verwenden wir den Mean Squared Error (MSE), um den RSS zu normalisieren und die durchschnittliche Abweichung pro Beobachtung zu erhalten. Der Mean Squared Error (MSE) ist der durchschnittliche quadratische Fehler, der durch Division des RSS durch die Anzahl der Beobachtungen \\(n\\) berechnet wird:\n\\[MSE = \\frac{1}{n} \\sum_{j=1}^n (y_j - \\hat{y}_j)^2\\]\nBeispiel: Wir berechnen den MSE f√ºr das obige Modell auf den Trainingsdaten.\n\nimport pandas as pd\nimport statsmodels.api as sm\nimport numpy as np\n\n# Lade den Datensatz\ndf = pd.read_csv(r\"../_assets/regression/Auto_Data_Set_963_49.csv\")\ndf[\"origin\"] = df[\"origin\"].map({1: \"USA\", 2: \"Europe\", 3: \"Japan\"})\n\n# Erstelle Dummy-Variablen\nX = pd.get_dummies(df[\"origin\"], prefix=\"origin\", drop_first=True)\nX[\"horsepower\"] = df[\"horsepower\"]\nX = sm.add_constant(X)\nX = X.astype(float)\n\n# Fitte das Modell\nmodel = sm.OLS(df[\"mpg\"], X).fit()\n\n# Berechne Vorhersagen\ny_pred = model.predict(X)\n\n# Berechne MSE\nmse = np.mean((df[\"mpg\"] - y_pred) ** 2)\nprint(f\"Trainings-MSE: {mse:.2f}\")\n\nTrainings-MSE: 20.53\n\n\nErgebnis: Ein MSE von z. B. 24.5 bedeutet, dass die quadrierten Abweichungen im Durchschnitt 24.5 betragen. Ein niedrigerer MSE deutet auf ein besseres Modell hin, aber der MSE auf Trainingsdaten kann durch √úberanpassung verzerrt sein.\n\n\n\n\n\n\nWeitere Fehlerma√üe: MSE, RMSE und MAPE\n\n\n\n\n\nNeben dem Mean Squared Error (MSE) gibt es weitere Fehlerma√üe, die die Modellgenauigkeit bewerten. Hier sind drei g√§ngige Ma√üe mit ihren Formeln und Beschreibungen sowie ein Beispiel, wie man sie in Python mit scikit-learn berechnet.\n\nMean Squared Error (MSE):\n\\[MSE = \\frac{1}{n} \\sum_{j=1}^n (y_j - \\hat{y}_j)^2\\]\nDer MSE misst den durchschnittlichen quadrierten Fehler zwischen tats√§chlichen (\\(y_j\\)) und vorhergesagten (\\(\\hat{y}_j\\)) Werten. Er ist empfindlich gegen√ºber gro√üen Abweichungen, da Fehler quadriert werden.\nRoot Mean Squared Error (RMSE):\n\\[RMSE = \\sqrt{\\frac{1}{n} \\sum_{j=1}^n (y_j - \\hat{y}_j)^2} = \\sqrt{MSE}\\]\nDer RMSE ist die Quadratwurzel des MSE und hat die gleiche Einheit wie die abh√§ngige Variable (z. B. mpg). Er ist intuitiver, da er die durchschnittliche Fehlergr√∂√üe direkt widerspiegelt.\nMean Absolute Percentage Error (MAPE):\n\\[MAPE = \\frac{100}{n} \\sum_{j=1}^n \\left| \\frac{y_j - \\hat{y}_j}{y_j} \\right|\\]\nDer MAPE misst den durchschnittlichen absoluten prozentualen Fehler. Er ist besonders n√ºtzlich, wenn relative Fehler (z. B. in Prozent) wichtiger sind als absolute Fehler, aber er kann problematisch sein, wenn \\(y_j\\) nahe null liegt.\n\nBeispiel: Berechnung von MSE, RMSE und MAPE mit scikit-learn basierend auf Vorhersagen eines Modells.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n\n# Angenommen, df[\"mpg\"] und y_pred sind gegeben (wie im urspr√ºnglichen Code)\n# Beispiel: Fortsetzung des vorherigen Codes\ny_true = df[\"mpg\"]\ny_pred = model.predict(X)\n\n# Berechne MSE\nmse = mean_squared_error(y_true, y_pred)\n\n# Berechne RMSE\nrmse = np.sqrt(mse)\n\n# Berechne MAPE\nmape = mean_absolute_percentage_error(y_true, y_pred) * 100  # In Prozent\n\nprint(f\"MSE: {mse:.2f}\")\nprint(f\"RMSE: {rmse:.2f} mpg\")\nprint(f\"MAPE: {mape:.2f}%\")\n\nMSE: 20.53\nRMSE: 4.53 mpg\nMAPE: 15.95%\n\n\nErgebnis: Zum Beispiel k√∂nnte die Ausgabe sein:\n- MSE: 24.50\n- RMSE: 4.95 mpg\n- MAPE: 15.20%\nInterpretation: Der RMSE von 4.95 mpg bedeutet, dass die Vorhersagen im Durchschnitt um etwa 4.95 Meilen pro Gallone von den tats√§chlichen Werten abweichen. Der MAPE von 15.20% zeigt, dass die Vorhersagen im Durchschnitt um 15.20% vom tats√§chlichen Verbrauch abweichen. Diese Ma√üe erg√§nzen den MSE und helfen, die Modellqualit√§t aus verschiedenen Perspektiven zu bewerten.\n\n\n\n\n\n9.1.3 Erkl√§rung Bias-Varianz-Tradeoff √ºber Modellkomplexit√§t\nWenn wir die Prognose eines Modells verbessern wollen, k√∂nnen wir die Komplexit√§t des Modells erh√∂hen, indem wir mehr Pr√§diktoren oder nichtlineare Transformationen hinzuf√ºgen. Hierzu k√∂nnen wir zum einem andere nicht-parametrische Modelle verwenden, wie z. B. Entscheidungsb√§ume, oder allgemein mehr Prediktoren in das Modell einf√ºgen. Dies kann jedoch zu √úberanpassung (Overfitting) f√ºhren, wenn das Modell zu komplex ist und die Trainingsdaten zu gut anpasst, aber auf neuen Daten schlecht generalisiert.\nDer Bias-Varianz-Tradeoff beschreibt das Zusammenspiel zweier Fehlerquellen bei der Modellierung:\n\nBias: Der Fehler, der durch die Vereinfachung eines komplexeren realen Problems durch ein Modell entsteht. Einfache Modelle (z. B. nur Intercept) haben hohen Bias, da sie die Datenstruktur schlecht abbilden.\nVarianz: Die Variabilit√§t der Modellvorhersagen, wenn das Modell auf unterschiedlichen Trainingsdaten gesch√§tzt wird. Komplexe Modelle (z. B. mit vielen Parametern) haben hohe Varianz, da sie stark von den Trainingsdaten abh√§ngen.\n\nDie Modellkomplexit√§t (z. B. Anzahl der Parameter) beeinflusst den Tradeoff:\n\nEinfache Modelle (wenige Parameter): Hoher Bias, niedrige Varianz, Gefahr der Unteranpassung (Underfitting).\nKomplexe Modelle (viele Parameter): Niedriger Bias, hohe Varianz, Gefahr der √úberanpassung (Overfitting).\n\nDas Ziel ist ein Modell mit minimalem Gesamtfehler, der sich aus Bias, Varianz und unvermeidbarem Fehler zusammensetzt:\n\\[\\text{Expected Error}^2 = \\text{Variance} + \\text{Bias}^2 + \\text{Variance of Error Terms}\\]\nBeispiel: Wir vergleichen drei Modelle mit unterschiedlicher Komplexit√§t.\n\nimport pandas as pd\nimport statsmodels.api as sm\nimport numpy as np\n\n# Lade den Datensatz\ndf = pd.read_csv(r\"../_assets/regression/Auto_Data_Set_963_49.csv\")\ndf[\"origin\"] = df[\"origin\"].map({1: \"USA\", 2: \"Europe\", 3: \"Japan\"})\n\n\n# Modell 1: Nur Intercept (sehr einfach)\nX1 = sm.add_constant(pd.DataFrame({\"const\": [1] * len(df)}))\nX1 = X1.astype(float)\nmodel1 = sm.OLS(df[\"mpg\"], X1).fit()\n\n# Modell 2: Horsepower (mittel komplex)\nX2 = sm.add_constant(df[[\"horsepower\"]])\nX2 = X2.astype(float)\nmodel2 = sm.OLS(df[\"mpg\"], X2).fit()\n\n# Modell 3: Horsepower, Quadratwurzel und Herkunft (komplex)\ndf[\"sqrt_horsepower\"] = np.sqrt(df[\"horsepower\"])\nX3 = pd.get_dummies(df[\"origin\"], prefix=\"origin\", drop_first=True)\nX3[\"horsepower\"] = df[\"horsepower\"]\nX3[\"sqrt_horsepower\"] = df[\"sqrt_horsepower\"]\nX3 = sm.add_constant(X3)\nX3= X3.astype(float)\nmodel3 = sm.OLS(df[\"mpg\"], X3).fit()\n\n# Modell 4: Mehre Potenzen (sehr komplex)\nX4 = X3.copy()\n# F√ºge weitere Gr√∂√üe\"cylinders\",\"displacement\",\"horsepower\",\"weight\",\"acceleration\",\"year\"\nX4[\"cylinders\"] = df[\"cylinders\"]\nX4[\"displacement\"] = df[\"displacement\"]\nX4[\"weight\"] = df[\"weight\"]\nX4[\"acceleration\"] = df[\"acceleration\"]\nX4[\"year\"] = df[\"year\"]\nX4 = X4.astype(float)\nmodel4 = sm.OLS(df[\"mpg\"], X4).fit()\n\n# Modell 5: Modell 4 mit einen Entscheidungsbaum\nfrom sklearn.tree import DecisionTreeRegressor\nX5 = X4.copy()\nmodel5 = DecisionTreeRegressor().fit(X5, df[\"mpg\"])\n\n\n# Berechne MSE f√ºr jedes Modell\nmse1 = np.mean((df[\"mpg\"] - model1.predict(X1)) ** 2)\nmse2 = np.mean((df[\"mpg\"] - model2.predict(X2)) ** 2)\nmse3 = np.mean((df[\"mpg\"] - model3.predict(X3)) ** 2)\nmse4 = np.mean((df[\"mpg\"] - model4.predict(X4)) ** 2)\nmse5 = np.mean((df[\"mpg\"] - model5.predict(X5)) ** 2)\n\nprint(f\"MSE Modell 1 (nur Intercept): {mse1:.2f}\")\nprint(f\"MSE Modell 2 (horsepower): {mse2:.2f}\")\nprint(f\"MSE Modell 3 (horsepower, sqrt, origin): {mse3:.2f}\")\nprint(f\"MSE Modell 4 (komplex): {mse4:.2f}\")\nprint(f\"MSE Modell 5 (Entscheidungsbaum): {mse5:.2f}\")\n\nMSE Modell 1 (nur Intercept): 60.76\nMSE Modell 2 (horsepower): 23.94\nMSE Modell 3 (horsepower, sqrt, origin): 17.06\nMSE Modell 4 (komplex): 8.55\nMSE Modell 5 (Entscheidungsbaum): 0.00\n\n\nErgebnis: Modell 1 hat den h√∂chsten MSE, da es zu einfach ist (hoher Bias). Modell 2 hat einen niedrigeren MSE , da es mehr Informationen nutzt. Modell 5 hat einen MSE von 0, da es die Trainingsdaten perfekt anpasst (hohe Varianz). Modell 3 hat einen MSE von 22.0, was auf eine gute Balance zwischen Bias und Varianz hinweist.\n\n\n9.1.4 Aufteilung in Training, Validation und Testset\nAuf den Trainingsdaten evaluiert scheint es, als ob die komplexeren Modelle besser abschneiden. Wir wissen aber nicht, ob ein Overfitting vorliegt, da wir die Modelle nur auf den Trainingsdaten getestet haben. Um die Modellleistung auf neuen, ungesehenen Daten zu bewerten und √úberanpassung zu vermeiden, teilen wir die Daten in drei Teile:\n\n\nTrainings-Set: Wird verwendet, um die Modellparameter (z. B. \\(\\beta_0, \\beta_1, \\ldots\\)) zu sch√§tzen.\nValidierungs-Set: Wird genutzt, um Modelle mit unterschiedlicher Komplexit√§t oder Pr√§diktoren zu vergleichen und das beste Modell auszuw√§hlen.\nTest-Set: Dient als Hold-Out-Set, um die endg√ºltige Leistung des ausgew√§hlten Modells auf ungesehenen Daten zu bewerten.\n\nEine typische Aufteilung ist 70 % Training, 15 % Validierung und 15 % Test. Um robuste Ergebnisse zu erzielen, kann Kreuzvalidierung (z. B. 5-fache Kreuzvalidierung) verwendet werden, um den Validierungs-MSE zu sch√§tzen.\n\n\n\n9.1.5 Erkl√§rung Bias-Varianz-Tradeoff √ºber Modellkomplexit√§t (mit TVT-Split)\nUm den Bias-Varianz-Tradeoff zu untersuchen, wiederholen wir das Experiment mit einem Trainings-Validierungs-Test-Split (TVT-Split) von 75-15-15%, um die Modelle auf Trainings-, Validierungs- und Testdaten zu bewerten. Dies erm√∂glicht es, die Generalisierungsf√§higkeit der Modelle zu √ºberpr√ºfen und √úberanpassung zu erkennen. Wir vergleichen f√ºnf Modelle mit zunehmender Komplexit√§t, einschlie√ülich eines nicht-linearen Entscheidungsbaums, und plotten die Mean Squared Error (MSE)-Werte f√ºr Training, Validation und Test, um den Einfluss der Modellkomplexit√§t zu visualisieren.\nExperiment: Wir teilen den Auto-Datensatz in 75% Training, 15% Validierung und 15% Test auf, trainieren die Modelle auf den Trainingsdaten, berechnen den MSE f√ºr alle Datensets und stellen die Ergebnisse grafisch dar.\n\nimport pandas as pd\nimport statsmodels.api as sm\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nimport matplotlib.pyplot as plt\n\n# Lade den Datensatz\ndf = pd.read_csv(r\"../_assets/regression/Auto_Data_Set_963_49.csv\")\ndf[\"origin\"] = df[\"origin\"].map({1: \"USA\", 2: \"Europe\", 3: \"Japan\"})\n\n# Erstelle transformierte Variable\ndf[\"sqrt_horsepower\"] = np.sqrt(df[\"horsepower\"])\n\n# Definiere Pr√§diktoren und Zielvariable\nX = pd.DataFrame({\n    \"horsepower\": df[\"horsepower\"],\n    \"sqrt_horsepower\": df[\"sqrt_horsepower\"],\n    \"cylinders\": df[\"cylinders\"],\n    \"displacement\": df[\"displacement\"],\n    \"weight\": df[\"weight\"],\n    \"acceleration\": df[\"acceleration\"],\n    \"year\": df[\"year\"]\n})\nX = pd.concat([X, pd.get_dummies(df[\"origin\"], prefix=\"origin\", drop_first=False)], axis=1)\ny = df[\"mpg\"]\n\n# TVT-Split: 75% Training, 15% Validierung, 15% Test\nX_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42, shuffle=True)\nX_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.1765, random_state=42)  # 0.1765 ‚âà 15/(100-15)\n\n# Setze Indizes zur√ºck, um Alignment-Probleme zu vermeiden\nX_train = X_train.reset_index(drop=True)\nX_val = X_val.reset_index(drop=True)\nX_test = X_test.reset_index(drop=True)\ny_train = y_train.reset_index(drop=True)\ny_val = y_val.reset_index(drop=True)\ny_test = y_test.reset_index(drop=True)\n\n# Modell 1: Nur Intercept (sehr einfach)\nX1_train = sm.add_constant(pd.DataFrame({\"const\": [1] * len(y_train)})).astype(float)\nX1_val = sm.add_constant(pd.DataFrame({\"const\": [1] * len(y_val)})).astype(float)\nX1_test = sm.add_constant(pd.DataFrame({\"const\": [1] * len(y_test)})).astype(float)\nmodel1 = sm.OLS(y_train, X1_train).fit()\n\n# Modell 2: Horsepower (mittel komplex)\nX2_train = sm.add_constant(X_train[[\"horsepower\"]]).astype(float)\nX2_val = sm.add_constant(X_val[[\"horsepower\"]]).astype(float)\nX2_test = sm.add_constant(X_test[[\"horsepower\"]]).astype(float)\nmodel2 = sm.OLS(y_train, X2_train).fit()\n\n# Modell 3: Horsepower, Quadratwurzel und Herkunft (komplex)\nX3_train = sm.add_constant(X_train[[\"horsepower\", \"sqrt_horsepower\", \"origin_Europe\", \"origin_Japan\"]]).astype(float)\nX3_val = sm.add_constant(X_val[[\"horsepower\", \"sqrt_horsepower\", \"origin_Europe\", \"origin_Japan\"]]).astype(float)\nX3_test = sm.add_constant(X_test[[\"horsepower\", \"sqrt_horsepower\", \"origin_Europe\", \"origin_Japan\"]]).astype(float)\nmodel3 = sm.OLS(y_train, X3_train).fit()\n\n# Modell 4: Mehrere Pr√§diktoren (sehr komplex)\nX4_train = sm.add_constant(X_train).astype(float)\nX4_val = sm.add_constant(X_val).astype(float)\nX4_test = sm.add_constant(X_test).astype(float)\nmodel4 = sm.OLS(y_train, X4_train).fit()\n\n# Modell 5: Entscheidungsbaum\nmodel5 = DecisionTreeRegressor(random_state=42).fit(X4_train, y_train)\n\n# Berechne MSE f√ºr jedes Modell auf Training, Validierung und Test\nmse1_train = np.mean((y_train - model1.predict(X1_train)) ** 2)\nmse1_val = np.mean((y_val - model1.predict(X1_val)) ** 2)\nmse1_test = np.mean((y_test - model1.predict(X1_test)) ** 2)\n\nmse2_train = np.mean((y_train - model2.predict(X2_train)) ** 2)\nmse2_val = np.mean((y_val - model2.predict(X2_val)) ** 2)\nmse2_test = np.mean((y_test - model2.predict(X2_test)) ** 2)\n\nmse3_train = np.mean((y_train - model3.predict(X3_train)) ** 2)\nmse3_val = np.mean((y_val - model3.predict(X3_val)) ** 2)\nmse3_test = np.mean((y_test - model3.predict(X3_test)) ** 2)\n\nmse4_train = np.mean((y_train - model4.predict(X4_train)) ** 2)\nmse4_val = np.mean((y_val - model4.predict(X4_val)) ** 2)\nmse4_test = np.mean((y_test - model4.predict(X4_test)) ** 2)\n\nmse5_train = np.mean((y_train - model5.predict(X4_train)) ** 2)\nmse5_val = np.mean((y_val - model5.predict(X4_val)) ** 2)\nmse5_test = np.mean((y_test - model5.predict(X4_test)) ** 2)\n\n# Ausgabe der MSE-Werte\nprint(f\"Modell 1 (nur Intercept): Train MSE = {mse1_train:.2f}, Val MSE = {mse1_val:.2f}, Test MSE = {mse1_test:.2f}\")\nprint(f\"Modell 2 (horsepower): Train MSE = {mse2_train:.2f}, Val MSE = {mse2_val:.2f}, Test MSE = {mse2_test:.2f}\")\nprint(f\"Modell 3 (horsepower, sqrt, origin): Train MSE = {mse3_train:.2f}, Val MSE = {mse3_val:.2f}, Test MSE = {mse3_test:.2f}\")\nprint(f\"Modell 4 (komplex): Train MSE = {mse4_train:.2f}, Val MSE = {mse4_val:.2f}, Test MSE = {mse4_test:.2f}\")\nprint(f\"Modell 5 (Entscheidungsbaum): Train MSE = {mse5_train:.2f}, Val MSE = {mse5_val:.2f}, Test MSE = {mse5_test:.2f}\")\n\n\n# Plot der MSE-Werte\nplt.figure(figsize=(8, 4))  # Kleinere Gr√∂√üe\nmodels = [\"Modell 1\", \"Modell 2\", \"Modell 3\", \"Modell 4\", \"Modell 5\"]\nsets = [\"Training\", \"Validation\", \"Test\"]\nmse_values = [\n    [mse1_train, mse1_val, mse1_test],\n    [mse2_train, mse2_val, mse2_test],\n    [mse3_train, mse3_val, mse3_test],\n    [mse4_train, mse4_val, mse4_test],\n    [mse5_train, mse5_val, mse5_test]\n]\n\n# Verwende viridis-Farbpalette\ncolors = plt.cm.viridis(np.linspace(0, 1, len(models)))\n\n# Scatterplot mit verbundenen Linien\nfor i, model in enumerate(models):\n    plt.plot(sets, mse_values[i], marker='o', linestyle='-', color=colors[i], label=model)\n\nplt.xlabel(\"Datensets\")\nplt.ylabel(\"Mean Squared Error (MSE)\")\nplt.title(\"MSE der Modelle auf Training, Validierung und Test\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\nModell 1 (nur Intercept): Train MSE = 61.40, Val MSE = 60.18, Test MSE = 58.54\nModell 2 (horsepower): Train MSE = 23.71, Val MSE = 26.24, Test MSE = 22.78\nModell 3 (horsepower, sqrt, origin): Train MSE = 15.61, Val MSE = 23.75, Test MSE = 17.52\nModell 4 (komplex): Train MSE = 8.62, Val MSE = 8.92, Test MSE = 8.30\nModell 5 (Entscheidungsbaum): Train MSE = 0.00, Val MSE = 11.47, Test MSE = 12.35\n\n\n\n\n\n\n\n\n\nErgebnis und Interpretation: Der Plot zeigt die MSE-Werte der f√ºnf Modelle f√ºr Training, Validierung und Test. Typischerweise gilt:\n\nModell 1 (nur Intercept): Hoher MSE auf allen Datensets, da das Modell zu einfach ist (hoher Bias, niedrige Varianz).\nModell 2 (horsepower): Niedrigerer MSE, da es mehr Informationen nutzt, aber immer noch relativ einfach.\nModell 3 (horsepower, sqrt, origin): Noch niedrigerer MSE, da es nicht-lineare Effekte und kategorische Variablen einbezieht.\nModell 4 (komplex): Sehr niedriger Trainings-MSE, aber potenziell h√∂herer Validierungs- und Test-MSE durch √úberanpassung (niedriger Bias, hohe Varianz).\nModell 5 (Entscheidungsbaum): Sehr niedriger Trainings-MSE (nahe 0), aber oft deutlich h√∂herer Validierungs- und Test-MSE, da Entscheidungsb√§ume stark √ºberanpassen k√∂nnen.\n\nEin Modell mit √§hnlichem MSE auf Validierung und Test generalisiert gut. Ein gro√üer Anstieg des MSE von Training zu Validierung/Test deutet auf √úberanpassung hin, wie es bei Modell 5 h√§ufig der Fall ist. Der Plot hilft, den Bias-Varianz-Tradeoff visuell zu verstehen: Einfache Modelle haben hohe Bias-Fehler, w√§hrend komplexe Modelle (insbesondere nicht-parametrische wie Entscheidungsb√§ume) hohe Varianz-Fehler aufweisen.\n\n\n\n\n\n\nResampling-Methoden\n\n\n\nResampling-Methoden wie Cross-Validation (CV) erm√∂glichen eine robuste Bewertung der Modellleistung, indem sie die Daten mehrfach aufteilen und den Fehler √ºber verschiedene Trainings- und Validierungssets mitteln. Dies ist besonders n√ºtzlich, um die Generalisierungsf√§higkeit eines Modells zu testen und √úberanpassung zu vermeiden. Hier sind drei g√§ngige Methoden mit ihren Beschreibungen und ein Beispiel, wie man sie mit scikit-learn implementiert.\n\nk-Fold Cross-Validation:\nDie Daten werden in \\(k\\) gleich gro√üe Teile (Folds) aufgeteilt. Das Modell wird \\(k\\)-mal trainiert, wobei jeweils ein Fold als Validierungsset und die restlichen \\(k-1\\) Folds als Trainingsset verwendet werden. Der MSE wird √ºber alle \\(k\\) Iterationen gemittelt.\nVorteile: Gute Balance zwischen Rechenaufwand und Stabilit√§t; typisch \\(k=5\\) oder \\(k=10\\).\nNachteile: Zuf√§llige Aufteilung kann f√ºr strukturierte Daten (z. B. Zeitreihen) problematisch sein.\nLeave-One-Out Cross-Validation (LOOCV):\nEine spezielle Form von k-Fold CV, bei der \\(k = n\\) (Anzahl der Datenpunkte). In jeder Iteration wird ein einzelner Datenpunkt als Validierungsset verwendet, und die restlichen \\(n-1\\) Datenpunkte dienen als Trainingsset. Der MSE wird √ºber alle \\(n\\) Iterationen gemittelt.\nVorteile: Nutzt fast alle Daten zum Training, sehr pr√§zise Sch√§tzung.\nNachteile: Sehr rechenintensiv f√ºr gro√üe Datens√§tze.\nZeitreihen-Cross-Validation:\nF√ºr Zeitreihendaten, bei denen die Reihenfolge wichtig ist, wird eine zeitliche Aufteilung verwendet. Das Modell wird auf einem wachsenden Trainingsfenster trainiert und auf einem nachfolgenden Validierungsfenster getestet, ohne zuk√ºnftige Daten zu verwenden.\nVorteile: Respektiert die zeitliche Struktur der Daten.\nNachteile: Weniger Daten f√ºr Training/Validierung, da keine zuf√§llige Aufteilung m√∂glich ist.\n\nBeispiel: Berechnung des MSE f√ºr Modell 4 (komplexes lineares Modell) mit k-Fold CV (\\(k=5\\)), LOOCV und Zeitreihen-CV unter Verwendung von scikit-learn. F√ºr Zeitreihen-CV wird angenommen, dass die Daten nach year sortiert sind.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold, LeaveOneOut, TimeSeriesSplit\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Lade den Datensatz\ndf = pd.read_csv(r\"../_assets/regression/Auto_Data_Set_963_49.csv\")\ndf[\"origin\"] = df[\"origin\"].map({1: \"USA\", 2: \"Europe\", 3: \"Japan\"})\n\n# Erstelle transformierte Variable\ndf[\"sqrt_horsepower\"] = np.sqrt(df[\"horsepower\"])\n\n# Definiere Pr√§diktoren und Zielvariable (wie Modell 4)\nX = pd.DataFrame({\n    \"horsepower\": df[\"horsepower\"],\n    \"sqrt_horsepower\": df[\"sqrt_horsepower\"],\n    \"cylinders\": df[\"cylinders\"],\n    \"displacement\": df[\"displacement\"],\n    \"weight\": df[\"weight\"],\n    \"acceleration\": df[\"acceleration\"],\n    \"year\": df[\"year\"]\n})\nX = pd.concat([X, pd.get_dummies(df[\"origin\"], prefix=\"origin\", drop_first=True)], axis=1)\ny = df[\"mpg\"]\n\n# Initialisiere Modell (LinearRegression statt statsmodels f√ºr CV-Kompatibilit√§t)\nmodel = LinearRegression()\n\n# 1. k-Fold Cross-Validation (k=5)\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\nmse_kf = []\nfor train_idx, val_idx in kf.split(X):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_val)\n    mse_kf.append(mean_squared_error(y_val, y_pred))\nmse_kf_mean = np.mean(mse_kf)\n\n# 2. Leave-One-Out Cross-Validation\nloo = LeaveOneOut()\nmse_loo = []\nfor train_idx, val_idx in loo.split(X):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_val)\n    mse_loo.append(mean_squared_error(y_val, y_pred))\nmse_loo_mean = np.mean(mse_loo)\n\n# 3. Zeitreihen-Cross-Validation (angenommen, Daten sind nach 'year' sortiert)\ndf_sorted = df.sort_values(\"year\")\nX_sorted = X.loc[df_sorted.index]\ny_sorted = y.loc[df_sorted.index]\ntscv = TimeSeriesSplit(n_splits=5)\nmse_tscv = []\nfor train_idx, val_idx in tscv.split(X_sorted):\n    X_train, X_val = X_sorted.iloc[train_idx], X_sorted.iloc[val_idx]\n    y_train, y_val = y_sorted.iloc[train_idx], y_sorted.iloc[val_idx]\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_val)\n    mse_tscv.append(mean_squared_error(y_val, y_pred))\nmse_tscv_mean = np.mean(mse_tscv)\n\n# Ausgabe der Ergebnisse\nprint(f\"5-Fold CV MSE: {mse_kf_mean:.2f}\")\nprint(f\"LOOCV MSE: {mse_loo_mean:.2f}\")\nprint(f\"Zeitreihen-CV MSE: {mse_tscv_mean:.2f}\")\n\n5-Fold CV MSE: 9.14\nLOOCV MSE: 9.10\nZeitreihen-CV MSE: 13.94\n\n\nErgebnis: Beispielausgabe k√∂nnte sein:\n\n5-Fold CV MSE: 15.50\n\nLOOCV MSE: 15.70\n\nZeitreihen-CV MSE: 16.20\n\nInterpretation: Die MSE-Werte der verschiedenen CV-Methoden geben Einblick in die Modellleistung. 5-Fold CV ist effizient und stabil, LOOCV ist pr√§zise, aber rechenintensiv, und Zeitreihen-CV respektiert die zeitliche Struktur, was f√ºr den Auto-Datensatz (sortiert nach year) relevant sein kann. Ein √§hnlicher MSE √ºber die Methoden hinweg deutet auf ein robustes Modell hin.",
    "crumbs": [
      "Regressions-Analyse",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Vorhersagen mittels Linearen Regression</span>"
    ]
  },
  {
    "objectID": "regression/tutorial.html",
    "href": "regression/tutorial.html",
    "title": "Tutorial 1: Vorhersage von Netzlasten",
    "section": "",
    "text": "Ziel\nNachdem wir in Tutorial 2 Muster in den Netzlasten analysiert und Unterschiede zwischen Wochentagen und Sonn-/Feiertagen identifiziert haben, ist das Ziel dieses Tutorials, die Netzlast basierend auf historischen Daten vorherzusagen. Wir nutzen die bereinigten Daten der Global Energy Forecasting Competition Hong, Pinson, and Fan (2014) und entwickeln zwei verschiedene Modelle, um die Netzlast zu prognostizieren. Die Vorhersagen und die tats√§chlichen Werte werden mit Plotly interaktiv visualisiert, um die Modellleistung intuitiv zu vergleichen. Zus√§tzlich berechnen wir den Mean Absolute Percentage Error (MAPE) f√ºr Trainings-, Validierungs- und Testsets, um die Genauigkeit der Modelle zu bewerten.",
    "crumbs": [
      "Regressions-Analyse",
      "Tutorial 1: Vorhersage von Netzlasten"
    ]
  },
  {
    "objectID": "regression/tutorial.html#ziel",
    "href": "regression/tutorial.html#ziel",
    "title": "Tutorial 1: Vorhersage von Netzlasten",
    "section": "",
    "text": "Aufgaben\n\nDatenaufteilung: Teilen Sie die Zeitreihendaten in 70% Training, 15% Validierung und 15% Test auf, wobei die zeitliche Reihenfolge erhalten bleibt (kein Shuffling).\nModellentwicklung: Entwickeln Sie zwei verschiedene Modelle zur Vorhersage der Netzlast z.B.:\n\nModell 1: Lineare Regression mit Pr√§diktoren wie Wochentag, Stunde des Tages, Temperatur und einer Interaktion zwischen Wochentag und Stunde.\nModell 2: Entscheidungsbaum mit denselben Pr√§diktoren, erg√§nzt um eine Transformation (z. B. quadratische Temperatur).\n\nFehlerbewertung: Berechnen Sie den MAPE f√ºr Trainings-, Validierungs- und Testsets f√ºr beide Modelle.\nVisualisierung: Erstellen Sie eine interaktive Zeitreihenvisualisierung mit Plotly, die die tats√§chliche Netzlast und die Vorhersagen beider Modelle f√ºr das Testset darstellt.\n\n\n\n\n\nHong, Tao, Pierre Pinson, and Shu Fan. 2014. ‚ÄúGlobal Energy Forecasting Competition 2012.‚Äù International Journal of Forecasting 30 (2): 357‚Äì63. https://doi.org/https://doi.org/10.1016/j.ijforecast.2013.07.001.",
    "crumbs": [
      "Regressions-Analyse",
      "Tutorial 1: Vorhersage von Netzlasten"
    ]
  },
  {
    "objectID": "regression/logistic_regression.html",
    "href": "regression/logistic_regression.html",
    "title": "10¬† Klassifikation",
    "section": "",
    "text": "10.1 Logistische Regression\nKlassifikation ist eine Methode des √ºberwachten Lernens, bei der die abh√§ngige Variable \\(Y\\) kategorisch ist, d.h., sie nimmt diskrete Werte an (z.B. ‚Äúja/nein‚Äù oder ‚ÄúUSA/nicht USA‚Äù). Ziel der Klassifikation ist es, die Wahrscheinlichkeit zu sch√§tzen, dass eine Beobachtung zu einer bestimmten Klasse geh√∂rt. Dieses Kapitel konzentriert sich auf bin√§re Klassifikation und verwendet den Auto-Datensatz.\nWir klassifizieren, ob ein Auto aus den USA stammt (origin == \"USA\", kodiert als 1) oder nicht (origin != \"USA\", kodiert als 0), z.B. basierend auf horsepower und weight. In diesem Fall haben wir eine einzelne Dummy-Variable als Zielvariable, die angibt, ob das Auto aus den USA stammt oder nicht.\nDie logistische Regression ist ein Standardmodell f√ºr bin√§re Klassifikation. Sie sch√§tzt die Wahrscheinlichkeit \\(p(Y=1 | X)\\), dass \\(Y=1\\) ist, gegeben die Pr√§diktoren \\(X\\). Die logistische Funktion (Sigmoid-Funktion) stellt sicher, dass die vorhergesagte Wahrscheinlichkeit zwischen 0 und 1 liegt:\n\\[\np(Y=1 | X) = \\frac{e^{\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p}}{1 + e^{\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p}}\n\\]\nDiese Darstellung ist nicht-linear, was bedeutet, dass die Beziehung zwischen den Pr√§diktoren und der Wahrscheinlichkeit nicht linear ist. Wir k√∂nnen Sie also nicht mit dem OLS-Ansatz (Ordinary Least Squares) die optimalen Koeffizienten \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) finden. Allerdings k√∂nnen wir das Problem auch so umformulieren, dass wir eine Linearit√§t herstellen:\n\\[\n\\log\\left(\\frac{p}{1 - p}\\right) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p\n\\]\nHierbei stellen \\(\\frac{p}{1 - p}\\) die Odds (Chancen) dar, dass \\(Y=1\\) ist. Die Odds sind das Verh√§ltnis der Wahrscheinlichkeit, dass \\(Y=1\\) ist, zu der Wahrscheinlichkeit, dass \\(Y=0\\) ist. Anwendung finden Sie heute z.B. in Sportswetten, wo die Quoten die Chancen f√ºr den Gewinn eines Teams darstellen.",
    "crumbs": [
      "Regressions-Analyse",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Klassifikation</span>"
    ]
  },
  {
    "objectID": "regression/logistic_regression.html#logistische-regression",
    "href": "regression/logistic_regression.html#logistische-regression",
    "title": "10¬† Klassifikation",
    "section": "",
    "text": "10.1.1 Beispiel mit dem Auto-Datensatz\nIn der Anwendung ver√§ndert sich f√ºr uns wenig. Unsere Zielvariable y ist bin√§r (Zugeh√∂rigkeit zur Klassen), und wir verwenden die logistische Regression, um die Wahrscheinlichkeit zu sch√§tzen, dass ein Auto aus den USA stammt. Wir verwenden horsepower und weight als Pr√§diktoren.\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Lade den Datensatz\ndf = pd.read_csv(r\"../_assets/regression/Auto_Data_Set_963_49.csv\")\ndf[\"origin\"] = df[\"origin\"].map({1: \"USA\", 2: \"Europe\", 3: \"Japan\"})\ndf[\"is_usa\"] = (df[\"origin\"] == \"USA\").astype(int)\n\n# Pr√§diktoren und Zielvariable\nX = df[[\"horsepower\", \"weight\"]]\nX = sm.add_constant(X)\ny = df[\"is_usa\"]\n\n# Fitte logistische Regression\nlogit_model = sm.Logit(y, X).fit()\nprint(logit_model.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.424129\n         Iterations 7\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                 is_usa   No. Observations:                  392\nModel:                          Logit   Df Residuals:                      389\nMethod:                           MLE   Df Model:                            2\nDate:                Tue, 03 Jun 2025   Pseudo R-squ.:                  0.3589\nTime:                        09:39:38   Log-Likelihood:                -166.26\nconverged:                       True   LL-Null:                       -259.33\nCovariance Type:            nonrobust   LLR p-value:                 3.788e-41\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -6.6482      0.720     -9.236      0.000      -8.059      -5.237\nhorsepower    -0.0025      0.009     -0.290      0.772      -0.020       0.015\nweight         0.0027      0.000      6.671      0.000       0.002       0.003\n==============================================================================\n\n\nErgebnis: Die Koeffizienten zeigen den Einfluss der Pr√§diktoren. Ein positiver Koeffizient f√ºr weight bedeutet, dass schwerere Autos eher aus den USA stammen, w√§hrend ein negativer Koeffizient f√ºr horsepower darauf hinweist, dass Autos mit h√∂herer Leistung weniger wahrscheinlich aus den USA kommen. Allerdings sin die genauen Werte der Koeffizienten nicht direkt interpretierbar, da sie die Ver√§nderung der Log-Odds darstellen.",
    "crumbs": [
      "Regressions-Analyse",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Klassifikation</span>"
    ]
  },
  {
    "objectID": "regression/logistic_regression.html#evaluation-der-ergebnisse",
    "href": "regression/logistic_regression.html#evaluation-der-ergebnisse",
    "title": "10¬† Klassifikation",
    "section": "10.2 Evaluation der Ergebnisse",
    "text": "10.2 Evaluation der Ergebnisse\nF√ºr jede Klassifikation mit dem Modell sch√§tzen wir eine Wahrscheinlichkeit \\(p(Y=1 | X)\\), die angibt, wie wahrscheinlich es ist, dass ein Auto aus den USA stammt. Hierbei k√∂nnen wir richtig liegen oder auch nicht.\n\n\n10.2.1 Confusion Matrix\nDie Confusion Matrix bewertet die Leistung eines Klassifikationsmodells, indem sie korrekte und falsche Vorhersagen in einer Tabelle darstellt:\n\nTrue Positives (TP): Korrekte positive Vorhersagen (USA, wenn wirklich aus USA).\nTrue Negatives (TN): Korrekte negative Vorhersagen (nicht USA, wenn nicht aus USA).\nFalse Positives (FP): Falsche positive Vorhersagen (Typ-I-Fehler) (USA, wenn nicht aus USA).\nFalse Negatives (FN): Falsche negative Vorhersagen (Typ-II-Fehler) (nicht USA, wenn wirklich aus USA).\n\nBeim T-Test sind diese Fehler vergleichbar, wenn wir die Nullhypothese ablehnen, obwohl sie wahr ist (FP) oder die Nullhypothese nicht ablehnen, obwohl sie falsch ist (FN).\n\n10.2.1.1 Beispiel mit dem Auto-Datensatz\nWir klassifizieren Autos als ‚ÄúUSA‚Äù oder ‚Äúnicht USA‚Äù mit einem Schwellenwert von 0.5:\n\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\n\n# Vorhersagen\ny_pred_prob = logit_model.predict(X)\ny_pred = (y_pred_prob &gt; 0.5).astype(int)\n\n# Confusion Matrix\ncm = confusion_matrix(y, y_pred)\nprint(\"Confusion Matrix:\\n\", cm)\n\n# Visualisierung\nplt.figure(figsize=(5, 5))\nplt.imshow(cm, cmap=\"Blues\")\nplt.colorbar()\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Vorhergesagt\")\nplt.ylabel(\"Tats√§chlich\")\nplt.xticks([0, 1], [\"nicht USA\", \"USA\"])\nplt.yticks([0, 1], [\"nicht USA\", \"USA\"])\nplt.show()\n\nConfusion Matrix:\n [[109  38]\n [ 40 205]]\n\n\n\n\n\n\n\n\n\nErgebnis: Die Matrix k√∂nnte z.B. zeigen:\n- TN = 200 (korrekt ‚Äúnicht USA‚Äù),\n- TP = 150 (korrekt ‚ÄúUSA‚Äù),\n- FP = 30 (falsch ‚ÄúUSA‚Äù),\n- FN = 20 (falsch ‚Äúnicht USA‚Äù).\n\n\n10.2.1.2 Beispiel: Corona-Test\nEin Corona-Test-Beispiel verdeutlicht die Fehlerarten:\n\n\n\n\n\n\n\n\n\nHat Corona (\\(y\\))\nTestwahrscheinlichkeit (\\(\\hat{p}(y)\\))\nKlassifikation (Schwelle 0.5)\nFehlerart\n\n\n\n\n0\n0.4\n0\nTN\n\n\n1\n0.9\n1\nTP\n\n\n0\n0.7\n1\nFP\n\n\n1\n0.4\n0\nFN\n\n\n\nDie Confusion Matrix dazu:\n\n\n\n\nVorhergesagt 0\nVorhergesagt 1\n\n\n\n\nTats√§chlich 0\nTN = 1\nFP = 1\n\n\nTats√§chlich 1\nFN = 1\nTP = 1\n\n\n\nHier sehen wir, dass ein FN (falsch negativ) gef√§hrlich sein k√∂nnte, da eine infizierte Person nicht erkannt wird. Eine M√∂glichkeit, dies zu verbessern, w√§re die Anpassung des Schwellenwerts (Thresholds), um die Sensitivit√§t zu erh√∂hen, auch wenn dies die Spezifit√§t verringert. Wenn wir z.B. sicherstellen wollen, dass alle infizierten Personen erkannt werden, k√∂nnten wir den Schwellenwert auf 0.3 senken, was zu mehr TP f√ºhren w√ºrde, aber auch die FP erh√∂hen k√∂nnte.\n\n\n\n\n\n\n\n\n\nHat Corona (\\(y\\))\nTestwahrscheinlichkeit (\\(\\hat{p}(y)\\))\nKlassifikation (Schwelle 0.3)\nFehlerart\n\n\n\n\n0\n0.4\n1\nFP\n\n\n1\n0.9\n1\nTP\n\n\n0\n0.7\n1\nFP\n\n\n1\n0.4\n1\nTP\n\n\n\nDie Confusion Matrix dazu:\n\n\n\n\nVorhergesagt 0\nVorhergesagt 1\n\n\n\n\nTats√§chlich 0\nTN = 0\nFP = 2\n\n\nTats√§chlich 1\nFN = 0\nTP = 2\n\n\n\nDie Anpassung des Schwellenwerts ist eine g√§ngige Praxis, um die Balance zwischen Sensitivit√§t und Spezifit√§t zu steuern, abh√§ngig von den Anforderungen der spezifischen Anwendung. Dabei wird das eigentliche Modell nicht ver√§ndert, sondern nur die Entscheidung, ab welcher Wahrscheinlichkeit eine Klasse zugeordnet wird.\n\n\n\n\n\n\nMulti-Class Klassifikation: Confusion Matrix f√ºr dreifache Klassifikation\n\n\n\n\n\nEs gibtverschiedene M√∂glichkeiten einen Klassifikator f√ºr mehr als zwei Klassen zu erstellen. Bei der logistischen Regression k√∂nnen einfach drei Klassen (z.B. ‚ÄúUSA‚Äù, ‚ÄúEurope‚Äù, ‚ÄúJapan‚Äù) als bin√§re Klassifikationen behandelt werden, indem f√ºr jede Klasse ein separates Modell trainiert wird. Die Confusion Matrix f√ºr bin√§re Klassifikation l√§sst sich auf Multi-Class Klassifikation erweitern, indem sie f√ºr jede Klasse die korrekten und falschen Vorhersagen darstellt. Bei einer dreifachen Klassifikation mit Klassen \\(C_1, C_2, C_3\\) (z.B. ‚ÄúUSA‚Äù, ‚ÄúEurope‚Äù, ‚ÄúJapan‚Äù) ist die Matrix eine \\(3 \\times 3\\)-Tabelle, wobei:\n\nDiagonalelemente (\\(cm_{ii}\\)): Anzahl der korrekten Vorhersagen f√ºr Klasse \\(i\\) (True Positives f√ºr diese Klasse).\nNicht-Diagonalelemente (\\(cm_{ij}\\), \\(i \\neq j\\)): Anzahl der falschen Vorhersagen, bei denen Klasse \\(i\\) tats√§chlich ist, aber Klasse \\(j\\) vorhergesagt wurde.\nFormel f√ºr Genauigkeit (Accuracy): \\(\\text{Accuracy} = \\frac{\\sum_{i=1}^k cm_{ii}}{\\sum_{i,j} cm_{ij}}\\), wobei \\(k\\) die Anzahl der Klassen ist.\n\nBeispiel mit dem Auto-Datensatz: Wir klassifizieren die Herkunft (origin) eines Autos in drei Klassen: ‚ÄúUSA‚Äù, ‚ÄúEurope‚Äù, ‚ÄúJapan‚Äù, basierend auf weight und horsepower. Eine logistische Regression (mit multi_class=\"multinomial\") wird verwendet, und die Confusion Matrix wird visualisiert.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\n\n# Lade den Datensatz\ndf = pd.read_csv(r\"../_assets/regression/Auto_Data_Set_963_49.csv\")\ndf[\"origin\"] = df[\"origin\"].map({1: \"USA\", 2: \"Europe\", 3: \"Japan\"})\n\n# Pr√§diktoren und Zielvariable\nX = df[[\"weight\", \"horsepower\"]]\ny_multi = df[\"origin\"]\n\n# Fitte logistische Regression f√ºr Multi-Class\nlog_reg = LogisticRegression(multi_class=\"multinomial\", max_iter=1000, random_state=42)\nlog_reg.fit(X, y_multi)\n\n# Vorhersagen\ny_pred_multi = log_reg.predict(X)\n\n# Confusion Matrix\ncm_multi = confusion_matrix(y_multi, y_pred_multi, labels=[\"USA\", \"Europe\", \"Japan\"])\nprint(\"Confusion Matrix:\\n\", cm_multi)\n\n# Visualisierung\nplt.figure(figsize=(6, 6))\nsns.heatmap(cm_multi, annot=True, fmt=\"d\", cmap=\"Blues\", \n            xticklabels=[\"USA\", \"Europe\", \"Japan\"], \n            yticklabels=[\"USA\", \"Europe\", \"Japan\"])\nplt.title(\"Confusion Matrix: Dreifache Klassifikation (USA, Europe, Japan)\")\nplt.xlabel(\"Vorhergesagt\")\nplt.ylabel(\"Tats√§chlich\")\nplt.show()\n\n# Berechne Genauigkeit\naccuracy = np.trace(cm_multi) / np.sum(cm_multi)\nprint(f\"Genauigkeit: {accuracy:.2f}\")\n\nConfusion Matrix:\n [[216   4  25]\n [ 29   6  33]\n [ 24   2  53]]\n\n\n/home/runner/work/MECH-B-4-MLDS-MLDS1/MECH-B-4-MLDS-MLDS1/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning:\n\n'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n\n\n\n\n\n\n\n\n\n\nGenauigkeit: 0.70\n\n\nFazit: Die Multi-Class Confusion Matrix ist ein m√§chtiges Werkzeug, um die Leistung eines Klassifikators f√ºr mehr als zwei Klassen zu bewerten. Sie erm√∂glicht eine detaillierte Analyse der Fehlerarten und hilft, Schw√§chen des Modells zu identifizieren.\n\n\n\n\n\n\n10.2.2 Fehlermetriken f√ºr Klassifikation\nBei der Bewertung eines Klassifikationsmodells liefert die Confusion Matrix detaillierte Informationen, aber einzelne Metriken wie Precision, Recall (Sensitivit√§t), Spezifit√§t und F1-Score fassen die Leistung pr√§gnant zusammen. Diese Metriken sind besonders n√ºtzlich, um die Balance zwischen korrekten und falschen Vorhersagen zu verstehen, insbesondere bei unbalancierten Datens√§tzen.\n\nGenauigkeit (Accuracy): Anteil der korrekten Vorhersagen (TP + TN) an allen Vorhersagen. Sie beantwortet: ‚ÄûWie viele Vorhersagen waren insgesamt korrekt?‚Äú \\(\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\\), wobei \\(TP\\) True Positives, \\(TN\\) True Negatives, \\(FP\\) False Positives und \\(FN\\) False Negatives sind.\nPrecision (Pr√§zision): Anteil der korrekten positiven Vorhersagen unter allen positiven Vorhersagen. Sie beantwortet: ‚ÄûWie viele der als positiv klassifizierten F√§lle sind tats√§chlich positiv?‚Äú \\(\\text{Precision} = \\frac{TP}{TP + FP}\\), wobei \\(TP\\) True Positives und \\(FP\\) False Positives sind.\nRecall (Sensitivit√§t, True Positive Rate): Anteil der korrekt identifizierten positiven F√§lle unter allen tats√§chlichen positiven F√§llen. Sie beantwortet: ‚ÄûWie viele der tats√§chlichen positiven F√§lle wurden gefunden?‚Äú \\(\\text{Recall} = \\frac{TP}{TP + FN}\\), wobei \\(FN\\) False Negatives sind.\nSpezifit√§t (True Negative Rate): Anteil der korrekt identifizierten negativen F√§lle unter allen tats√§chlichen negativen F√§llen. Sie beantwortet: ‚ÄûWie gut erkennt das Modell die negativen F√§lle?‚Äú \\(\\text{Spezifit√§t} = \\frac{TN}{TN + FP}\\), wobei \\(TN\\) True Negatives sind.\nF1-Score: Harmonisches Mittel von Precision und Recall, um eine ausgewogene Metrik zu erhalten. Er ist besonders n√ºtzlich, wenn Precision und Recall unterschiedlich stark sind. \\(F_1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\\).\n\n\n# Confusion Matrix\ncm = confusion_matrix(y, y_pred)\ntn, fp, fn, tp = cm.ravel()\nprint(\"Confusion Matrix:\\n\", cm)\n\n# Berechne Metriken\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nprecision = precision_score(y, y_pred)\nrecall= recall_score(y, y_pred)\nspecificity = tn / (tn + fp)  # Spezifit√§t manuell berechnet\nf1 = f1_score(y, y_pred)\n\n# Ausgabe in einer Tabelle\nmetrics = pd.DataFrame({\n    \"Metrik\": [\"Precision\", \"Recall (Sensitivit√§t)\", \"Spezifit√§t\", \"F1-Score\"],\n    \"Wert\": [precision, recall, specificity, f1]\n})\nprint(\"\\nEvaluierungsmetriken:\")\nprint(metrics.round(2))\n\nConfusion Matrix:\n [[109  38]\n [ 40 205]]\n\nEvaluierungsmetriken:\n                  Metrik  Wert\n0              Precision  0.84\n1  Recall (Sensitivit√§t)  0.84\n2             Spezifit√§t  0.74\n3               F1-Score  0.84\n\n\n\n\n10.2.3 ROC-Kurve und AUC\nDie ROC-Kurve (Receiver Operating Characteristic) zeigt die True Positive Rate (TPR) gegen die False Positive Rate (FPR) √ºber verschiedene Schwellenwerte. Dies l√§sst einen Vergleich verschidener Modelle zu, da sie die Leistung √ºber alle m√∂glichen Schwellenwerte hinweg darstellt.\n\nTPR (Recall): \\(\\frac{TP}{TP + FN}\\)\nFPR: \\(\\frac{FP}{FP + TN}\\)\n\nDie AUC (Area Under the Curve) misst die Gesamtleistung: AUC = 1 bedeutet perfekte Klassifikation, AUC = 0.5 entspricht einem Zufallsklassifikator.\n\n10.2.3.1 Beispiel mit dem Auto-Datensatz\nWir berechnen die ROC-Kurve und AUC f√ºr unser Modell:\n\nfrom sklearn.metrics import roc_curve, auc\n\n# ROC-Kurve und AUC\nfpr, tpr, thresholds = roc_curve(y, y_pred_prob)\nroc_auc = auc(fpr, tpr)\n\n# Plot\nplt.figure(figsize=(6, 6))\nplt.plot(fpr, tpr, color=\"blue\", label=f\"AUC = {roc_auc:.2f}\")\nplt.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC-Kurve\")\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\n\n\n\n\n\n\nErgebnis: Ein AUC von z.B. 0.85 zeigt eine gute Unterscheidungsf√§higkeit des Modells zwischen ‚ÄúUSA‚Äù und ‚Äúnicht USA‚Äù.",
    "crumbs": [
      "Regressions-Analyse",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Klassifikation</span>"
    ]
  },
  {
    "objectID": "regression/logistic_regression.html#fazit",
    "href": "regression/logistic_regression.html#fazit",
    "title": "10¬† Klassifikation",
    "section": "10.3 Fazit",
    "text": "10.3 Fazit\nDie logistische Regression ist ein effektives Werkzeug zur bin√§ren Klassifikation, wie am Beispiel des Auto-Datensatzes gezeigt. Die Confusion Matrix liefert detaillierte Einblicke in die Vorhersagefehler, w√§hrend die ROC-Kurve und AUC die Modellleistung √ºber verschiedene Schwellenwerte bewerten. Diese Methoden helfen, fundierte Entscheidungen √ºber die Klassifikation von Autos basierend auf horsepower und weight zu treffen.",
    "crumbs": [
      "Regressions-Analyse",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Klassifikation</span>"
    ]
  },
  {
    "objectID": "aufgaben/uebungsaufgaben.html",
    "href": "aufgaben/uebungsaufgaben.html",
    "title": "√úbungsaufgaben Klausurvorbereitung",
    "section": "",
    "text": "Theorieaufgaben f√ºr einen Bachelorstudiengang",
    "crumbs": [
      "√úbungsaufgaben Klausurvorbereitung"
    ]
  },
  {
    "objectID": "aufgaben/uebungsaufgaben.html#theorieaufgaben-f√ºr-einen-bachelorstudiengang",
    "href": "aufgaben/uebungsaufgaben.html#theorieaufgaben-f√ºr-einen-bachelorstudiengang",
    "title": "√úbungsaufgaben Klausurvorbereitung",
    "section": "",
    "text": "1. Variablentypen und Tidy Data\n\nBeschreiben Sie die Konzepte von ‚ÄúTidy Data‚Äù nach Hadley Wickham. Geben Sie ein Beispiel f√ºr einen ‚Äúunordentlichen‚Äù Datensatz und zeigen Sie, wie er in ein Tidy Data-Format umgewandelt werden k√∂nnte.\nNennen und erkl√§ren Sie mindestens vier verschiedene Variablentypen (z.B. nominal, ordinal, intervall, verh√§ltnis/ratio) und geben Sie f√ºr jeden Typ ein Beispiel\n\n\n\n\n\n\n\nL√∂sung Aufgabe 1\n\n\n\n\n\nTidy Data nach Hadley Wickham: ‚ÄúTidy Data‚Äù ist ein Standard f√ºr die Strukturierung von Datens√§tzen, der drei Hauptprinzipien umfasst: 1. Jede Variable bildet eine Spalte. 2. Jede Beobachtung bildet eine Zeile. 3. Jede Art von Beobachtungseinheit bildet eine Tabelle.\nBeispiel f√ºr ‚Äúunordentlichen‚Äù Datensatz und Umwandlung:\n\nUnordentlich: Eine Tabelle, in der die Spalten die Jahre darstellen und die Zeilen Produkte mit ihren Verkaufszahlen f√ºr diese Jahre sind.\n\n\n\n\nProdukt\n2020\n2021\n2022\n\n\n\n\nA\n100\n120\n110\n\n\nB\n50\n60\n70\n\n\n\n\nTidy Data: Die Jahre werden zu einer Variablen (Spalte ‚ÄúJahr‚Äù), die Verkaufszahlen zu einer Variablen (Spalte ‚ÄúVerk√§ufe‚Äù).\n\n\n\n\nProdukt\nJahr\nVerk√§ufe\n\n\n\n\nA\n2020\n100\n\n\nA\n2021\n120\n\n\nA\n2022\n110\n\n\nB\n2020\n50\n\n\nB\n2021\n60\n\n\nB\n2022\n70\n\n\n\nVariablentypen und Beispiele aus dem ‚ÄúAuto‚Äù-Datensatz:\n\nNominalskala: Kategorien ohne nat√ºrliche Ordnung.\n\nBeispiel: origin (USA, Europe, Japan). Die Reihenfolge hat keine Bedeutung.\n\nOrdinalskala: Kategorien mit einer nat√ºrlichen Ordnung, aber ohne gleichm√§√üige Abst√§nde zwischen den Kategorien.\n\nBeispiel: Eine hypothetische Variable f√ºr Fahrzeugzustand wie ‚Äúschlecht‚Äù, ‚Äúmittel‚Äù, ‚Äúgut‚Äù. Die Reihenfolge ist klar, aber der Unterschied zwischen ‚Äúschlecht‚Äù und ‚Äúmittel‚Äù muss nicht gleich dem zwischen ‚Äúmittel‚Äù und ‚Äúgut‚Äù sein. (Im echten ‚ÄúAuto‚Äù-Datensatz gibt es keine direkte Variable daf√ºr; cylinders k√∂nnte als ordinal betrachtet werden, da es eine Ordnung (4, 6, 8 Zylinder) gibt, die Abst√§nde aber nicht intervallbasiert sind).\n\nIntervallskala: Numerische Daten mit gleichm√§√üigen Abst√§nden zwischen den Werten, aber ohne einen nat√ºrlichen Nullpunkt (Verh√§ltnisse sind nicht sinnvoll).\n\nBeispiel: model_year. Der Unterschied zwischen 70 und 71 ist derselbe wie zwischen 78 und 79, aber ein Jahr 0 existiert nicht im Sinne eines absoluten Nullpunkts der Zeitrechnung f√ºr dieses Dataset.\n\nVerh√§ltnisskala: Numerische Daten mit gleichm√§√üigen Abst√§nden und einem nat√ºrlichen Nullpunkt (Verh√§ltnisse sind sinnvoll).\n\nBeispiel: mpg, horsepower, weight. Ein Auto mit 100 PS hat doppelt so viel Leistung wie ein Auto mit 50 PS. Ein Gewicht von 0 lbs bedeutet kein Gewicht.\n\n\n\n\n\n\n\n2. Deskriptive Statistik\nErkl√§ren Sie die folgenden deskriptiven Statistiken: Arithmetischer Mittelwert, Median, Standardabweichung, Minimum, Maximum und den Interquartilsabstand (IQR) und erkl√§ren Sie jeweils, wie empfindlich sie gegen√ºber Ausrei√üern sind. Welche beiden Definitionen der Standardabweichung kennen Sie und wie unterscheiden sie sich?\nHypothetische Ergebnisse f√ºr den ‚ÄúAuto‚Äù-Datensatz:\n\n\n\nStatistik\nmpg\nhorsepower\nweight\n\n\n\n\nMittelwert\n23.5\n104.5\n2970\n\n\nMedian\n23.0\n93.5\n2800\n\n\nStandardabw.\n7.8\n38.5\n840\n\n\nMinimum\n9.0\n46\n1600\n\n\nMaximum\n46.6\n230\n5100\n\n\nIQR\n10.0\n50.0\n1400\n\n\n\n\n\n\n\n\n\nL√∂sung Aufgabe 2\n\n\n\n\n\nInterpretation der hypothetischen Ergebnisse:\n-Arithmetischer Mittelwert: Gleich-gewichteter Durchschnittswert der Variablen ist ein Lagema√ü, das die zentrale Tendenz beschreibt. Es ist empfindlich gegen√ºber Ausrei√üern. - Median: Der Median teilt die Daten in zwei H√§lften und ist robuster gegen√ºber Ausrei√üern. - Standardabweichung: Ein Ma√ü f√ºr die Streuung der Daten um den Mittelwert. Eine hohe Standardabweichung zeigt, dass die Daten weit gestreut sind. - Es gibt die populationsbezogene Standardabweichung (\\(\\sigma\\)) und die stichprobenbezogene Standardabweichung (\\(s\\)). Die Stichprobenbezogene Standardabweichung wird mit \\(n-1\\) im Nenner berechnet, um die Sch√§tzung der Populationsstandardabweichung zu korrigieren. Die populationsbezogene Standardabweichung verwendet \\(n\\) im Nenner. - Minimum: Der kleinste Wert in der Datenreihe. Es ist empfindlich gegen√ºber Ausrei√üern, da ein einzelner extrem niedriger Wert den Minimumwert stark beeinflussen kann. - Maximum: Der gr√∂√üte Wert in der Datenreihe. Auch hier kann ein einzelner extrem hoher Wert den Maximumwert stark beeinflussen. - Interquartilsabstand (IQR): Der IQR misst die Spannweite der mittleren 50% der Daten und ist robust gegen√ºber Ausrei√üern, da er nur die Quartile ber√ºcksichtigt.\n\n\n\n\n\n3. Grundlagen der Verteilungen\n\nErkl√§ren Sie den Unterschied zwischen diskreten und kontinuierlichen Verteilungen. Nennen Sie jeweils zwei Beispiele f√ºr jede Art von Verteilung und beschreiben Sie kurz, wo sie in der Praxis Anwendung finden k√∂nnten.\n\n\n\n\n\n\n\nL√∂sung Aufgabe 3\n\n\n\n\n\nDiskrete Verteilungen: Eine diskrete Verteilung beschreibt Zufallsvariablen, deren Werte nur abz√§hlbare, separate Werte annehmen k√∂nnen. Zwischen zwei aufeinanderfolgenden Werten gibt es keine weiteren m√∂glichen Werte.\n\nBeispiel 1: Binomialverteilung\n\nBeschreibung: Beschreibt die Anzahl der Erfolge in einer festen Anzahl von unabh√§ngigen Bernoulli-Versuchen (z.B. M√ºnzw√ºrfe oder ja/nein-Fragen), wobei jeder Versuch nur zwei m√∂gliche Ergebnisse hat.\nAnwendung: Anzahl der fehlerhaften Produkte in einer Stichprobe von 100 produzierten Artikeln.\n\nBeispiel 2: Poisson-Verteilung\n\nBeschreibung: Beschreibt die Anzahl der Ereignisse, die in einem festen Zeitintervall oder Raumgebiet auftreten, wenn diese Ereignisse mit einer bekannten durchschnittlichen Rate unabh√§ngig voneinander geschehen.\nAnwendung: Anzahl der Kundenanrufe in einem Callcenter pro Stunde; Anzahl der Verkehrsunf√§lle an einer Kreuzung pro Monat.\n\n\nKontinuierliche Verteilungen: Eine kontinuierliche Verteilung beschreibt Zufallsvariablen, deren Werte jeden beliebigen Wert innerhalb eines bestimmten Intervalls annehmen k√∂nnen. Die Wahrscheinlichkeit, dass die Variable einen bestimmten einzelnen Wert annimmt, ist Null; stattdessen werden Wahrscheinlichkeiten f√ºr Intervalle berechnet.\n\nBeispiel 1: Normalverteilung (Gau√ü-Verteilung)\n\nBeschreibung: Eine symmetrische, glockenf√∂rmige Verteilung, die h√§ufig in der Natur und in vielen Messprozessen auftritt. Sie ist durch ihren Mittelwert (\\(\\mu\\)) und ihre Standardabweichung (\\(\\sigma\\)) vollst√§ndig definiert.\nAnwendung: K√∂rpergr√∂√üe von Menschen in einer Population; Messfehler bei physikalischen Experimenten; Intelligenzquotient (IQ) in einer Bev√∂lkerung.\n\nBeispiel 2: Exponentialverteilung\n\nBeschreibung: Beschreibt die Zeit bis zum Eintreten eines Ereignisses in einem Poisson-Prozess, d.h., die Zeit bis zum ersten Ereignis oder die Zeit zwischen zwei aufeinanderfolgenden Ereignissen. Sie ist ‚Äúged√§chtnislos‚Äù.\nAnwendung: Lebensdauer von elektronischen Bauteilen; Zeit zwischen aufeinanderfolgenden Erdbeben; Wartezeit auf den n√§chsten Kunden in einem Servicegesch√§ft.\n\n\n\n\n\n\n\n4. Datenvisualisierung\n\nW√§hlen Sie zwei geeignete Variablen aus dem ‚ÄúAuto‚Äù-Datensatz oder einem anderen Datensatz ihrer Wahl und skizzieren Sie:\n\nEin Histogramm f√ºr eine kontinuierliche Variable.\nEin Box-Plot f√ºr eine kontinuierliche Variable in Abh√§nigkeit einer kategorialen Variable.\n\nErl√§utern Sie, welche Informationen aus diesen Visualisierungen gewonnen werden k√∂nnen.\n\n\n\n\n\n\n\nL√∂sung Aufgabe 4\n\n\n\n\n\nAuswahl der Variablen aus dem ‚ÄúAuto‚Äù-Datensatz: * Kontinuierliche Variable: mpg (Meilen pro Gallone) * Kategoriale Variable: mpg √ºber origin (Herkunftsland: USA, Europe, Japan)\n1. Histogramm f√ºr mpg: * Visualisierung (Beschreibung): Ein Histogramm w√ºrde die Verteilung der mpg-Werte in verschiedenen ‚ÄúBins‚Äù (Intervalle) darstellen. Die x-Achse w√ºrde die mpg-Werte zeigen, und die y-Achse die H√§ufigkeit (oder Dichte) der Autos, die in jeden mpg-Bereich fallen. * Informationen/Interpretation: * Form der Verteilung: Man k√∂nnte erkennen, ob die Verteilung symmetrisch, links- oder rechtsschief ist (z.B. ob es mehr Autos mit geringem oder hohem Verbrauch gibt). * Zentrale Tendenz: Der Bereich mit der h√∂chsten H√§ufigkeit (der ‚ÄúGipfel‚Äù des Histogramms) w√ºrde den h√§ufigsten Kraftstoffverbrauch anzeigen. * Streuung: Die Breite des Histogramms w√ºrde Auskunft √ºber die Variabilit√§t des Kraftstoffverbrauchs geben. * Ausrei√üer/Anomalien: Einzelne Balken weit entfernt vom Hauptteil der Verteilung k√∂nnten auf Ausrei√üer hindeuten. * Multimodalit√§t: Mehrere ‚ÄúGipfel‚Äù k√∂nnten auf verschiedene Subgruppen innerhalb des Datensatzes hindeuten (z.B. verschiedene Fahrzeugtypen mit unterschiedlichem Verbrauch).\n2. Box-Plot f√ºr origin: * Visualisierung (Beschreibung): Ein Box-Plot w√ºrde die Verteilung der mpg-Werte f√ºr jede Kategorie von origin (USA, Europe, Japan) darstellen. Jede Box zeigt den Interquartilsabstand (IQR), den Median und m√∂gliche Ausrei√üer.\n\n\n\n\n\n5. Korrelation vs.¬†Kausalit√§t\n\nErl√§utern Sie den Unterschied zwischen Korrelation und Kausalit√§t anhand eines selbstgew√§hlten Beispiels (nicht das Piraten-Beispiel aus dem Skript). Diskutieren Sie, warum es wichtig ist, diese Unterscheidung zu verstehen, insbesondere im Kontext von Datenanalyse und maschinellem Lernen. Was von beiden kann direkt aus den Daten gemessen werden und was nicht? Wie k√∂nnen wir Kausalit√§t in der Praxis untersuchen?\n\n\n\n\n\n\n\nL√∂sung Aufgabe 5\n\n\n\n\n\nKorrelation vs.¬†Kausalit√§t:\n\nKorrelation: Beschreibt einen statistischen Zusammenhang zwischen zwei oder mehr Variablen, bei dem √Ñnderungen in einer Variablen mit √Ñnderungen in einer oder mehreren anderen Variablen einhergehen. Eine Korrelation sagt nichts √ºber die Ursache-Wirkungs-Beziehung aus. Sie zeigt lediglich, dass die Variablen in einer bestimmten Weise zusammenh√§ngen. Die Korrelation kann positiv (beide Variablen steigen oder fallen zusammen), negativ (eine Variable steigt, w√§hrend die andere f√§llt) oder gar nicht vorhanden sein. Gemessen wird die Korrelation oft durch den Pearson-Korrelationskoeffizienten, der zwischen -1 und 1 liegt, wobei 0 keine Korrelation bedeutet.\nKausalit√§t: Bedeutet, dass eine Variable (die Ursache) direkt eine Ver√§nderung in einer anderen Variablen (die Wirkung) hervorruft. Eine kausale Beziehung impliziert immer eine Korrelation, aber eine Korrelation impliziert nicht zwangsl√§ufig eine Kausalit√§t. Hierzu sind oft Experimente oder spezielle statistische Methoden erforderlich, um die Richtung und das Vorhandensein der Ursache-Wirkungs-Beziehung zu best√§tigen.\n\nSelbstgew√§hltes Beispiel:\n\nBeobachtung: Im Sommer steigt sowohl der Verkauf von Eiscreme als auch die Anzahl der Ertrinkungsf√§lle in einem See.\nKorrelation: Es gibt eine positive Korrelation zwischen dem Verkauf von Eiscreme und der Anzahl der Ertrinkungsf√§lle. Wenn der Eiscremeverkauf steigt, steigen auch die Ertrinkungsf√§lle.\nFehlerhafte Kausalit√§t: Es w√§re ein Fehler zu schlussfolgern, dass der Kauf von Eiscreme dazu f√ºhrt, dass Menschen ertrinken, oder dass Ertrinkungsf√§lle den Eiscremeverkauf ankurbeln.\nWahre Kausalit√§t/Dritte Variable: Die wahre Ursache f√ºr beide Ph√§nomene ist eine dritte, nicht direkt beobachtete Variable: die Au√üentemperatur. Wenn die Temperatur steigt (Sommer), essen mehr Menschen Eiscreme und mehr Menschen gehen im See schwimmen, was zu einer erh√∂hten Anzahl von Ertrinkungsf√§llen f√ºhrt. Die Temperatur ist hier die gemeinsame Ursache f√ºr beide Effekte.\n\nWichtigkeit der Unterscheidung in Datenanalyse und maschinellem Lernen:\nDas Verst√§ndnis des Unterschieds ist aus mehreren Gr√ºnden von entscheidender Bedeutung:\n\nFehlgeleitete Entscheidungen und Interventionen: Wenn wir Korrelationen f√§lschlicherweise als Kausalit√§ten interpretieren, k√∂nnten wir ineffektive oder sogar sch√§dliche Ma√ünahmen ergreifen. Im Beispiel w√ºrde das Verbot von Eiscreme den Ertrinkungsf√§llen nicht entgegenwirken. Im Business-Kontext k√∂nnte dies zu falschen Investitionen oder Strategien f√ºhren.\nModellinterpretation und Erkl√§rbarkeit: In der Datenanalyse wollen wir oft nicht nur Vorhersagen treffen, sondern auch verstehen, warum etwas passiert. Wenn ein Modell eine hohe Korrelation zwischen zwei Variablen findet, dies aber keine Kausalit√§t ist, liefert das Modell keine wahren kausalen Erkl√§rungen. Dies ist besonders wichtig in Bereichen wie Medizin, Sozialwissenschaften oder Politikgestaltung, wo kausale Zusammenh√§nge f√ºr effektive Interventionen unerl√§sslich sind.\nRobuste Vorhersagemodelle: Obwohl viele maschinelle Lernmodelle auch mit korrelativen Beziehungen gute Vorhersagen treffen k√∂nnen, sind Modelle, die kausale Zusammenh√§nge ber√ºcksichtigen, oft robuster und generalisierbarer, insbesondere wenn sich die zugrunde liegenden Verteilungen der Daten √§ndern. Wenn wir die Kausalit√§t verstehen, k√∂nnen wir Modelle entwickeln, die auch unter neuen Bedingungen zuverl√§ssig sind.\nUmgang mit St√∂rvariablen (Confoundern): Das Ignorieren von St√∂rvariablen (wie die Temperatur im Beispiel) kann zu Scheinkorrelationen f√ºhren. Das Bewusstsein f√ºr Kausalit√§t hilft uns, solche Variablen zu identifizieren und zu kontrollieren, um aussagekr√§ftigere Analysen zu erm√∂glichen.\n\nKurz gesagt: Korrelation ist ein Werkzeug zur Entdeckung von Mustern, aber Kausalit√§t ist der Schl√ºssel zum Verst√§ndnis und zur Beeinflussung der realen Welt.\n\n\n\n\n\n6. Zentraler Grenzwertsatz\n\nErkl√§ren Sie den Zentralen Grenzwertsatz in eigenen Worten. Warum ist er f√ºr die statistische Inferenz so wichtig?\n\n\n\n\n\n\n\nL√∂sung Aufgabe 6\n\n\n\n\n\nDer Zentrale Grenzwertsatz in eigenen Worten: Der Zentrale Grenzwertsatz (ZGS) besagt, dass, wenn man gen√ºgend gro√üe Stichproben aus irgendeiner Population zieht (egal welche Verteilungsform diese Population hat ‚Äì sie kann schief, uniform oder jede andere Form haben), dann wird die Verteilung der Mittelwerte dieser Stichproben ann√§hernd normalverteilt sein. Je gr√∂√üer die Stichprobengr√∂√üe ist, desto besser n√§hert sich diese Stichprobenmittelwertverteilung einer Normalverteilung an. Das ist erstaunlich, da die urspr√ºngliche Population selbst nicht normalverteilt sein muss. Der Mittelwert dieser Verteilung der Stichprobenmittelwerte ist der wahre Populationsmittelwert, und ihre Standardabweichung ist der Standardfehler des Mittelwerts.\nBedeutung f√ºr die statistische Inferenz: Der Zentrale Grenzwertsatz ist fundamental f√ºr die statistische Inferenz, da er es uns erm√∂glicht, Schlussfolgerungen √ºber eine Population zu ziehen, selbst wenn wir ihre urspr√ºngliche Verteilung nicht kennen.\n\nHypothesentests: Er ist die Grundlage f√ºr viele parametrische Hypothesentests (z.B. \\(t\\)-Tests, \\(z\\)-Tests). Durch ihn k√∂nnen wir annehmen, dass die Stichprobenmittelwerte normalverteilt sind, selbst wenn die Einzeldaten nicht normalverteilt sind. Das erm√∂glicht die Verwendung von Standard-Normalverteilungs- oder \\(t\\)-Verteilungs-Tabellen, um p-Werte zu berechnen und Hypothesen zu testen.\nKonfidenzintervalle: Er erlaubt uns, Konfidenzintervalle f√ºr Populationsparameter (insbesondere den Mittelwert) zu konstruieren. Wir k√∂nnen mit einer bestimmten Wahrscheinlichkeit (z.B. 95%) angeben, in welchem Bereich der wahre Populationsmittelwert liegt, basierend auf einer Stichprobe.\nSch√§tzung von Parametern: Der ZGS rechtfertigt die Verwendung des Stichprobenmittelwerts als einen guten Sch√§tzer f√ºr den Populationsmittelwert.\n\nOhne den ZGS m√ºssten wir f√ºr viele Tests und Sch√§tzungen annehmen, dass die zugrunde liegende Population normalverteilt ist, was in der Realit√§t oft nicht der Fall ist.\n\n\n\n\n\n7. Einfache Lineare Regression\n\nNehmen Sie an, Sie m√∂chten den Kraftstoffverbrauch (mpg) eines Autos mithilfe der Leistung (horsepower) vorhersagen.\n\nFormulieren Sie ein einfaches lineares Regressionsmodell.\nErl√§utern Sie die Bedeutung der Parameter \\(\\beta_0\\) und \\(\\beta_1\\) in diesem Kontext.\nDiskutieren Sie, was ein hohes \\(R^2\\) in diesem Modell bedeuten w√ºrde.\n\n\n\n\n\n\n\n\nL√∂sung Aufgabe 7\n\n\n\n\n\n1. Formulierung des einfachen linearen Regressionsmodells: Das einfache lineare Regressionsmodell, das den Kraftstoffverbrauch (mpg) als abh√§ngige Variable (\\(Y\\)) und die Leistung (horsepower) als unabh√§ngige Variable (\\(X\\)) verwendet, kann wie folgt formuliert werden:\n\\[\nY = \\beta_0 + \\beta_1 X + \\epsilon\n\\] oder spezifischer f√ºr dieses Beispiel: \\[\n\\text{mpg} = \\beta_0 + \\beta_1 \\cdot \\text{horsepower} + \\epsilon\n\\] Wobei: * \\(\\text{mpg}\\) ist der zu modellierende Kraftstoffverbrauch (abh√§ngige Variable). * \\(\\text{horsepower}\\) ist die Leistung des Autos (unabh√§ngige Variable). * \\(\\beta_0\\) ist der Achsenabschnitt (Intercept). * \\(\\beta_1\\) ist der Steigungskoeffizient (Slope). * \\(\\epsilon\\) ist der Fehlerterm, der die nicht durch das Modell erkl√§rten Variationen und zuf√§llige Fehler darstellt.\n2. Erl√§uterung der Bedeutung der Parameter \\(\\beta_0\\) und \\(\\beta_1\\):\n\n\\(\\beta_0\\) (Achsenabschnitt):\n\nDieser Parameter stellt den gesch√§tzten Wert von mpg dar, wenn horsepower Null ist.\nInterpretation im Kontext: Ein \\(\\beta_0\\) von, sagen wir, 35 w√ºrde bedeuten, dass ein hypothetisches Auto mit 0 PS einen Verbrauch von 35 mpg h√§tte. In vielen realen Szenarien, wie diesem, ist die Interpretation des Achsenabschnitts als tats√§chlicher Wert oft nicht sinnvoll oder sogar unm√∂glich, da 0 PS in der Praxis nicht vorkommen. Er dient eher dazu, die Position der Regressionsgeraden im Koordinatensystem festzulegen.\n\n\\(\\beta_1\\) (Steigungskoeffizient):\n\nDieser Parameter gibt an, um wie viele Einheiten sich der gesch√§tzte Wert von mpg √§ndert, wenn horsepower um eine Einheit (hier: 1 PS) steigt, unter der Annahme, dass alle anderen Faktoren konstant bleiben (was hier nur horsepower ist).\nInterpretation im Kontext: Ein \\(\\beta_1\\) von, sagen wir, -0.15 w√ºrde bedeuten, dass der Kraftstoffverbrauch um 0.15 mpg sinkt, wenn die Leistung des Autos um 1 PS steigt. Dies w√§re ein negatives Verh√§ltnis, was typisch ist, da leistungsst√§rkere Autos tendenziell mehr Kraftstoff verbrauchen (geringere mpg-Werte).\n\n\n3. Diskussion eines hohen \\(R^2\\) in diesem Modell:\n\nDefinition von \\(R^2\\): Der \\(R^2\\)-Wert (Bestimmtheitsma√ü) ist ein Ma√ü daf√ºr, welcher Anteil der Gesamtvariation in der abh√§ngigen Variablen (\\(Y\\), hier mpg) durch die unabh√§ngige Variable (\\(X\\), hier horsepower) erkl√§rt wird. Er liegt immer zwischen 0 und 1 (oder 0% und 100%).\nBedeutung eines hohen \\(R^2\\):\n\nEin hoher \\(R^2\\)-Wert (z.B. 0.8 oder 80%) w√ºrde bedeuten, dass ein gro√üer Teil der Variabilit√§t im Kraftstoffverbrauch (mpg) durch die Leistung (horsepower) erkl√§rt werden kann.\nDies w√ºrde darauf hindeuten, dass horsepower ein sehr starker Pr√§diktor f√ºr mpg ist. Das Modell passt gut zu den beobachteten Daten, und die Residuen (die unerkl√§rten Variationen) w√§ren relativ klein.\nPraktische Implikation: Wenn das \\(R^2\\) hoch ist, k√∂nnten wir relativ genaue Vorhersagen des Kraftstoffverbrauchs eines Autos allein auf Basis seiner Leistung machen.\nWichtiger Hinweis: Ein hohes \\(R^2\\) bedeutet nicht unbedingt, dass das Modell kausal ist oder dass es das ‚Äúbeste‚Äù Modell ist. Es zeigt nur, wie gut die Linie die Punkte ‚Äúerkl√§rt‚Äù. Es sagt nichts √ºber die Richtigkeit der Modellannahmen oder das Vorhandensein von Ausrei√üern aus. Es k√∂nnte auch √ºberz√§hligen Variablen (Overfitting) geschuldet sein, wenn andere Faktoren im Modell enthalten w√§ren, hier aber nicht der Fall, da es sich um eine einfache Regression handelt.\n\n\n\n\n\n\n\n8. Hypothesentest - T-Test Grundlagen\n\nErl√§utern Sie die Schritte eines Hypothesentests. Nehmen Sie an, Sie m√∂chten testen, ob der durchschnittliche Kraftstoffverbrauch (mpg) von US-Autos signifikant von 20 Meilen pro Gallone abweicht. Formulieren Sie die Null- und Alternativhypothese f√ºr diesen Test.\n\n\n\n\n\n\n\nL√∂sung Aufgabe 8\n\n\n\n\n\nSchritte eines Hypothesentests:\nEin Hypothesentest ist ein statistisches Verfahren, um eine Aussage (Hypothese) √ºber eine Population anhand von Stichprobendaten zu √ºberpr√ºfen. Die typischen Schritte sind:\n\nFormulierung der Hypothesen:\n\nNullhypothese (\\(H_0\\)): Dies ist die Aussage, die man widerlegen m√∂chte. Sie repr√§sentiert typischerweise den ‚ÄúStatus quo‚Äù oder keine Effekt/kein Unterschied.\nAlternativhypothese (\\(H_1\\)): Dies ist die Aussage, die man beweisen m√∂chte, wenn die Nullhypothese abgelehnt wird. Sie stellt einen Effekt oder einen Unterschied dar.\n\nFestlegung des Signifikanzniveaus (\\(\\alpha\\)):\n\n\\(\\alpha\\) ist die Wahrscheinlichkeit, die Nullhypothese abzulehnen, obwohl sie wahr ist (Fehler 1. Art). √úbliche Werte sind 0.05 (5%) oder 0.01 (1%).\n\nAuswahl des geeigneten Testverfahrens:\n\nBasierend auf der Art der Daten (kategorial, numerisch), der Anzahl der Stichproben, der Verteilung und der Fragestellung wird ein passender statistischer Test gew√§hlt (z.B. \\(t\\)-Test, Chi-Quadrat-Test, ANOVA).\n\nBerechnung der Teststatistik:\n\nMithilfe der Stichprobendaten wird ein Wert berechnet, der als Teststatistik bezeichnet wird. Dieser Wert quantifiziert, wie stark die Stichprobendaten von der Nullhypothese abweichen.\n\nBestimmung des kritischen Wertes oder des p-Wertes:\n\nKritischer Wert-Ansatz: Man bestimmt einen oder mehrere kritische Werte aus der Verteilung der Teststatistik unter \\(H_0\\). Liegt die berechnete Teststatistik au√üerhalb des Annahmebereichs (d.h., im Ablehnungsbereich), wird \\(H_0\\) abgelehnt.\np-Wert-Ansatz: Man berechnet die Wahrscheinlichkeit (den p-Wert), einen so extremen oder extremeren Wert der Teststatistik zu beobachten, unter der Annahme, dass die Nullhypothese wahr ist.\n\nEntscheidung und Schlussfolgerung:\n\nWenn der p-Wert \\(\\le \\alpha\\): Die Nullhypothese wird abgelehnt. Es gibt gen√ºgend statistische Evidenz f√ºr die Alternativhypothese.\nWenn der p-Wert \\(&gt; \\alpha\\): Die Nullhypothese wird nicht abgelehnt. Es gibt nicht gen√ºgend statistische Evidenz, um die Alternativhypothese zu unterst√ºtzen.\nDie Schlussfolgerung sollte im Kontext des urspr√ºnglichen Problems formuliert werden.\n\n\nHypothesen f√ºr den Test des durchschnittlichen Kraftstoffverbrauchs von US-Autos:\nFragestellung: Weicht der durchschnittliche Kraftstoffverbrauch (mpg) von US-Autos signifikant von 20 Meilen pro Gallone ab?\n\nNullhypothese (\\(H_0\\)): Der wahre durchschnittliche Kraftstoffverbrauch von US-Autos betr√§gt 20 Meilen pro Gallone.\n\nIn Symbolen: \\(H_0: \\mu_{\\text{US-Autos}} = 20\\)\n\nAlternativhypothese (\\(H_1\\)): Der wahre durchschnittliche Kraftstoffverbrauch von US-Autos weicht von 20 Meilen pro Gallone ab.\n\nIn Symbolen: \\(H_1: \\mu_{\\text{US-Autos}} \\neq 20\\)\n\n\nDies ist ein zweiseitiger Test, da wir pr√ºfen, ob der Wert abweicht (gr√∂√üer oder kleiner sein kann) und nicht spezifisch, ob er gr√∂√üer oder kleiner ist als 20.\n\n\n\n\n\n9. Multiple Lineare Regression\n\nErweitern Sie das Modell aus Aufgabe 7, indem Sie zus√§tzlich das Gewicht (weight) als Pr√§diktor f√ºr mpg aufnehmen.\n\nFormulieren Sie das multiple lineare Regressionsmodell.\nW√ºrden Sie erwarten, dass die Koeffizienten f√ºr den Intercept und horsepower signifikant anders sind als im einfachen linearen Modell? Warum?\n\n\n\n\n\n\n\n\nL√∂sung Aufgabe 9\n\n\n\n\n\n1. Formulierung des multiplen linearen Regressionsmodells: Das multiple lineare Regressionsmodell f√ºr mpg unter Verwendung von horsepower und weight lautet:\n\\[\n\\text{mpg} = \\beta_0 + \\beta_1 \\cdot \\text{horsepower} + \\beta_2 \\cdot \\text{weight} + \\epsilon\n\\] Wobei: * \\(\\text{mpg}\\) ist der Kraftstoffverbrauch (abh√§ngige Variable). * \\(\\text{horsepower}\\) ist die Leistung des Autos (unabh√§ngige Variable 1). * \\(\\text{weight}\\) ist das Gewicht des Autos (unabh√§ngige Variable 2). * \\(\\beta_0\\) ist der Achsenabschnitt. * \\(\\beta_1\\) ist der Steigungskoeffizient f√ºr horsepower. * \\(\\beta_2\\) ist der Steigungskoeffizient f√ºr weight. * \\(\\epsilon\\) ist der Fehlerterm.\n\n2. Erwartung bez√ºglich der Koeffizienten f√ºr den Intercept und horsepower:\nIntercept (\\(\\beta_0\\)):\n\nDer Intercept k√∂nnte sich √§ndern, da wir nun zwei Pr√§diktoren haben. Der Wert von \\(\\beta_0\\) repr√§sentiert den gesch√§tzten mpg, wenn sowohl horsepower als auch weight Null sind. Da es in der Praxis keine Autos mit 0 PS und 0 Gewicht gibt, ist die Interpretation des Intercepts in diesem Kontext oft weniger sinnvoll. Er dient haupts√§chlich dazu, die Regressionsgerade korrekt zu positionieren.\n\n\n\n\n\n\n\n10. Fehlerma√üe in der Regression\n\nErkl√§ren Sie die Bedeutung von ‚ÄúMean Squared Error‚Äù (MSE) und ‚ÄúRoot Mean Squared Error‚Äù (RMSE) als Fehlerma√üe in der linearen Regression. Warum wird oft der RMSE dem MSE vorgezogen?\n\n\n\n\n\n\n\nL√∂sung Aufgabe 10\n\n\n\n\n\nBedeutung von Mean Squared Error (MSE) und Root Mean Squared Error (RMSE):\nSowohl MSE als auch RMSE sind Ma√üe f√ºr die durchschnittliche Gr√∂√üe der Fehler in einem Regressionsmodell. Sie quantifizieren, wie stark die vorhergesagten Werte von den tats√§chlichen Werten abweichen.\n\nMean Squared Error (MSE):\n\nDefinition: Das MSE ist der Durchschnitt der quadrierten Differenzen zwischen den vorhergesagten Werten (\\(\\hat{Y}_i\\)) und den tats√§chlichen Werten (\\(Y_i\\)).\nFormel: \\(MSE = \\frac{1}{n} \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2\\)\nEigenschaften:\n\nGro√üe Fehler werden st√§rker bestraft als kleine Fehler, da sie quadriert werden.\nDer Wert liegt immer im Bereich \\([0, \\infty)\\), wobei 0 ein perfektes Modell anzeigt.\nDie Einheit des MSE ist die quadrierte Einheit der abh√§ngigen Variablen (z.B. wenn \\(Y\\) in Metern ist, ist MSE in Quadratmetern).\n\n\nRoot Mean Squared Error (RMSE):\n\nDefinition: Das RMSE ist die Quadratwurzel des MSE.\nFormel: \\(RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2} = \\sqrt{MSE}\\)\nEigenschaften:\n\nWie MSE bestraft es gro√üe Fehler st√§rker.\nDer Wert liegt immer im Bereich \\([0, \\infty)\\), wobei 0 ein perfektes Modell anzeigt.\nDie Einheit des RMSE ist dieselbe wie die Einheit der abh√§ngigen Variablen (z.B. wenn \\(Y\\) in Metern ist, ist RMSE in Metern).\n\n\n\nWarum wird oft der RMSE dem MSE vorgezogen?\nDer RMSE wird dem MSE aus einem entscheidenden Grund oft vorgezogen:\n\nInterpretierbarkeit in der Originaleinheit: Das gr√∂√üte Argument f√ºr RMSE ist, dass seine Einheit dieselbe ist wie die der abh√§ngigen Variablen, w√§hrend die Einheit des MSE die quadrierte Einheit ist. Dies macht den RMSE wesentlich leichter interpretierbar im Kontext des Problems.\n\nWenn Sie beispielsweise den Preis eines Hauses vorhersagen (in Euro), w√§re ein RMSE von 10.000 ‚Ç¨ intuitiver zu verstehen als ein MSE von 100.000.000 \\(‚Ç¨^2\\). Der RMSE gibt direkt an, wie gro√ü die durchschnittliche Abweichung der Vorhersagen in den originalen Einheiten der abh√§ngigen Variablen ist.\n\n\nObwohl beide Ma√üe √§hnliche Informationen √ºber die Modellleistung liefern (ein niedrigerer Wert ist immer besser), ist die intuitive Verst√§ndlichkeit des RMSE ein gro√üer Vorteil f√ºr die Kommunikation der Modellergebnisse. Das MSE ist jedoch n√ºtzlich, weil die Quadrierung die Ableitung erleichtert, was es zu einem bevorzugten Kriterium f√ºr die Optimierung in vielen Algorithmen macht. F√ºr die Pr√§sentation der Ergebnisse wird dann oft der RMSE berechnet.\n\n\n\n\n\n11. Bias-Varianz-Tradeoff\n\nBeschreiben Sie den Bias-Varianz-Tradeoff im Kontext von Vorhersagemodellen (Prognosemodellen).\n\n\n\n\n\n\n\nL√∂sung Aufgabe 11\n\n\n\n\n\nDer Bias-Varianz-Tradeoff:\nDer Bias-Varianz-Tradeoff ist ein zentrales Konzept im maschinellen Lernen, das die Beziehung zwischen der Komplexit√§t eines Modells und seiner F√§higkeit, genaue Vorhersagen auf unbekannten Daten zu treffen, beschreibt. Ziel ist es, ein Modell zu finden, das eine gute Balance zwischen Bias und Varianz hat, um eine optimale Generalisierungsf√§higkeit zu erreichen.\n\nBias (Verzerrung):\n\nWas es ist: Der Bias ist der Fehler, der durch vereinfachende Annahmen im Lernalgorithmus entsteht. Ein Modell mit hohem Bias ist zu einfach, um den zugrunde liegenden komplexen Zusammenhang in den Daten zu erfassen. Es ‚Äúuntersch√§tzt‚Äù die Komplexit√§t der wahren Beziehung.\nFolge: F√ºhrt zu Underfitting (Unteranpassung), d.h., das Modell ist auf den Trainingsdaten und erst recht auf neuen, ungesehenen Daten schlecht.\nBeispiel: Eine lineare Regression, die versucht, eine nicht-lineare Beziehung zu modellieren.\n\nVarianz:\n\nWas es ist: Die Varianz ist der Fehler, der durch die Sensitivit√§t des Modells gegen√ºber kleinen Schwankungen in den Trainingsdaten entsteht. Ein Modell mit hoher Varianz ist zu komplex oder zu ‚Äúempfindlich‚Äù f√ºr die Trainingsdaten und lernt dabei auch das Rauschen in den Daten.\nFolge: F√ºhrt zu Overfitting (√úberanpassung), d.h., das Modell ist auf den Trainingsdaten sehr gut, aber auf neuen, ungesehenen Daten schlecht, weil es die spezifischen Merkmale des Trainingsdatensatzes zu stark verinnerlicht hat.\nBeispiel: Ein sehr komplexes Polynommodell, das alle Datenpunkte perfekt trifft, aber dann stark oszilliert, wenn neue Datenpunkte au√üerhalb der Trainingsdaten kommen.\n\nTradeoff: Es gibt einen inh√§renten Kompromiss: Wenn wir den Bias reduzieren (Modell komplexer machen), steigt typischerweise die Varianz, und umgekehrt. Ein gutes Modell muss eine Balance finden, um den Gesamtfehler (oft gemessen als Mean Squared Error) auf ungesehenen Daten zu minimieren.\n\n\n\n\n\n\n12. Kreuzvalidierung\n\nErl√§utern Sie das Prinzip der \\(k\\)-Fold-Kreuzvalidierung. Warum ist diese Methode dem einfachen Train-Test-Split bei der Modellbewertung oft √ºberlegen? Nennen Sie mindestens einen Vorteil. Was ist ein Vorteil der \\(k\\)-Fold-Kreuzvalidierung gegen√ºbert der Leave-One-Out-Kreuzvalidierung (LOOCV)?\n\n\n\n\n\n\n\nL√∂sung Aufgabe 12\n\n\n\n\n\nPrinzip der \\(k\\)-Fold-Kreuzvalidierung:\nDie \\(k\\)-Fold-Kreuzvalidierung ist eine Technik zur Modellbewertung und -auswahl, die die Leistung eines Modells auf einem Datensatz zuverl√§ssiger einsch√§tzt, indem sie den Datensatz in \\(k\\) gleich gro√üe Teilmengen (Folds) aufteilt. Der Prozess l√§uft wie folgt ab:\n\nAufteilung: Der gesamte Datensatz wird in \\(k\\) (z.B. 5 oder 10) ann√§hernd gleich gro√üe, disjunkte Folds aufgeteilt.\nIterativer Trainings- und Validierungsprozess:\n\nIn jeder der \\(k\\) Iterationen (oder ‚ÄúFolds‚Äù) wird ein anderer Fold als Validierungs- (oder Test-)Set verwendet.\nDie verbleibenden \\(k-1\\) Folds werden zusammen als Trainingsset verwendet.\nDas Modell wird auf dem Trainingsset trainiert und seine Leistung (z.B. MSE, Genauigkeit) auf dem Validierungsset bewertet.\n\nAggregation: Nach \\(k\\) Iterationen hat jede Beobachtung genau einmal zum Validierungsset geh√∂rt. Die \\(k\\) erhaltenen Leistungsmetriken werden gemittelt, um eine robustere Sch√§tzung der Modellleistung zu erhalten.\n\nWarum ist die \\(k\\)-Fold-Kreuzvalidierung dem einfachen Train-Test-Split √ºberlegen?\nDie \\(k\\)-Fold-Kreuzvalidierung bietet mehrere Vorteile gegen√ºber dem einfachen Train-Test-Split:\n\nBessere Nutzung der Daten:\n\nVorteil: Beim einfachen Train-Test-Split wird ein signifikanter Teil der Daten (das Testset) nicht zum Training des Modells verwendet. Dies ist besonders bei kleinen Datens√§tzen problematisch, da das Modell weniger Daten zum Lernen hat. Bei der \\(k\\)-Fold-Kreuzvalidierung wird jede Beobachtung sowohl zum Training (in \\(k-1\\) Folds) als auch zur Validierung (in einem Fold) verwendet. Dies f√ºhrt zu einer effizienteren Nutzung des gesamten Datensatzes f√ºr Training und Bewertung.\n\nRobustere Sch√§tzung der Modellleistung (geringere Varianz):\n\nVorteil: Der einfache Train-Test-Split h√§ngt stark von der zuf√§lligen Aufteilung ab. Eine ‚Äúungl√ºckliche‚Äù Aufteilung k√∂nnte dazu f√ºhren, dass das Testset nicht repr√§sentativ f√ºr die Gesamtpopulation ist, was die Bewertung der Modellleistung verzerrt. Die \\(k\\)-Fold-Kreuzvalidierung mittelt die Leistung √ºber \\(k\\) verschiedene Train-Test-Splits. Dies reduziert die Varianz der Leistungssch√§tzung und liefert eine stabilere und weniger zuf√§lligkeitsanf√§llige Bewertung der Generalisierungsf√§higkeit des Modells.\n\nReduziertes Risiko von Overfitting auf das Testset:\n\nVorteil: Wenn beim einfachen Train-Test-Split das Testset wiederholt f√ºr Modelloptimierung oder Hyperparameter-Tuning verwendet wird, besteht die Gefahr, dass das Modell am Ende auf dieses spezifische Testset √ºberangepasst wird. Die \\(k\\)-Fold-Kreuzvalidierung bietet einen eingebauten Mechanismus zur Validierung auf verschiedenen Teilmengen der Daten, was hilft, die Generalisierungsf√§higkeit besser einzusch√§tzen, bevor das Modell eventuell auf einem endg√ºltigen, ungesehenen Testset evaluiert wird (falls vorhanden).\n\n\nZusammenfassend l√§sst sich sagen, dass die \\(k\\)-Fold-Kreuzvalidierung eine zuverl√§ssigere und stabilere Methode zur Bewertung der Modellleistung darstellt, insbesondere bei kleineren oder heterogenen Datens√§tzen, da sie die Auswirkungen einer einzelnen, zuf√§lligen Datenaufteilung minimiert und die verf√ºgbaren Daten effizienter nutzt.\nGegen√ºber der Leave-One-Out-Kreuzvalidierung (LOOCV): Die \\(k\\)-Fold-Kreuzvalidierung hat gegen√ºber der LOOCV (Leave-One-Out-Kreuzvalidierung) mehrere Vorteile: 1. Rechenaufwand: * Vorteil: LOOCV erfordert \\(n\\) Trainingsl√§ufe (wobei \\(n\\) die Anzahl der Beobachtungen im Datensatz ist), da jede einzelne Beobachtung einmal als Testset verwendet wird. Dies kann bei gro√üen Datens√§tzen sehr rechenintensiv sein. Bei der \\(k\\)-Fold-Kreuzvalidierung wird das Modell nur \\(k\\) Mal trainiert, was den Rechenaufwand erheblich reduziert, insbesondere bei gro√üen Datens√§tzen.\n\n\n\n\n\n13. Logistische Regression - Grundlagen\n\nErkl√§ren Sie, warum lineare Regression f√ºr bin√§re Klassifikationsprobleme ungeeignet ist. Wie l√∂st die logistische Regression dieses Problem?\n\n\n\n\n\n\n\nL√∂sung Aufgabe 13\n\n\n\n\n\nWarum Lineare Regression f√ºr bin√§re Klassifikationsprobleme ungeeignet ist:\nEin bin√§res Klassifikationsproblem liegt vor, wenn die abh√§ngige Variable (\\(Y\\)) nur zwei diskrete Werte annehmen kann, typischerweise 0 oder 1 (z.B. ‚ÄúKunde kauft‚Äù vs.¬†‚ÄúKunde kauft nicht‚Äù, ‚ÄúKredit gew√§hrt‚Äù vs.¬†‚ÄúKredit abgelehnt‚Äù).\nDie lineare Regression ist aus folgenden Gr√ºnden f√ºr solche Probleme ungeeignet:\n\nUngeeignete Vorhersagewerte:\n\nDie lineare Regression versucht, \\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\) zu sch√§tzen. Die vorhergesagten Werte \\(\\hat{Y}\\) k√∂nnen jedoch beliebige reelle Zahlen annehmen (\\(\\pm \\infty\\)).\nBei bin√§ren Daten ist \\(Y\\) jedoch 0 oder 1. Wenn wir die linearen Vorhersagen als Wahrscheinlichkeiten interpretieren w√ºrden (\\(P(Y=1|X)\\)), k√∂nnten diese Werte kleiner als 0 oder gr√∂√üer als 1 sein, was als Wahrscheinlichkeit mathematisch unsinnig ist.\n\nFalsche Annahmen √ºber den Fehlerterm:\n\nLineare Regression geht davon aus, dass die Fehlerterme (\\(\\epsilon\\)) normalverteilt sind und eine konstante Varianz (Homoskedastizit√§t) aufweisen.\nBei bin√§ren Daten ist der Fehlerterm nicht normalverteilt. Da \\(Y\\) nur 0 oder 1 sein kann, ist der Fehler \\(\\epsilon = Y - \\hat{Y}\\). Wenn \\(\\hat{Y}\\) beispielsweise eine Wahrscheinlichkeit ist, k√∂nnen die Fehler nur \\(1 - \\hat{Y}\\) oder \\(0 - \\hat{Y}\\) sein, was zu einer nicht-normalen und heteroskedastischen Fehlerverteilung f√ºhrt (die Varianz h√§ngt von der Wahrscheinlichkeit \\(p\\) ab).\n\nFehlinterpretierbarkeit der Koeffizienten:\n\nIn der linearen Regression wird \\(\\beta_1\\) als die durchschnittliche √Ñnderung in \\(Y\\) f√ºr eine Einheit √Ñnderung in \\(X\\) interpretiert. Bei bin√§ren Daten w√§re dies eine √Ñnderung in \\(0/1\\), was schwer sinnvoll zu interpretieren ist.\n\n\nWie die logistische Regression dieses Problem l√∂st:\nDie logistische Regression l√∂st diese Probleme, indem sie die Lineare Regression nicht direkt auf die bin√§re Variable \\(Y\\) anwendet, sondern auf eine Transformation der Wahrscheinlichkeit, dass \\(Y=1\\).\n\nModellierung der Wahrscheinlichkeit √ºber die Logit-Funktion:\n\nAnstatt \\(Y\\) direkt zu modellieren, modelliert die logistische Regression die Log-Odds (oder Logit) der Wahrscheinlichkeit \\(P(Y=1|X)\\).\nDie Logit-Funktion ist definiert als: \\(\\text{logit}(p) = \\ln\\left(\\frac{p}{1-p}\\right)\\).\nDie lineare Beziehung wird auf diese Log-Odds angewendet: \\[\n  \\ln\\left(\\frac{P(Y=1|X)}{1-P(Y=1|X)}\\right) = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p\n  \\]\nDie linke Seite der Gleichung kann Werte von \\(-\\infty\\) bis \\(+\\infty\\) annehmen, was mit der linearen Regression vereinbar ist.\n\nTransformation zur√ºck zur Wahrscheinlichkeit mit der Sigmoid-Funktion:\n\nUm die Wahrscheinlichkeit \\(P(Y=1|X)\\) zu erhalten, wird die inverse der Logit-Funktion verwendet, die Sigmoid-Funktion (oder logistische Funktion): \\[\n  P(Y=1|X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p)}}\n  \\]\nDiese Sigmoid-Funktion hat die Eigenschaft, dass sie jeden reellen Wert als Eingabe akzeptiert und eine Ausgabe im Bereich von 0 bis 1 liefert. Dadurch k√∂nnen die Vorhersagen direkt als Wahrscheinlichkeiten interpretiert werden.\n\nNat√ºrliche Modellierung der binomischen Verteilung:\n\nDie logistische Regression geht von der Annahme aus, dass die abh√§ngige Variable einer Bernoulli-Verteilung (oder Binomialverteilung f√ºr mehrere Versuche) folgt, was f√ºr bin√§re Daten angemessen ist.\n\n\nZusammenfassend l√∂st die logistische Regression das Problem der ungeeigneten Vorhersagewerte und Annahmen der linearen Regression, indem sie eine nicht-lineare Transformation verwendet, um die Wahrscheinlichkeit eines bin√§ren Ereignisses vorherzusagen, deren Werte nat√ºrlicherweise zwischen 0 und 1 liegen.\n\n\n\n\n\n14. Konfidenzintervalle\n\nErl√§utern Sie das Konzept eines Konfidenzintervalls. Was bedeutet es, wenn Sie ein 95%-Konfidenzintervall f√ºr den Mittelwert einer Stichprobe berechnet haben?\n\n\n\n\n\n\n\nL√∂sung Aufgabe 14\n\n\n\n\n\nKonzept eines Konfidenzintervalls:\nEin Konfidenzintervall ist ein Bereich von Werten, der dazu dient, einen unbekannten Populationsparameter (z.B. den Populationsmittelwert \\(\\mu\\), die Populationsstandardabweichung \\(\\sigma\\) oder den Populationsanteil \\(p\\)) zu sch√§tzen. Es wird auf Basis von Stichprobendaten berechnet.\nEs besteht aus zwei Hauptkomponenten:\n\nPunktsch√§tzer: Das ist der beste Sch√§tzwert f√ºr den Parameter, der direkt aus der Stichprobe abgeleitet wird (z.B. der Stichprobenmittelwert \\(\\bar{X}\\) f√ºr den Populationsmittelwert \\(\\mu\\)).\nFehlermarge: Dies ist ein Bereich um den Punktsch√§tzer herum, der die Unsicherheit der Sch√§tzung widerspiegelt. Die Gr√∂√üe der Fehlermarge h√§ngt von der Variabilit√§t der Daten, der Stichprobengr√∂√üe und dem gew√§hlten Konfidenzniveau ab.\n\nDas Konfidenzintervall gibt an, wie pr√§zise die Sch√§tzung ist und mit welcher ‚ÄúSicherheit‚Äù der wahre Populationsparameter in diesem Intervall liegt.\nWas bedeutet es, wenn Sie ein 95%-Konfidenzintervall f√ºr den Mittelwert einer Stichprobe berechnet haben?\nWenn Sie ein 95%-Konfidenzintervall f√ºr den Mittelwert einer Population auf Basis einer Stichprobe berechnet haben, bedeutet dies Folgendes:\n\nInterpretation der ‚ÄúSicherheit‚Äù: Es ist nicht die Wahrscheinlichkeit, dass der eine berechnete Intervall den wahren Populationsmittelwert enth√§lt. Der wahre Populationsmittelwert ist ein fester, unbekannter Wert; er liegt entweder im Intervall oder nicht.\nWiederholte Stichprobenziehung: Die korrekte Interpretation bezieht sich auf das Verfahren: Wenn wir den Prozess der Stichprobenziehung und der Berechnung des Konfidenzintervalls sehr oft wiederholen w√ºrden (z.B. 100 Mal), dann w√ºrden wir erwarten, dass ungef√§hr 95% dieser Intervalle den wahren, unbekannten Populationsmittelwert enthalten.\n‚ÄúWir sind zu 95% sicher, dass‚Ä¶‚Äù: In der Praxis formulieren wir dies oft als: ‚ÄúWir sind zu 95% zuversichtlich/sicher, dass der wahre Populationsmittelwert zwischen der unteren und oberen Grenze dieses spezifischen Intervalls liegt.‚Äù Dies ist eine praktische, wenn auch leicht vereinfachte Formulierung der obigen, formal korrekten Interpretation.\n\nBeispiel: Wenn Sie aus einer Stichprobe ein 95%-Konfidenzintervall f√ºr die durchschnittliche K√∂rpergr√∂√üe von Studierenden zwischen 170 cm und 175 cm berechnet haben, bedeutet dies, dass, wenn Sie diesen Prozess viele Male wiederholen w√ºrden, etwa 95% der so erzeugten Intervalle die tats√§chliche durchschnittliche K√∂rpergr√∂√üe aller Studierenden (die wir nicht kennen) umfassen w√ºrden.\n\n\n\n\n\n15. Chi-Quadrat-Test\n\nSie m√∂chten untersuchen, ob es einen Zusammenhang zwischen der Herkunft (origin) eines Autos und dem Vorhandensein eines Turboladers (hypothetische kategoriale Variable, die Sie erstellen k√∂nnten) gibt.\n\nWelchen statistischen Test w√ºrden Sie verwenden? Begr√ºnden Sie Ihre Wahl.\nFormulieren Sie die Null- und Alternativhypothese f√ºr diesen Test.\nErkl√§ren Sie, warum dieser Test als nicht-parametrisch gilt.\n\n\n\n\n\n\n\n\nL√∂sung Aufgabe 15\n\n\n\n\n\n1. Welchen statistischen Test w√ºrden Sie verwenden? Begr√ºnden Sie Ihre Wahl.\nIch w√ºrde den Chi-Quadrat-Unabh√§ngigkeitstest (engl. Chi-squared test of independence) verwenden.\nBegr√ºndung: * Die Fragestellung zielt darauf ab, einen Zusammenhang (oder dessen Fehlen) zwischen zwei kategorialen Variablen zu pr√ºfen: origin (z.B. USA, Europa, Japan) und Turbolader (z.B. Ja, Nein). * Der Chi-Quadrat-Unabh√§ngigkeitstest ist speziell daf√ºr konzipiert, zu testen, ob eine statistische Abh√§ngigkeit zwischen zwei oder mehr kategorialen Variablen besteht. Er vergleicht die beobachteten H√§ufigkeiten in einer Kontingenztafel mit den erwarteten H√§ufigkeiten unter der Annahme, dass die Variablen unabh√§ngig sind.\n2. Formulieren Sie die Null- und Alternativhypothese f√ºr diesen Test.\n\nNullhypothese (\\(H_0\\)): Es gibt keinen statistischen Zusammenhang zwischen der Herkunft eines Autos und dem Vorhandensein eines Turboladers. Die beiden Variablen sind statistisch unabh√§ngig.\n\n(Formal: Die Verteilung der Turbolader h√§ngt nicht von der Herkunft ab, und umgekehrt.)\n\nAlternativhypothese (\\(H_1\\)): Es gibt einen statistischen Zusammenhang zwischen der Herkunft eines Autos und dem Vorhandensein eines Turboladers. Die beiden Variablen sind statistisch abh√§ngig.\n\n(Formal: Die Verteilung der Turbolader h√§ngt von der Herkunft ab.)\n\n\n3. Erkl√§ren Sie, warum dieser Test als nicht-parametrisch gilt.\nDer Chi-Quadrat-Test gilt als nicht-parametrisch, weil er keine Annahmen √ºber die spezifische Verteilungsform (z.B. Normalverteilung) der zugrunde liegenden Populationen macht.\n\nIm Gegensatz zu parametrischen Tests (wie dem \\(t\\)-Test oder ANOVA), die Annahmen √ºber Parameter wie Mittelwert, Varianz und Normalit√§t der Daten treffen, basiert der Chi-Quadrat-Test auf der H√§ufigkeitsverteilung von Daten in Kategorien.\nEr arbeitet direkt mit H√§ufigkeiten oder Z√§hlungen und vergleicht, ob die beobachteten H√§ufigkeiten signifikant von den erwarteten H√§ufigkeiten abweichen, die unter der Annahme der Unabh√§ngigkeit berechnet werden.\nDiese Eigenschaft macht ihn besonders geeignet f√ºr kategoriale Daten, wo metrische Annahmen (wie Mittelwerte oder Normalverteilung) nicht anwendbar sind.\n\n\n\n\n\n\n16. ROC-Kurve und AUC\n\nErkl√§ren Sie, was eine ROC-Kurve darstellt und wie sie interpretiert wird. Was misst die AUC (Area Under the Curve) und welche Werte sind w√ºnschenswert? Warum sind diese Metriken besonders n√ºtzlich f√ºr die Bewertung von Klassifikationsmodellen?\n\n\n\n\n\n\n\nL√∂sung Aufgabe 16\n\n\n\n\n\nWas eine ROC-Kurve darstellt und wie sie interpretiert wird:\nDie ROC-Kurve (Receiver Operating Characteristic Curve) ist ein grafisches Werkzeug zur Bewertung der Leistung eines bin√§ren Klassifikationsmodells bei verschiedenen Klassifikationsschwellenwerten.\n\nSie wird geplottet, indem die True Positive Rate (TPR) (auch Sensitivit√§t oder Recall genannt) auf der y-Achse gegen die False Positive Rate (FPR) auf der x-Achse aufgetragen wird.\n\nTPR (Sensitivit√§t / Recall): Der Anteil der tats√§chlich positiven F√§lle, die korrekt als positiv identifiziert wurden (\\(TPR = \\frac{TP}{TP + FN}\\)).\nFPR (1 - Spezifit√§t): Der Anteil der tats√§chlich negativen F√§lle, die f√§lschlicherweise als positiv identifiziert wurden (\\(FPR = \\frac{FP}{FP + TN}\\)).\n\nInterpretation der ROC-Kurve:\n\nJeder Punkt auf der ROC-Kurve repr√§sentiert ein bestimmtes Schwellenwertpaar f√ºr TPR und FPR. Durch Variieren des Schwellenwerts, ab dem eine Beobachtung als positiv klassifiziert wird (z.B. Wahrscheinlichkeit &gt; 0.5), bewegen wir uns entlang der Kurve.\nEine ideale Kurve w√ºrde im oberen linken Eck des Diagramms verlaufen (TPR = 1, FPR = 0), was bedeutet, dass alle positiven F√§lle korrekt identifiziert werden, ohne dass falsche positive F√§lle erzeugt werden.\nEine Diagonallinie von (0,0) nach (1,1) repr√§sentiert einen Zufallsklassifikator (z.B. M√ºnzwurf). Ein Modell unterhalb dieser Linie ist schlechter als zuf√§llig.\nJe weiter die Kurve nach oben links verl√§uft, desto besser ist die Unterscheidungsf√§higkeit des Modells zwischen den positiven und negativen Klassen.\n\n\nWas misst die AUC (Area Under the Curve) und welche Werte sind w√ºnschenswert?\n\nDie AUC (Area Under the Curve) ist ein einzelner Skalarwert, der die gesamte Fl√§che unter der ROC-Kurve misst. Sie fasst die Gesamtleistung des Modells √ºber alle m√∂glichen Klassifikationsschwellenwerte hinweg zusammen.\nWertebereich: Die AUC liegt typischerweise zwischen 0 und 1.\nW√ºnschenswerte Werte:\n\nAUC = 1.0: Perfekte Klassifikation (Modell unterscheidet positive und negative Klassen fehlerfrei).\nAUC = 0.5: Zufallsklassifikator (Modell ist nicht besser als zuf√§lliges Raten).\nAUC &lt; 0.5: Das Modell ist schlechter als Zufall (kann oft durch Invertieren der Vorhersagen behoben werden).\nAllgemein: H√∂here AUC-Werte sind w√ºnschenswert, wobei Werte √ºber 0.70 oft als akzeptabel und Werte √ºber 0.85 als sehr gut angesehen werden.\n\n\nWarum sind diese Metriken besonders n√ºtzlich f√ºr die Bewertung von Klassifikationsmodellen?\nROC-Kurve und AUC sind aus mehreren Gr√ºnden n√ºtzlich:\n\nSchwellenwert-unabh√§ngig: Im Gegensatz zu anderen Metriken wie Accuracy, Precision oder Recall, die von einem bestimmten Klassifikationsschwellenwert abh√§ngen, bewerten ROC und AUC die Leistung eines Modells √ºber alle m√∂glichen Schwellenwerte. Dies ist entscheidend, da der optimale Schwellenwert je nach Anwendungsfall (z.B. h√∂here Sensitivit√§t vs.¬†h√∂here Spezifit√§t) variieren kann.\nUmgang mit unbalancierten Daten: Bei stark unbalancierten Datens√§tzen (z.B. 99% negative F√§lle, 1% positive F√§lle) kann die Accuracy irref√ºhrend sein. Ein Modell, das immer ‚Äúnegativ‚Äù vorhersagt, h√§tte eine hohe Genauigkeit (99%), w√§re aber nutzlos. ROC und AUC sind weniger anf√§llig f√ºr dieses Problem, da sie die F√§higkeit des Modells bewerten, positive F√§lle von negativen zu unterscheiden, unabh√§ngig von deren H√§ufigkeit.\nVergleich von Modellen: Sie erm√∂glichen einen fairen Vergleich verschiedener Klassifikationsmodelle, selbst wenn diese unterschiedliche Schwellenwerte verwenden oder auf unterschiedlich gro√üen Datens√§tzen trainiert wurden. Das Modell mit der gr√∂√üeren AUC wird im Allgemeinen als das leistungsst√§rkere Modell angesehen.\nVisualisierung der Trade-offs: Die ROC-Kurve visualisiert den Kompromiss zwischen TPR (Nutzen) und FPR (Kosten) bei verschiedenen Schwellenwerten, was f√ºr die Entscheidungsfindung wichtig ist. Man kann den Punkt auf der Kurve w√§hlen, der am besten zu den spezifischen Anforderungen des Problems passt.\n\n\n\n\n\n\n17. Overfitting und Underfitting\n\nDefinieren Sie Overfitting und Underfitting im Kontext von maschinellem Lernen. Welche Auswirkungen haben sie auf die Generalisierungsf√§higkeit eines Modells? Nennen Sie jeweils eine Strategie zur Vermeidung von Overfitting und Underfitting.\n\n\n\n\n\n\n\nL√∂sung Aufgabe 17\n\n\n\n\n\nDefinition von Overfitting und Underfitting:\nIm Kontext des maschinellen Lernens beschreiben Overfitting und Underfitting zwei grundlegende Probleme, die die Leistung eines Modells beeinflussen k√∂nnen, insbesondere seine F√§higkeit, auf neuen, ungesehenen Daten genaue Vorhersagen zu treffen.\n\nUnderfitting (Unteranpassung):\n\nDefinition: Underfitting tritt auf, wenn ein Modell zu einfach ist, um die zugrunde liegenden Muster und die Struktur in den Trainingsdaten zu erfassen. Es ‚Äúlernt‚Äù die Daten nicht ausreichend.\nSymptome: Das Modell zeigt sowohl auf den Trainingsdaten als auch auf den Testdaten eine schlechte Leistung. Es ist nicht komplex genug, um die Variabilit√§t in den Daten zu modellieren.\nBeispiel: Eine lineare Regression, die versucht, eine sehr komplexe, nicht-lineare Beziehung zu modellieren. Das Modell ist zu ‚Äústarr‚Äù.\n\nOverfitting (√úberanpassung):\n\nDefinition: Overfitting tritt auf, wenn ein Modell zu komplex oder zu flexibel ist und nicht nur die zugrunde liegenden Muster, sondern auch das Rauschen und die zuf√§lligen Schwankungen (Fehler) in den Trainingsdaten lernt. Es ‚Äúmemorisiert‚Äù die Trainingsdaten.\nSymptome: Das Modell zeigt eine ausgezeichnete Leistung auf den Trainingsdaten, aber eine deutlich schlechtere Leistung auf neuen, ungesehenen Testdaten. Es hat die spezifischen Merkmale des Trainingsdatensatzes zu stark verinnerlicht und kann nicht auf die allgemeine Population verallgemeinern.\nBeispiel: Ein sehr komplexes Polynommodell oder ein tiefer Entscheidungsbaum, der so viele Verzweigungen hat, dass er jeden einzelnen Trainingsdatenpunkt perfekt klassifiziert oder vorhersagt.\n\n\nAuswirkungen auf die Generalisierungsf√§higkeit eines Modells:\nDie Generalisierungsf√§higkeit ist die F√§higkeit eines Modells, gute Vorhersagen oder Klassifikationen auf neuen, ungesehenen Daten zu treffen, die nicht Teil des Trainingsdatensatzes waren.\n\nAuswirkung von Underfitting: Ein unterangepasstes Modell hat eine schlechte Generalisierungsf√§higkeit, weil es die grundlegenden Beziehungen in den Daten nicht einmal auf den Trainingsdaten gelernt hat. Es ist sowohl auf bekannten als auch auf unbekannten Daten ungenau.\nAuswirkung von Overfitting: Ein √ºberangepasstes Modell hat ebenfalls eine schlechte Generalisierungsf√§higkeit, obwohl es auf den Trainingsdaten gut abschneidet. Es hat Rauschen und spezifische Details der Trainingsdaten gelernt, die nicht repr√§sentativ f√ºr die Gesamtpopulation sind. Daher versagt es, wenn es mit neuen Daten konfrontiert wird.\n\nDas Ziel beim Modelltraining ist es, ein Modell zu finden, das eine gute Balance zwischen Underfitting und Overfitting aufweist, um eine optimale Generalisierungsf√§higkeit zu erreichen.\nStrategien zur Vermeidung:\n\nStrategie zur Vermeidung von Underfitting:\n\nModellkomplexit√§t erh√∂hen: Verwenden Sie ein komplexeres Modell (z.B. von linearer Regression zu Polynomregression wechseln, von einfachen Entscheidungsb√§umen zu komplexeren Strukturen oder neuronalen Netzen).\nMehr relevante Features hinzuf√ºgen: Die Aufnahme weiterer Variablen, die f√ºr das Problem relevant sind, kann dem Modell helfen, komplexere Muster zu erfassen.\nFeature Engineering: Erstellen Sie neue Features aus bestehenden Daten, die dem Modell helfen, den Zusammenhang besser zu verstehen (z.B. interaktive Terme, polynomiale Terme).\n\nStrategie zur Vermeidung von Overfitting:\n\nMehr Trainingsdaten sammeln: Der einfachste und oft effektivste Weg. Mehr Daten helfen dem Modell, die wahren Muster vom Rauschen zu unterscheiden.\nModellkomplexit√§t reduzieren (Regularisierung):\n\nRegularisierungstechniken: F√ºgen Sie dem Verlustfunktionsterm eine Strafe f√ºr die Modellkomplexit√§t hinzu (z.B. L1-Regularisierung (Lasso) oder L2-Regularisierung (Ridge) bei linearen Modellen, Dropout bei neuronalen Netzen). Dies zwingt das Modell, einfachere Gewichte zu lernen.\nWeniger Features verwenden/Feature-Auswahl: Wenn zu viele irrelevante Features vorhanden sind, kann das Modell Rauschen lernen. Entfernen Sie Features oder nutzen Sie Feature-Auswahlmethoden.\nEinfachere Modelle verwenden: Wechseln Sie zu einem weniger komplexen Modelltyp (z.B. von einem tiefen neuronalen Netz zu einem flacheren, oder von einem komplexen Ensemble zu einem einfacheren Baum).\n\nKreuzvalidierung: Verwenden Sie Methoden wie \\(k\\)-Fold-Kreuzvalidierung, um die Modellleistung auf verschiedenen Teilen der Daten zu bewerten und ein robusteres Leistungsma√ü zu erhalten, was hilft, Overfitting fr√ºhzeitig zu erkennen.\nEarly Stopping: Beim iterativen Training (z.B. bei neuronalen Netzen) das Training beenden, wenn die Leistung auf einem Validierungsset beginnt zu sinken, auch wenn sie auf dem Trainingsset noch steigt.\n\n\n\n\n\n\n\n18. Anwendung des CRISP-DM Modells\n\nNehmen Sie ein beliebiges Problem, das Sie mit Datenanalyse l√∂sen m√∂chten (z.B. Vorhersage von Immobilienpreisen). Beschreiben Sie, wie Sie die ersten drei Phasen des CRISP-DM Modells (Business Understanding, Data Understanding, Data Preparation) auf dieses Problem anwenden w√ºrden, basierend auf den im Kurs behandelten Konzepten und den bereitgestellten Dokumenten.\n\n\n\n\n\n\n\nL√∂sung Aufgabe 18\n\n\n\n\n\nProblem: Vorhersage von Immobilienpreisen in einer bestimmten Region.\nAnwendung des CRISP-DM Modells (Phasen 1-3):\nPhase 1: Business Understanding (Gesch√§ftsverst√§ndnis)\nIn dieser Phase geht es darum, die Gesch√§ftsziele und das Problem aus der Perspektive des Auftraggebers zu verstehen.\n\nZiele definieren:\n\nGesch√§ftsziel: Eine pr√§zise und zuverl√§ssige Vorhersage der Immobilienpreise erm√∂glichen, um K√§ufern und Verk√§ufern eine fundierte Entscheidungsgrundlage zu bieten, Banken bei der Kreditbewertung zu unterst√ºtzen oder Immobilienmaklern bei der Preisgestaltung zu helfen.\nData-Mining-Ziel: Ein Vorhersagemodell entwickeln, das den Immobilienpreis (Zielvariable) basierend auf verschiedenen Immobilienmerkmalen (Pr√§diktoren) mit hoher Genauigkeit sch√§tzt.\nErfolgskriterien: Das Modell sollte einen RMSE (Root Mean Squared Error) von unter X Euro erreichen oder eine durchschnittliche Abweichung von weniger als Y% vom tats√§chlichen Preis aufweisen.\n\nProblemformulierung:\n\nWie k√∂nnen wir den Einfluss verschiedener Faktoren (z.B. Lage, Gr√∂√üe, Zustand, Anzahl der Zimmer, Baujahr) auf den Immobilienpreis quantifizieren?\nGibt es bestimmte ‚ÄúHotspots‚Äù oder Immobilienmerkmale, die den Preis √ºberproportional beeinflussen?\nWelche Art von Modell w√§re am besten geeignet (z.B. Lineare Regression, Baum-basierte Modelle)?\n\nKontextualisierung: Verst√§ndnis der Marktbedingungen (Angebot/Nachfrage, Zinss√§tze, lokale Wirtschaft), der Zielgruppe (Privatpersonen, Investoren) und der Nutzung des Modells (einmalige Sch√§tzung, kontinuierliche √úberwachung).\n\nPhase 2: Data Understanding (Datenverst√§ndnis)\nIn dieser Phase geht es darum, die Daten zu sammeln, zu untersuchen und ihre Qualit√§t zu bewerten, um ein erstes Verst√§ndnis zu entwickeln.\n\nDatenerhebung:\n\nIdentifikation potenzieller Datenquellen: Immobilienportale, Kataster√§mter, amtliche Statistiken, Maklerdatenbanken.\nSammeln von Daten √ºber verkaufte Immobilien in der Region, einschlie√ülich Merkmalen wie:\n\nprice (Preis - Zielvariable)\nliving_area (Wohnfl√§che in m¬≤)\nnumber_of_rooms (Anzahl der Zimmer)\nnumber_of_bathrooms (Anzahl der B√§der)\nlot_size (Grundst√ºcksgr√∂√üe in m¬≤)\nyear_built (Baujahr)\nproperty_type (Immobilientyp: Haus, Wohnung, Reihenhaus - kategorial)\nlocation (PLZ, Stadtteil - kategorial/geografisch)\ncondition (Zustand: z.B. neuwertig, gut, renovierungsbed√ºrftig - ordinal)\nlast_renovation_year (Letztes Renovierungsjahr)\n\n\nDatenerkundung (Exploratory Data Analysis - EDA):\n\nDeskriptive Statistik (Referenz: data_sets.md): Berechnen von Mittelwert, Median, Standardabweichung, Min/Max f√ºr numerische Variablen (price, living_area, lot_size, year_built). H√§ufigkeiten f√ºr kategoriale Variablen (property_type, location, condition).\nDatenvisualisierung (Referenz: data_sets.md, distributions.md):\n\nHistogramme f√ºr price, living_area, year_built, um Verteilungen zu sehen (z.B. ob Preise rechtsschief verteilt sind).\nBalkendiagramme f√ºr property_type, location, condition, um die Verteilung der Kategorien zu sehen.\nStreudiagramme zwischen price und numerischen Variablen (z.B. living_area vs.¬†price), um lineare oder nicht-lineare Beziehungen zu erkennen.\nBoxplots von price nach property_type oder condition, um Preisunterschiede zwischen Kategorien zu visualisieren.\n\nKorrelationsanalyse (Referenz: data_sets.md): Berechnung von Korrelationskoeffizienten (z.B. Pearson) zwischen price und numerischen Pr√§diktoren (living_area, number_of_rooms). Erste Einsch√§tzung von Multikollinearit√§t zwischen Pr√§diktoren.\n\nQualit√§tspr√ºfung:\n\nFehlende Werte: Identifizieren, welche Variablen fehlende Werte haben und wie viele (z.B. last_renovation_year k√∂nnte fehlen, wenn nie renoviert).\nAusrei√üer: Erkennen von extremen Werten (z.B. sehr gro√üe/kleine Immobilien, unrealistische Preise).\nDatenkonsistenz: √úberpr√ºfung auf Inkonsistenzen (z.B. year_built &gt; aktuelles Jahr, living_area &gt; lot_size bei einem Haus).\nDatenformate: Sicherstellen, dass alle Variablen im korrekten Format vorliegen (numerisch, kategorial).\n\n\nPhase 3: Data Preparation (Datenaufbereitung)\nDiese Phase konzentriert sich auf die Bereinigung und Transformation der Rohdaten in ein Format, das f√ºr die Modellierung geeignet ist.\n\nDatenauswahl:\n\nEntscheidung, welche Variablen relevant sind und welche ausgeschlossen werden (z.B. irrelevante IDs).\nUmgang mit unvollst√§ndigen Daten (z.B. Spalten mit zu vielen fehlenden Werten entfernen).\n\nDatenbereinigung (Referenz: tutorial.md):\n\nUmgang mit fehlenden Werten:\n\nStrategien: Imputation (z.B. mit Mittelwert, Median, Modus), Entfernung von Zeilen/Spalten (falls wenige). Begr√ºndung der Wahl.\n\nUmgang mit Ausrei√üern:\n\nStrategien: Entfernen, Transformieren (z.B. Log-Transformation, wenn die Verteilung schief ist), Capping/Flooring.\n\nUmgang mit Duplikaten: Identifizieren und Entfernen von doppelten Eintr√§gen.\nInkonsistenzen beheben: Korrektur von fehlerhaften oder inkonsistenten Eintr√§gen (z.B. Vereinheitlichung von Stadtnamen).\n\nDatenkonstruktion (Feature Engineering):\n\nErstellung neuer Features:\n\nage_of_property = aktuelles Jahr - year_built\nrenovated_since = aktuelles Jahr - last_renovation_year (falls vorhanden)\nprice_per_sqm = price / living_area (f√ºr weitere Analysen oder als Zielvariable, wenn sinnvoll)\nInteraktionsterme (z.B. living_area * number_of_rooms).\n\nKategorien zusammenfassen: Zusammenfassen seltener Kategorien in property_type oder location zu einer ‚ÄúOther‚Äù-Kategorie.\n\nDatenintegration:\n\nFalls Daten aus mehreren Quellen stammen, diese zusammenf√ºhren (z.B. Immobilienmerkmale mit Daten zur Infrastruktur des Stadtteils).\n\nDatenformatierung/Transformation (Referenz: advanced_linear_regression.md, logistic_regression.md):\n\nNumerische Transformationen: Log-Transformation von schiefen Variablen (price, living_area) zur besseren Anpassung an die Annahmen linearer Modelle.\nKategoriale Variablen kodieren: One-Hot-Encoding f√ºr property_type, location und condition (wenn nicht ordinal interpretiert). F√ºr condition k√∂nnte auch eine numerische Kodierung 1, 2, 3‚Ä¶ in Betracht gezogen werden, wenn die Ordnung sinnvoll ist.\nSkalierung: Standardisierung (Z-Transformation) oder Normalisierung von numerischen Variablen (z.B. living_area, horsepower), besonders wichtig f√ºr Modelle, die auf Abst√§nden basieren (z.B. k-NN, SVM) oder Regularisierung verwenden. Dies verhindert, dass Variablen mit gr√∂√üeren Werten den Modelllernprozess dominieren.\n\n\nDiese drei Phasen legen das Fundament f√ºr die anschlie√üenden Schritte der Modellierung und Evaluation, indem sie sicherstellen, dass das Problem klar definiert ist und die Daten in einem geeigneten Format und ausreichender Qualit√§t vorliegen.\n\n\n\n\n\n19. Datenaufbereitung in der Praxis\n\nAngenommen, Sie erhalten einen neuen Datensatz, der unsauber ist (z.B. fehlende Werte, inkonsistente Formate, Duplikate). Beschreiben Sie mindestens drei Schritte zur Datenbereinigung, die Sie durchf√ºhren w√ºrden, um den Datensatz f√ºr eine weitere Analyse vorzubereiten. Erl√§utern Sie die Wichtigkeit einer reproduzierbaren Datenaufbereitung.\n\n\n\n\n\n\n\nL√∂sung Aufgabe 19\n\n\n\n\n\nAngenommen, der Datensatz enth√§lt Informationen √ºber Kundenbestellungen mit Feldern wie KundenID, Bestelldatum, Produktname, Menge, Preis und Versandstatus.\nDrei Schritte zur Datenbereinigung:\n\nUmgang mit fehlenden Werten:\n\nProblem: Fehlende Werte sind h√§ufig und k√∂nnen die Analyse und Modellierung stark beeinflussen oder zu Fehlern f√ºhren. Sie k√∂nnen als NaN, NULL, leere Strings oder spezielle Platzhalter (?, -999) auftreten.\nVorgehen:\n\nIdentifizieren: Zuerst w√ºrde ich fehlende Werte in jeder Spalte identifizieren und ihren Anteil am Datensatz pr√ºfen.\nStrategie definieren:\n\nL√∂schen: Wenn eine Spalte einen sehr hohen Anteil an fehlenden Werten hat (z.B. &gt; 70-80%) oder wenn es nur sehr wenige Zeilen mit fehlenden Werten gibt, k√∂nnte ich die gesamte Spalte oder die betroffenen Zeilen entfernen. F√ºr eine Variable wie Versandstatus, die f√ºr die Analyse essenziell ist, w√ºrde ich Zeilen mit fehlenden Werten entfernen, wenn es wenige sind.\nImputation: F√ºr numerische Variablen wie Menge oder Preis k√∂nnte ich fehlende Werte durch den Mittelwert, Median oder Modus der Spalte ersetzen. F√ºr kategoriale Variablen wie Produktname k√∂nnte der Modus oder eine neue Kategorie ‚ÄúUnbekannt‚Äù verwendet werden. Die Wahl h√§ngt von der Verteilung und der Bedeutung der Variable ab.\n\n\nBeispiel: Wenn Menge bei 5% der Bestellungen fehlt und die Verteilung rechtsschief ist, w√ºrde ich den Median zur Imputation verwenden, um Ausrei√üereinfluss zu minimieren.\n\nStandardisierung und Korrektur inkonsistenter Formate/Eintr√§ge:\n\nProblem: Daten k√∂nnen in unterschiedlichen Formaten vorliegen oder inkonsistente Schreibweisen aufweisen, was eine korrekte Analyse verhindert.\nVorgehen:\n\nDatentypen pr√ºfen: Sicherstellen, dass jede Spalte den korrekten Datentyp hat (z.B. Bestelldatum als Datumsobjekt, Menge als Integer, Preis als Float).\nTextuelle Inkonsistenzen: Einheitliche Gro√ü-/Kleinschreibung (z.B. alles in Kleinbuchstaben) f√ºr kategoriale Textfelder (Produktname). Korrektur von Tippfehlern oder Varianten (z.B. ‚ÄúTV‚Äù, ‚Äút.v.‚Äù, ‚ÄúTelevision‚Äù zu ‚ÄúTelevision‚Äù vereinheitlichen).\nNumerische/Datum-Formatierung: Entfernung unerw√ºnschter Zeichen (z.B. W√§hrungssymbole, Kommas als Tausendertrenner), Konvertierung von Datumsstrings in ein einheitliches Datumsformat (z.B. YYYY-MM-DD).\n\nBeispiel: F√ºr Bestelldatum, das mal als ‚Äú2023-01-15‚Äù, mal als ‚Äú15/01/2023‚Äù vorliegt, w√ºrde ich eine einheitliche Konvertierung zu YYYY-MM-DD durchf√ºhren, um Datumsberechnungen zu erm√∂glichen. F√ºr Preis, der als ‚Äú\\(123.45\" vorliegt, w√ºrde ich das '\\)‚Äô-Symbol entfernen und zu einem numerischen Typ konvertieren.\n\nUmgang mit Duplikaten:\n\nProblem: Doppelte Eintr√§ge k√∂nnen Analysen verzerren und die statistische Validit√§t beeintr√§chtigen. Sie k√∂nnen durch Fehler bei der Datenerfassung oder -zusammenf√ºhrung entstehen.\nVorgehen:\n\nIdentifizieren: Bestimmen, was eine ‚Äúduplikate‚Äù Zeile ausmacht. Oft ist dies eine Kombination aus mehreren Spalten (z.B. KundenID, Bestelldatum, Produktname, Menge). Manchmal ist eine KundenID allein nicht ausreichend, wenn ein Kunde mehrere Bestellungen hat.\nStrategie: Entscheiden, ob die Duplikate entfernt werden sollen und welche der doppelten Zeilen (z.B. die erste, die letzte) behalten werden soll.\nEntfernen: Duplikate basierend auf der definierten Spaltenkombination entfernen.\n\nBeispiel: Wenn zwei Zeilen exakt dieselbe KundenID, Bestelldatum, Produktname, Menge und Preis aufweisen, deutet dies stark auf einen doppelten Eintrag hin, der entfernt werden sollte, um keine √úberbewertung dieser ‚ÄúBestellung‚Äù zu riskieren.\n\n\nWichtigkeit einer reproduzierbaren Datenaufbereitung:\nDie Reproduzierbarkeit ist in der Datenanalyse von entscheidender Bedeutung, da sie Transparenz, Validierung und Effizienz gew√§hrleistet:\n\nValidierung und Fehlerbehebung: Wenn die Datenaufbereitung nicht reproduzierbar ist, kann man im Falle von Fehlern oder unerwarteten Ergebnissen nicht nachvollziehen, welche Schritte unternommen wurden. Dies erschwert die Fehlerbehebung und die Validierung der Analyse.\nTransparenz und Vertrauen: Eine reproduzierbare Datenaufbereitung bedeutet, dass jeder die Schritte nachvollziehen und √ºberpr√ºfen kann. Dies schafft Vertrauen in die Ergebnisse, sowohl bei internen Stakeholdern als auch in wissenschaftlichen oder regulatorischen Kontexten.\nZusammenarbeit: In Teams k√∂nnen andere Mitglieder die Arbeit leichter verstehen, √ºberpr√ºfen und weiterf√ºhren. Ein standardisiertes Skript stellt sicher, dass alle mit denselben bereinigten Daten arbeiten.\nWartbarkeit und Aktualisierung: Daten √§ndern sich im Laufe der Zeit. Wenn neue Daten hinzukommen oder die Analyse aktualisiert werden muss, kann ein reproduzierbares Skript einfach erneut ausgef√ºhrt werden, anstatt die Bereinigungsschritte manuell zu wiederholen. Dies spart Zeit und reduziert das Fehlerrisiko erheblich (Referenz tutorial.md Abschnitt ‚ÄúData Preprocessing‚Äù).\nAuditierbarkeit: Insbesondere in regulierten Branchen ist es oft erforderlich, jeden Schritt der Datenverarbeitung dokumentieren und bei Bedarf nachweisen zu k√∂nnen.\n\nUm Reproduzierbarkeit zu gew√§hrleisten, sollten alle Datenbereinigungs- und Transformationsschritte in einem Skript (z.B. Python oder R) festgehalten werden, das ohne manuelle Eingriffe ausgef√ºhrt werden kann. Kommentare und eine klare Struktur sind dabei essenziell.",
    "crumbs": [
      "√úbungsaufgaben Klausurvorbereitung"
    ]
  },
  {
    "objectID": "aufgaben/uebungsaufgaben.html#rechenaufgaben",
    "href": "aufgaben/uebungsaufgaben.html#rechenaufgaben",
    "title": "√úbungsaufgaben Klausurvorbereitung",
    "section": "Rechenaufgaben",
    "text": "Rechenaufgaben\n\n1. Deskriptive Statistik & Z-Transformation\nGegeben sei eine Stichprobe von Autogeschwindigkeiten (in mph): \\([60, 65, 70, 72, 75, 80, 85, 90]\\).\n\nBerechnen Sie den Mittelwert und die Standardabweichung dieser Stichprobe.\nAngenommen, diese Geschwindigkeiten sind ann√§hernd normalverteilt. Wie w√ºrden Sie die Geschwindigkeit von 72 mph in einen Z-Wert umwandeln? Interpretieren Sie das Ergebnis.\n\n\n\n\n\n\n\nL√∂sung Aufgabe 1\n\n\n\n\n\nMittelwert (\\(\\bar{X}\\)): \\[\n\\bar{X} = \\frac{60 + 65 + 70 + 72 + 75 + 80 + 85 + 90}{8} = \\frac{597}{8} = 74.625\n\\]\nStandardabweichung (\\(S\\)): \\[\nS = \\sqrt{\\frac{1}{n-1} \\sum (X_i - \\bar{X})^2}\n\\] \\(S = \\sqrt{\\frac{(60-74.625)^2 + (65-74.625)^2 + (70-74.625)^2 + (72-74.625)^2 + (75-74.625)^2 + (80-74.625)^2 + (85-74.625)^2 + (90-74.625)^2}{8-1}}\\) \\(S = \\sqrt{\\frac{213.90625 + 92.640625 + 21.390625 + 6.990625 + 0.140625 + 28.890625 + 107.640625 + 236.390625}{7}}\\) \\(S = \\sqrt{\\frac{708}{7}} \\approx \\sqrt{101.14} \\approx 10.057\\)\nZ-Transformation f√ºr 72 mph: \\[\nZ = \\frac{X - \\bar{X}}{S} = \\frac{72 - 74.625}{10.057} \\approx \\frac{-2.625}{10.057} \\approx -0.261\n\\] Interpretation: Ein Z-Wert von ca. -0.261 bedeutet, dass 72 mph etwa 0.26 Standardabweichungen unter dem Stichprobenmittelwert liegt.\n\n\n\n\n\n2. Wahrscheinlichkeitsberechnung - Normalverteilung\nDie Lebensdauer von Gl√ºhbirnen sei normalverteilt mit einem Mittelwert von \\(1200\\) Stunden und einer Standardabweichung von \\(150\\) Stunden.\n\nWie gro√ü ist die Wahrscheinlichkeit, dass eine zuf√§llig ausgew√§hlte Gl√ºhbirne weniger als \\(1000\\) Stunden brennt?\nWie gro√ü ist die Wahrscheinlichkeit, dass sie zwischen \\(1100\\) und \\(1300\\) Stunden brennt? (Verwenden Sie eine Z-Tabelle oder einen Taschenrechner/Software f√ºr die Normalverteilung).\n\n\n\n\n\n\n\nL√∂sung Aufgabe 2\n\n\n\n\n\nGegeben: \\(\\mu = 1200\\), \\(\\sigma = 150\\).\nWahrscheinlichkeit, dass eine Gl√ºhbirne weniger als \\(1000\\) Stunden brennt (\\(P(X &lt; 1000)\\)): Z-Wert f√ºr \\(X = 1000\\): \\[\nZ = \\frac{X - \\mu}{\\sigma} = \\frac{1000 - 1200}{150} = \\frac{-200}{150} \\approx -1.33\n\\] Mit einer Z-Tabelle (oder Software) finden wir \\(P(Z &lt; -1.33) \\approx 0.0918\\). Die Wahrscheinlichkeit betr√§gt ca. \\(9.18\\%\\).\nWahrscheinlichkeit, dass sie zwischen \\(1100\\) und \\(1300\\) Stunden brennt (\\(P(1100 &lt; X &lt; 1300)\\)): Z-Wert f√ºr \\(X = 1100\\): \\[\nZ_1 = \\frac{1100 - 1200}{150} = \\frac{-100}{150} \\approx -0.67\n\\] Z-Wert f√ºr \\(X = 1300\\): \\[\nZ_2 = \\frac{1300 - 1200}{150} = \\frac{100}{150} \\approx 0.67\n\\] \\(P(1100 &lt; X &lt; 1300) = P(-0.67 &lt; Z &lt; 0.67) = P(Z &lt; 0.67) - P(Z &lt; -0.67)\\) Mit einer Z-Tabelle (oder Software): \\(P(Z &lt; 0.67) \\approx 0.7486\\) \\(P(Z &lt; -0.67) \\approx 0.2514\\) \\(P(1100 &lt; X &lt; 1300) \\approx 0.7486 - 0.2514 = 0.4972\\). Die Wahrscheinlichkeit betr√§gt ca. \\(49.72\\%\\).\n\n\n\n\n\n3. Konfidenzintervall f√ºr den Mittelwert\nEine Stichprobe von \\(n=49\\) Studierenden hatte eine durchschnittliche Bearbeitungszeit f√ºr eine Pr√ºfung von \\(\\bar{X} = 75\\) Minuten mit einer Stichprobenstandardabweichung von \\(S = 14\\) Minuten.\n\nKonstruieren Sie ein \\(95\\%\\)-Konfidenzintervall f√ºr die wahre durchschnittliche Bearbeitungszeit aller Studierenden. (Nutzen Sie den passenden kritischen Wert f√ºr die \\(t\\)-Verteilung mit \\(df = n-1\\)).\n\n\n\n\n\n\n\nL√∂sung Aufgabe 3\n\n\n\n\n\nGegeben: \\(n = 49\\), \\(\\bar{X} = 75\\), \\(S = 14\\). Freiheitsgrade \\(df = n - 1 = 49 - 1 = 48\\). F√ºr ein \\(95\\%\\)-Konfidenzintervall und \\(df=48\\) ist der kritische \\(t\\)-Wert (zweiseitig) ungef√§hr \\(t_{krit} \\approx 2.011\\).\nKonfidenzintervall Formel: \\[\n\\bar{X} \\pm t_{krit} \\cdot \\frac{S}{\\sqrt{n}}\n\\] Berechnung des Standardfehlers des Mittelwerts: \\[\nSE = \\frac{S}{\\sqrt{n}} = \\frac{14}{\\sqrt{49}} = \\frac{14}{7} = 2\n\\] Berechnung des Konfidenzintervalls: \\[\n75 \\pm 2.011 \\cdot 2\n\\] \\[\n75 \\pm 4.022\n\\] Untere Grenze: \\(75 - 4.022 = 70.978\\) Obere Grenze: \\(75 + 4.022 = 79.022\\)\nDas \\(95\\%\\)-Konfidenzintervall f√ºr die wahre durchschnittliche Bearbeitungszeit liegt zwischen \\(70.978\\) und \\(79.022\\) Minuten.\n\n\n\n\n\n4. Einfache Lineare Regression - Parameter Sch√§tzung\nSie haben die folgenden Beobachtungen f√ºr \\(X\\) (Werbeausgaben in Tsd. ‚Ç¨) und \\(Y\\) (Verkaufszahlen in Tsd. St√ºck):\n\n\\(X = [10, 20, 30, 40, 50]\\)\n\\(Y = [5, 12, 18, 25, 30]\\)\nSch√§tzen Sie die Parameter \\(\\beta_0\\) (Achsenabschnitt) und \\(\\beta_1\\) (Steigung) der einfachen linearen Regression \\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\) mittels der Formeln f√ºr die Kleinste-Quadrate-Sch√§tzung oder eine Skizze:\n\n\\(\\beta_1 = \\frac{\\sum (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum (X_i - \\bar{X})^2}\\)\n\\(\\beta_0 = \\bar{Y} - \\beta_1 \\bar{X}\\)\n\nInterpretieren Sie die gesch√§tzten Koeffizienten.\n\n\n\n\n\n\n\nL√∂sung Aufgabe 4\n\n\n\n\n\n1. Mittelwerte berechnen: \\(\\bar{X} = \\frac{10+20+30+40+50}{5} = \\frac{150}{5} = 30\\) \\(\\bar{Y} = \\frac{5+12+18+25+30}{5} = \\frac{90}{5} = 18\\)\n2. Summen f√ºr \\(\\beta_1\\) berechnen:\n\n\n\n\n\n\n\n\n\n\n\n\\(X_i\\)\n\\(Y_i\\)\n\\(X_i - \\bar{X}\\)\n\\(Y_i - \\bar{Y}\\)\n\\((X_i - \\bar{X})(Y_i - \\bar{Y})\\)\n\\((X_i - \\bar{X})^2\\)\n\n\n\n\n10\n5\n-20\n-13\n260\n400\n\n\n20\n12\n-10\n-6\n60\n100\n\n\n30\n18\n0\n0\n0\n0\n\n\n40\n25\n10\n7\n70\n100\n\n\n50\n30\n20\n12\n240\n400\n\n\n\n\n\nSumme:\n630\n1000\n\n\n\n3. \\(\\beta_1\\) berechnen: \\[\n\\hat{\\beta}_1 = \\frac{630}{1000} = 0.63\n\\]\n4. \\(\\beta_0\\) berechnen: \\[\n\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1 \\bar{X} = 18 - (0.63 \\cdot 30) = 18 - 18.9 = -0.9\n\\]\nGesch√§tztes Regressionsmodell: \\(Y = -0.9 + 0.63 X\\)\nInterpretation der Koeffizienten: * \\(\\hat{\\beta}_0 = -0.9\\) (Achsenabschnitt): Wenn die Werbeausgaben (X) 0 Tsd. ‚Ç¨ betragen, w√§ren die gesch√§tzten Verkaufszahlen -0.9 Tsd. St√ºck. Dieser Wert ist in diesem Kontext nicht sinnvoll interpretierbar, da Verkaufszahlen nicht negativ sein k√∂nnen und eine Extrapolation zu 0 Werbeausgaben au√üerhalb des beobachteten Bereichs liegt. * \\(\\hat{\\beta}_1 = 0.63\\) (Steigung): Eine Erh√∂hung der Werbeausgaben um 1 Tsd. ‚Ç¨ ist mit einem Anstieg der Verkaufszahlen um 0.63 Tsd. St√ºck verbunden, unter der Annahme, dass alle anderen Faktoren konstant bleiben. Dies impliziert eine positive Beziehung zwischen Werbeausgaben und Verkaufszahlen.\n\n\n\n\n\n5. Hypothesentest - Einstichproben-t-Test\nEin Hersteller behauptet, dass seine neuen Batterien eine durchschnittliche Lebensdauer von \\(50\\) Stunden haben (\\(\\mu_0 = 50\\)). Eine Stichprobe von \\(25\\) Batterien ergibt eine durchschnittliche Lebensdauer von \\(48\\) Stunden mit einer Standardabweichung von \\(5\\) Stunden.\n\nF√ºhren Sie einen Einstichproben-t-Test durch, um zu pr√ºfen, ob die wahre Lebensdauer signifikant von \\(50\\) Stunden abweicht (zweiseitiger Test) bei einem Signifikanzniveau von \\(\\alpha = 0.05\\).\nBerechnen Sie die Teststatistik \\(t = \\frac{\\bar{X} - \\mu_0}{\\frac{S}{\\sqrt{n}}}\\).\nVergleichen Sie den berechneten \\(t\\)-Wert mit dem kritischen \\(t\\)-Wert oder dem p-Wert (wenn Sie eine \\(t\\)-Tabelle oder Software verwenden k√∂nnen). Was ist Ihre Schlussfolgerung?\n\n\n\n\n\n\n\nL√∂sung Aufgabe 5\n\n\n\n\n\nGegeben: \\(\\mu_0 = 50\\), \\(n = 25\\), \\(\\bar{X} = 48\\), \\(S = 5\\). Signifikanzniveau \\(\\alpha = 0.05\\).\n1. Hypothesen aufstellen:\n\nNullhypothese (\\(H_0\\)): Die wahre durchschnittliche Lebensdauer betr√§gt 50 Stunden (\\(\\mu = 50\\)).\nAlternativhypothese (\\(H_1\\)): Die wahre durchschnittliche Lebensdauer weicht von 50 Stunden ab (\\(\\mu \\neq 50\\)). (Zweiseitiger Test)\n\n2. Teststatistik berechnen: \\[\nt = \\frac{\\bar{X} - \\mu_0}{\\frac{S}{\\sqrt{n}}} = \\frac{48 - 50}{\\frac{5}{\\sqrt{25}}} = \\frac{-2}{\\frac{5}{5}} = \\frac{-2}{1} = -2\n\\] Die Teststatistik ist \\(t = -2\\).\n3. Kritischen Wert bestimmen: Freiheitsgrade \\(df = n - 1 = 25 - 1 = 24\\). F√ºr einen zweiseitigen Test mit \\(\\alpha = 0.05\\) und \\(df = 24\\) ist der kritische \\(t\\)-Wert (aus \\(t\\)-Tabelle oder Software) ungef√§hr \\(t_{krit} = \\pm 2.064\\).\n4. Entscheidung treffen: Der berechnete \\(t\\)-Wert (\\(|-2| = 2\\)) ist kleiner als der absolute kritische \\(t\\)-Wert (\\(2.064\\)). Das bedeutet, der Wert liegt innerhalb des Annahmebereichs. Alternativ, der p-Wert f√ºr \\(t = -2\\) bei \\(df = 24\\) (zweiseitig) ist ca. \\(0.0566\\). Da \\(p = 0.0566 &gt; \\alpha = 0.05\\), lehnen wir die Nullhypothese nicht ab.\nSchlussfolgerung: Es gibt keine ausreichenden statistischen Beweise, um zu behaupten, dass die wahre durchschnittliche Lebensdauer der Batterien signifikant von 50 Stunden abweicht. Die beobachtete Abweichung von 48 Stunden k√∂nnte rein zuf√§llig sein.\n\n\n\n\n\n6. Multiple Lineare Regression - Vorhersage\nEin Modell zur Vorhersage des Kraftstoffverbrauchs (mpg) hat folgende gesch√§tzte Parameter:\n\n\\(\\hat{\\beta}_0 = 45\\) (Konstante)\n\\(\\hat{\\beta}_1 = -0.1\\) (f√ºr horsepower)\n\\(\\hat{\\beta}_2 = -0.005\\) (f√ºr weight)\nBerechnen Sie den vorhergesagten mpg f√ºr ein Auto mit \\(150\\) PS (horsepower) und \\(3000\\) lbs (weight).\nWie w√ºrde sich die Vorhersage √§ndern, wenn das Auto \\(100\\) PS und \\(2000\\) lbs h√§tte?\n\n\n\n\n\n\n\nL√∂sung Aufgabe 6\n\n\n\n\n\nDas multiple lineare Regressionsmodell lautet: \\[\n\\widehat{mpg} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot \\text{horsepower} + \\hat{\\beta}_2 \\cdot \\text{weight}\n\\] Einsetzen der gegebenen Parameter: \\[\n\\widehat{mpg} = 45 - 0.1 \\cdot \\text{horsepower} - 0.005 \\cdot \\text{weight}\n\\]\n1. Vorhersage f√ºr Auto 1: * horsepower = 150 * weight = 3000\n\\[\n\\widehat{mpg}_1 = 45 - (0.1 \\cdot 150) - (0.005 \\cdot 3000)\n\\] \\[\n\\widehat{mpg}_1 = 45 - 15 - 15\n\\] \\[\n\\widehat{mpg}_1 = 15\n\\] Der vorhergesagte Kraftstoffverbrauch f√ºr Auto 1 betr√§gt \\(15\\) mpg.\n2. Vorhersage f√ºr Auto 2: * horsepower = 100 * weight = 2000\n\\[\n\\widehat{mpg}_2 = 45 - (0.1 \\cdot 100) - (0.005 \\cdot 2000)\n\\] \\[\n\\widehat{mpg}_2 = 45 - 10 - 10\n\\] \\[\n\\widehat{mpg}_2 = 25\n\\] Der vorhergesagte Kraftstoffverbrauch f√ºr Auto 2 betr√§gt \\(25\\) mpg.\n√Ñnderung der Vorhersage: Die Vorhersage √§ndert sich von 15 mpg auf 25 mpg. Ein Auto mit weniger Leistung und geringerem Gewicht hat einen h√∂heren vorhergesagten Kraftstoffverbrauch, was der Erwartung entspricht.\n\n\n\n\n\n7. W√ºrfelsumme\nZwei faire W√ºrfel werden geworfen.\n\nWie hoch ist die Wahrscheinlichkeit, dass die Summe der Augenzahlen kleiner als 5 ist?\nWie hoch ist die Wahrscheinlichkeit, dass mindestens ein W√ºrfel eine 6 zeigt?\n\n\n\n\n\n\n\nL√∂sung Aufgabe 7\n\n\n\n\n\nUm die Wahrscheinlichkeit zu berechnen, dass die Summe der Augenzahlen kleiner als 5 ist, betrachten wir alle m√∂glichen Kombinationen, bei denen die Summe 2, 3 oder 4 ergibt:\n\nSumme 2: (1, 1)\nSumme 3: (1, 2), (2, 1)\nSumme 4: (1, 3), (2, 2), (3, 1)\n\nEs gibt insgesamt 6 g√ºnstige Kombinationen. Da es insgesamt 36 m√∂gliche Kombinationen beim Werfen von zwei W√ºrfeln gibt (6 Seiten pro W√ºrfel), ist die Wahrscheinlichkeit:\n\\[\nP(\\text{Summe} &lt; 5) = \\frac{6}{36} = \\frac{1}{6} \\approx 0.1667\n\\]\nUm die Wahrscheinlichkeit zu berechnen, dass mindestens ein W√ºrfel eine 6 zeigt, betrachten wir alle Kombinationen, bei denen mindestens ein W√ºrfel eine 6 zeigt:\n\n(6, 1), (6, 2), (6, 3), (6, 4), (6, 5), (6, 6)\n(1, 6), (2, 6), (3, 6), (4, 6), (5, 6)\n\nEs gibt insgesamt 11 g√ºnstige Kombinationen. Die Wahrscheinlichkeit ist:\n\\[\nP(\\text{Mindestens eine 6}) = \\frac{11}{36} \\approx 0.3056\n\\]\n\n\n\n\n\n8. Fehlerma√üe in der Regression\nSie haben die folgenden beobachteten (\\(Y\\)) und vorhergesagten (\\(\\hat{Y}\\)) Werte:\n\n\\(Y = [10, 15, 20, 25]\\)\n\\(\\hat{Y} = [11, 14, 21, 23]\\)\nBerechnen Sie den Mean Squared Error (MSE) und den Root Mean Squared Error (RMSE) f√ºr diese Vorhersagen.\nMSE \\(= \\frac{1}{n} \\sum (Y_i - \\hat{Y}_i)^2\\)\nRMSE \\(= \\sqrt{MSE}\\)\n\n\n\n\n\n\n\nL√∂sung Aufgabe 8\n\n\n\n\n\nGegeben: \\(Y = [10, 15, 20, 25]\\) \\(\\hat{Y} = [11, 14, 21, 23]\\) Anzahl der Beobachtungen \\(n = 4\\).\n1. Differenzen und quadrierte Differenzen berechnen: * \\((Y_1 - \\hat{Y}_1)^2 = (10 - 11)^2 = (-1)^2 = 1\\) * \\((Y_2 - \\hat{Y}_2)^2 = (15 - 14)^2 = (1)^2 = 1\\) * \\((Y_3 - \\hat{Y}_3)^2 = (20 - 21)^2 = (-1)^2 = 1\\) * \\((Y_4 - \\hat{Y}_4)^2 = (25 - 23)^2 = (2)^2 = 4\\)\n2. Summe der quadrierten Differenzen berechnen: \\(\\sum (Y_i - \\hat{Y}_i)^2 = 1 + 1 + 1 + 4 = 7\\)\n3. Mean Squared Error (MSE) berechnen: \\[\nMSE = \\frac{1}{n} \\sum (Y_i - \\hat{Y}_i)^2 = \\frac{7}{4} = 1.75\n\\]\n4. Root Mean Squared Error (RMSE) berechnen: \\[\nRMSE = \\sqrt{MSE} = \\sqrt{1.75} \\approx 1.3228\n\\] Der MSE betr√§gt \\(1.75\\) und der RMSE betr√§gt ungef√§hr \\(1.3228\\).\n\n\n\n\n\n9. Konfusionsmatrix & Metriken\nEin Klassifikationsmodell hat folgende Ergebnisse f√ºr ein bin√§res Problem geliefert:\n\nWahre Positive (TP): 80\nWahre Negative (TN): 150\nFalsche Positive (FP): 20\nFalsche Negative (FN): 50\nBerechnen Sie:\n\nGenauigkeit (Accuracy)\nPr√§zision (Precision)\nSensitivit√§t (Recall / True Positive Rate)\nF1-Score (Optional, aber empfehlenswert f√ºr tiefere Einsicht)\n\nFormeln:\n\nAccuracy \\(= \\frac{TP+TN}{TP+TN+FP+FN}\\)\nPrecision \\(= \\frac{TP}{TP+FP}\\)\nRecall \\(= \\frac{TP}{TP+FN}\\)\nF1-Score \\(= 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\\)\n\n\n\n\n\n\n\n\nL√∂sung Aufgabe 9\n\n\n\n\n\nGegeben: TP = 80, TN = 150, FP = 20, FN = 50.\n1. Genauigkeit (Accuracy) berechnen: Gesamtzahl der Beobachtungen: \\(80 + 150 + 20 + 50 = 300\\) \\[\n\\text{Accuracy} = \\frac{TP+TN}{TP+TN+FP+FN} = \\frac{80+150}{300} = \\frac{230}{300} \\approx 0.7667\n\\]\n2. Pr√§zision (Precision) berechnen: \\[\n\\text{Precision} = \\frac{TP}{TP+FP} = \\frac{80}{80+20} = \\frac{80}{100} = 0.80\n\\]\n3. Sensitivit√§t (Recall) berechnen: \\[\n\\text{Recall} = \\frac{TP}{TP+FN} = \\frac{80}{80+50} = \\frac{80}{130} \\approx 0.6154\n\\]\n4. F1-Score berechnen: \\[\n\\text{F1-Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} = 2 \\cdot \\frac{0.80 \\cdot 0.6154}{0.80 + 0.6154} = 2 \\cdot \\frac{0.49232}{1.4154} \\approx 2 \\cdot 0.3478 \\approx 0.6956\n\\]\nErgebnisse: * Accuracy: ca. \\(76.67\\%\\) * Precision: \\(80\\%\\) * Recall: ca. \\(61.54\\%\\) * F1-Score: ca. \\(69.56\\%\\)\n\n\n\n\n\n10. Kreditnehmer-Datensatz\nF√ºr diese Aufgabe nutzen wir den loan50-Datensatz, der 50 Beobachtungen √ºber Kreditnehmer enth√§lt. Gehen Sie davon aus, dass dieser Datensatz eine repr√§sentative Zufallsstichprobe der Grundgesamtheit von Kreditnehmern darstellt, und dass die relativen H√§ufigkeiten in dieser Stichprobe als Sch√§tzungen f√ºr die tats√§chlichen Wahrscheinlichkeiten in der Grundgesamtheit verwendet werden k√∂nnen.\nDie relevante Kreuztabelle aus dem Skript ist wie folgt gegeben:\nhomeownership  False  True\nhomeownership             \nMortgage          20     6\nOwn                2     1\nRent              18     3\nBeantworten Sie die folgenden Fragen:\n\nWie hoch ist die Wahrscheinlichkeit, dass ein zuf√§llig ausgew√§hlter Kreditnehmer eine Hypothek (Mortgage) hat?\nWie hoch ist die Wahrscheinlichkeit, dass ein zuf√§llig ausgew√§hlter Kreditnehmer ein zweites Einkommen (has_second_income = True) hat, wenn bekannt ist, dass er eine Hypothek (homeownership = Mortgage) hat?\n\n\n\n\n\n\n\nL√∂sung Aufgabe 10\n\n\n\n\n\nAus der Tabelle geht hervor, dass von 50 Kreditnehmern 26 eine Hypothek haben. Die Wahrscheinlichkeit ist also:\n\\[\nP(\\text{Hypothek}) = \\frac{26}{50} = 0.52\n\\]\nAus der Kreuztabelle geht hervor, dass von 26 Kreditnehmern mit Hypothek 6 ein zweites Einkommen haben. Die bedingte Wahrscheinlichkeit ist:\n\\[\nP(\\text{Zweiteinkommen} | \\text{Hypothek}) = \\frac{6}{26} \\approx 0.2308\n\\]",
    "crumbs": [
      "√úbungsaufgaben Klausurvorbereitung"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Carroll, Lewis. 2015. Alice‚Äôs Adventures in Wonderland\nUnfolded.\n\n\nDATAtab. retrieved 2025. ‚ÄúKorrelationskoeffizient Tutorial\nImage.‚Äù https://datatab.de/assets/tutorial/Korrelationskoeffizient.png.\n\n\nDiez, David M., Christopher D. Barr, and Mine Cetinkaya-Rundel. 2019.\nOpenIntro Statistics. 4th ed. Boston, MA: OpenIntro. \\url{https://www.openintro.org/stat/textbook.php?stat_book=os}.\n\n\nFrisch, Max. 1957. Homo Faber. Ein Bericht. Frankfurt am Main:\nSuhrkamp.\n\n\nHong, Tao, Pierre Pinson, and Shu Fan. 2014. ‚ÄúGlobal Energy\nForecasting Competition 2012.‚Äù International Journal of\nForecasting 30 (2): 357‚Äì63. https://doi.org/https://doi.org/10.1016/j.ijforecast.2013.07.001.\n\n\nHyndman, RJ. 2018. Forecasting: Principles and Practice.\nOTexts.\n\n\nInductiveload, based on original work by Jhguch. n.d. ‚ÄúTwo Sample\nt-Test.‚Äù https://commons.wikimedia.org/wiki/File:Two_sample_ttest.svg.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, et al.\n2013. An Introduction to Statistical Learning. Vol. 112.\nSpringer.\n\n\nKozyrkov, Cassie. 2018. ‚ÄúWhat on Earth Is Data Science?‚Äù\nmedium.com. \\url{https://kozyrkov.medium.com/what-on-earth-is-data-science-eb1237d8cb37}.\n\n\nMatthes, Eric. 2023. Python Crash Course - a Hands-on, Project-Based\nIntroduction to Programming. 3rd ed. No Starch Press. https://ehmatthes.github.io/pcc_3e/.\n\n\nRedAndr and Mikhail Ryazanov. 2011. ‚ÄúSatirical diagram illustrating the influence of pirates\ndecreasing on global warming as per Pastafarian beliefs.‚Äù\nhttps://commons.wikimedia.org/wiki/File:PiratesVsTemp(en).svg.\n\n\nShearer, Colin. 2000. ‚ÄúThe CRISP-DM Model: The New Blueprint for\nData Mining.‚Äù Journal of Data Warehousing 5 (4): 13‚Äì22.\n\n\nWickham, Hadley. 2014. ‚ÄúTidy Data.‚Äù Journal of\nStatistical Software 59: 1‚Äì23.\n\n\nWikimedia Commons contributors. retrieved 2025. ‚ÄúCRISP-DM\nProcess Diagram.‚Äù https://upload.wikimedia.org/wikipedia/commons/b/b9/CRISP-DM_Process_Diagram.png.",
    "crumbs": [
      "References"
    ]
  }
]