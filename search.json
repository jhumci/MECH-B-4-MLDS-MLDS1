[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning and Data Science 1",
    "section": "",
    "text": "Preface\nDies sind die Vorlesungsnotizen f√ºr die Lehrveranstaltung Machine Learning + Data Science I (Lecture) am MCI | The Entrepreneurial School Bachelor-Studiengangs Mechatronik im Sommer-Semester 2025.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#danksagungen",
    "href": "index.html#danksagungen",
    "title": "Machine Learning and Data Science 1",
    "section": "Danksagungen",
    "text": "Danksagungen\nWir danken der Open-Source-Community f√ºr die Bereitstellung exzellenter Tutorials und Anleitungen zu Data-Science-Themen in und mit Python im Web.\nEinzelne Quellen werden an den entsprechenden Stellen im Dokument zitiert.\nBesonderer Dank gilt Peter Kandof, f√ºr das Aufsetzen eines Beispielprojekts f√ºr die Vorlesung anhand von MECH-M-DUAL-1-DBM - Grundlagen datenbasierter Methoden. Diese Notizen wurden mit Quarto erstellt.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Einleitung",
    "section": "",
    "text": "Leistungs√ºberpr√ºfung\nIn dieser Lehrveranstaltung werden wir uns mit den grundlegenden Konzepten moderner Data-Science-Techniken befassen und eine solide Basis in Statistik und maschinellem Lernen legen. Wir behandeln alle essenziellen Grundlagen, die notwendig sind, um sich in diesem Bereich sicher zu bewegen. Dabei beschr√§nken wir uns nicht nur auf die theoretischen Aspekte, sondern nutzen auch Python, um die Inhalte programmatisch zu veranschaulichen.\nDie verwendeten Referenzen wurden nach Qualit√§t, freier Verf√ºgbarkeit und der Nutzung von Python ausgew√§hlt. F√ºr die statistischen Grundlagen greifen wir auf Beispiele aus Diez, Barr, and Cetinkaya-Rundel (2019) zur√ºck, w√§hrend f√ºr die Einf√ºhrung in das statistische Lernen James et al. (2013) herangezogen wird.\nDieses Skriptum richtet sich an Studierende der Ingenieurwissenschaften, weshalb mathematische Konzepte nur selten mit strengen Beweisen versehen sind.\nDie Vorlesungsteil der Lehrveranstaltung wird mit einer Klausur bewertet.",
    "crumbs": [
      "Einleitung"
    ]
  },
  {
    "objectID": "intro.html#leistungs√ºberpr√ºfung",
    "href": "intro.html#leistungs√ºberpr√ºfung",
    "title": "Einleitung",
    "section": "",
    "text": "Diez, David M., Christopher D. Barr, and Mine Cetinkaya-Rundel. 2019. OpenIntro Statistics. 4th ed. Boston, MA: OpenIntro. \\url{https://www.openintro.org/stat/textbook.php?stat_book=os}.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, et al. 2013. An Introduction to Statistical Learning. Vol. 112. Springer.",
    "crumbs": [
      "Einleitung"
    ]
  },
  {
    "objectID": "dataexploratory/index.html",
    "href": "dataexploratory/index.html",
    "title": "Umgang mit Daten und Explorative Analyse",
    "section": "",
    "text": "Alice: Would you tell me, please, which way I ought to go from here?\nThe Cheshire Cat: That depends a good deal on where you want to get to.\n‚Äî Carroll (2015)\n\n\nIn dieser Vorlesung erarbeiten wir gemeinsam die Kompetenzen, um Daten zu analysieren, Modelle zu erstellen und Vorhersagen zu treffen. Hierbei lernen wir alle Grundlagen kennen, die notwendig sind, um sich dabei zurechtzufinden. Zun√§chst gibt, es aber die Richtung zu kl√§ren, in die wir uns bewegen wollen.\n\n\n\n\n\n\nNote\n\n\n\nDiese Notizen setzen voraus, dass Sie √ºber grundlegende Programmierkenntnisse in Python verf√ºgen und wir bauen auf diesem Wissen auf. In diesem Sinne verwenden wir Python als Werkzeug und beschreiben die inneren Abl√§ufe nur, wenn es uns hilft, die behandelten Themen besser zu verstehen.\nFalls dies nicht der Fall ist, schauen Sie sich MCI-MECH-B-3-SWD-SWD-ILV an, einen Kurs √ºber Softwaredesign im selben Bachelor-Programm und von denselben Autoren.\n\n\nZus√§tzlich k√∂nnen wir die folgenden B√ºcher √ºber Python empfehlen:\n\nMatthes (2023): Python Crash Course - A hands-on, project-based introduction to programming: Online Material.\nPython Cheat Sheet provided by Matthes (2023).\n\nF√ºr die Statistik und Machine Learning ortientieren wir uns an folgenden B√ºchern:\n\nDiez, Barr, and Cetinkaya-Rundel (2019): OpenIntro Statistics: Online Material\nJames et al. (2013): Introduction to Statistical Learning: Online Material\n\n\n\n\n\nCarroll, Lewis. 2015. Alice‚Äôs Adventures in Wonderland Unfolded.\n\n\nDiez, David M., Christopher D. Barr, and Mine Cetinkaya-Rundel. 2019. OpenIntro Statistics. 4th ed. Boston, MA: OpenIntro. \\url{https://www.openintro.org/stat/textbook.php?stat_book=os}.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, et al. 2013. An Introduction to Statistical Learning. Vol. 112. Springer.\n\n\nMatthes, Eric. 2023. Python Crash Course - a Hands-on, Project-Based Introduction to Programming. 3rd ed. No Starch Press. https://ehmatthes.github.io/pcc_3e/.",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse"
    ]
  },
  {
    "objectID": "dataexploratory/data_science.html",
    "href": "dataexploratory/data_science.html",
    "title": "1¬† Data Science, Statisik, und Machine Learning",
    "section": "",
    "text": "1.1 Data Sciene: Projektvorgehen\nData Science, Statisik, Machine Learning und K√ºstliche Intelligenz sind Begriffe, die in den letzten Jahren immer h√§ufiger in den Medien und in der Wissenschaft auftauchen.\nEs gibt unterschiedlichte Definitionen f√ºr diese Begriffe, die sich je nach Kontext und Anwendungsbereich unterscheiden. Damit wir uns verstehen, versuchen wir es so:\nUm uns in unseren folgenden Abenteuern zurrechtzufinden, besch√§ftigen wir uns zun√§chst hier in Chapter 1 mit dem gundlegenden Herangehen an Data Science-Probleme. In Chapter 2 mit den Daten, die wir analysieren wollen.\nFigure¬†1.1 zeigt einige der vielen Entscheidungen, die wir treffen m√ºssen, wenn wir uns in unserem eigenen Projekt bewegen.\nSelbst, wenn uns das Ziel klar ist, k√∂nnen wir uns immernoch verlaufen. Um dies zu verhindern, gibt es verschiedene Vorgehensweisen, die uns helfen auf dem Pfad zu bleiben und uns helfen ein tiefgehendes Verst√§ndnis f√ºr einen Datensatz zu entwickeln. Hierbei unterst√ºtzen Standardvorgehensweisen wie CRISP-DM (Cross-Industry Standard Process for Data Mining Figure¬†1.2) Shearer (2000). Es gibts aber auch neuere und spezifische Prozesse, die in bestimmten Bereichen, wie z.B. im Feld der Zeitreihenprognosen Hyndman (2018), angewendet werden. F√ºr den Anfang begn√ºgen wir uns jedoch mit dem etabliertesten und verbreitesten Prozess.",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Data Science, Statisik, und Machine Learning</span>"
    ]
  },
  {
    "objectID": "dataexploratory/data_science.html#ssec-dataexploratory-data_science-project",
    "href": "dataexploratory/data_science.html#ssec-dataexploratory-data_science-project",
    "title": "1¬† Data Science, Statisik, und Machine Learning",
    "section": "",
    "text": "Figure¬†1.2: CRISP-DM Prozessdiagramm Wikimedia Commons contributors (retrieved 2025)\n\n\n\n\n1.1.1 CRISP-DM: Ein systematischer Ansatz f√ºr datenbezogene Projekte\nDer CRISP-DM-Prozess stellt ein generisches Rahmenwerk dar, das es erm√∂glicht, datengetriebene Projekte von der Problemdefinition bis hin zur operativen Anwendung zu strukturieren. Der Prozess ist in sechs zentrale Phasen unterteilt:\n\nBusiness Understanding\nZiel ist es, die gesch√§ftlichen Anforderungen und Problemstellungen klar zu definieren. Diese Phase umfasst Fragestellungen wie: Was m√∂chten wir mit den Daten l√∂sen? oder Welche Ergebnisse und Metriken sind entscheidend f√ºr den Erfolg?\nData Understanding\nHier wird das vorliegende Datenmaterial genauer untersucht, einschlie√ülich seiner Struktur, St√∂rfaktoren und potenzieller Verzerrungen. Eine erste Erkundung der Daten kann entscheidend sein, um Hypothesen zu entwickeln.\nDatenaufbereitung\nIn dieser Phase werden die Rohdaten bereinigt und in ein Format gebracht, das f√ºr die Analyse geeignet ist (vgl. Chapter 2). Aktivit√§ten umfassen das Entfernen fehlender Werte, Transformation von Variablen und die Auswahl relevanter Features.\nModellierung\nAufbau eines Modells zur Beantwortung der Kernfrage. Hierbei kann z. B. ein Regressionsmodell f√ºr Prognosen oder ein Klassifikationsmodell bei Entscheidungsproblematiken im Vordergrund stehen.\nEvaluierung\n√úberpr√ºfung, ob das Modell tats√§chlich valide und praktisch anwendbar ist. Kernfragen sind: Passt das Modell zu unseren Zielen? und Sind die Ergebnisse sinnvoll und umsetzbar?\nDeployment (Inbetriebnahme)\nDas Modell wird implementiert, um tats√§chliche Entscheidungen oder Vorhersagen zu unterst√ºtzen. Dies k√∂nnte z. B. bedeuten, ein automatisiertes System zu schaffen, das regelm√§√üig aktualisierte Prognosen liefert.\n\nTipp: Obwohl der Prozess linear erscheint, sind R√ºckspr√ºnge oft unvermeidlich, z. B. wenn das Modell nicht ausreichend Performanz liefert oder die Anforderungen sich √§ndern. Auch werden wir mehr √ºber der Daten und die Prozesse (Schritte 2 und 3) lernen, je mehr wir uns mit den Daten besch√§ftigen.\n\n\n1.1.2 Data Science als People Business?\nEs ist wichtig zu erkennen, dass Daten allein nur einen Ausschnitt der Realit√§t darstellen und ohne Kontext wenig n√ºtzen. Ein Kernelement ist daher, sich ausreichendes Dom√§nenwissen anzueignen, um die Semantik der zugrunde liegenden Daten zu interpretieren. H√§ufig kann es dabei hilfreich sein, Daten aus angrenzenden Kontexten hinzuzuziehen und den Dialog mit Expert:innen oder Personen mit Erfahrungswissen zu suchen.\nAls Standardvorgehen f√ºr viele datenbasierte Projekte empfiehlt Hyndman (2018) in seinem einflussreichen Werk zur Zeitreihenanalyse, diese Prinzipien auch auf Prognosen anzuwenden, √§hnliches gibt aber f√ºr alle Probblem.\n\n\n\n\n\n\nBeispiel:\n\n\n\nWenn wir historische Verkaufsdaten eines Gesch√§fts analysieren, um zuk√ºnftige Trends zu prognostizieren, sollten wir sowohl mit der sp√§teren Nutzer:in des Forecasts (z.B. Produktionsplaner:in f√ºr das Business Understanding), als auch mit den Erzeugen der Daten (z.B. Sales-Abteilung f√ºr das Data Understanding) sprechen.\n\nDieses bestimmt, wie unsere Modellierung aussehen soll.\n\nWie weit in die Zukunft m√ºssen wir vorhersagen (Prognosehorizont)?\nWelche Aufl√∂sung ben√∂tigt unsere Prognose (z.B. tagescharf oder w√∂chtentlich)?\nIn welche Systeme (z.B. Dashboards) muss die Prognose sp√§ter deployed werden?\nWie soll das Modell evaluiert werden (z.B. ist es wichtiger an keinem Tag gro√üe Ausrei√üer zu haben oder die kumulativen Absatzzahlen √ºber das Jahr hinweg genau zu treffen?)\n\nDurch Gespr√§che mit weiteren Stakeholdern ergibt sich zudem Business & Data Understanding\n\nGibt es saisonale Effekte (z.B. Insekten-Schutz-Produkte)\nGibt es systematische Fehler in der Datenaufzeichntung (End-of-Year-Effecs)\nGab es Systemumstellungen in der Datenerfassung oder andere Externe Br√ºche (z.B. Markteintritte)\n\n\nDie Herausforderung ist hierbei jedoch, dass Prognosen fehleranf√§llig sind. Ein pl√∂tzlicher Markteinbruch oder ein externes Ereignis, wie ein sozio√∂konomischer Schock, k√∂nnte die Vorhersagen unbrauchbar machen. Evtl. geh√∂rt zum Business Understanding auch die Grenzen einer datanbasierenden L√∂sung zu verstehen.\n\n\n\n\n1.1.3 Ein Beispiel-Datenset: loan50\nIm Folgenden nutzen wir das loan50-Datenset, das aus dem Lehrbuch von Diez, Barr, and Cetinkaya-Rundel (2019) stammt und zur Erkundung solcher Fragestellungen dient.\nDas loan50-Datenset enth√§lt Informationen zu 50 vergebenen Krediten, die √ºber die Lending Club Plattform vermittelt wurden. Diese Plattform erm√∂glicht es Einzelpersonen, untereinander Kredite zu vergeben. Wie in vielen Finanzanwendungen sind jedoch nicht alle Kreditnehmer:innen gleich:\n\nKandidat:innen mit hoher R√ºckzahlungssicherheit werden bevorzugt behandelt und erhalten in der Regel Kredite mit niedrigeren Zinss√§tzen.\nRisikoreichere Antragsteller:innen k√∂nnten hingegen keine Angebote erhalten oder hohe Zinss√§tze ablehnen.\n\n\n\n\n\n\n\nWarning\n\n\n\nAchtung: Dieses Datenset enth√§lt nur tats√§chlich vergebene Kredite und repr√§sentiert daher nur eine Teilmenge aller m√∂glichen Kreditanfragen. Diese Einschr√§nkung kann dazu f√ºhren, dass wir relevante Informationen √ºber nicht vergebene Kreditantr√§ge nicht betrachten. Solche Probeme bezeichnet man gemeinhin als Bias (Verzerrung).\n\n\nEinige der verf√ºgbaren Variablen:\nDie unten aufgelisteten Variablen beschreiben die Eigenschaften des umfassenderen loans_full_schema-Datensatzes, wovon eine Teilmenge im loan50-Datenset enthalten ist.\n\n\n\nVariable\nBeschreibung\n\n\n\n\nemp_title\nBerufsbezeichnung\n\n\nemp_length\nAnzahl der Jahre im Beruf (aufgerundet, Werte √ºber 10 Jahre werden als 10 dargestellt)\n\n\nstate\nUS-Bundesstaat (zweistellige Abk√ºrzung)\n\n\nhome_ownership\nWohnsituation der Bewerber:innen (z. B. Eigentum, gemietet)\n\n\nannual_income\nJ√§hrliches Einkommen\n\n\nverified_income\nArt der Verifikation des Einkommens\n\n\ndebt_to_income\nSchulden-Einkommens-Verh√§ltnis\n\n\ngrade\nBewertung des Kredits, wobei A die h√∂chste Stufe ist\n\n\n‚Ä¶\nWeitere Variablen finden Sie in der vollst√§ndigen Beschreibung: loan50 - OpenIntro Dataset\n\n\n\n\n\n\n\n\n\nBusiness und Data Understanding\n\n\n\n\nWelche Fragestellungen k√∂nnten mit diesem Datensatz beantwortet werden?\n\nBeispielsweise: Gibt es einen Zusammenhang zwischen dem Schulden-Einkommens-Verh√§ltnis und der Kreditbewilligung?\n\nWas m√ºsste noch bekannt sein, um die Daten besser zu verstehen?\n\nWelche spezifischen Regeln wurden aufgestellt, um Kredite zu vergeben oder abzulehnen?\nWo bestehen potenzielle Verzerrungen (z. B. durch die fehlenden Daten zu abgelehnten Antr√§gen)?\n\n\n\n\n\n\n\n\nDiez, David M., Christopher D. Barr, and Mine Cetinkaya-Rundel. 2019. OpenIntro Statistics. 4th ed. Boston, MA: OpenIntro. \\url{https://www.openintro.org/stat/textbook.php?stat_book=os}.\n\n\nHyndman, RJ. 2018. Forecasting: Principles and Practice. OTexts.\n\n\nKozyrkov, Cassie. 2018. ‚ÄúWhat on Earth Is Data Science?‚Äù medium.com. \\url{https://kozyrkov.medium.com/what-on-earth-is-data-science-eb1237d8cb37}.\n\n\nShearer, Colin. 2000. ‚ÄúThe CRISP-DM Model: The New Blueprint for Data Mining.‚Äù Journal of Data Warehousing 5 (4): 13‚Äì22.\n\n\nWikimedia Commons contributors. retrieved 2025. ‚ÄúCRISP-DM Process Diagram.‚Äù https://upload.wikimedia.org/wikipedia/commons/b/b9/CRISP-DM_Process_Diagram.png.",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Data Science, Statisik, und Machine Learning</span>"
    ]
  },
  {
    "objectID": "dataexploratory/data_sets.html",
    "href": "dataexploratory/data_sets.html",
    "title": "2¬† Data Sets",
    "section": "",
    "text": "2.1 Tidy-Data\nEin sauber aufbereiteter Datensatz ist eine grundlegende Voraussetzung f√ºr jede datenbasierte Analyse und ist im CRISP-DM Teil der Datenaufbereitung. Wir starten zun√§chst mit dem Konzept von Tidy Data Section 2.1, welches sich mit der sauberen Strukturierung von Daten befasst. Anschlie√üend werden wir uns mit den Typen von Variablen Section 2.2 befassen, die unter anderem den Ausschlag gibt, welche verschiedenen Visualisierungen Section 2.3 sinnvoll sind, um sich ein Data Understanding zu erarbeiten. Zum Schluss werden wir uns mit den verschiedenen Ma√üen f√ºr Variablen Section 2.4 auseinandersetzen, mit welchen man Datens√§tze beschreiben kann.\nWenn wir mit Computern automatisiert arbeiten m√∂chten, ist neben der Semantik der Daten auch deren Syntax essenziell. Das bedeutet, dass die Daten in einer Struktur vorliegen m√ºssen, die ihre Semantik sinnvoll abbildet.\nEin weitverbreiteter Standard, der in diesem Zusammenhang h√§ufig genutzt wird, sind die von Wickham (2014) beschriebenen Tidy Data Conventions. Dieses Datenformat ist de facto eine Grundlage f√ºr viele Softwarepakete wie pandas, statsmodels, sklearn, tensorflow und andere Werkzeuge im Bereich Datenanalyse und des maschinelles Lernen.",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data Sets</span>"
    ]
  },
  {
    "objectID": "dataexploratory/data_sets.html#sec-dataexploratory-data_tidy-data",
    "href": "dataexploratory/data_sets.html#sec-dataexploratory-data_tidy-data",
    "title": "2¬† Data Sets",
    "section": "",
    "text": "Hinweis: Datenbanknormalisierung\n\n\n\n\n\nIm Grunde handelt es sich bei diesem Format um ein Prinzip, das auch in der Datenbanknormalisierung nach Codd verfolgt wird. Ihnen wird dieses Konzept in relationalen Datenbanken (SQL) erneut begegnen.\n\n\n\n\n2.1.1 Was bedeutet ‚ÄúTidy Data‚Äù?\nTidy Data folgt drei Grundprinzipien:\n\n\n\n\n\n\nGrundprinzipien von Tidy Data\n\n\n\n\nJede Zeile repr√§sentiert eine Beobachtung (bzw. eine Einheit).\nJede Spalte repr√§sentiert eine Variable (bzw. ein Attribut).\nJede Zelle enth√§lt genau einen pr√§zisen Wert (einen primitiven Datentyp wie int, float, str oder bool ‚Äì keine Listen, Tupel oder geschachtelten Objekte).\n\n\n\nEin Beispiel f√ºr nicht-Tidy-Daten k√∂nnte eine Spalte enthalten, in der mehrere Werte in einer Liste zusammengefasst sind. Solche Daten sind schwerer zu verarbeiten und unflexibler beim Einsatz in Analysetools.\nTidy Data hilft uns bei der Datenbereinigung und Datenanalyse. Es erleichtert die Automatisierung und Standardisierung von Prozessen und reduziert die Wahrscheinlichkeit von Fehlern.\n\nKompatibilit√§t: Viele Python-Bibliotheken wie pandas, statsmodels oder seaborn setzen voraus, dass die verwendeten Daten im Tidy-Format vorliegen.\nAutomatisierung: Tidy-Daten erleichtern Standardoperationen wie Filtern, Gruppieren und Pivotieren erheblich.\nFehlerpr√§vention: Unstrukturierte oder verschachtelte Datenstrukturen sind fehleranf√§llig und schwer zu debuggen.\n\n\n\n\n\n\n\nDaten in das Tidy-Format transformieren\n\n\n\nEs gibt viele hilfreiche Funktionen und Methoden in pandas, um Daten zu ‚Äútidy-fizieren‚Äù. Ein Beispiel ist die Verwendung der Methoden stack, unstack und melt. Diese helfen dabei, Daten umzustrukturieren und in die gew√ºnschte lange (viele Zeilen) oder weite (viele Spalten) Form zu bringen. Ein hilfreicher Artikel hierzu ist Reshape with Pandas.\nüí° Tipp: Wenn Sie unsicher sind, wie Sie Ihre Daten umorganisieren sollten, k√∂nnen Sie ein Beispiel (z.B. head() eines DataFrames) und die gew√ºnschte Struktur (also Spaltennamen) in ein Large Language Model eingeben. Oft erhalten Sie klare Vorschl√§ge zur Umstrukturierung!\n\n\n\n\n2.1.2 Positive Beispiele f√ºr Tidy Data\nDer folgende Beispielcode zeigt, wie Sie ein CSV-Datei laden und sich mit den ersten Zeilen vertraut machen k√∂nnen. Gl√ºcklicher Weise ist dieser Datensatz bereits im Tidy-Format. Jede Zeile repr√§sentiert eine Beobachtung (Kreditnehmer) und jede Spalte eine Variable (Attribut).\n\nimport pandas as pd\n\n# Lesen der CSV-Datei in einen DataFrame\ndf = pd.read_csv(r\"../_assets/dataexploratory/loan50.csv\")\n# Ausgabe der ersten Zeilen des Datensatzes\nprint(df.head())\n\n  state  emp_length  term homeownership  annual_income verified_income  \\\n0    NJ         3.0    60          rent          59000    Not Verified   \n1    CA        10.0    36          rent          60000    Not Verified   \n2    SC         NaN    36      mortgage          75000        Verified   \n3    CA         0.0    36          rent          75000    Not Verified   \n4    OH         4.0    60      mortgage         254000    Not Verified   \n\n   debt_to_income  total_credit_limit  total_credit_utilized  \\\n0        0.557525               95131                  32894   \n1        1.305683               51929                  78341   \n2        1.056280              301373                  79221   \n3        0.574347               59890                  43076   \n4        0.238150              422619                  60490   \n\n   num_cc_carrying_balance        loan_purpose  loan_amount grade  \\\n0                        8  debt_consolidation        22000     B   \n1                        2         credit_card         6000     B   \n2                       14  debt_consolidation        25000     E   \n3                       10         credit_card         6000     B   \n4                        2    home_improvement        25000     B   \n\n   interest_rate  public_record_bankrupt loan_status  has_second_income  \\\n0          10.90                       0     Current              False   \n1           9.92                       1     Current              False   \n2          26.30                       0     Current              False   \n3           9.92                       0     Current              False   \n4           9.43                       0     Current              False   \n\n   total_income  \n0         59000  \n1         60000  \n2         75000  \n3         75000  \n4        254000  \n\n\n\n\n2.1.3 Negative Beispiele f√ºr Tidy Data\nFolgendes Datenbeispiel zeigt, wie ein Datensatz nicht im Tidy-Format aussieht. Wir sehen die Strombedarfe von verschiedenen Netzgebieten zone_id zu verschiedenen Zeitpunkten. Allerdings ist es ung√ºnstig, dass nicht jede Kombination aus Zone und Zeitpunkt eine eigene Zeile hat. Stattdessen sind die Werte f√ºr alle 24 Stunden in einer eigenen Spalte.\n\nimport pandas as pd\n\ndf = pd.read_csv(r\"../_assets/dataexploratory/GEFCom2012/GEFCOM2012_Data/Load/Load_history.csv\")\nprint(df.head())\n\n   zone_id  year  month  day      h1      h2      h3      h4      h5      h6  \\\n0        1  2004      1    1  16,853  16,450  16,517  16,873  17,064  17,727   \n1        1  2004      1    2  14,155  14,038  14,019  14,489  14,920  16,072   \n2        1  2004      1    3  14,439  14,272  14,109  14,081  14,775  15,491   \n3        1  2004      1    4  11,273  10,415   9,943   9,859   9,881  10,248   \n4        1  2004      1    5  10,750  10,321  10,107  10,065  10,419  12,101   \n\n   ...     h15     h16     h17     h18     h19     h20     h21     h22  \\\n0  ...  13,518  13,138  14,130  16,809  18,150  18,235  17,925  16,904   \n1  ...  16,127  15,448  15,839  17,727  18,895  18,650  18,443  17,580   \n2  ...  13,507  13,414  13,826  15,825  16,996  16,394  15,406  14,278   \n3  ...  14,207  13,614  14,162  16,237  17,430  17,218  16,633  15,238   \n4  ...  13,845  14,350  15,501  17,307  18,786  19,089  19,192  18,416   \n\n      h23     h24  \n0  16,162  14,750  \n1  16,467  15,258  \n2  13,315  12,424  \n3  13,580  11,727  \n4  17,006  16,018  \n\n[5 rows x 28 columns]\n\n\nEine Umwandlung in das Tidy-Format w√ºrde, wie folgt aussehen, wobei wird darauf achten sollten, dass der timestamp als datetime-Objekt und die load als int gespeichert wird:\n\n\n\nzone_id\ntimestamp\nload (kW)\n\n\n\n\n1\n2012-01-01 00:00:00\n1000\n\n\n1\n2012-01-01 01:00:00\n1100",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data Sets</span>"
    ]
  },
  {
    "objectID": "dataexploratory/data_sets.html#sec-dataexploratory-data_variable-types",
    "href": "dataexploratory/data_sets.html#sec-dataexploratory-data_variable-types",
    "title": "2¬† Data Sets",
    "section": "2.2 Messniveaus von Variablen",
    "text": "2.2 Messniveaus von Variablen\nVariablen sind die Bausteine von Daten und repr√§sentieren die Merkmale, die wir messen oder beobachten. Im Kapitel Section 2.2 werden wir uns noch tiefer mit Variablen auseinandersetzen. In der Datenanalyse ist es wichtig, die Art der Variablen zu kennen, da dies beeinflusst, welche Methoden und Visualisierungen f√ºr die Daten geeignet sind. Variablen lassen sich nach ihrem Messniveau klassifizieren, was wiederum die Art der Informationen beschreibt, die sie enthalten. Die bekannteste Klassifikation von Variablen basiert auf den vier Messniveaus von Stanley Smith Stevens: Nominal, Ordinal, Intervall und Ratio (Verh√§ltnis).\n\nNutzen Sie den oben gezeigten Datensatz loan50, um die folgenden Aufgaben f√ºr unterschiedliche Variablenarten zu l√∂sen:\n\n\nSortieren: Wie k√∂nnte man die Werte der Variablen\n\nstate,\ngrade,\nein Beispiel f√ºr ein Intervallniveau (aber kein Ratio),\nannual_income\nsinnvoll in aufsteigender Reihenfolge anordnen?\n\nZentrale Werte bestimmen: Wie l√§sst sich ein zentraler Wert bestimmen, sei es durch den Modus, die Median oder den Mittelwert?\nBeziehungen beschreiben: Welche Aussage k√∂nnte man √ºber die Beziehung zwischen zwei Werten einer Variablen machen?\n\n\n2.2.1 Nominale Variablen\nDefinition: Nominale Variablen kategorisieren Daten ohne eine festgelegte Reihenfolge.\n\ndf = pd.read_csv(r\"../_assets/dataexploratory/loan50.csv\")\nprint(df[\"state\"].head())\n\n0    NJ\n1    CA\n2    SC\n3    CA\n4    OH\nName: state, dtype: object\n\n\n\nprint(df[\"state\"].value_counts().head())\n\nstate\nCA    9\nTX    5\nIL    4\nNJ    3\nMD    3\nName: count, dtype: int64\n\n\n\nprint(df[\"state\"].mode().head())\n\n0    CA\nName: state, dtype: object\n\n\n\nSortieren: Es gibt keine inh√§rente Methode, diese Werte zu sortieren. Dies liegt daran, dass nominale Daten keine Reihenfolge implizieren.\nZentraler Wert: Modus, da dieser Wert am h√§ufigsten vorkommt.\nBeziehungen: Die Beziehung zwischen zwei Werten kann nur beschreiben, ob sie in derselben Kategorie sind oder nicht.\n\n\n\n2.2.2 Ordinale Variablen\nOrdinale Variablen haben eine nat√ºrliche Ordnung, aber der Abstand zwischen den Werten ist nicht zwingend gleichm√§√üig.\n\nprint(df[\"grade\"].head())\n\n0    B\n1    B\n2    E\n3    B\n4    B\nName: grade, dtype: object\n\n\n\nprint(df[\"grade\"].sort_values().head())\n\n49    A\n18    A\n33    A\n36    A\n14    A\nName: grade, dtype: object\n\n\n\nprint(df[\"grade\"].value_counts().head())\n\ngrade\nB    19\nA    15\nD     8\nC     6\nE     2\nName: count, dtype: int64\n\n\n\nprint(df[\"grade\"].mode())\n\n0    B\nName: grade, dtype: object\n\n\n\n# Define the order for the categorical values\ngrade_order = sorted(df[\"grade\"].unique())\n\n# Convert the 'grade' column to a categorical type with the specified order\ndf['grade'] = pd.Categorical(df['grade'], categories=grade_order, ordered=True)\n\n# Convert categorical data to numerical codes\ngrade_codes = df['grade'].cat.codes\n\nprint(grade_codes.median())\n\n1.0\n\n\n\nSortieren: Mit geeigneten Regeln ist es m√∂glich, diese Werte in aufsteigender Reihenfolge zu ordnen: [\"C\", \"B\", \"A\"].\nZentraler Wert:: Der Modus ist geeignet, und der Median zeigt auf, dass 50 % der Werte gleich oder niedriger als \"B\" sind.\nBeziehungen: Zwei Werte lassen sich nach ihrer Position der Reihenfolge vergleichen: h√∂her oder niedriger.\n\n\n\n2.2.3 Intervallskalierte Variablen\nIntervallskalierte Variablen haben geordnete Werte mit gleichm√§√üigen Abst√§nden zwischen ihnen, aber sie besitzen keinen absoluten Nullpunkt. Ein Beispiel ist das j√§hrliche Einkommen.\n\nprint(df[\"annual_income\"].head())\n\nprint(df[\"annual_income\"].mean())\n\n0     59000\n1     60000\n2     75000\n3     75000\n4    254000\nName: annual_income, dtype: int64\n86170.0\n\n\n\nSortieren:: Daten k√∂nnen numerisch in aufsteigender Reihenfolge sortiert werden.\nZentraler Wert:: Der Modus und Median sind geeignete Ma√üe. Der arithmetische Mittelwert berechnet sich als: \\(\\mu = \\frac{1}{n} \\sum x_i\\)\nBeziehungen: Der Abstand (Intervall) zwischen zwei Werten kann quantifiziert werden.\n\n\n\n\n\n\n\nUnterschied zwischen Intervall- und Ratiodaten\n\n\n\nIm Gegensatz zum Ratio-Messniveau besitzen Intervall-Daten keinen absoluten Nullpunkt. Aussagen wie ‚Äúdas Doppelte‚Äù sind daher nicht sinnvoll.\n\n\n\n\n\n\n\n\nTip\n\n\n\nEs ist oft hilfreich, sich das Messniveau einer Variablen vor Beginn der Analyse klar zu machen. Das Messniveau entscheidet auch, welche Visualisierung sinnvoll sind. M√∂gliche Fehleinsch√§tzungen k√∂nnen zu falschen oder unzul√§ssigen Berechnungen f√ºhren, z. B. Mittelwerte bei Nominaldaten. Daten sollten entsprechend ihrem Typ gereinigt und transformiert werden.",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data Sets</span>"
    ]
  },
  {
    "objectID": "dataexploratory/data_sets.html#sec-dataexploratory-data_visualization",
    "href": "dataexploratory/data_sets.html#sec-dataexploratory-data_visualization",
    "title": "2¬† Data Sets",
    "section": "2.3 Visualisierungen",
    "text": "2.3 Visualisierungen\n\n\n\n\n\n\nTip\n\n\n\nEs gibt viele M√∂glichkeiten, Daten zu visualisieren, um Muster und Trends zu erkennen. Zwei weit verbreitete Pakete sind matplotlib und plotly. Im folgenden benutzen wir vorallem seaborn, welches eine Erweiterung von matplotlib ist und speziell f√ºr statistische Visualisierungen entwickelt wurde.\n\n\n\n2.3.1 Histogramme\nEin Histogramm ist eine angen√§herte Darstellung der Verteilung einer intervallskalierten Variable. Es liefert wertvolle Informationen √ºber:\n\nZentralwert: Wo liegen die Daten?\nVarianz: Wie stark streuen die Daten?\nVerteilung: Wie h√§ufig treten bestimmte Werte auf?\n\n(fig:sec-dataexploratory-sets-histogram?) zeigt ein Histogramm des j√§hrlichen Einkommens aus dem Datensatz loan50.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create a histogram of the annual income\nsns.histplot(df[\"annual_income\"], bins=10, stat = 'count')\n\n\n\n\nHistogramm des j√§hrlichen Einkommens\n\n\n\n\n\n2.3.1.1 Konstruktion eines Histogramms\nEin Histogramm wird in wenigen Schritten erstellt. Meinst wird dies bereits f√ºr uns wie in seaborn erledigt, es ist jedoch hilfreich, die Schritte zu kennen, um die Visualisierung besser zu verstehen, da sie manchmal abgewandelt wird.\n\nBinning: Teilen Sie die Werte der beobachteten Variablen \\(x_i\\) in eine Reihe von Intervallen (Bins oder Buckets) auf.\nZ√§hlen: Erfassen Sie, wie viele Werte in jedes Intervall fallen (z. B. 5% der Werte).\nIntervall-Eigenschaften: Die Intervalle der Bins sollten aufeinander folgen, sich nicht √ºberlappen und idealerweise die gleiche Breite haben.\nDarstellung: Die Anzahl der Werte in jedem Intervall wird entlang der y-Achse aufgetragen. F√ºr relative H√§ufigkeiten wird durch die Stichprobengr√∂√üe geteilt.\nWenn die Intervalle gleich breit sind, wird die y-Achse als H√§ufigkeit interpretiert. Wenn die Intervalle unterschiedlich breit sind, wird die y-Achse als Dichte interpretiert. Dazu wird die H√∂he der Balken so skaliert, dass die Fl√§che unter dem Histogramm \\(1\\) ergibt.",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data Sets</span>"
    ]
  },
  {
    "objectID": "dataexploratory/data_sets.html#sec-dataexploratory-data_variable-measures",
    "href": "dataexploratory/data_sets.html#sec-dataexploratory-data_variable-measures",
    "title": "2¬† Data Sets",
    "section": "2.4 Ma√üe f√ºr Variablen",
    "text": "2.4 Ma√üe f√ºr Variablen\nVariablen lassen sich auf verschiedene Weisen beschreiben. Lagema√üe bzw. die zentrale Tendenz gibt an, wo die Daten liegen, w√§hrend die Streuung angibt, wie weit die Daten von diesem Wert entfernt sind. Die Zusammenh√§nge zwischen Variablen k√∂nnen durch Korrelationen und Kovarianzen beschrieben werden.\n\n2.4.1 Lagema√üe\nVariablen k√∂nnen auf verschiedene Weisen beschrieben werden. Beispielweise k√∂nnen Lagema√üe wie der Arithmetischer Mittelwert (Mean), Median oder Modus genutzt werden, um die zentrale Tendenz der Daten zu beschreiben. Welche wir einsetzen, h√§ngt vom Messniveau der Variablen ab.\nBetrachten wir eine mindestens intervall-skalierte Variable \\(x \\in \\mathbb{R}^n\\) aus den Datensatz, so k√∂nnen wir die folgenden Lagema√üe berechnen:\n\ndas maximale Element bzw. der H√∂chstwert: \\[\nx^{max} = \\max_i x_i,\n\\]\n\n\nincome_max = df[\"annual_income\"].max()\nprint(f\"{income_max=}\")\n\nincome_max=np.int64(325000)\n\n\n\nder minimale Wert bzw. das Minimum:\n\n\nincome_min = df[\"annual_income\"].min()\nprint(f\"{income_min=}\")\n\nincome_min=np.int64(28800)\n\n\n\\[\nx^{min} = \\min_i x_i,\n\\] - der arithmetische Mittelwert: \\[\n\\overline{x} = \\frac1n \\sum_{i=1}^n x_i = \\frac{x_1 + x_2 + \\cdots + x_n}{n},\n\\]\n\nincome_mean = df[\"annual_income\"].mean()\nprint(f\"{income_mean=}\")\n\nincome_mean=np.float64(86170.0)\n\n\n\nder Median ist der Wert, der die Daten in zwei gleich gro√üe Teile teilt:\n\n\\[\n\\widetilde{x} = \\begin{cases}\n                x_{(n+1)/2}& n\\quad \\text{odd}\\\\\n                \\frac{x_{n/2} + x_{n/2+1}}{2}& n\\quad \\text{even}\n                \\end{cases},\n\\]\n\nincome_median = df[\"annual_income\"].median()\nprint(f\"{income_median=}\")\n\nincome_median=np.float64(74000.0)\n\n\n\nVerallgemeinert f√ºr \\(p\\in(0,1)\\) ist das p-Quantil \\(\\overline{x}_p\\) der Wert, der die Daten in zwei Teile teilt, wobei \\(p\\) der Anteil der Daten ist, die kleiner oder gleich \\(\\overline{x}_p\\) sind.\n\n\nincome_quartiles = df[\"annual_income\"].quantile([0.25, 0.5, 0.75])\nprint(f\"{income_quartiles=}\")\n\nincome_quartiles=0.25    55750.0\n0.50    74000.0\n0.75    99500.0\nName: annual_income, dtype: float64\n\n\n\\[\n\\overline{x}_p = \\begin{cases}\n                 \\frac12\\left(x_{np} + x_{np+1}\\right) & pn \\in \\mathbb{N}\\\\\n                x_{\\lfloor np+1\\rfloor} & pn \\not\\in \\mathbb{N}\n                \\end{cases}.\n\\]\nEinige Quantile haben spezielle Namen, wie der Median f√ºr \\(p=0.5\\), das untere und obere Quartil f√ºr \\(p=0.25\\) und \\(p=0.75\\) (oder erstes, zweites (Median) und drittes Quartil), respektive.\n\n\n\n\n\n\nCaution\n\n\n\nWie gut lassen sich Arithmetischer Mittelwert, Median und Mode aus dem Histogramm ablesen?\n\n\n\n\n2.4.2 Kumulative Histogramme und Empirische Verteilungsfunktionen\nAls Alternative haben sich kumulative Histogramme, wie in ?fig-sec-dataexploratory-sets-kum-histogram, etabliert, die die kumulative Verteilungsfunktion (Cumulative Density Function / CDF) visualisieren. Diese Funktion gibt an, wie viele Werte kleiner oder gleich einem bestimmten Wert sind. Zur Konstruktion der CDF werden die Daten in aufsteigender Reihenfolge sortiert und die relative H√§ufigkeit der Werte berechnet.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create a histogram of the annual income\nsns.histplot(df[\"annual_income\"], bins=10, stat = 'density', cumulative=True)\n\n\n\n\n\n\n\n\n\n\n2.4.3 Steuungsma√üe\nSteuungsma√üe beschreiben die Streuung der Daten um den zentralen Wert. Beispiele sind die Spannweite, Varianz und die Standardabweichung.\nDie Spannweite ist die Differenz zwischen dem gr√∂√üten und kleinsten Wert: \\[\n\\text{Spannweite} = x^{max} - x^{min}.\n\\] Die Varianz ist ein Ma√ü f√ºr die mittlere quadratische Abweichung der Daten vom Mittelwert. Die Einheit der Varianz ist das Quadrat der Einheit der Daten: \\[\n\\sigma = \\sqrt{\\operatorname{Var}(x)}.\n\\] \\[\n\\operatorname{Var}(x) = \\frac1n \\sum_{i=1}^n (x_i - \\mu)^2.\n\\]\nDie Standardabweichung ist die Quadratwurzel der Varianz. Damit hat sie die gleiche Einheit wie die Daten:\n\\[\n\\sigma = \\sqrt{\\operatorname{Var}(x)}.\n\\]\nIn Python k√∂nnen wir die Varianz und Standardabweichung mit pandas oder numpy berechnen:\n\nprint(f\"Varianz: {df['annual_income'].var()}\")\nprint(f\"Standardabweichung: {df['annual_income'].std()}\")\n\nVarianz: 3313901734.6938777\nStandardabweichung: 57566.49837096119\n\n\n\n\n\n\n\n\nKorrigerte Stichproben-Varianz\n\n\n\nDie Varianz einer Stichprobe ist kein erwartungstreuer Sch√§tzer f√ºr die Varianz der Grundgesamtheit. Die Begriffe werden wir in Section 5.1 noch genauer betrachten. Einfach gesagt, die Varianz einer Stichprobe ist tendenziell kleiner als die Varianz der Grundgesamtheit, da wird beim zuf√§lligen Ziehen wahrscheinlich eher aus der Mitte als von den Extremen ziehen. Die korrigierte Stichproben-Varianz wird durch \\(n-1\\) statt \\(n\\) im Nenner definiert. In pandas wird die korrigierte Stichproben-Varianz als Standard verwendet, die unkorrigierte Varianz kann mit dem Parameter ddof=0 berechnet werden.\n\n\n\n\n2.4.4 Zusammenhangsma√üe\nIn der Statistik beschreiben Zusammenhangsma√üe die Beziehung zwischen zwei Variablen. Beispiele sind die Kovarianz und der Korrelationskoeffizient. Diese geben einen Hinweis darauf, ob und wie stark zwei Variablen zusammenh√§ngen.\n\n2.4.4.1 Korrrelation\nIn der Statistik beschreibt der Begriff Korrelation oder Abh√§ngigkeit jede statistische Beziehung zwischen bivariaten Daten (gepaarte Daten) oder Zufallsvariablen.\nIn unserem Datensatz k√∂nnen wir beispielsweise untersuchen: - emp_length: Anzahl der Jahre im Beruf - annual_income: J√§hrliches Einkommen - debt_to_income: Schulden-Einkommens-Verh√§ltnis\nUm die Daten besser zu verstehen k√∂nnen wir zun√§chst einen Scatterplot ?fig-sec-dataexploratory-sets-scatterplot erstellen:\n\nimport seaborn as sns\n\ndf_reduced = df[[\"emp_length\", \"annual_income\", \"debt_to_income\"]]\n\nsns.pairplot(df_reduced)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiskussion\n\n\n\nWie interpretieren Sie den Zusammenhang zwischen den Variablen emp_length, annual_income und debt_to_income? Was w√ºrde entsprechend Ihres Dom√§nenwissens Sinn ergeben?\n\n\n\n\n2.4.4.2 Kovarianz\nDie Kovarianz ist ein Ma√ü f√ºr die gemeinsame Variabilit√§t zweier Variablen. Sie ist definiert als der Erwartungswert des Produkts der Abweichungen der Zufallsvariablen von ihren Erwartungswerten:\n\\[\n\\operatorname{cov}(x, y) = \\frac1n \\sum_{i=1}^n (x_i - \\overline{x})(y_i - \\overline{y}).\n\\]\nIn Python k√∂nnen wir die Kovarianz-Matrix mit pandas direkt berechnen:\n\ndf_reduced.cov()\n\n\n\n\n\n\n\n\nemp_length\nannual_income\ndebt_to_income\n\n\n\n\nemp_length\n12.393174\n1.924082e+04\n-0.051111\n\n\nannual_income\n19240.824468\n3.313902e+09\n-8584.039227\n\n\ndebt_to_income\n-0.051111\n-8.584039e+03\n0.918269\n\n\n\n\n\n\n\nDie Kovarianz kann Wertebereiche von \\(-\\infty\\) bis \\(+\\infty\\) annehmen und ist nicht normiert. Um die St√§rke der Beziehung zu quantifizieren, verwenden wir den Korrelationskoeffizienten, der leichter zu interpretieren ist.\n\n\n2.4.4.3 Korrelationskoeffizient\nDer Korrelationskoeffizient nach Pearson ist ein Ma√ü f√ºr den linearen Zusammenhang zwischen zwei Variablen. Er ist definiert als das Verh√§ltnis der Kovarianz der beiden Variablen zur Multiplikation ihrer Standardabweichungen:\n\\[\n\\rho_{x,y} = \\operatorname{corr}(x, y) = \\frac{\\operatorname{cov}(x, y)}{\\sigma_x \\sigma_y},\n\\]\nwobei \\(\\sigma_x\\) und \\(\\sigma_y\\) die Standardabweichungen der Variablen sind.\nIn Python k√∂nnen wir den Korrelationskoeffizienten mit numpy berechnen:\n\ndf_reduced.corr()\n\n\n\n\n\n\n\n\nemp_length\nannual_income\ndebt_to_income\n\n\n\n\nemp_length\n1.000000\n0.093156\n-0.014857\n\n\nannual_income\n0.093156\n1.000000\n-0.155610\n\n\ndebt_to_income\n-0.014857\n-0.155610\n1.000000\n\n\n\n\n\n\n\nEin Korrelationskoeffizient von \\(1\\) bedeutet eine perfekte positive Korrelation, \\(-1\\) eine perfekte negative Korrelation und \\(0\\) keine Korrelation. In diesem fall beobachten wir eine leichte negative Korrelation zwischen emp_length und debt_to_income und eine leichte positive Korrelation zwischen emp_length und annual_income. Eine Variable kann auch mit sich selbst perfekt korreliert sein, was zu einem Korrelationskoeffizienten von \\(1\\) f√ºhrt.\n\n\n\n\n\n\nVorsicht\n\n\n\nDer Korrelationskoeffizient misst nur lineare Zusammenh√§nge. Nicht-lineare Zusammenh√§nge werden nicht erfasst. Es kann auch Zusammenh√§nge geben, die nicht durch den Korrelationskoeffizienten erfasst werden, z.B. wenn die Daten nicht normalverteilt sind. In Figure¬†2.1 sehen wir einige Beispiele in denen definitiv Korrelationen bestehen, die aber nicht durch den Korrelationskoeffizienten erfasst werden.\n\n\n\n\n\n\nFigure¬†2.1: Beispiele Korrelationskoeffizient DATAtab (retrieved 2025)\n\n\n\n\n\n\n\n\n\n\n\nKorrelation vs.¬†Kausalit√§t\n\n\n\nEine hohe Korrelation bedeutet nicht notwendigerweise Kausalit√§t. Es ist wichtig, die Daten und den Kontext zu verstehen, um sinnvolle Schlussfolgerungen zu ziehen. Ansonsten besteht die Gefahr, dass Zusammenh√§nge fehlinterpretiert werden. Ein bekanntes Beispiel ist die Korrelation zwischen der Anzahl der Piraten und der globalen Temperatur, die in Figure¬†2.2 dargestellt ist. In der Wissenschaft begegnet man diesem Problem mit kontrollierten Experimenten.\n\n\n\n\n\n\nFigure¬†2.2: Korrelation Piraten Klima RedAndr and Mikhail Ryazanov (2011)\n\n\n\n\n\n\n\n\n\nDATAtab. retrieved 2025. ‚ÄúKorrelationskoeffizient Tutorial Image.‚Äù https://datatab.de/assets/tutorial/Korrelationskoeffizient.png.\n\n\nRedAndr and Mikhail Ryazanov. 2011. ‚ÄúSatirical diagram illustrating the influence of pirates decreasing on global warming as per Pastafarian beliefs.‚Äù https://commons.wikimedia.org/wiki/File:PiratesVsTemp(en).svg.\n\n\nWickham, Hadley. 2014. ‚ÄúTidy Data.‚Äù Journal of Statistical Software 59: 1‚Äì23.",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data Sets</span>"
    ]
  },
  {
    "objectID": "dataexploratory/tutorial.html",
    "href": "dataexploratory/tutorial.html",
    "title": "Tutorial: Getting Data Tidy and finding Correlation",
    "section": "",
    "text": "Objective\nThe Global Energy Forecasting Competition Hong, Pinson, and Fan (2014) is a data science competition that aims to advance the field of energy forecasting. The competition is held every two years and the data is made available to the public for research purposes. Different teams from around the world participate in the competition and the best models are selected based on their performance.\nIn 2012 one of the goals was to forecast the load of a power system. The data consists of hourly load data for a period of 5 years. The data was not provided in a tidy format and we need to clean it up before we can start working with it. The main objective of the load forecasting competition was to predict an accurate system load for each hour in each of the zones.",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse",
      "Tutorial: Getting Data Tidy and finding Correlation"
    ]
  },
  {
    "objectID": "dataexploratory/tutorial.html#data",
    "href": "dataexploratory/tutorial.html#data",
    "title": "Tutorial: Getting Data Tidy and finding Correlation",
    "section": "Data",
    "text": "Data\nThe data is provided in a zip file that contains different files:\n\nHoliday_List.csv\nLoad_history.csv\ntemperature_history.csv\n\nYour task is to make sense from the data an bring it into a tidy format. Store the data in a pandas DataFrame and save it as a csv file. Also reload it, to make shure it works.",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse",
      "Tutorial: Getting Data Tidy and finding Correlation"
    ]
  },
  {
    "objectID": "dataexploratory/tutorial.html#crisp-dm",
    "href": "dataexploratory/tutorial.html#crisp-dm",
    "title": "Tutorial: Getting Data Tidy and finding Correlation",
    "section": "Crisp-DM",
    "text": "Crisp-DM\nWe start by using the first four steps of the CRISP-DM process to make sense of the data using exploratory data analysis.\n\nBusiness Understanding\n\nWhat is the system load of a power system and why is it important to forecast it?\nWhat are the benefits of accurate load forecasting?\nWhat factors should influence the load of a power system?\n\nData Understanding\nHow many systems are there in the data? What are the features of the data? What is the time period of the data?\nDatenaufbereitung\nWhat is a meaningful structure for the data? What should be colums and what should be rows? How can we bring the data into a tidy format?\nModellierung\nIs there a seasionality in the data? Plot the average load for each hour of the day, day of the week and month of the year in a Boxplot. How do the distributions between the different systems compare? Are there any outliers? Are there any correlations between the load and the temperature? Plot the load against the temperature and compute the correlation coefficient.\n\n\n\n\n\n\n\nTip\n\n\n\nMake sure to store not only the processed, but also the processed data in a csv file. This way you can always go back to the original data and start over if you make a mistake. Also store the preprocessing steps in a separate script, so you can reproduce the results later. It is not uncommon, to recieve new data that needs to be processed in the same way. This is also the time to think of a proper folder structure for your project. Do not forget all the things you learned in the software design courses. You can also use modules like cookiecutter to create a project structure like this:\n‚îú‚îÄ‚îÄ LICENSE            &lt;- Open-source license if one is chosen\n‚îú‚îÄ‚îÄ Makefile           &lt;- Makefile with convenience commands like `make data` or `make train`\n‚îú‚îÄ‚îÄ README.md          &lt;- The top-level README for developers using this project.\n‚îú‚îÄ‚îÄ data\n‚îÇ   ‚îú‚îÄ‚îÄ external       &lt;- Data from third party sources.\n‚îÇ   ‚îú‚îÄ‚îÄ interim        &lt;- Intermediate data that has been transformed.\n‚îÇ   ‚îú‚îÄ‚îÄ processed      &lt;- The final, canonical data sets for modeling.\n‚îÇ   ‚îî‚îÄ‚îÄ raw            &lt;- The original, immutable data dump.\n‚îÇ\n‚îú‚îÄ‚îÄ docs               &lt;- A default mkdocs project; see www.mkdocs.org for details\n‚îÇ\n‚îú‚îÄ‚îÄ models             &lt;- Trained and serialized models, model predictions, or model summaries\n‚îÇ\n‚îú‚îÄ‚îÄ notebooks          &lt;- Jupyter notebooks. Naming convention is a number (for ordering),\n‚îÇ                         the creator's initials, and a short `-` delimited description, e.g.\n‚îÇ                         `1.0-jqp-initial-data-exploration`.\n...\n\n\n\n\n\n\nHong, Tao, Pierre Pinson, and Shu Fan. 2014. ‚ÄúGlobal Energy Forecasting Competition 2012.‚Äù International Journal of Forecasting 30 (2): 357‚Äì63. https://doi.org/https://doi.org/10.1016/j.ijforecast.2013.07.001.",
    "crumbs": [
      "Umgang mit Daten und Explorative Analyse",
      "Tutorial: Getting Data Tidy and finding Correlation"
    ]
  },
  {
    "objectID": "statistics/index.html",
    "href": "statistics/index.html",
    "title": "Statistik",
    "section": "",
    "text": "‚ÄúIch bin Ingenieur und gewohnt, in Wahrscheinlichkeiten zu denken, nach der Mathematik der Vernunft.‚Äù - Frisch (1957)\n\nDie klassische Statistik ist ein Teilbereich der Mathematik, der sich mit der Sammlung, Analyse, Interpretation, Pr√§sentation und Modellierung von Daten besch√§ftigt. Ein gro√üen Teil der Statistik ist die Wahrscheinlichkeitstheorie, die sich mit dem Verst√§ndnis von Zufallsereignissen befasst.\nIn Kapitel 3¬† Stichproben und Zufallsvariablen besch√§ftigen wir uns zun√∂chst damit, wie Daten erhoben werden und wie man die Wahrscheinlichkeiten von Ereignissen berechnet. In zweiten Kapitel ?sec-statistics-distributions betrachten wir typische Verteilungen, die geeigenet sind zuf√§llige Ereignisse zu beschreiben.\n?sec-statistics-estimates\n?sec-statistics-hypothesis\n\n\n\n\nFrisch, Max. 1957. Homo Faber. Ein Bericht. Frankfurt am Main: Suhrkamp.",
    "crumbs": [
      "Statistik"
    ]
  },
  {
    "objectID": "statistics/sampling.html",
    "href": "statistics/sampling.html",
    "title": "3¬† Stichproben und Zufallsvariablen",
    "section": "",
    "text": "3.1 Stichprobenziehung aus einer Grundgesamtheit\nIn diesem Abschnitt {#sec-statistics-sampling} behandeln wir Stichproben und Zufallsvariablen.\nEine Stichprobe umfasst \\(n\\) Beobachtungen aus einer Grundgesamtheit, der Menge \\(N\\) aller m√∂glichen Beobachtungen. Sie ist eine Teilmenge der Grundgesamtheit und sollte idealerweise R√ºckschl√ºsse auf diese erm√∂glichen.\nDie Grundgesamtheit (population) ist die Gesamtheit aller untersuchbaren Beobachtungen, die Stichprobe (sample) eine Teilmenge davon. Eine repr√§sentative Stichprobe erlaubt Verallgemeinerungen. Da die vollst√§ndige Datenerhebung der Grundgesamtheit oft zu aufwendig und kostspielig ist, ziehen wir R√ºckschl√ºsse aus Stichproben (siehe Figure Figure¬†3.1). Dies gelingt am besten mit einer gro√üen, zuf√§llig ausgew√§hlten Stichprobe.\nFigure¬†3.1: Visualisierung der Stichprobenziehung aus einer Grundgesamtheit.\nAllerdings kommt es hier zu einen Unterschied zwischen der Sichtweise der klassischen Statistik und dem Ansatz den den viele Data Scientists verfolgen. In der klassischen Statistik wird die Stichprobe so gew√§hlt, dass sie repr√§sentativ f√ºr die Grundgesamtheit ist.\nAls Data Scientist hingegen, sind wir oft an den Daten interessiert, die uns zur Verf√ºgung stehen. Wir haben keine M√∂glichkeit, die Grundgesamtheit zu beeinflussen. Wir m√ºssen also mit den Daten arbeiten, die wir haben und uns dabei bewusst sein, dass wir einem Sampling-Bias unterliegen.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Stichproben und Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "statistics/sampling.html#sec-sampling-population",
    "href": "statistics/sampling.html#sec-sampling-population",
    "title": "3¬† Stichproben und Zufallsvariablen",
    "section": "",
    "text": "Note\n\n\n\nWenn wir die Leistungsf√§higkeit in Mathematik unter Studierenden auswerten wollen, dann sollten wir unsere Stichprobe nicht nur im Studiengang Mechatronik nachfragen.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWenn wir die die Lebensdauer eines Werkzeugs auf einer 5-Achs-Fr√§smaschinene prognostizieren wollen, k√∂nnen wir die Modelle nicht zwischen Betrieben vergleichen, die die Maschine regelm√§√üig warten und solchen, die das nicht tun.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Stichproben und Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "statistics/sampling.html#sec-classical-vs-datascience",
    "href": "statistics/sampling.html#sec-classical-vs-datascience",
    "title": "3¬† Stichproben und Zufallsvariablen",
    "section": "3.2 Unterschied zwischen klassischer Statistik und Data Science",
    "text": "3.2 Unterschied zwischen klassischer Statistik und Data Science\nDie klassische Statistik w√§hlt Stichproben so, dass sie die Grundgesamtheit repr√§sentieren, w√§hrend Data Scientists oft mit verf√ºgbaren Daten arbeiten und keinen Einfluss auf die Grundgesamtheit haben. Dies f√ºhrt zu einem m√∂glichen Sampling-Bias.\n\n\n\n\n\n\nNote\n\n\n\nZur Bewertung der Mathematikleistung von Studierenden w√§re eine Stichprobe nur aus Mechatronik nicht repr√§sentativ.\n\n\n\n\n\n\n\n\nNote\n\n\n\nBei der Prognose der Werkzeuglebensdauer einer 5-Achs-Fr√§smaschine sind Vergleiche zwischen gewarteten und ungewarteten Maschinen verzerrt.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Stichproben und Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "statistics/sampling.html#sec-grouping-data",
    "href": "statistics/sampling.html#sec-grouping-data",
    "title": "3¬† Stichproben und Zufallsvariablen",
    "section": "3.3 Gruppieren von Daten",
    "text": "3.3 Gruppieren von Daten\nEin bewusster Bias kann bei der Datenauswahl entstehen. Fragen wir z. B. nur Personen mit Hypothek (mortgage) nach ihrem Einkommen und nicht Mieter (rent), ist die Stichprobe nicht repr√§sentativ (siehe Figure Figure¬†3.2).\n\nimport pandas as pd\nimport seaborn as sns\n\ndf = pd.read_csv(\"../_assets/dataexploratory/loan50.csv\")\nsns.histplot(data=df, x=\"annual_income\", bins=30, hue=\"homeownership\")\n\n\n\n\n\n\n\nFigure¬†3.2: Verteilung des j√§hrlichen Einkommens nach Wohneigentum.\n\n\n\n\n\n\ndf.groupby(\"homeownership\")[\"annual_income\"].mean()\n\nhomeownership\nmortgage    99807.692308\nown         67666.666667\nrent        71928.571429\nName: annual_income, dtype: float64",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Stichproben und Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "statistics/sampling.html#sec-data-analysis",
    "href": "statistics/sampling.html#sec-data-analysis",
    "title": "3¬† Stichproben und Zufallsvariablen",
    "section": "3.4 Analyse der Daten",
    "text": "3.4 Analyse der Daten\n\n\n\n\n\n\nNote\n\n\n\nInwiefern entsprechen die Daten den Erwartungen?\n\n\nIm Beispiel haben wir eine ordinal skalierte Variable (homeownership) und eine metrisch skalierte Variable (annual_income). Wir untersuchen ihren Zusammenhang. Korrelation und Kausalit√§t, wie in der letzten Einheit besprochen, sind hier nicht anwendbar, da homeownership ordinal ist.\n\n3.4.1 Boxplot\nEin Boxplot eignet sich zum Vergleich ordinaler und metrischer Variablen. Er zeigt die Verteilung der metrischen Variable (annual_income) f√ºr die Auspr√§gungen der ordinalen Variable (homeownership). Die Box umfasst Median sowie erstes und drittes Quartil, die Whisker die Datenreichweite (1,5-fache Interquartilsdistanz ab den Quartilen), und Punkte markieren Ausrei√üer (siehe Figure Figure¬†3.3).\n\nimport pandas as pd\nimport seaborn as sns\n\ndf = pd.read_csv(\"../_assets/dataexploratory/loan50.csv\")\nsns.boxplot(data=df, x=\"homeownership\", y=\"annual_income\")\nsns.stripplot(data=df, x=\"homeownership\", y=\"annual_income\", color=\"black\", size=3, alpha=0.5)\n\n\n\n\n\n\n\nFigure¬†3.3: Boxplot des j√§hrlichen Einkommens nach Wohneigentum.\n\n\n\n\n\nDer Boxplot ist einfach zu erstellen und zu interpretieren, jedoch bei stark unterschiedlichen Verteilungen wenig aussagekr√§ftig. Hier k√∂nnen Daten transformationen oder alternative Visualisierungen helfen.\n\n\n\n\n\n\nTip\n\n\n\nEine moderne Alternative zum Boxplot ist der Violinplot. Er zeigt die Verteilung als gesch√§tzte Wahrscheinlichkeitsdichte (bisher f√ºr uns ein gegl√§ttetes Histogramm) und ist informativer, da er die Datenverteilung detaillierter darstellt (siehe Figure ?fig-violin-plot). \n\n\n\n\n3.4.2 Experimente\nIn der Statistik unterscheiden wir Beobachtungsstudien, bei denen Daten ohne Eingriff beobachtet werden, von Experimenten, bei denen Daten manipuliert werden, um Effekte zu pr√ºfen. Experimente sind aufwendiger und teurer, erm√∂glichen aber die Untersuchung von Kausalzusammenh√§ngen.\n\n\n\n\n\n\nNote\n\n\n\nBeobachtungsstudien k√∂nnen longitudinal sein, z. B. die Mathematikleistung von Studierenden √ºber die Zeit, oder Querschnittsstudien, z. B. die Leistung nach Studieng√§ngen zu einem Zeitpunkt.\n\n\n\n\n\n\n\n\nNote\n\n\n\nUm die Wirkung von Studieng√§ngen auf die Mathematikleistung zu pr√ºfen, k√∂nnten wir ein Experiment durchf√ºhren: Studierende zuf√§llig Studieng√§ngen zuweisen und ihre Leistung messen. Dies erfordert Zufallsauswahl und -zuweisung (ethische Bedenken beachten). Eine Kontrollgruppe ohne Studiengang schlie√üt externe Einfl√ºsse (z. B. Alter) aus. Eine Messung vor dem Studium ist bei zuf√§lliger Zuweisung entbehrlich.\n\n\nExperimente sind der Goldstandard f√ºr Kausalit√§t. Eine unabh√§ngige Variable (z. B. Studiengang) wird manipuliert, andere Variablen konstant gehalten oder durch gro√üe Stichproben ausgeglichen. Findet sich eine Korrelation zur abh√§ngigen Variable (z. B. Mathematikleistung), ist Kausalit√§t plausibel. Im Data Science wird oft pragmatisch mit vorhandenen Daten gearbeitet ‚Äì schneller, aber weniger zuverl√§ssig.\n\n3.4.2.1 Beispiel: Ist ein W√ºrfel gezinkt?\nStellen wir uns vor, eine Kollegin besteht auf ihrem eigenen W√ºrfel. Ist er gezinkt? Eine Beobachtungsstudie k√∂nnte die Augenzahlen mit einem fairen W√ºrfel (Kontrollgruppe) vergleichen. Wir werfen beide W√ºrfel 1000-mal und pr√ºfen die Verteilung (siehe Figure Figure¬†3.4). Auff√§llige Abweichungen machen misstrauisch.\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\nfair_dice_rolls = np.random.randint(1, 7, 1000)\nmanipulated_dice_rolls = np.random.choice([1, 2, 3, 4, 5, 6], 1000, p=[1.9/12, 1.9/12, 1.9/12, 1.9/12, 1.9/12, 2.5/12])\n\nsns.histplot(fair_dice_rolls, bins=6, discrete=True, color=\"lightblue\", alpha=0.2, label=\"Fairer W√ºrfel\")\nsns.histplot(manipulated_dice_rolls, bins=6, discrete=True, color=\"red\", alpha=0.2, label=\"Manipulierter W√ºrfel\")\nplt.legend()\n\n\n\n\n\n\n\nFigure¬†3.4: Verteilung der W√ºrfelergebnisse: fairer vs.¬†manipulierter W√ºrfel.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Stichproben und Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "statistics/sampling.html#sec-variable-perspectives",
    "href": "statistics/sampling.html#sec-variable-perspectives",
    "title": "3¬† Stichproben und Zufallsvariablen",
    "section": "3.5 Blickpunkte auf Variablen",
    "text": "3.5 Blickpunkte auf Variablen\nWir haben Variablen (Spalten in tidy data) untersucht und betrachten sie aus verschiedenen Perspektiven.\n\n3.5.1 Skalenniveaus\nSkalenniveaus klassifizieren Variablen in nominal, ordinal, metrisch und verh√§ltnisskaliert. Sie bestimmen, welche statistischen Methoden zur Analyse geeignet sind.\n\n\n3.5.2 Im Kontext von Experimenten\nIn Experimenten und Beobachtungsstudien unterscheiden wir unabh√§ngige (Einflussgr√∂√üe) und abh√§ngige (gemessene Effekte) Variablen. Bezeichnungen variieren je nach Fachgebiet (siehe Table (tab-variable-terms?)). Sp√§ter erkennen wir, dass mehrere unabh√§ngige und abh√§ngige Variablen m√∂glich sind, doch zun√§chst bleiben wir bei Singular.\n\n\n\nAnwendungsfeld\nUnabh√§ngige Variable\nAbh√§ngige Variable\n\n\n\n\nStatistik\nExplanatory Variable\nResponse Variable\n\n\nMachine Learning\nFeatures\nTarget\n\n\nExperimente\nTreatment\nOutcome\n\n\nPsychologie\nIndependent Variable\nDependent Variable\n\n\nForecasts\nPredictor\nPredicted Variable\n\n\n√ñkonometrie\nExplanatory Variable\nDependent Variable\n\n\nInformatik\nInput\nOutput\n\n\nProgramming\nArgument\nReturn Value\n\n\nProgramming\nX\ny",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Stichproben und Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "statistics/sampling.html#sec-probability",
    "href": "statistics/sampling.html#sec-probability",
    "title": "3¬† Stichproben und Zufallsvariablen",
    "section": "3.6 Wahrscheinlichkeitsrechnung",
    "text": "3.6 Wahrscheinlichkeitsrechnung\nEine weitere Perspektive sind Prozesse hinter Beobachtungen: deterministisch (gleiches Ergebnis bei gleichen Bedingungen) oder zuf√§llig (unterschiedliche Ergebnisse trotz gleicher Bedingungen). Zuf√§llige Prozesse werden durch Wahrscheinlichkeiten beschrieben.\n\n\n\n\n\n\nImportant\n\n\n\nOb das Universum deterministisch oder zuf√§llig ist, spielt keine Rolle. Zuf√§lligkeit bedeutet hier, dass Ergebnisse nicht a priori vorhersagbar sind ‚Äì sei es durch echte Zufallsprozesse (z. B. W√ºrfeln) oder unvollst√§ndige Modellierung (z. B. fehlende Variablen).\n\n\n\n3.6.1 Zufallsvariablen\nEine Zufallsvariable nimmt zuf√§llig Werte an ‚Äì diskret (bestimmte Werte, z. B. W√ºrfelaugenzahl: 1‚Äì6) oder kontinuierlich (Werte in einem Intervall, z. B. Temperatur). Die Augenzahl eines W√ºrfels ist eine diskrete Zufallsvariable; jeder Wurf ist eine Realisierung (siehe Figure Figure¬†3.5).\n\nimport numpy as np\nimport seaborn as sns\n\nnp.random.seed(42)\ndice_rolls = np.random.randint(1, 7, 1000)\nsns.histplot(dice_rolls, bins=6, discrete=True)\n\n\n\n\n\n\n\nFigure¬†3.5: Verteilung der W√ºrfelergebnisse bei 1000 W√ºrfen.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Stichproben und Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "statistics/sampling.html#sec-random-variables-coin",
    "href": "statistics/sampling.html#sec-random-variables-coin",
    "title": "3¬† Stichproben und Zufallsvariablen",
    "section": "3.7 Zufallsvariablen und M√ºnzwurf",
    "text": "3.7 Zufallsvariablen und M√ºnzwurf\n√Ñhnlich wie beim W√ºrfel ist beim M√ºnzwurf die Zufallsvariable die Seite, die oben liegt (Kopf oder Zahl). F√ºr numerische Analysen wandeln wir diese kategorialen Werte in 0 (Zahl) und 1 (Kopf) um, wodurch der Ereignisraum {Kopf, Zahl} zu {0, 1} wird.\n\n3.7.1 Begriffe der Wahrscheinlichkeit\nWir definieren: - Zufallsexperiment/-prozess: Ein Prozess mit unvorhersagbarem Ergebnis, z. B. M√ºnzwurf, W√ºrfeln oder Kartenziehen. - Ereignisraum (sample space): Alle m√∂glichen Ergebnisse eines Zufallsexperiments, z. B. {Kopf, Zahl} beim M√ºnzwurf. - Zufallsvariable: Eine Funktion, die jedem Ergebnis eine Zahl zuordnet, z. B. 1 f√ºr Kopf und 0 f√ºr Zahl.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Stichproben und Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "statistics/sampling.html#sec-frequentist-probability",
    "href": "statistics/sampling.html#sec-frequentist-probability",
    "title": "3¬† Stichproben und Zufallsvariablen",
    "section": "3.8 Frequentistische Wahrscheinlichkeit",
    "text": "3.8 Frequentistische Wahrscheinlichkeit\n\n\n\n\n\n\nImportant\n\n\n\nDie Wahrscheinlichkeit eines Ergebnisses ist der Anteil, wie oft es bei unendlich vielen Wiederholungen eines Zufallsprozesses eintritt. Sie liegt zwischen 0 und 1 und kann als Prozentsatz (0‚Äì100 %) angegeben werden.\n\n\nIn Figure Figure¬†3.5 trat die Augenzahl 2 etwa 167-mal in 1000 W√ºrfen auf, was einer Wahrscheinlichkeit von ca. 1/6 (0,167) entspricht ‚Äì ebenso f√ºr die anderen Augenzahlen. Bei endlichen Beobachtungen ist dies eine Sch√§tzung; die exakte Wahrscheinlichkeit gilt nur f√ºr unendlich viele Versuche (siehe Figure Figure¬†3.6).\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nnumber_of_rolls = 1000\nnp.random.seed(10)\ndice_rolls = np.random.randint(1, 7, number_of_rolls)\n\nproportion_ones = np.cumsum(dice_rolls == 1) / np.arange(1, number_of_rolls + 1)\n\nsns.lineplot(x=np.arange(1, number_of_rolls + 1), y=proportion_ones)\nsns.lineplot(x=[1, number_of_rolls + 1], y=[1/6, 1/6], color=\"red\", linestyle=\"--\")\nplt.xlabel(\"$n$ - Anzahl der W√ºrfe\")\nplt.ylabel(\"$\\hat{p}_n$ - Anteil der 1en\")\n\n&lt;&gt;:14: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n&lt;&gt;:14: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n/tmp/ipykernel_3137/965827469.py:14: SyntaxWarning:\n\ninvalid escape sequence '\\h'\n\n\n\n\n\n\n\n\nText(0, 0.5, '$\\\\hat{p}_n$ - Anteil der 1en')\n\n\n(a) Anteil der W√ºrfelergebnisse ‚Äò1‚Äô √ºber die Anzahl der W√ºrfe.\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure¬†3.6\n\n\n\n\n\n3.8.1 Konvergenz und Sch√§tzungen\nFigure Figure¬†3.6 zeigt den Anteil \\(\\hat{p}_n\\) der Augenzahl 1 bei jedem Schritt \\(n\\) einer Simulation. Er konvergiert gegen die Wahrscheinlichkeit 1/6 (ca. 0,167). Der beobachtete Anteil \\(\\hat{p}_n\\) sch√§tzt die Wahrscheinlichkeit \\(p\\) und wird mit mehr Beobachtungen genauer; \\(p\\) ist der Grenzwert.\n\n\n\n\n\n\nImportant\n\n\n\nIn der Statistik arbeiten wir oft mit Sch√§tzungen, da unendlich viele Beobachtungen fehlen. Die Unsicherheit der Sch√§tzungen muss ber√ºcksichtigt werden. Sch√§tzungen kennzeichnen wir mit \\(\\hat{p}\\).\n\n\n\n\n\n\n\n\nImportant\n\n\n\nGesetz der gro√üen Zahlen: Mit steigender Beobachtungszahl konvergiert der Anteil eines Ergebnisses gegen dessen Wahrscheinlichkeit.\n\n\n\n\n3.8.2 Wahrscheinlichkeitsnotation\nF√ºr verschiedene Ergebnisse schreiben wir \\(P(X = x)\\) als Wahrscheinlichkeit, dass die Zufallsvariable \\(X\\) (z. B. M√ºnzwurf) den Wert \\(x\\) annimmt.\n\nF√ºr einen fairen M√ºnzwurf:\n\\(P(X = \\text{Kopf}) = 0.5\\), \\(P(X = \\text{Zahl}) = 0.5\\)\noder: \\(P(X = 1) = 0.5\\), \\(P(X = 0) = 0.5\\).\n\nF√ºr einen fairen W√ºrfelwurf:\n\\(P(X = 1) = P(X = 2) = P(X = 3) = P(X = 4) = P(X = 5) = P(X = 6) = 1/6\\).\n\nDie Summe der Wahrscheinlichkeiten aller Ergebnisse eines Zufallsexperiments ist stets 1, da mindestens ein Ergebnis eintritt. Dies entspricht der Fl√§che unter einem normalisierten Histogramm oder einer Dichtefunktion.\n\n\n3.8.3 Disjunkte Ereignisse\nZwei Ereignisse \\(A\\) und \\(B\\) sind disjunkt (sich ausschlie√üend), wenn sie nicht gleichzeitig eintreten k√∂nnen. Beispiel: Bei einem W√ºrfelwurf sind ‚ÄûAugenzahl 1‚Äú und ‚ÄûAugenzahl 2‚Äú disjunkt. Die Wahrscheinlichkeit, dass eines von beiden eintritt, ist \\(P(A \\cup B)\\) (logisches ‚Äûoder‚Äú, ‚ÄûA vereinigt B‚Äú):\n\\[P(X=1 \\cup X=2) = P(X=1) + P(X=2) = \\frac{1}{6} + \\frac{1}{6} = \\frac{1}{3}.\\]\n\n\n\n\n\n\nImportant\n\n\n\nAdditionsregel: F√ºr sich ausschlie√üende Ereignisse \\(A\\) und \\(B\\) gilt:\n\\[P(A \\cup B) = P(A) + P(B).\\]\nF√ºr mehrere disjunkte Ereignisse \\(A_1, \\ldots, A_n\\):\n\\[P(A_1 \\cup A_2 \\cup \\ldots \\cup A_n) = P(A_1) + P(A_2) + \\ldots + P(A_n).\\]\n\n\n\n3.8.3.1 Beispiel: Kreditnehmer-Datensatz\nIm Datensatz aus Kapitel 2 beschreibt homeownership, ob ein Kreditnehmer mietet, eine Hypothek hat oder Eigent√ºmer ist. Von 50 Kreditnehmern (siehe Code-Ausgabe) sind die Verteilungen: Miete (21), Hypothek (26), Eigentum (3).\n\nimport pandas as pd\n\ndf = pd.read_csv(\"../_assets/dataexploratory/loan50.csv\")\nprint(f\"Anzahl Beobachtungen: {df['homeownership'].shape[0]}\")\nprint(df[\"homeownership\"].value_counts())\n\nAnzahl Beobachtungen: 50\nhomeownership\nmortgage    26\nrent        21\nown          3\nName: count, dtype: int64\n\n\n\nSind Miete, Hypothek und Eigentum disjunkt?\nBestimmen Sie den Anteil der Kredite mit Hypothek und Eigentum separat.\nNutzen Sie die Additionsregel f√ºr disjunkte Ereignisse, um die Wahrscheinlichkeit zu berechnen, dass ein zuf√§llig ausgew√§hlter Kreditnehmer eine Hypothek hat oder Eigent√ºmer ist.\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nJa, die Kategorien sind disjunkt, da ein Kreditnehmer nur eine davon haben kann.\nAnteil Hypothek: \\(\\frac{26}{50}\\), Anteil Eigentum: \\(\\frac{3}{50}\\).\nWahrscheinlichkeit (Hypothek oder Eigentum): \\(\\frac{26}{50} + \\frac{3}{50} = \\frac{29}{50}\\). Dies entspricht der Wahrscheinlichkeit, nicht zu mieten.\n\n\n\n\n\n\n\n3.8.4 Das Komplement eines Ereignisses\nDas Komplement eines Ereignisses \\(A\\), bezeichnet als \\(A^c\\) oder \\(\\bar{A}\\), ist das Nicht-Eintreten von \\(A\\). Es umfasst alle Ergebnisse au√üer \\(A\\) und ist zu \\(A\\) disjunkt. Die Wahrscheinlichkeit des Komplements ist:\n\\[\nP(A^c) = 1 - P(A)\n\\]\nBeispiel: Die Wahrscheinlichkeit, dass ein W√ºrfel nicht 1 zeigt:\n\\[\n1 - P(X \\neq 1) =1-P(X=1) = 1 - \\frac{1}{6} = \\frac{5}{6}.\n\\]\nDie Summe \\(P(A) + P(A^c) = 1\\) gilt stets, da entweder \\(A\\) oder \\(A^c\\) eintritt.\n\n\n3.8.5 Nicht-disjunkte Ereignisse\nNicht-disjunkte Ereignisse k√∂nnen √ºberlappen. Beispiel: Bei einem Kartenspiel (siehe Figure Figure¬†3.7) interessiert die Wahrscheinlichkeit, eine Bildkarte (Bube, Dame, K√∂nig) oder eine Karo-Karte zu ziehen. Bild und Karo sind nicht disjunkt, da Bildkarten in Karo beide Eigenschaften haben. Einfaches Addieren √ºbersch√§tzt die Wahrscheinlichkeit durch doppelte Z√§hlung.\n\n\n\n\n\n\nFigure¬†3.7: Deck of 52 Cards\n\n\n\nEin Venn-Diagramm (siehe Figure Figure¬†3.8) zeigt: Die Schnittmenge (\\(A \\cap B\\), logisches ‚Äûund‚Äú) sind Karten, die beide Eigenschaften haben; die Vereinigung (\\(A \\cup B\\), logisches ‚Äûoder‚Äú) umfasst alle Karten mit mindestens einer Eigenschaft.\n\n\n\n\n\n\nFigure¬†3.8: Venn Diagramm of Cards\n\n\n\nVon 52 Karten sind 12 Bildkarten, 13 Karo-Karten und 3 sowohl Bild- als auch Karo-Karten. Die Wahrscheinlichkeit f√ºr ‚ÄûBild oder Karo‚Äú ist:\n\\[\nP(\\text{Bild} \\cup \\text{Karo}) = P(\\text{Bild}) + P(\\text{Karo}) - P(\\text{Bild} \\cap \\text{Karo}).\n= \\frac{12}{52} + \\frac{13}{52} - \\frac{3}{52} = \\frac{22}{52}.\n\\]\nHierraus k√∂nnen wir die Additionsregel f√ºr nicht-disjunkte Ereignisse formulieren:\n\n\n\n\n\n\nImportant\n\n\n\nGenerelle Additionsregel: F√ºr nicht-disjunkte Ereignisse \\(A\\) und \\(B\\) gilt:\n\\[\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B).\n\\]\n\n\n\n3.8.5.1 Beispiel: Summe zweier W√ºrfel\nSei \\(A\\) das Ereignis, dass die Summe der Augenzahlen zweier fairer W√ºrfel kleiner als 12 ist.\n\nWas ist das Komplement von \\(A\\)?\nWie gro√ü ist \\(P(A)\\)?\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nDas Komplement \\(A^c\\) ist die Summe \\(\\geq 12\\), also genau 12 (da die maximale Summe 12 betr√§gt).\n\\(P(A)\\) ist die Summe der Wahrscheinlichkeiten f√ºr Summen 2 bis 11.\nM√∂gliche Summen und Kombinationen (siehe Table (tab-dice-sums?)):\n\n\n\n\n\n\n\n\n\\(A = W_1 + W_2\\)\nM√∂gliche Kombinationen\n\n\n\n\n2\n\\((W_1=1 \\cap W_2=1)\\)\n\n\n3\n\\((W_1=1 \\cap W_2=2) \\cup (W_1=2 \\cap W_2=1)\\)\n\n\n4\n\\((W_1=1 \\cap W_2=3) \\cup (W_1=2 \\cap W_2=2) \\cup (W_1=3 \\cap W_2=1)\\)\n\n\n5\n\\((W_1=1 \\cap W_2=4) \\cup (W_1=2 \\cap W_2=3) \\cup (W_1=3 \\cap W_2=2) \\cup (W_1=4 \\cap W_2=1)\\)\n\n\n6\n\\((W_1=1 \\cap W_2=5) \\cup (W_1=2 \\cap W_2=4) \\cup (W_1=3 \\cap W_2=3) \\cup (W_1=4 \\cap W_2=2) \\cup (W_1=5 \\cap W_2=1)\\)\n\n\n7\n\\((W_1=1 \\cap W_2=6) \\cup (W_1=2 \\cap W_2=5) \\cup (W_1=3 \\cap W_2=4) \\cup (W_1=4 \\cap W_2=3) \\cup (W_1=5 \\cap W_2=2) \\cup (W_1=6 \\cap W_2=1)\\)\n\n\n8\n\\((W_1=2 \\cap W_2=6) \\cup (W_1=3 \\cap W_2=5) \\cup (W_1=4 \\cap W_2=4) \\cup (W_1=5 \\cap W_2=3) \\cup (W_1=6 \\cap W_2=2)\\)\n\n\n9\n\\((W_1=3 \\cap W_2=6) \\cup (W_1=4 \\cap W_2=5) \\cup (W_1=5 \\cap W_2=4) \\cup (W_1=6 \\cap W_2=3)\\)\n\n\n10\n\\((W_1=4 \\cap W_2=6) \\cup (W_1=5 \\cap W_2=5) \\cup (W_1=6 \\cap W_2=4)\\)\n\n\n11\n\\((W_1=5 \\cap W_2=6) \\cup (W_1=6 \\cap W_2=5)\\)\n\n\n12\n\\((W_1=6 \\cap W_2=6)\\)\n\n\n\nDie Gesamtzahl der Kombinationen betr√§gt \\(6 \\times 6 = 36\\). F√ºr \\(A^c\\) (Summe = 12) gibt es 1 Fall, also \\(P(A^c) = \\frac{1}{36}\\) und \\(P(A) = 1 - P(A^c) = \\frac{35}{36}\\).\n\n\n\nBei komplexeren Berechnungen hilft eine Monte-Carlo-Simulation: Das Experiment wird mehrfach simuliert, und die Wahrscheinlichkeit ergibt sich aus dem Anteil der Treffer (siehe Code-Ausgabe).\n\nimport numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(np.random.randint(1, 7, (10000, 2)), columns=[\"W1\", \"W2\"])\ndf[\"Sum\"] = df[\"W1\"] + df[\"W2\"]\np = (df[\"Sum\"] &lt; 12).mean()\n\nprint(f\"Die Wahrscheinlichkeit, dass die Summe &lt; 12 ist, betr√§gt {p:.3f}\")\n\nDie Wahrscheinlichkeit, dass die Summe &lt; 12 ist, betr√§gt 0.973\n\n\n\n\n3.8.5.2 Simulation der W√ºrfelsumme (Fortsetzung)\nDie Verteilung der Summen zweier W√ºrfel kann auch grafisch dargestellt werden (siehe Figure Figure¬†3.9). Die Simulation best√§tigt, dass die Wahrscheinlichkeit f√ºr eine Summe &lt; 12 hoch ist.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(np.random.randint(1, 7, (10000, 2)), columns=[\"W1\", \"W2\"])\ndf[\"Sum\"] = df[\"W1\"] + df[\"W2\"]\n\nsns.histplot(df[\"Sum\"], bins=11, discrete=True, stat='density')\nplt.axvline(11, color=\"red\", linestyle=\"--\")\nplt.xlabel(\"Summe der Augenzahlen\")\nplt.ylabel(\"Dichte\")\nplt.show()\n\n\n\n\n\n\n\nFigure¬†3.9: Verteilung der Summen zweier W√ºrfel bei 10.000 W√ºrfen; rote Linie markiert Summe = 11.\n\n\n\n\n\n\n\n\n3.8.6 Wahrscheinlichkeit der W√ºrfelsumme (Fortsetzung)\nFigure Figure¬†3.9 zeigt die Verteilung der Summen zweier W√ºrfel. Die Wahrscheinlichkeit, dass die Summe &lt; 12 ist, betr√§gt ca. 0,97 (Monte-Carlo-Sch√§tzung).\n- 3. Die Wahrscheinlichkeit f√ºr eine Summe \\(\\geq 12\\) ist:\n\\[P(A^c) = 1 - P(A) = 1 - \\frac{35}{36} = \\frac{1}{36} \\approx 0.03.\\]",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Stichproben und Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "statistics/sampling.html#sec-independent-events",
    "href": "statistics/sampling.html#sec-independent-events",
    "title": "3¬† Stichproben und Zufallsvariablen",
    "section": "3.9 Unabh√§ngige Ereignisse",
    "text": "3.9 Unabh√§ngige Ereignisse\nZwei Ereignisse \\(A\\) und \\(B\\) sind unabh√§ngig, wenn das Eintreten des einen das andere nicht beeinflusst (vgl. Korrelation und Kausalit√§t in Chapter 2). Die Wahrscheinlichkeit eines Ereignisses h√§ngt nicht vom anderen ab. Beispiele: M√ºnzwurf und W√ºrfelwurf oder die Ergebnisse zweier W√ºrfel ‚Äì das Ergebnis des ersten W√ºrfels beeinflusst den zweiten nicht.\nDie Wahrscheinlichkeit, dass zwei unabh√§ngige Ereignisse gleichzeitig eintreten, ist:\n\\[P(A \\cap B) = P(A) \\cdot P(B).\\]\n\n\n\n\n\n\nImportant\n\n\n\nMultiplikationsregel f√ºr unabh√§ngige Ereignisse: F√ºr unabh√§ngige Ereignisse \\(A\\) und \\(B\\) gilt:\n\\[P(A \\cap B) = P(A) \\cdot P(B).\\]\nF√ºr mehrere unabh√§ngige Ereignisse \\(A_1, \\ldots, A_n\\):\n\\[P(A_1 \\cap A_2 \\cap \\ldots \\cap A_n) = P(A_1) \\cdot P(A_2) \\cdot \\ldots \\cdot P(A_n).\\]\n\n\n\n3.9.1 Beispiel 1: W√ºrfelsummen\nDa die Ergebnisse zweier W√ºrfel unabh√§ngig sind, k√∂nnen wir die Wahrscheinlichkeiten multiplizieren (siehe Table (tab-dice-sum-probabilities?)).\n\n\n\n\n\n\n\n\n\\(A = W_1 + W_2\\)\nM√∂gliche Kombinationen\n\\(P(A)\\)\n\n\n\n\n2\n\\(W_1=1, W_2=1\\)\n\\(P(A=2) = P(W_1=1) \\cdot P(W_2=1) = \\frac{1}{6} \\cdot \\frac{1}{6} = \\frac{1}{36}\\)\n\n\n3\n\\(W_1=1, W_2=2\\), \\(W_1=2, W_2=1\\)\n\\(P(A=3) = P(W_1=1) \\cdot P(W_2=2) + P(W_1=2) \\cdot P(W_2=1) = \\frac{1}{6} \\cdot \\frac{1}{6} + \\frac{1}{6} \\cdot \\frac{1}{6} = \\frac{2}{36}\\)\n\n\n4\n\\(W_1=1, W_2=3\\), \\(W_1=2, W_2=2\\), \\(W_1=3, W_2=1\\)\n\\(P(A=4) = P(W_1=1) \\cdot P(W_2=3) + P(W_1=2) \\cdot P(W_2=2) + P(W_1=3) \\cdot P(W_2=1) = \\frac{1}{6} \\cdot \\frac{1}{6} + \\frac{1}{6} \\cdot \\frac{1}{6} + \\frac{1}{6} \\cdot \\frac{1}{6} = \\frac{3}{36}\\)\n\n\n5\n\\(W_1=1, W_2=4\\), \\(W_1=2, W_2=3\\), \\(W_1=3, W_2=2\\), \\(W_1=4, W_2=1\\)\n\\(P(A=5) = P(W_1=1) \\cdot P(W_2=4) + P(W_1=2) \\cdot P(W_2=3) + P(W_1=3) \\cdot P(W_2=2) + P(W_1=4) \\cdot P(W_2=1) =\n\\frac{4}{36}\\)\n\n\n6\n\\(W_1=1, W_2=5\\), \\(W_1=2, W_2=4\\), \\(W_1=3, W_2=3\\), \\(W_1=4, W_2=2\\), \\(W_1=5, W_2=1\\)\n\\(P(A=6) = P(W_1=1) \\cdot P(W_2=5) + P(W_1=2) \\cdot P(W_2=4) + P(W_1=3) \\cdot P(W_2=3) + P(W_1=4) \\cdot P(W_2=2) + P(W_1=5) \\cdot P(W_2=1) =\n\\frac{5}{36}\\)\n\n\n7\n\\(W_1=1, W_2=6\\), \\(W_1=2, W_2=5\\), \\(W_1=3, W_2=4\\), \\(W_1=4, W_2=3\\), \\(W_1=5, W_2=2\\), \\(W_1=6, W_2=1\\)\n\\(P(A=7) = P(W_1=1) \\cdot P(W_2=6) + P(W_1=2) \\cdot P(W_2=5) + P(W_1=3) \\cdot P(W_2=4) + P(W_1=4) \\cdot P(W_2=3) + P(W_1=5) \\cdot P(W_2=2) + P(W_1=6) \\cdot P(W_2=1) = \\frac{6}{36}\\)\n\n\n8\n\\(W_1=2, W_2=6\\), \\(W_1=3, W_2=5\\), \\(W_1=4, W_2=4\\), \\(W_1=5, W_2=3\\), \\(W_1=6, W_2=2\\)\n\\(P(A=8) = P(W_1=2) \\cdot P(W_2=6) + P(W_1=3) \\cdot P(W_2=5) + P(W_1=4) \\cdot P(W_2=4) + P(W_1=5) \\cdot P(W_2=3) + P(W_1=6) \\cdot P(W_2=2) = \\frac{5}{36}\\)\n\n\n9\n\\(W_1=3, W_2=6\\), \\(W_1=4, W_2=5\\), \\(W_1=5, W_2=4\\), \\(W_1=6, W_2=3\\)\n\\(P(A=9) = P(W_1=3) \\cdot P(W_2=6) + P(W_1=4) \\cdot P(W_2=5) + P(W_1=5) \\cdot P(W_2=4) + P(W_1=6) \\cdot P(W_2=3) = \\frac{4}{36}\\)\n\n\n10\n\\(W_1=4, W_2=6\\), \\(W_1=5, W_2=5\\), \\(W_1=6, W_2=4\\)\n\\(P(A=10) = P(W_1=4) \\cdot P(W_2=6) + P(W_1=5) \\cdot P(W_2=5) + P(W_1=6) \\cdot P(W_2=4) = \\frac{3}{36}\\)\n\n\n11\n\\(W_1=5, W_2=6\\), \\(W_1=6, W_2=5\\)\n\\(P(A=11) = P(W_1=5) \\cdot P(W_2=6) + P(W_1=6) \\cdot P(W_2=5) = \\frac{2}{36}\\)\n\n\n12\n\\(W_1=6, W_2=6\\)\n\\(P(A=12) = P(W_1=6) \\cdot P(W_2=6) = \\frac{1}{36}\\)\n\n\n\n\n\n3.9.2 Beispiel 2: Wahr oder Falsch\nBestimmen Sie, ob die folgenden Aussagen wahr oder falsch sind, und begr√ºnden Sie Ihre Antwort.\n\nWenn eine faire M√ºnze oft geworfen wird und die letzten acht W√ºrfe Kopf waren, ist die Wahrscheinlichkeit, dass der n√§chste Wurf Kopf ist, etwas weniger als 50 %.\nDas Ziehen einer Bildkarte (Bube, Dame, K√∂nig) und das Ziehen einer roten Karte aus einem vollst√§ndigen Kartenspiel sind sich gegenseitig ausschlie√üende Ereignisse.\nDas Ziehen einer Bildkarte und das Ziehen eines Asses aus einem vollst√§ndigen Kartenspiel sind sich gegenseitig ausschlie√üende Ereignisse.\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nFalsch. Bei einer fairen M√ºnze ist \\(P(\\text{Kopf}) = 0.5\\). Vorherige W√ºrfe beeinflussen den n√§chsten nicht, da sie unabh√§ngig sind.\nFalsch. Bildkarten k√∂nnen rot sein (z. B. Karo-Dame). ‚ÄûBildkarte‚Äú und ‚Äûrote Karte‚Äú sind nicht disjunkt.\nWahr. Eine Bildkarte (Bube, Dame, K√∂nig) kann kein Ass sein; die Ereignisse sind disjunkt.\n\n\n\n\n\n\n3.9.3 Bedingte Wahrscheinlichkeit\nBedingte Wahrscheinlichkeit beschreibt die Wahrscheinlichkeit eines Ereignisses \\(A\\), gegeben dass ein anderes Ereignis \\(B\\) eingetreten ist, notiert als \\(P(A | B)\\) (‚Äû\\(A\\) gegeben \\(B\\)‚Äú). Beispiel: Wie wahrscheinlich ist eine Bildkarte, wenn die Karte eine Karo-Karte ist? Aus Figure Figure¬†3.8:\n\\[P(\\text{Bild} | \\text{Karo}) = \\frac{3}{13},\\]\nda von 13 Karo-Karten 3 Bildkarten sind.\nEine Kreuztabelle (contingency table) zeigt die H√§ufigkeiten von Ereigniskombinationen, z. B. im loan50-Datensatz (siehe Table (tab-contingency-loan?)).\n\nimport pandas as pd\n\ndf = pd.read_csv(\"../_assets/dataexploratory/loan50.csv\")\ncontingency_table = pd.crosstab(df['homeownership'], df['has_second_income'])\nprint(contingency_table)\n\nhas_second_income  False  True \nhomeownership                  \nmortgage              20      6\nown                    3      0\nrent                  19      2\n\n\n\n\n3.9.4 Bedingte Wahrscheinlichkeit (Fortsetzung)\nAus der Kreuztabelle (Table (tab-contingency-loan?)) ergibt sich: Von 26 Kreditnehmern mit Hypothek haben 6 ein zweites Einkommen. Die bedingte Wahrscheinlichkeit ist:\n\\[P(\\text{Zweiteinkommen} | \\text{Hypothek}) = \\frac{6}{26}.\\]\n\n\n\n\n\n\nImportant\n\n\n\nDie bedingte Wahrscheinlichkeit \\(P(A|B)\\) ist die Wahrscheinlichkeit von \\(A\\), gegeben dass \\(B\\) eingetreten ist. Sie wird berechnet als:\n\\[P(A|B) = \\frac{P(A \\cap B)}{P(B)}.\\]\nIm Beispiel:\n\\[P(\\text{Zweiteinkommen} | \\text{Hypothek}) = \\frac{P(\\text{Zweiteinkommen} \\cap \\text{Hypothek})}{P(\\text{Hypothek})} = \\frac{\\frac{6}{50}}{\\frac{26}{50}} = \\frac{6}{26}.\\]\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\\(P(A \\cap B)\\) kann hier nicht via Multiplikationsregel berechnet werden, da die Ereignisse nicht unabh√§ngig sind. Stattdessen werden beobachtete H√§ufigkeiten verwendet.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nSumme bedingter Wahrscheinlichkeiten: Sind \\(A_1, \\ldots, A_k\\) alle disjunkten Ergebnisse einer Variable, gilt f√ºr ein Ereignis \\(B\\):\n\\[P(A_1|B) + \\cdots + P(A_k|B) = 1.\\]\nF√ºr ein Ereignis und sein Komplement:\n\\[P(A|B) = 1 - P(A^c|B).\\]\n\n\n\n\n3.9.5 Beispiel: AIDS-Test\n\n\n\n\n\n\nFigure¬†3.10: Meme AIDS Test\n\n\n\nWie hoch ist die Wahrscheinlichkeit, dass eine Person AIDS hat, wenn folgendes bekannt ist:\n\nDie Wahrscheinlichkeit, dass eine Person AIDS hat, betr√§gt 0,1 % (\\(P(\\text{AIDS}) = 0.001\\)).\nBei AIDS betr√§gt die Wahrscheinlichkeit eines positiven Tests 99 % (\\(P(\\text{Positiv} | \\text{AIDS}) = 0.99\\), Sensitivit√§t).\nOhne AIDS betr√§gt die Wahrscheinlichkeit eines positiven Tests 5 % (\\(P(\\text{Positiv} | \\text{Kein AIDS}) = 0.05\\), falsch-positiv, 1 - Spezifit√§t).\n\nEin Baumdiagramm hilft: \\(P(\\text{AIDS}) = 0.001\\), \\(P(\\text{Kein AIDS}) = 0.999\\), \\(P(\\text{Positiv} | \\text{AIDS}) = 0.99\\), \\(P(\\text{Positiv} | \\text{Kein AIDS}) = 0.05\\).\n\n\n\n\n\ngraph LR\n    U[Person] --&gt;|0.001| A[AIDS] \n    U[Person] --&gt;|0.999| D[Kein AIDS]\n    A[AIDS] --&gt;|0.99| B[Positiv]\n    A[AIDS] --&gt;|0.01| C[Negativ]\n    D[Kein AIDS] --&gt;|0.05| E[Positiv]\n    D[Kein AIDS] --&gt;|0.95| F[Negativ]\n\n\n\n\n\n\nPfade mit positivem Test (disjunkt):\n\nOberer Pfad: \\(P(\\text{AIDS} \\cap \\text{Positiv}) = P(\\text{AIDS}) \\cdot P(\\text{Positiv} | \\text{AIDS}) = 0.001 \\cdot 0.99 = 0.00099\\).\nUnterer Pfad: \\(P(\\text{Kein AIDS} \\cap \\text{Positiv}) = P(\\text{Kein AIDS}) \\cdot P(\\text{Positiv} | \\text{Kein AIDS}) = 0.999 \\cdot 0.05 = 0.04995\\).\nGesamt: \\(P(\\text{Positiv}) = 0.00099 + 0.04995 = 0.05094\\).\n\nBedingte Wahrscheinlichkeit:\n\\[\nP(\\text{AIDS} | \\text{Positiv}) = \\frac{P(\\text{AIDS} \\cap \\text{Positiv})}{P(\\text{Positiv})} = \\frac{0.00099}{0.05094} = 0.0194.\n\\]\nTrotz positivem Test ist die Wahrscheinlichkeit f√ºr AIDS gering (ca. 1,94 %). In der Praxis folgen Best√§tigungstests, und die Pr√§valenz in Risikogruppen ist h√∂her als 0,1 %.\n\n\n3.9.6 Satz von Bayes\nIm AIDS-Test-Beispiel kennen wir \\(P(\\text{Positiv} | \\text{AIDS})\\) ‚Äì die Wahrscheinlichkeit eines positiven Tests bei AIDS ‚Äì und m√∂chten die Umkehrung, \\(P(\\text{AIDS} | \\text{Positiv})\\) ‚Äì die Wahrscheinlichkeit von AIDS bei einem positiven Test. Diese Umkehrung nennt sich bedingte Wahrscheinlichkeit in umgekehrter Richtung. Doch wie gelangen wir von der einen zur anderen? Der Satz von Bayes liefert die L√∂sung, indem er bedingte Wahrscheinlichkeiten umkehrt.\n\n3.9.6.1 Motivation und Herleitung\nStellen wir uns vor, wir wollen \\(P(\\text{AIDS} | \\text{Positiv})\\) berechnen. Aus der Definition der bedingten Wahrscheinlichkeit wissen wir:\n\\[P(\\text{AIDS} | \\text{Positiv}) = \\frac{P(\\text{AIDS} \\cap \\text{Positiv})}{P(\\text{Positiv})}.\\]\nGleichzeitig gilt f√ºr die umgekehrte Richtung:\n\\[P(\\text{Positiv} | \\text{AIDS}) = \\frac{P(\\text{Positiv} \\cap \\text{AIDS})}{P(\\text{AIDS})}.\\]\nDa \\(P(\\text{AIDS} \\cap \\text{Positiv}) = P(\\text{Positiv} \\cap \\text{AIDS})\\) (Schnittmengen sind symmetrisch), k√∂nnen wir die zweite Gleichung umstellen:\n\\[P(\\text{AIDS} \\cap \\text{Positiv}) = P(\\text{Positiv} | \\text{AIDS}) \\cdot P(\\text{AIDS}).\\]\nSetzen wir dies in die erste Gleichung ein:\n\\[P(\\text{AIDS} | \\text{Positiv}) = \\frac{P(\\text{Positiv} | \\text{AIDS}) \\cdot P(\\text{AIDS})}{P(\\text{Positiv})}.\\]\nDas ist der Satz von Bayes! Er verbindet die bekannte Bedingung (\\(P(\\text{Positiv} | \\text{AIDS})\\)) mit der gesuchten (\\(P(\\text{AIDS} | \\text{Positiv})\\)), wobei \\(P(\\text{Positiv})\\) die Gesamtwahrscheinlichkeit eines positiven Tests ist.\n\n\n3.9.6.2 Formale Definition\nDer Satz von Bayes lautet allgemein:\n\\[P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)},\\]\nwobei:\n\n\\(P(A|B)\\) die Posterior-Wahrscheinlichkeit ist (z. B. AIDS bei positivem Test),\n\n\\(P(B|A)\\) die Likelihood (z. B. positiver Test bei AIDS),\n\n\\(P(A)\\) der Prior (z. B. Grundwahrscheinlichkeit f√ºr AIDS),\n\n\\(P(B)\\) die Normalisierungskonstante (z. B. Gesamtwahrscheinlichkeit eines positiven Tests).\n\nWir k√∂nnen also unser Vorwissen (Prior) mit neuen Daten (Likelihood) kombinieren, um die Wahrscheinlichkeit f√ºr ein Ereignis zu aktualisieren (Posterior).\n\n\n3.9.6.3 Anwendung\nIm AIDS-Beispiel:\n\n\\(P(\\text{Positiv} | \\text{AIDS}) = 0.99\\),\n\n\\(P(\\text{AIDS}) = 0.001\\),\n\n\\(P(\\text{Positiv}) = P(\\text{Positiv} | \\text{AIDS}) \\cdot P(\\text{AIDS}) + P(\\text{Positiv} | \\text{Kein AIDS}) \\cdot P(\\text{Kein AIDS}) = 0.00099 + 0.04995 = 0.05094\\).\nDamit:\n\\[P(\\text{AIDS} | \\text{Positiv}) = \\frac{0.99 \\cdot 0.001}{0.05094} \\approx 0.01943.\\]\nDer Satz von Bayes ist in Medizin, Wirtschaft und Technik essenziell, um aus bekannten Daten (z. B. Testresultaten) auf Ursachen (z. B. Krankheiten) zu schlie√üen.\n\n\n\n\n\n\n\nTip\n\n\n\nYoutube-Videos:\n\nThree Blue One Brown: Bayes Theorem\nVeritasium: The Bayesian Trap\n\n\n\n\n\n\n\n\n\nAssoziationsanalyse mit A-Priori-Algorithmus\n\n\n\nDie Assoziationsanalyse ist ein Verfahren, um Zusammenh√§nge in Daten zu finden. Ein bekannter Algorithmus ist der A-Priori-Algorithmus, der z.B. in f√ºr Predictive Maintainance oder in der Warenkorbanalyse verwendet wird. Der A-Priori-Algorithmus findet heraus, welche Produkte oft zusammen gekauft werden. Ein Beispiel ist, dass Kunden, die Windeln kaufen, oft auch Bier kaufen.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Stichproben und Zufallsvariablen</span>"
    ]
  },
  {
    "objectID": "statistics/distributions.html",
    "href": "statistics/distributions.html",
    "title": "4¬† Verteilungen",
    "section": "",
    "text": "4.1 Diskrete Verteilungen\nIn diesem Abschnitt untersuchen wir Verteilungen einzelner Variablen. Histogramme und Balkendiagramme veranschaulichen die H√§ufigkeits- oder Wahrscheinlichkeitsverteilung, wie in den folgenden Abbildungen dargestellt. Verteilungen sind die Grundlage f√ºr Simulationen, z. B. in der Monte-Carlo-Methode, wie im Tutorial zur Fahrzeugausfallzeit gezeigt.\nDiskrete Verteilungen beschreiben Zufallsvariablen mit abz√§hlbaren Werten. Wir betrachten die Bernoulli-Verteilung, Binomial-Verteilung und Poisson-Verteilung, die h√§ufig in der Statistik und realen Anwendungen vorkommen.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Verteilungen</span>"
    ]
  },
  {
    "objectID": "statistics/distributions.html#sec-discrete-distributions",
    "href": "statistics/distributions.html#sec-discrete-distributions",
    "title": "4¬† Verteilungen",
    "section": "",
    "text": "4.1.1 Bernoulli-Verteilung\nDie Bernoulli-Verteilung modelliert eine Zufallsvariable mit genau zwei m√∂glichen Ergebnissen, z. B. einen M√ºnzwurf: ‚ÄûKopf‚Äú (\\(X = 1\\)) mit Wahrscheinlichkeit \\(p\\) oder ‚ÄûZahl‚Äú (\\(X = 0\\)) mit Wahrscheinlichkeit \\(1-p\\). Die Wahrscheinlichkeitsfunktion lautet:\n\\[P(X = x) = \\begin{cases}\np & \\text{f√ºr } x = 1, \\\\\n1-p & \\text{f√ºr } x = 0.\n\\end{cases}\\]\nWir schreiben \\(X \\sim \\text{Bernoulli}(p)\\). F√ºr eine faire M√ºnze gilt \\(p = 0.5\\), also \\(X \\sim \\text{Bernoulli}(0.5)\\).\nFigure¬†4.1 zeigt die Verteilung f√ºr eine faire M√ºnze. Diese einfache Verteilung ist die Basis f√ºr komplexere Modelle wie die Binomial-Verteilung und findet Anwendung in Entscheidungen mit Ja/Nein-Ergebnissen.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\np = 0.5\nx = [0, 1]  # Einfache Liste statt NumPy-Array f√ºr Klarheit\nP_X = [1-p, p]\n\nplt.bar(x, P_X, color='skyblue', width=0.6)\nplt.xlabel('Ergebnis (0 = Zahl, 1 = Kopf)')\nplt.ylabel('Wahrscheinlichkeit')\nplt.title('Bernoulli-Verteilung ($p = 0.5$)')\nplt.ylim(0, 1)\nplt.xticks(x)\nplt.grid(axis='y', alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\nFigure¬†4.1: Wahrscheinlichkeitsverteilung der Bernoulli-Verteilung f√ºr eine faire M√ºnze (p = 0.5).\n\n\n\n\n\nIn Figure¬†4.1 wird jeder m√∂glichen Realisierung der Zufallsvariablen \\(X\\) die theoretische Wahrscheinlichkeit zugeordnet. Im Gegensatz zu Histogrammen, die empirische H√§ufigkeiten darstellen, zeigt diese Abbildung die Wahrscheinlichkeitsverteilung direkt.\n\n4.1.1.1 Erwartungswert und Varianz der Bernoulli-Verteilung\n\n\n\n\n\n\nImportant\n\n\n\nDer Erwartungswert \\(E(X)\\) einer Verteilung beschreibt ihre zentrale Tendenz ‚Äì den durchschnittlichen Wert der Zufallsvariablen. F√ºr eine diskrete Zufallsvariable \\(X\\) gilt:\n\\[E(X) = \\sum_{x} x \\cdot P(X = x).\\]\nBei der Bernoulli-Verteilung (\\(X \\sim \\text{Bernoulli}(p)\\)) ist:\n\\[E(X) = 0 \\cdot (1-p) + 1 \\cdot p = p.\\]\nF√ºr eine faire M√ºnze (\\(p = 0.5\\)) ist \\(E(X) = 0.5\\).\n\n\n\n\n\n\n\n\nImportant\n\n\n\nDie Varianz \\(\\text{Var}(X)\\) misst die Streuung der Verteilung ‚Äì wie stark die Werte um den Erwartungswert schwanken. Sie wird berechnet als:\n\\[\\text{Var}(X) = \\sum_{x} (x - E(X))^2 \\cdot P(X = x).\\]\nF√ºr die Bernoulli-Verteilung ergibt sich:\n\\[\\text{Var}(X) = (0 - p)^2 \\cdot (1-p) + (1 - p)^2 \\cdot p = p \\cdot (1-p).\\]\nBei \\(p = 0.5\\) ist \\(\\text{Var}(X) = 0.5 \\cdot 0.5 = 0.25\\).\n\n\n\n\n\n4.1.2 Binomialverteilung\nStellen wir uns vor, wir wiederholen einen M√ºnzwurf \\(n\\)-mal unabh√§ngig mit der Wahrscheinlichkeit \\(p\\) f√ºr ‚ÄûKopf‚Äú. Die Zufallsvariable \\(Y\\) z√§hlt die Anzahl der Kopfw√ºrfe und folgt einer Binomialverteilung: \\(Y \\sim \\text{Bin}(n, p)\\). Die Wahrscheinlichkeitsfunktion lautet:\n\\[P(Y = k) = \\binom{n}{k} p^k (1-p)^{n-k},\\]\nwobei \\(\\binom{n}{k}\\) die Anzahl der M√∂glichkeiten ist, \\(k\\) Erfolge in \\(n\\) Versuchen zu erzielen.\nFigure¬†4.2 zeigt die Verteilung f√ºr \\(n = 10\\) und \\(p = 0.5\\). Die Binomialverteilung ist eine Erweiterung der Bernoulli-Verteilung und n√ºtzlich, um H√§ufigkeiten in wiederholten Experimenten zu modellieren.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import binom\n\nn = 10\np = 0.5\ny = np.arange(0, n + 1)\nP_Y = binom.pmf(y, n, p)\n\nplt.bar(y, P_Y, color='skyblue', width=0.8)\nplt.xlabel('Anzahl der Kopfw√ºrfe ($Y$)')\nplt.ylabel('Wahrscheinlichkeit')\nplt.title(f'Binomialverteilung ($n = {n}$, $p = {p}$)')\nplt.xticks(y)\nplt.grid(axis='y', alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\nFigure¬†4.2: Wahrscheinlichkeitsverteilung der Binomialverteilung f√ºr eine faire M√ºnze (n = 10, p = 0.5).\n\n\n\n\n\nStellen wir uns vor, wir wiederholen einen M√ºnzwurf \\(n\\)-mal. Die W√ºrfe sind unabh√§ngig und identisch verteilt (i.i.d.), d.¬†h., jeder Wurf hat die gleiche Wahrscheinlichkeit \\(p\\) f√ºr ‚ÄûKopf‚Äú und ist unbeeinflusst von den anderen. Die Zufallsvariable \\(Y\\) z√§hlt die Anzahl der Kopfw√ºrfe und folgt einer Binomialverteilung: \\(Y \\sim \\text{Bin}(n, p)\\). Die Wahrscheinlichkeitsfunktion ist:\n\\[P(Y = y) = \\binom{n}{y} \\cdot p^y \\cdot (1-p)^{n-y},\\]\nwobei \\(\\binom{n}{y}\\) der Binomialkoeffizient ist. Dieser gibt an, auf wie viele Arten \\(y\\) Erfolge in \\(n\\) Versuchen auftreten k√∂nnen und wird definiert als:\n\\[\\binom{n}{y} = \\frac{n!}{y! \\cdot (n - y)!}.\\]\n\n4.1.2.1 Beispiel: M√ºnzwurf\nF√ºr \\(n = 10\\) W√ºrfe mit einer fairen M√ºnze (\\(p = 0.5\\)) ist \\(Y \\sim \\text{Bin}(10, 0.5)\\). Dies wurde in Figure¬†4.2 gezeigt.\n\n\n4.1.2.2 Beispiel: Gewinnlose\nPassen wir die Werte an: Beim Ziehen von 10 Losen, wobei die Wahrscheinlichkeit f√ºr ein Gewinnlos \\(p = 0.1\\) betr√§gt, gilt \\(Y \\sim \\text{Bin}(10, 0.1)\\). Die Zufallsvariable \\(Y\\) z√§hlt die Anzahl der Gewinnlose. Figure¬†4.3 zeigt diese Verteilung. Solche Modelle sind n√ºtzlich, um Erfolge in wiederholten, unabh√§ngigen Versuchen zu analysieren ‚Äì √§hnlich wie im Tutorial zur Fahrzeugausfallzeit, wo Verteilungen f√ºr Komponenten genutzt werden.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import binom\n\nn = 10\np = 0.1\ny = np.arange(0, n + 1)\nP_Y = binom.pmf(y, n, p)\n\nplt.bar(y, P_Y, color='skyblue', width=0.8)\nplt.xlabel('Anzahl der Gewinnlose ($Y$)')\nplt.ylabel('Wahrscheinlichkeit')\nplt.title(f'Binomialverteilung ($n = {n}$, $p = {p}$)')\nplt.xticks(y)\nplt.grid(axis='y', alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\nFigure¬†4.3: Wahrscheinlichkeitsverteilung der Binomialverteilung f√ºr das Ziehen von Gewinnlosen (n = 10, p = 0.1).\n\n\n\n\n\n\n\n4.1.2.3 Erwartungswert und Varianz der Binomialverteilung\nDer Erwartungswert \\(E(Y)\\) der Binomialverteilung (\\(Y \\sim \\text{Bin}(n, p)\\)) gibt die erwartete Anzahl der Erfolge in \\(n\\) Versuchen an:\n\\[E(Y) = n \\cdot p.\\]\nF√ºr \\(n = 10\\) und \\(p = 0.1\\) (Gewinnlose) ist \\(E(Y) = 10 \\cdot 0.1 = 1\\).\nDie Varianz \\(\\text{Var}(Y)\\) misst die Streuung um diesen Erwartungswert:\n\\[\\text{Var}(Y) = n \\cdot p \\cdot (1-p).\\]\nIm Beispiel ist \\(\\text{Var}(Y) = 10 \\cdot 0.1 \\cdot 0.9 = 0.9\\). Diese Werte helfen, die Verteilung zu charakterisieren, z. B. in Simulationen wie im Tutorial.\n\n\n\n4.1.3 Diskrete Gleichverteilung\nDie diskrete Gleichverteilung (auch Uniformverteilung) beschreibt eine Zufallsvariable, bei der alle m√∂glichen Werte innerhalb eines Intervalls die gleiche Wahrscheinlichkeit haben. Im Gegensatz zum W√ºrfelbeispiel, wo die Werte bei 1 beginnen, kann das Intervall beliebig gew√§hlt werden, z. B. \\(x = a, a+1, \\ldots, b\\), wobei \\(a\\) und \\(b\\) ganze Zahlen sind und \\(a \\leq b\\). Die Anzahl der Werte ist \\(n = b - a + 1\\), und die Wahrscheinlichkeitsfunktion lautet:\n\\[P(X = x) = \\frac{1}{b - a + 1}, \\quad \\text{f√ºr } x = a, a+1, \\ldots, b.\\]\nWir notieren \\(X \\sim \\text{DU}(a, b)\\) f√ºr die diskrete Gleichverteilung auf \\([a, b]\\).\nFigure¬†4.4 zeigt die Verteilung f√ºr \\(a = 3\\) und \\(b = 8\\) (z. B. ein modifizierter W√ºrfel). Dieses flexible Intervall ist n√ºtzlich, um spezifische Szenarien zu modellieren, z. B. in Simulationen mit nicht standardisierten Bereichen.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nn = 6\nx = np.arange(1, n + 1)\nP_X = np.full(n, 1/n)  # Array mit konstanter Wahrscheinlichkeit 1/n\n\nplt.bar(x, P_X, color='skyblue', width=0.8)\nplt.xlabel('W√ºrfelzahl ($X$)')\nplt.ylabel('Wahrscheinlichkeit')\nplt.title(f'Diskrete Gleichverteilung ($n = {n}$)')\nplt.xticks(x)\nplt.grid(axis='y', alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\nFigure¬†4.4: Wahrscheinlichkeitsverteilung der diskreten Gleichverteilung f√ºr einen W√ºrfel (n = 6).\n\n\n\n\n\nIn der Abbildung (fig:sec-dataexploratory-distributions-discrete-uniform-distribution?) wird die Wahrscheinlichkeitsverteilung der diskreten Gleichverteilung f√ºr das W√ºrfeln eines W√ºrfels dargestellt. Die Wahrscheinlichkeitsverteilung zeigt, dass alle m√∂glichen Werte der Zufallsvariablen die gleiche Wahrscheinlichkeit haben.\n\n4.1.3.1 Erwartungswert und Varianz der diskreten Gleichverteilung\nDer Erwartungswert \\(E(X)\\) einer diskreten Gleichverteilung auf dem Intervall \\([a, b]\\) liegt in der Mitte des Intervalls:\n\\[E(X) = \\frac{a + b}{2} = \\sum_{x} x \\cdot P(X = x).$.\\]\nF√ºr \\(a = 3\\) und \\(b = 8\\) ergibt sich:\n\\[E(X) = \\frac{3 + 8}{2} = 5.5.\\]\nDie Varianz \\(\\text{Var}(X)\\) beschreibt die Streuung der Verteilung:\n\\[\\text{Var}(X) = \\frac{(b - a + 1)^2 - 1}{12} = \\sum_{x} (x - E(X))^2 \\cdot P(X = x)..\\]\nBei \\(a = 3\\) und \\(b = 8\\) ist \\(n = b - a + 1 = 6\\), also:\n\\[\\text{Var}(X) = \\frac{6^2 - 1}{12} = \\frac{36 - 1}{12} = \\frac{35}{12} \\approx 2.9167.\\]\nDiese Formeln gelten f√ºr jedes Intervall \\([a, b]\\), wobei \\(a\\) und \\(b\\) ganze Zahlen sind und \\(a \\leq b\\).",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Verteilungen</span>"
    ]
  },
  {
    "objectID": "statistics/distributions.html#sec-continuous-distributions",
    "href": "statistics/distributions.html#sec-continuous-distributions",
    "title": "4¬† Verteilungen",
    "section": "4.2 Stetige Verteilungen",
    "text": "4.2 Stetige Verteilungen\nStetige Verteilungen modellieren Zufallsvariablen, die kontinuierliche Werte annehmen k√∂nnen, z. B. Zeit oder L√§nge. Im Gegensatz zu diskreten Verteilungen gibt es hier unendlich viele m√∂gliche Werte innerhalb eines Intervalls. Wir betrachten die Normalverteilung, Exponentialverteilung und stetige Gleichverteilung, die in Simulationen wie im Tutorial zur Fahrzeugausfallzeit eine Rolle spielen.\n\n4.2.1 Stetige Gleichverteilung\nDie stetige Gleichverteilung beschreibt eine Zufallsvariable, bei der alle Werte in einem Intervall \\([a, b]\\) gleich wahrscheinlich sind. Die Wahrscheinlichkeitsdichtefunktion (Dichte) lautet:\n\\[f(x) = \\begin{cases}\n\\frac{1}{b - a} & \\text{f√ºr } a \\leq x \\leq b, \\\\\n0 & \\text{sonst}.\n\\end{cases}\\]\nWir schreiben \\(X \\sim \\text{U}(a, b)\\). Bei stetigen Verteilungen wird die Wahrscheinlichkeit als Fl√§che unter der Dichte berechnet, wobei die Gesamtfl√§che stets 1 betr√§gt.\nFigure¬†4.5 zeigt die Dichte f√ºr \\(a = 0\\) und \\(b = 1\\). Im Tutorial wird die stetige Gleichverteilung f√ºr die Steuereinheit (\\(X \\sim \\text{U}(4000, 8000)\\)) verwendet, um Ausfallzeiten zu modellieren.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\na = 0\nb = 1\nx = np.linspace(a - 0.5, b + 0.5, 1000)\nf_X = np.where((x &gt;= a) & (x &lt;= b), 1 / (b - a), 0)\n\nplt.plot(x, f_X, color='skyblue', linewidth=2)\nplt.fill_between(x, f_X, alpha=0.2, color='skyblue')  # Fl√§che einf√§rben\nplt.xlabel('Werte der Zufallsvariablen ($X$)')\nplt.ylabel('Wahrscheinlichkeitsdichte')\nplt.title(f'Stetige Gleichverteilung ($a = {a}$, $b = {b}$)')\nplt.grid(alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\nFigure¬†4.5: Wahrscheinlichkeitsdichte der stetigen Gleichverteilung auf dem Intervall [0, 1].\n\n\n\n\n\nBei stetigen Verteilungen ist die Wahrscheinlichkeit eines exakten Wertes 0, da es unendlich viele m√∂gliche Werte gibt. Stattdessen berechnen wir die Wahrscheinlichkeit f√ºr einen Wertebereich als Fl√§che unter der Dichtefunktion:\n\\[P(a \\leq X \\leq b) = \\int_{a}^{b} f(x) \\, dx.\\]\nDer Erwartungswert \\(E(X)\\) einer gleichverteilten Zufallsvariable \\(X \\sim \\text{U}(a, b)\\) ist:\n\\[E(X) = \\frac{a + b}{2}=\\int\\limits_{ - \\infty }^\\infty {x \\cdot f\\left( x \\right)} \\,\\,dx.\\]\nDie Varianz \\(\\text{Var}(X)\\) betr√§gt:\n\\[\\text{Var}(X) = \\frac{(b - a)^2}{12} = \\int\\limits_{ - \\infty }^\\infty {{{\\left( {x - {\\mu _x}} \\right)}^2}} \\cdot f\\left( x \\right)\\,\\,dx.\\]\n\n4.2.1.1 Beispiel: Ausfallwahrscheinlichkeit eines Bauteils\nDer Ausfall eines elektronischen Bauteils folgt einer stetigen Gleichverteilung auf \\([0, 3650]\\) Tagen (\\(X \\sim \\text{U}(0, 3650)\\)). Wie hoch ist die Wahrscheinlichkeit, dass es innerhalb der ersten 1000 Tage ausf√§llt?\nDie Dichtefunktion ist:\n\\[f(x) = \\begin{cases}\n\\frac{1}{3650 - 0} = \\frac{1}{3650} & \\text{f√ºr } 0 \\leq x \\leq 3650, \\\\\n0 & \\text{sonst}.\n\\end{cases}\\]\nDie Wahrscheinlichkeit ergibt sich durch Integration:\n\\[P(0 \\leq X \\leq 1000) = \\int_{0}^{1000} \\frac{1}{3650} \\, dx = \\frac{1000}{3650} \\approx 0.274.\\]\nDas Bauteil hat also eine 27,4 % Chance, innerhalb von 1000 Tagen auszusetzen. Dieses Prinzip wird im Tutorial bei der Steuereinheit (\\(X \\sim \\text{U}(4000, 8000)\\)) angewendet.\n\n\n\n\n\n\nL√∂sungsschritte\n\n\n\n\n\n\nDichte: \\(f(x) = \\frac{1}{3650}\\) f√ºr \\(0 \\leq x \\leq 3650\\).\n\nIntegral: \\(P(0 \\leq X \\leq 1000) = \\int_{0}^{1000} \\frac{1}{3650} \\, dx\\).\n\nErgebnis: \\(\\frac{1000}{3650} = 0.274\\).\n\n\n\n\n\n\n\n4.2.2 Normalverteilung\nDie Normalverteilung ist eine stetige Verteilung, die in Statistik und Naturwissenschaften weit verbreitet ist. Ihre Glockenform zeigt eine symmetrische Verteilung der Werte um den Erwartungswert. Die Dichtefunktion lautet:\n\\[f(x) = \\frac{1}{\\sqrt{2\\pi} \\sigma} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right),\\]\nf√ºr \\(-\\infty &lt; x &lt; \\infty\\). Wir schreiben \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), wobei \\(\\mu\\) der Erwartungswert und \\(\\sigma^2\\) die Varianz ist.\nFalls \\(\\mu = 0\\) und \\(\\sigma = 1\\), spricht man von der Standardnormalverteilung (\\(X \\sim \\mathcal{N}(0, 1)\\)), eine spezielle Form mit Erwartungswert 0 und Varianz 1. Figure¬†4.6 zeigt diese Verteilung. Im Tutorial wird die Normalverteilung f√ºr den Sensor (\\(X \\sim \\mathcal{N}(6000, 100^2)\\)) genutzt, um Ausfallzeiten zu modellieren.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nmu = 0\nsigma = 1\nx = np.linspace(-5, 5, 1000)\nf_X = 1 / (np.sqrt(2 * np.pi) * sigma) * np.exp(-((x - mu) ** 2) / (2 * sigma ** 2))\n\nplt.plot(x, f_X, color='skyblue', linewidth=2)\nplt.fill_between(x, f_X, alpha=0.2, color='skyblue')  # Fl√§che einf√§rben\nplt.xlabel('Werte der Zufallsvariablen ($X$)')\nplt.ylabel('Wahrscheinlichkeitsdichte')\nplt.title(f'Standardnormalverteilung ($\\mu = {mu}$, $\\sigma = {sigma}$)')\nplt.grid(alpha=0.3)\nplt.show()\n\n&lt;&gt;:13: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:13: SyntaxWarning:\n\ninvalid escape sequence '\\s'\n\n&lt;&gt;:13: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:13: SyntaxWarning:\n\ninvalid escape sequence '\\s'\n\n/tmp/ipykernel_3173/513147542.py:13: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n/tmp/ipykernel_3173/513147542.py:13: SyntaxWarning:\n\ninvalid escape sequence '\\s'\n\n\n\n\n\n\n\n\n\nFigure¬†4.6: Wahrscheinlichkeitsdichte der Standardnormalverteilung (\\(\\mu = 0\\), \\(\\sigma = 1\\)).\n\n\n\n\n\nFr√ºher, ohne Computer, war die Berechnung der Fl√§che unter der Normalverteilungskurve (\\(P(X \\leq x)\\)) schwierig. Man nutzte \\(Z\\)-Wert-Tabellen, um die Wahrscheinlichkeit f√ºr eine standardnormalverteilte Zufallsvariable \\(Z \\sim \\mathcal{N}(0, 1)\\) nachzuschlagen. Beispiele:\n\n\\(P(Z \\leq 0) = 0.5\\) (50 %).\n\n\\(P(Z \\leq 1) = 0.8413\\) (84,13 %).\n\n\\(P(Z \\leq -1) = 1 - P(Z \\leq 1) = 0.1587\\) (15,87 %).\nUmgekehrt: \\(P(Z \\leq 1.645) = 0.95\\) (95 %). Heute ersetzen Computerprogramme solche Tabellen, wie im Folgenden gezeigt.\n\n\n4.2.2.1 Erwartungswert und Varianz der Normalverteilung\nDer Erwartungswert \\(E(X)\\) einer normalverteilten Zufallsvariable \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) ist:\n\\[E(X) = \\mu = \\int\\limits_{ - \\infty }^\\infty {x \\cdot f\\left( x \\right)} \\,\\,dx.\\]\nDie Varianz \\(\\text{Var}(X)\\) betr√§gt:\n\\[{\\sigma _x}^2 = Var\\left( X \\right) = E{\\left( {X - {\\mu _x}} \\right)^2} = \\int\\limits_{ - \\infty }^\\infty {{{\\left( {x - {\\mu _x}} \\right)}^2}} \\cdot f\\left( x \\right)\\,\\,dx.\\]\nF√ºr die Standardnormalverteilung (\\(X \\sim \\mathcal{N}(0, 1)\\)) gilt \\(E(X) = 0\\) und \\(\\text{Var}(X) = 1\\). Im Tutorial wird dies f√ºr den Sensor (\\(X \\sim \\mathcal{N}(6000, 100^2)\\)) genutzt.\n\n4.2.2.1.1 Standardisierung der Normalverteilung\nViele Zufallsvariablen folgen keiner Standardnormalverteilung, sondern haben andere Werte f√ºr \\(\\mu\\) und \\(\\sigma\\). Um diese auf \\(Z \\sim \\mathcal{N}(0, 1)\\) zu √ºberf√ºhren, wird standardisiert:\n\\[Z = \\frac{X - \\mu}{\\sigma}.\\]\n\\(Z\\) hat dann \\(E(Z) = 0\\) und \\(\\text{Var}(Z) = 1\\), sodass \\(Z\\)-Tabellen oder Software genutzt werden k√∂nnen.\n\n\n\n4.2.2.2 Beispiel: Intelligenz-Quotient (IQ)\nDer IQ ist normalverteilt mit \\(X \\sim \\mathcal{N}(100, 15^2)\\) (\\(\\mu = 100\\), \\(\\sigma = 15\\)). Wie wahrscheinlich ist ein IQ von 130 oder mehr (\\(P(X \\geq 130)\\))?\n\n4.2.2.2.1 Analytische Berechnung\nDie Dichtefunktion ist:\n\\[f(x) = \\frac{1}{\\sqrt{2\\pi} \\cdot 15} \\exp\\left(-\\frac{(x - 100)^2}{2 \\cdot 15^2}\\right).\\]\nDie Wahrscheinlichkeit \\(P(X \\geq 130)\\) ergibt sich als:\n\\[P(X \\geq 130) = 1 - P(X \\leq 130) = 1 - \\int_{-\\infty}^{130} f(x) \\, dx.\\]\nMit Standardisierung:\n\\[Z = \\frac{130 - 100}{15} = 2, \\quad P(X \\leq 130) = P(Z \\leq 2).\\]\nAus Tabellen oder Software: \\(P(Z \\leq 2) \\approx 0.9772\\), also:\n\\[P(X \\geq 130) = 1 - 0.9772 = 0.0228 \\text{ (2,28 %)}.\\]\n\n\n4.2.2.2.2 Berechnung mit Python\nMit scipy.stats.norm k√∂nnen wir die kumulative Verteilungsfunktion (cdf) direkt berechnen:\n\n\n\nimport numpy as np\nfrom scipy.stats import norm\n\nmu = 100\nsigma = 15\nP_X_geq_130 = 1 - norm.cdf(130, loc=mu, scale=sigma)\n\nprint(f'P(X &gt;= 130) = {P_X_geq_130:.4f}')  # Ausgabe: 0.0228\n\nP(X &gt;= 130) = 0.0228\n\n\n\nFigure¬†4.7\n\n\n\n\n\n\n4.2.2.3 Visualisierung der Wahrscheinlichkeiten\nOft interessiert der Anteil einer Population innerhalb von ein oder zwei Standardabweichungen vom Mittelwert. F√ºr eine normalverteilte Zufallsvariable \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) berechnen wir:\n\\[P(\\mu - \\sigma \\leq X \\leq \\mu + \\sigma) = P(X \\leq \\mu + \\sigma) - P(X \\leq \\mu - \\sigma),\\]\nwobei \\(P(X \\leq x)\\) die kumulative Verteilungsfunktion (CDF) ist. F√ºr die Standardnormalverteilung gilt:\n- \\(P(\\mu - \\sigma \\leq X \\leq \\mu + \\sigma) \\approx 0.6826\\) (ca. 68 %),\n- \\(P(\\mu - 2\\sigma \\leq X \\leq \\mu + 2\\sigma) \\approx 0.9544\\) (ca. 95 %).\nIm IQ-Beispiel (\\(X \\sim \\mathcal{N}(100, 15^2)\\)) pr√ºfen wir den Bereich von zwei Standardabweichungen (\\(\\mu - 2\\sigma = 70\\), \\(\\mu + 2\\sigma = 130\\)). Figure¬†4.8 zeigt diese Wahrscheinlichkeit. Solche Berechnungen sind im Tutorial n√ºtzlich, z. B. um Sensor-Ausfallzeiten zu analysieren.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\nmu = 100\nsigma = 15\nx = np.linspace(mu - 4*sigma, mu + 4*sigma, 1000)\nf_X = norm.pdf(x, loc=mu, scale=sigma)\n\n# Wahrscheinlichkeit f√ºr Œº ¬± 2œÉ\nP_X_2_sigma = norm.cdf(mu + 2*sigma, loc=mu, scale=sigma) - norm.cdf(mu - 2*sigma, loc=mu, scale=sigma)\nprint(f'P({mu - 2*sigma} &lt;= X &lt;= {mu + 2*sigma}) = {P_X_2_sigma:.4f}')  # Ausgabe: 0.9545\n\n# Plot\nplt.plot(x, f_X, color='skyblue', linewidth=2)\nplt.fill_between(x, f_X, where=(x &gt;= mu - 2*sigma) & (x &lt;= mu + 2*sigma), color='lightblue', alpha=0.3, label=f'P = {P_X_2_sigma:.4f}')\nplt.axvline(mu - 2*sigma, color='red', linestyle='--', label=f'{mu - 2*sigma}')\nplt.axvline(mu + 2*sigma, color='red', linestyle='--', label=f'{mu + 2*sigma}')\nplt.xlabel('IQ-Wert ($X$)')\nplt.ylabel('Wahrscheinlichkeitsdichte')\nplt.title(f'IQ-Verteilung ($\\mu = {mu}$, $\\sigma = {sigma}$)')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\n&lt;&gt;:21: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:21: SyntaxWarning:\n\ninvalid escape sequence '\\s'\n\n&lt;&gt;:21: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:21: SyntaxWarning:\n\ninvalid escape sequence '\\s'\n\n/tmp/ipykernel_3173/3074097218.py:21: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n/tmp/ipykernel_3173/3074097218.py:21: SyntaxWarning:\n\ninvalid escape sequence '\\s'\n\n\n\nP(70 &lt;= X &lt;= 130) = 0.9545\n\n\n\n\n\n\n\n\nFigure¬†4.8: Wahrscheinlichkeit, dass ein IQ-Wert innerhalb von zwei Standardabweichungen liegt.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nDer IQ ist so skaliert, dass \\(\\mu = 100\\) und \\(\\sigma = 15\\). Die Wahrscheinlichkeit f√ºr einen IQ \\(\\geq 130\\) betr√§gt etwa 2,28 %, w√§hrend 95,45 % der Bev√∂lkerung einen IQ zwischen 70 und 130 haben (innerhalb von \\(\\pm 2\\sigma\\)). Dies zeigt die praktische Relevanz der Normalverteilung.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Verteilungen</span>"
    ]
  },
  {
    "objectID": "statistics/distributions.html#sec-other-distributions",
    "href": "statistics/distributions.html#sec-other-distributions",
    "title": "4¬† Verteilungen",
    "section": "4.3 Weitere Verteilungen",
    "text": "4.3 Weitere Verteilungen\nNeben der Normalverteilung spielen weitere stetige und diskrete Verteilungen eine Rolle in Statistik und Simulationen, wie im Tutorial zur Fahrzeugausfallzeit genutzt:\n\nDie Exponentialverteilung modelliert die Zeit zwischen unabh√§ngigen Ereignissen, z. B. in der Zuverl√§ssigkeitsanalyse oder Warteschlangentheorie.\n\nDie Poissonverteilung (diskret) beschreibt die Anzahl von Ereignissen in einem festen Zeitintervall, etwa in Zufallsprozessen.\n\nDiese Verteilungen sind im Python-Modul scipy.stats verf√ºgbar, das Funktionen f√ºr Wahrscheinlichkeiten und Zufallszahlen bietet. Im Tutorial wird z. B. die Poissonverteilung f√ºr den Motor (\\(X \\sim \\text{Poisson}(5000)\\)) eingesetzt.\n\n4.3.1 Exponentialverteilung\nDie Dichtefunktion der Exponentialverteilung ist:\n\\[f(x) = \\lambda e^{-\\lambda x}, \\quad x \\geq 0,\\]\nwobei \\(\\lambda\\) die Rate ist. Wir schreiben \\(X \\sim \\text{Exp}(\\lambda)\\), mit \\(E(X) = \\frac{1}{\\lambda}\\) und \\(\\text{Var}(X) = \\frac{1}{\\lambda^2}\\). Sie ist im Tutorial f√ºr Ausfallzeiten relevant, wenn Ereignisse exponentiell verteilt w√§ren.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Verteilungen</span>"
    ]
  },
  {
    "objectID": "statistics/distributions.html#sec-fitting-distributions",
    "href": "statistics/distributions.html#sec-fitting-distributions",
    "title": "4¬† Verteilungen",
    "section": "4.4 Fitting von Verteilungen",
    "text": "4.4 Fitting von Verteilungen\nIn der Praxis m√ºssen wir oft die Verteilung von Daten bestimmen ‚Äì ein Prozess namens Fitting. Ziel ist es, die Verteilung zu finden, die die Daten am besten beschreibt. Methoden wie Maximum-Likelihood-Sch√§tzung oder die Methode der Momente werden daf√ºr genutzt.\nAls Beispiel simulieren wir Lotterie-Daten mit \\(X \\sim \\text{Bin}(10, 0.1)\\) und passen verschiedene Verteilungen an, um zu vergleichen, welche die Daten am besten modelliert.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import binom, norm, uniform\n\nnp.random.seed(42)\ndata = np.random.binomial(n=10, p=0.1, size=1000)  # 1000 Ziehungen\n\n# Binomialverteilung sch√§tzen\nn_est = 10  # Bekannt aus Simulation\np_est = np.mean(data) / n_est\nbinom_x = np.arange(0, n_est + 1)\nbinom_y = binom.pmf(binom_x, n=n_est, p=p_est)\n\n# Normalverteilung sch√§tzen\nmu, sigma = norm.fit(data)\nnorm_x = np.linspace(0, 10, 1000)\nnorm_y = norm.pdf(norm_x, mu, sigma)\n\n# Gleichverteilung sch√§tzen\na, b = uniform.fit(data)\nuniform_x = np.linspace(0, 10, 1000)\nuniform_y = uniform.pdf(uniform_x, a, b - a)\n\n# Plot\nplt.hist(data, bins=range(11), density=True, color='skyblue', alpha=0.7, label='Daten')\nplt.plot(binom_x, binom_y, 'ro--', label=f'Binomial ($n={n_est}$, $p={p_est:.2f}$)')\nplt.plot(norm_x, norm_y, 'g-', label=f'Normal ($\\mu={mu:.2f}$, $\\sigma={sigma:.2f}$)')\nplt.plot(uniform_x, uniform_y, 'p-', label=f'Gleichverteilung ($a={a:.2f}$, $b={b:.2f}$)')\nplt.xlabel('Anzahl der Gewinnlose ($X$)')\nplt.ylabel('Dichte')\nplt.title('Fitting von Verteilungen an Binomialdaten')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\n&lt;&gt;:27: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:27: SyntaxWarning:\n\ninvalid escape sequence '\\s'\n\n&lt;&gt;:27: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:27: SyntaxWarning:\n\ninvalid escape sequence '\\s'\n\n/tmp/ipykernel_3173/1255964559.py:27: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n/tmp/ipykernel_3173/1255964559.py:27: SyntaxWarning:\n\ninvalid escape sequence '\\s'\n\n\n\n\n\n\n\n\n\nFigure¬†4.9: Fitting verschiedener Verteilungen an simulierte Binomialdaten (n = 10, p = 0.1).",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Verteilungen</span>"
    ]
  },
  {
    "objectID": "statistics/distributions.html#sec-computing-distributions",
    "href": "statistics/distributions.html#sec-computing-distributions",
    "title": "4¬† Verteilungen",
    "section": "4.5 Rechnen mit Verteilungen, Zufallsvariablen und Erwartungswert",
    "text": "4.5 Rechnen mit Verteilungen, Zufallsvariablen und Erwartungswert\nDer zentrale Grenzwertsatz besagt, dass die Summe einer gro√üen Anzahl von unabh√§ngigen und identisch verteilten (i.i.d.) Zufallsvariablen einer Normalverteilung folgt ‚Äì unabh√§ngig von ihrer urspr√ºnglichen Verteilung. Dies ist ein Grundpfeiler der Statistik und erkl√§rt, warum Normalverteilungen in Simulationen wie im Tutorial oft auftreten.\n\n4.5.1 Rechenregeln f√ºr Erwartungswert und Varianz\nF√ºr unabh√§ngige Zufallsvariablen \\(X_1, X_2, \\ldots, X_n\\) gelten folgende Regeln:\n\nErwartungswert: Die Summe der Erwartungswerte gilt immer, auch ohne Unabh√§ngigkeit:\n\\[E(X_1 + X_2 + \\cdots + X_n) = E(X_1) + E(X_2) + \\cdots + E(X_n).\\]\nVarianz: Bei Unabh√§ngigkeit (Kovarianz = 0) ist die Varianz die Summe der Varianzen:\n\\[\\text{Var}(X_1 + X_2 + \\cdots + X_n) = \\text{Var}(X_1) + \\text{Var}(X_2) + \\cdots + \\text{Var}(X_n).\\]\nAndernfalls: \\(\\text{Var}(X_1 + X_2) = \\text{Var}(X_1) + \\text{Var}(X_2) + 2 \\text{Cov}(X_1, X_2)\\).\nLinearkombinationen: F√ºr \\(aX + bY\\):\n\\[E(aX + bY) = aE(X) + bE(Y),\\]\n\\[\\text{Var}(aX + bY) = a^2 \\text{Var}(X) + b^2 \\text{Var}(Y) + 2ab \\text{Cov}(X, Y),\\]\nwobei \\(\\text{Cov}(X, Y) = 0\\) bei Unabh√§ngigkeit.\n\n\n\n\n\n\n\nImportant\n\n\n\nDie Kovarianz \\(\\text{Cov}(X, Y)\\) misst den linearen Zusammenhang zwischen \\(X\\) und \\(Y\\). Bei Unabh√§ngigkeit ist sie 0, was die Varianzberechnung vereinfacht.\n\n\n\n\n4.5.2 Beispiel: Summe von Zufallsvariablen aus verschiedenen Verteilungen\nDer zentrale Grenzwertsatz gilt auch, wenn die Zufallsvariablen nicht identisch verteilt sind, solange sie unabh√§ngig sind und die Anzahl gro√ü ist. Figure¬†4.10 zeigt, wie die Summe von Uniform-, Exponential- und Binomialverteilungen einer Normalverteilung √§hnelt ‚Äì ein Prinzip, das in Monte-Carlo-Simulationen wie im Tutorial genutzt wird.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\nN = 10000\n\n# Zufallsvariablen definieren\nX_uni_1 = np.random.uniform(0, 10, N)      # Uniform [0, 10]\nX_uni_2 = np.random.uniform(2, 8, N)       # Uniform [2, 8]\nX_exp_1 = np.random.exponential(2, N)      # Exponential, Scale = 2\nX_exp_2 = np.random.exponential(3, N)      # Exponential, Scale = 3\nX_bin_1 = np.random.binomial(20, 0.3, N)   # Binomial (n=20, p=0.3)\nX_bin_2 = np.random.binomial(15, 0.4, N)   # Binomial (n=15, p=0.4)\n\nX_sum = X_uni_1 + X_uni_2 + X_exp_1 + X_exp_2 + X_bin_1 + X_bin_2\n\n# Plot\nplt.figure(figsize=(12, 6))\nplt.hist(X_sum, bins=50, density=True, color='skyblue', alpha=0.7, label='Summe')\nplt.hist(X_uni_1, bins=50, density=True, alpha=0.3, label='Uniform 1')\nplt.hist(X_uni_2, bins=50, density=True, alpha=0.3, label='Uniform 2')\nplt.hist(X_exp_1, bins=50, density=True, alpha=0.3, label='Exponential 1')\nplt.hist(X_exp_2, bins=50, density=True, alpha=0.3, label='Exponential 2')\nplt.hist(X_bin_1, bins=50, density=True, alpha=0.3, label='Binomial 1')\nplt.hist(X_bin_2, bins=50, density=True, alpha=0.3, label='Binomial 2')\nplt.xlabel('Wert')\nplt.ylabel('Dichte')\nplt.title('Summe von Zufallsvariablen aus verschiedenen Verteilungen')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\nFigure¬†4.10: Summe von Zufallsvariablen aus verschiedenen Verteilungen (N = 10.000).\n\n\n\n\n\n\n4.5.2.1 Normalverteilungs-Fit der Summe\nUm den zentralen Grenzwertsatz zu veranschaulichen, passen wir eine Normalverteilung an die Summe an. Figure¬†4.11 zeigt, wie gut die Summe einer Normalverteilung entspricht.\n\nfrom scipy.stats import norm\n\n# Normalverteilung an Summe anpassen\nX_sum_mu, X_sum_sigma = norm.fit(X_sum)\n\n# Plot\nplt.hist(X_sum, bins=50, density=True, color='skyblue', alpha=0.7, label='Summe')\nx = np.linspace(min(X_sum), max(X_sum), 1000)\ny = norm.pdf(x, X_sum_mu, X_sum_sigma)\nplt.plot(x, y, 'r-', label=f'Normal ($\\mu={X_sum_mu:.1f}$, $\\sigma={X_sum_sigma:.1f}$)')\nplt.xlabel('Wert')\nplt.ylabel('Dichte')\nplt.title('Fit der Summe mit einer Normalverteilung')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\n&lt;&gt;:10: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:10: SyntaxWarning:\n\ninvalid escape sequence '\\s'\n\n&lt;&gt;:10: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:10: SyntaxWarning:\n\ninvalid escape sequence '\\s'\n\n/tmp/ipykernel_3173/3238649060.py:10: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n/tmp/ipykernel_3173/3238649060.py:10: SyntaxWarning:\n\ninvalid escape sequence '\\s'\n\n\n\n\n\n\n\n\n\nFigure¬†4.11: Fit der Summe von Zufallsvariablen mit einer Normalverteilung (N = 10.000).",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Verteilungen</span>"
    ]
  },
  {
    "objectID": "statistics/tutorial.html",
    "href": "statistics/tutorial.html",
    "title": "Tutorial: Monte Carlo Simulation of Vehicle Component Failure",
    "section": "",
    "text": "Ziel\nIn dieser √úbung erstellen Studierende eine Monte-Carlo-Simulation zur Modellierung der Ausfallzeit von Fahrzeugkomponenten. Dabei werden Wahrscheinlichkeitskonzepte wie bedingte Wahrscheinlichkeit, Additions- und Multiplikationsregeln, statistische Unabh√§ngigkeit sowie Visualisierung durch ein Histogramm vertieft.",
    "crumbs": [
      "Statistik",
      "Tutorial: Monte Carlo Simulation of Vehicle Component Failure"
    ]
  },
  {
    "objectID": "statistics/tutorial.html#sec-monte-carlo-story",
    "href": "statistics/tutorial.html#sec-monte-carlo-story",
    "title": "Tutorial: Monte Carlo Simulation of Vehicle Component Failure",
    "section": "Szenario und Anwendungsfall",
    "text": "Szenario und Anwendungsfall\n\nSzenario\nEin Auto besteht aus kritischen Komponenten: Motor, Sensoren und Steuereinheit. F√§llt der Motor oder die Steuereinheit aus, stoppt das Fahrzeug. Der Besitzer m√∂chte die erwartete Betriebsdauer bis zum Ausfall absch√§tzen, um Wartungsintervalle zu planen.\n\n\nAnwendungsfall\nDie Studierenden simulieren das Ausfallverhalten eines Fahrzeugs mit drei Komponenten:\n- Motor: Ausfallzeit folgt einer Poissonverteilung mit Mittelwert 5000 Stunden.\n- Sensor: Ausfallzeit folgt einer Normalverteilung mit \\(\\mu = 6000\\) Stunden, \\(\\sigma = 100\\) Stunden.\n- Steuereinheit: Ausfallzeit folgt einer Gleichverteilung zwischen 4000 und 8000 Stunden.\n- Bedingung: F√§llt der Sensor vor dem Motor aus, verk√ºrzt sich die Motor-Ausfallzeit um 1000 Stunden.\n- Stopp: Das Fahrzeug stoppt, wenn Motor oder Steuereinheit ausf√§llt.\n\nAufgaben\n\nSimulation\n\nSimuliere Ausfallzeiten f√ºr jede Komponente des Fahrzeugs.\nBestimme die Zeit bis zum Ausfall des Fahrzeugs.\nWiederhole dies f√ºr 10.000 Durchl√§ufe.\nStelle die Zeit bis zum Ausfall in einem Histogramm dar (Figure¬†1).\n\n\n\nAnalyse\nF√ºr die weitere Anaylse nehmen wir an, dass wir den Zufallsprozess der Ausf√§lle nicht kennen. Wir k√∂nnen jedoch die Ausfallereignisse aufzeichnen und analysieren. Wir interessieren uns nur f√ºr Ausf√§lle, die zu einem Stopp des Fahrzeugs vor 5000 Betriebs-Stunden f√ºhren.\n\nBerechne f√ºr jede Komponente die Wahrscheinlichkeit, dass sie ausgefallen ist, gegeben das Fahrzeug f√§llt vor 5000 Stunden aus.\nGibt es eine Korrelation zwischen allen Zeiten bis zum Ausfall aller Komponenten und des Fahrzeugs? Berechne die Korrelationskoeffizienten und visualisiere die Korrelation in einer Scatterplot-Matrix.\nWie hoch ist die Bedingte Wahrscheinlichkeit, dass die Steuereinheit den ausgefallen ist, wenn das Fahrzeug vor 4000 Stunden ausgefallen ist.\nW√§hle eine Verteilung f√ºr die Ausfallzeit des Fahrzeugs fitte diese. Vergleiche die Verteilung mit dem Histogramm und gibt die Parameter der Verteilung an.\nNutze die gefittete Verteilung, um die Wahrscheinlichkeit zu berechnen, dass das Fahrzeug vor 3000 Stunden ausf√§llt.",
    "crumbs": [
      "Statistik",
      "Tutorial: Monte Carlo Simulation of Vehicle Component Failure"
    ]
  },
  {
    "objectID": "statistics/tutorial.html#sec-monte-carlo-simulation",
    "href": "statistics/tutorial.html#sec-monte-carlo-simulation",
    "title": "Tutorial: Monte Carlo Simulation of Vehicle Component Failure",
    "section": "Simulation",
    "text": "Simulation\nZuerst erstellen wir einen leeren DataFrame, um die Struktur der simulierten Ausfallzeiten zu zeigen:\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\nn_trials = 10000\n\n# Leerer DataFrame f√ºr Ausfallzeiten\ncolumns = ['motor', 'sensor', 'control', 'vehicle_failure']\ndf = pd.DataFrame(index=range(n_trials), columns=columns)\nprint(\"Leerer DataFrame (Auszug):\")\nprint(df.head())\n\nLeerer DataFrame (Auszug):\n  motor sensor control vehicle_failure\n0   NaN    NaN     NaN             NaN\n1   NaN    NaN     NaN             NaN\n2   NaN    NaN     NaN             NaN\n3   NaN    NaN     NaN             NaN\n4   NaN    NaN     NaN             NaN\n\n\nNun simulieren wir die Ausfallzeiten und berechnen die Zeit bis zum Fahrzeugausfall:\n\nfor trial in range(n_trials):\n    # Ausfallzeiten f√ºr das Fahrzeug\n    motor_time = np.random.poisson(5000)\n    sensor_time = np.random.normal(6000, 100)\n    control_time = np.random.uniform(4000, 8000)\n\n    # Bedingung: Sensor-Ausfall verk√ºrzt Motorzeit\n    if sensor_time &lt; motor_time:\n        motor_time -= 1000\n        if motor_time &lt; sensor_time:\n            motor_time = sensor_time\n\n\n    # Ausfallzeit des Fahrzeugs (Motor oder Steuereinheit)\n    vehicle_failure = min(motor_time, control_time)\n\n    # Daten in DataFrame speichern\n    df.loc[trial, 'motor'] = motor_time\n    df.loc[trial, 'sensor'] = sensor_time\n    df.loc[trial, 'control'] = control_time\n    df.loc[trial, 'vehicle_failure'] = vehicle_failure\n\nprint(\"DataFrame mit simulierten Ausfallzeiten (Auszug):\")\nprint(df.head())\n\nplt.hist(df['vehicle_failure'], bins=50, density=True, color='skyblue', alpha=0.7)\nplt.xlabel('Ausfallzeit des Fahrzeugs (Stunden)')\nplt.ylabel('Dichte')\nplt.title('Verteilung der Ausfallzeiten (Fahrzeug)')\nplt.show()\n\nDataFrame mit simulierten Ausfallzeiten (Auszug):\n  motor       sensor      control vehicle_failure\n0  4974  6064.768854  4624.074562     4624.074562\n1  4919  6152.302986  7464.704583            4919\n2  5020  5953.052561  4727.299869     4727.299869\n3  4928  6054.256004  6099.025727            4928\n4  4986  5898.716888  5168.578594            4986\n\n\n\n\n\n\n\n\nFigure¬†1: Histogramm der Ausfallzeiten des Fahrzeugs (10.000 Durchl√§ufe).",
    "crumbs": [
      "Statistik",
      "Tutorial: Monte Carlo Simulation of Vehicle Component Failure"
    ]
  },
  {
    "objectID": "statistics/interference_basics.html",
    "href": "statistics/interference_basics.html",
    "title": "5¬† Interferenz",
    "section": "",
    "text": "5.1 Punktsch√§tzer und Konfidenzintervalle\nBei der Frage der Interferenz geht es darum, was wir mit ausreichender Sicherheit √ºber eine Population aussagen k√∂nnen, wenn wir nur eine Stichprobe haben. Ein Beispiel hierf√ºr k√∂nnte sein, ob ein Parameter einer Verteilung signifikant von einem bestimmten Wert abweicht oder ob sich die Mittelwerte zweier Verteilungen signifikant unterscheiden.\nEin Punktsch√§tzer ist eine Sch√§tz-Funktion, die eine Sch√§tzung f√ºr einen Parameter (z.B. Mittelwert oder Varianz) einer Verteilung liefert. Da wir die wahre Verteilung der Grundgesamtheit nie direkt beobachten k√∂nnen, m√ºssen wir R√ºckschl√ºsse daraus aus unserer Stichprobe ziehen. Dabei hilft uns wieder der Zentrale Grenzwertsatz.\nWir beginnen mit einem Beispiel einer Zufallsvariable \\(X \\sim \\text{Bernoulli}(p)\\), wobei uns der Parameter \\(p\\) unbekannt ist. Diesen Parameter wollen wir nun anhand unserer Stichprobe sch√§tzen. Eine Sch√§tzung f√ºr \\(p\\) ist der Anteil der Einsen in der Stichprobe \\(\\hat{p}\\). Diesen Sch√§tzer k√∂nnen wir berechnen als\n\\[\n\\hat{p} = \\frac{1}{n} \\sum_{i=1}^n X_i,\n\\]\nUm dies zu illustrieren, schauen wir uns die Grundgesamtheit und eine Stichprobe an. Wir ziehen \\(N=1000\\) Zufallszahlen aus einer Bernoulli-Verteilung mit \\(p=0.3\\) und w√§hlen eine Stichprobe der Gr√∂√üe \\(n=100\\). Diesen Vorgang wiederholen wir \\(m=100\\) mal. Den Anteil der Einsen in der Stichprobe speichern wir als Stichprobenh√§ufigkeit \\(\\hat{p}\\).\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\n\n# Definiere Anzahl der Versuche \nm = 100\n\nresults = np.zeros(m)\n\n# Grundgesamtheit ist Bernoulli verteilt\np = 0.3\nN = 1000\n\nfor i in range(m):\n    # Grundgesamtheit\n    X = np.random.binomial(1, p, N)\n\n    # Stichprobe mit n=100\n    n = 100\n    X_sample = np.random.choice(X, n)\n\n    # Speichere das Anzahl der Einsen\n    results[i] = np.sum(X_sample)/n\n\n# Wie verteilt sich die Anzahl der Einsen in der Stichprobe\nsns.histplot(results)\n\n# Wie gut passt eine Normalverteilung\nmu, std = norm.fit(results)\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = norm.pdf(x, mu, std)\nplt.plot(x, p, 'k', linewidth=2)\n\nplt.title('Verteilung der Anzahl der Einsen in der Stichprobe')\nplt.show()\n\nprint(f'Mittelwert von p_hat: {np.mean(results)/n:.2f}')\nprint(f'Standardabweichung von p_hat: {np.std(results)/n:.2f}')\n\n\n\n\n\n\n\n\nMittelwert von p_hat: 0.00\nStandardabweichung von p_hat: 0.00\nVerglichen mit den theoretischen Werten √ºbergibt dies eine hohe √úbereinstimmung.\n\\[\n\\mu_{\\hat{p}} = p = 0.3,\n\\]\n\\[\nSE_{\\hat{p}}=\\sigma_{\\hat{p}} = \\sqrt{\\frac{p(1-p)}{n}} = 0.046.\n\\]",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Interferenz</span>"
    ]
  },
  {
    "objectID": "statistics/interference_basics.html#sec-statistics-pointestimates",
    "href": "statistics/interference_basics.html#sec-statistics-pointestimates",
    "title": "5¬† Interferenz",
    "section": "",
    "text": "Bestimmung von Punktsch√§tzern mittels Maximum-Likelihood-Sch√§tzung\n\n\n\n\n\nUm den Maximum-Likelihood-Sch√§tzer (MLE) f√ºr den Parameter \\(p\\) einer Bernoulli-Verteilung zu finden, geht man wie folgt vor. Man stellt die Likelihood-Funktion der Beobachteten Werte in der Stichprobe auf und sucht dann nach dem Wert von \\(p\\), bei dem dieses Auftreten am wahrscheinlichsten ist.\n\n5.1.0.0.1 1. Likelihood-Funktion aufstellen:\nAngenommen, du hast \\(n\\) unabh√§ngige und identisch verteilte Bernoulli-Zufallsvariablen \\(X_1, X_2, \\ldots, X_n\\). Jede dieser Variablen \\(X_i\\) hat eine Bernoulli-Verteilung mit Parameter \\(p\\), was bedeutet, dass die Wahrscheinlichkeit, dass \\(X_i = 1\\) ist, gleich \\(p\\) ist und die Wahrscheinlichkeit, dass \\(X_i = 0\\) ist, gleich \\(1-p\\) ist.\nDie Likelihood-Funktion ist das Produkt der Wahrscheinlichkeiten aller beobachteten Werte. Falls wir \\(k\\) Einsen und \\(n-k\\) Nullen in unserer Stichprobe haben, ist die Likelihood-Funktion gegeben durch:\n\\[\nL(p) = \\prod_{i=1}^{n} p^{x_i} (1-p)^{1-x_i} = P(X_1=x_1) \\cdot P(X_2=x_2) \\cdot \\ldots \\cdot P(X_n=x_n)\n\\]\nDas vereinfacht sich zu:\n\\[\nL(p) = p^k (1-p)^{n-k}\n\\]\nwobei \\(k = \\sum_{i=1}^{n} x_i\\) die Anzahl der Einsen in der Stichprobe ist.\n\n\n5.1.0.0.2 2. Log-Likelihood-Funktion aufstellen:\nUm die Berechnungen zu vereinfachen, nimmt man den nat√ºrlichen Logarithmus der Likelihood-Funktion, da nun die Produkte zu Summen werden und dies die Ableitungen vereinfacht:\n\\[\n\\log L(p) = k \\log p + (n-k) \\log (1-p)\n\\]\n\n\n5.1.0.0.3 3. Log-Likelihood maximieren:\nUm die Log-Likelihood-Funktion zu maximieren, leiten wir sie nach \\(p\\) ab und setzen diese Ableitung gleich null:\n\\[\n\\frac{d}{dp} \\log L(p) = \\frac{k}{p} - \\frac{n-k}{1-p} = 0\n\\]\n\n\n5.1.0.0.4 4. Gleichung nach \\(p\\) aufl√∂sen:\nUm diese Gleichung nach \\(p\\) aufzul√∂sen, bringen wir sie auf einen gemeinsamen Nenner:\n\\[\n\\frac{k(1-p) - (n-k)p}{p(1-p)} = 0\n\\]\nDer Z√§hler muss null sein, damit der Bruch null wird und wir die Nullstellen finden:\n\\[\nk - kp - np + kp = k - np = 0\n\\]\nDas ergibt:\n\\[\nk = np\n\\]\nDaraus folgt, dass der MLE f√ºr \\(p\\):\n\\[\n\\hat{p} = \\frac{k}{n} = \\frac{1}{n} \\sum_{i=1}^{n} x_i.\n\\]\n\n\n5.1.0.0.5 Fazit:\nDer Maximum-Likelihood-Sch√§tzer \\(\\hat{p}\\) f√ºr den Parameter \\(p\\) einer Bernoulli-Verteilung ist also gleich dem Stichprobenmittelwert \\(frac{k}{n}\\), was intuitiv der Anteil der beobachteten Einsen in der Stichprobe ist. Der Maximum-Likelihood-Sch√§tzer ergibt sich, aus dem Gedanken, dass der Parameter \\(p\\) der Wert ist, bei dem die beobachteten Werte in der Stichprobe am wahrscheinlichsten sind.\n\n\n\n\n\n\n\n\n\n\nAnwendund des Zentralen Grenzwertsatzes f√ºr Sch√§tzer\n\n\n\nWenn eine Stichprobe gro√ü genug ist und die Ziehungen unabh√§ngig und identisch Verteilt sind, dann ist \\(\\hat{p}\\) normalverteilt mit\n\\[\n\\mu_{\\hat{p}} = p,\n\\]\nwas bedeutet, dass wird bei unendlich vielen Wiederholungen im Mittel den wahren Wert von \\(p\\) sch√§tzen. Einen Sch√§tzer mit dieser Eigenschaft nennen wir erwartungstreu.\nDie Standardabweichung, welche wir bei einem Sch√§tzer als Standardfehler bezeichnen, gegeben durch \\[\nSE_{\\hat{p}}=\\sigma_{\\hat{p}} = \\sqrt{\\frac{p(1-p)}{n}}.\n\\]\nDamit dies gilt muss \\(np \\geq 10\\) und \\(n(1-p) \\geq 10\\) sein.\n\n\n\n\n\n\n\n\n5.1.1 Konfidenzintervalle\nEin Konfidenzintervall ist ein Intervall, das den wahren Wert eines Parameters mit einer bestimmten Wahrscheinlichkeit enth√§lt. Bei einer Normalverteilung haben wir schon beobachtet, dass ca. 95% der Werte innerhalb von zwei Standardabweichungen liegen. Wir k√∂nnen dieses Wissen nutzen, um ein Konfidenzintervall zu berechnen.\n\n\n\n\n\n\nImportant\n\n\n\nEin Konfidenzintervall \\(\\text{CI}_{1-\\alpha}\\) gibt f√ºr einen Sch√§tzer \\(\\hat{\\theta}\\) eines Parameters \\(\\theta\\) einen Intervall an, in dem der wahre Wert von \\(\\theta\\) mit einer bestimmten Wahrscheinlichkeit (\\(1-\\alpha\\)) liegt. Das Konfidenzintervall f√ºr den Mittelwert \\(\\mu\\) einer Normalverteilung mit bekannter Varianz \\(\\sigma^2\\) ist gegeben durch\n\\[\n\\left(\\bar{\\theta} - z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}, \\bar{\\theta} + z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\right),\n\\]\nwobei \\(z_{\\alpha/2}\\) das \\(\\alpha/2\\)-Quantil der Standardnormalverteilung ist und wir dieses durch die Multiplikation mit der Standardabweichung \\(\\sigma\\) und Division durch die Wurzel der Stichprobengr√∂√üe \\(n\\) auf unsere Ursprungsverteilung skalieren.\nF√ºr den Fall, dass die Varianz unbekannt ist, k√∂nnen wir die Stichprobenvarianz \\(S^2\\) verwenden und erhalten\n\n\n\n\n\n\n\n\nErkl√§rung\n\n\n\nZun√§chst √ºberf√ºhren wir die Normalverteilung von \\(\\theta\\) in eine Standardnormalverteilung. Dazu subtrahieren wir den Erwartungswert \\(\\mu\\) und teilen durch die Standardabweichung \\(\\sigma\\). Damit erhalten wir eine Standardnormalverteilung mit Erwartungswert 0 und Varianz 1.\nNun k√∂nnen wir die Wahrscheinlichkeit berechnen, dass der Wert innerhalb eines Intervalls liegt. F√ºr ein Konfidenzniveau von 95% ist \\(\\alpha = 0.05\\). Das bedeutet, dass wird an beiden Seiten der Verteilung \\(\\alpha/2 = 0.025\\) abtrennen.\nIn der Tabelle \\(\\Phi(z)\\) der Standardnormalverteilung suchen wir nun den Wert, ab dem 97.5% der Werte unterhalb liegen. Dieser Wert ist \\(z_{\\alpha/2} = 1.96\\). Aus der Symmetrie der Normalverteilung folgt, dass der Wert f√ºr \\(-z_{\\alpha/2}\\) ebenfalls 1.96 ist.\n\n\nIn unserem Beispiel ist die Stichprobenh√§ufigkeit \\(\\hat{p}\\) normalverteilt mit \\(\\mu_{\\hat{p}} = p\\) und \\(\\sigma_{\\hat{p}} = \\sqrt{\\frac{p(1-p)}{n}}\\). Wir k√∂nnen also ein Konfidenzintervall f√ºr \\(p\\) berechnen.\n\\[\n\\left(\\hat{p} - z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}, \\hat{p} + z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\right).\n\\]\nAus der Tabelle der Standardnormalverteilung k√∂nnen wir den Wert f√ºr \\(z_{\\alpha/2}\\) ablesen. F√ºr ein Konfidenzniveau von 95% ist \\(\\alpha = 0.05\\) und damit \\(z_{\\alpha/2} = 1.96\\). Alternativ k√∂nnen wir dieses auch mit der Funktion norm.ppf berechnen.\n\n# Konfidenzintervall\nalpha = 0.05\nz = norm.ppf(1-alpha/2)\nprint(f'z_alpha/2: {z:.2f}')\n\nz_alpha/2: 1.96\n\n\nDamit ergibt sich f√ºr unser Beispiel ein Konfidenzintervall von\n\\[\n\\left(0.3 - 1.96 \\sqrt{\\frac{0.3(1-0.3)}{100}}, 0.3 + 1.96 \\sqrt{\\frac{0.3(1-0.3)}{100}}\\right) = (0.21, 0.39).\n\\]\nWir sind uns also zu 95% sicher, dass der wahre Wert von \\(p\\) in diesem Intervall liegt.\n\n\n\n\n\n\nCaution\n\n\n\nWelchen Faktor k√∂nnen wir anpassen, wenn wir den Konfidenzintervall verkleineren wollen?\nWelcher Konfidenzintervall w√ºrde sich f√ºr ein Konfidenzniveau von 99% ergeben?\n\n\n\n\n\n\n\n\nL√∂sung\n\n\n\n\n\nDer Faktor, den wir anpassen k√∂nnen, die gr√∂√üe der Stichprobe \\(n\\), da diese im Nennern steht.\nF√ºr ein Konfidenzniveau von 99% ist \\(\\alpha = 0.01\\) und damit \\(z_{\\alpha/2} = 2.58\\). Damit ergibt sich f√ºr unser Beispiel ein Konfidenzintervall von\n\\[\n\\left(0.3 - 2.58 \\sqrt{\\frac{0.3(1-0.3)}{100}}, 0.3 + 2.58 \\sqrt{\\frac{0.3(1-0.3)}{100}}\\right) = (0.18, 0.42).\n\\]\n\n5.1.2 Darstellung des Konfidenzintervalls eines normalverteilten Parameters\n\n# Konfidenzniveau\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Konfidenzniveaus\nalphas = [0.05, 0.01]\n\ncmap = plt.get_cmap('tab10')\ncolors = cmap(np.linspace(0, 1, len(alphas)))\n\n# Normalverteilung\nmu = 0.3\nsigma = np.sqrt(0.3*(1-0.3)/100)\n\n# Konfidenzintervall\nx = np.linspace(0.1, 0.5, 100)\ny = norm.pdf(x, mu, sigma)\nplt.plot(x, y)\n\nfor alpha in alphas:\n    z = norm.ppf(1-alpha/2)\n    lower = mu - z*sigma\n    upper = mu + z*sigma\n\n    print(f'Konfidenzintervall f√ºr alpha={alpha}: ({lower:.2f}, {upper:.2f})')\n\n    # Darstellung\n    plt.fill_between(x, y, where=(x &gt;= lower) & (x &lt;= upper), alpha=0.1, color=colors[alphas.index(alpha)])\n    # Text\n    plt.text(mu+0.1, norm.pdf(mu, mu, sigma) * (0.2+ 0.1 * alphas.index(alpha)), f'CI {1-alpha} %', color=colors[alphas.index(alpha)])\n    \n\n\nplt.legend()\nplt.title('Konfidenzintervall f√ºr p')\nplt.show()\n\nKonfidenzintervall f√ºr alpha=0.05: (0.21, 0.39)\nKonfidenzintervall f√ºr alpha=0.01: (0.18, 0.42)\n\n\n/tmp/ipykernel_3247/598074221.py:35: UserWarning:\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.1.3 Punktsch√§tzer und Konfidenzintervall f√ºr andere F√§lle\nF√ºr andere Verteilungen und Sch√§tzer k√∂nnen wir die gleichen Prinzipien anwenden. Wir k√∂nnen die Sch√§tzer und Konfidenzintervalle f√ºr den Mittelwert, die Varianz oder andere Parameter berechnen. Dabei m√ºssen wir nur die Verteilung des Sch√§tzers kennen und die entsprechenden Formeln anwenden.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Interferenz</span>"
    ]
  },
  {
    "objectID": "statistics/interference_basics.html#sec-statistics-hypothesistests",
    "href": "statistics/interference_basics.html#sec-statistics-hypothesistests",
    "title": "5¬† Interferenz",
    "section": "5.2 Hypothesentests",
    "text": "5.2 Hypothesentests\nEin Hypothesentest ist ein statistisches Verfahren, um zu entscheiden, ob eine Hypothese √ºber eine Population auf Basis einer Stichprobe abgelehnt oder beibehalten wird. Dabei wird eine Nullhypothese \\(H_0\\) aufgestellt, die wir widerlegen wollen. Die Nullhypothese ist meist eine Aussage √ºber den Wert eines Parameters, z.B. dass der Mittelwert einer Verteilung gleich einem bestimmten Wert ist. Die Alternative Hypothese \\(H_1\\) ist die Aussage, die wir beweisen wollen.\n\n\n\n\n\n\nDialektik und Falsifikationismus\n\n\n\nZun√§chst wirkt es ungewohnt, dass wir etwas aufstellen, nur um es zu widerlegen. Dieser Ansatz ist jedoch ein zentraler Bestandteil der wissenschaftlichen Methode und fu√üt tief in der westlichen Philosophie.\nDie Dialektik ist eine Methode, um Wahrheit zu finden, indem eine These aufgestellt wird und diese durch eine Antithese widerlegt wird. Die Synthese ist dann der n√§chste Schritt in Richtung Wahrheit. Dieser Prozess wird so lange wiederholt, bis die Wahrheit gefunden ist.\nDer Philisoph Karl Popper hat diesen Ansatz weiterentwickelt und den Falsifikationismus gepr√§gt, um den wissenschaftlichen Erkennungsprozess zu beschreiben. Laut Popper l√§sst sich eine These nie beweisen, sondern nur widerlegen. Wenn eine These widerlegt wird, muss sie verworfen werden. Wenn sie widersteht, ist sie nicht bewiesen, sondern nur nicht widerlegt.\n\n\nEnsprechend gehen wir in der Statistik wie folgt vor:\n\nWir formulieren die Nullhypothese \\(H_0\\) und die Alternativhypothese \\(H_1\\). Die Nullhypothese ist die Aussage, die wir widerlegen wollen. Die Alternativhypothese ist die Aussage, die wir beweisen wollen.\nWir w√§hlen ein Signifikanzniveau \\(\\alpha\\), das die Wahrscheinlichkeit angibt, mit der wir die Nullhypothese ablehnen.\nWir berechnen den Teststatistik \\(t\\) und bestimmen die Verteilung der Teststatistik unter der Nullhypothese.\n\n\n5.2.1 Entscheidungsfehler\nUm uns das Problem noch einmal zu vergegenw√§rtigen, betrachten wir nochmal das Beispiel eines medizinischen Tests. Wir haben eine Person, die entweder Krebs hat oder nicht. Der Test kann entweder positiv oder negativ sein.\n\n\n\n\n\ngraph LR\n    U[Person] --&gt;|0.001| A[Krebs] \n    U[Person] --&gt;|0.999| D[Kein Krebs]\n    A[Krebs] --&gt;|0.99| B[Positiv]\n    A[Krebs] --&gt;|0.01| C[Negativ]\n    D[Kein Krebs] --&gt;|0.05| E[Positiv]\n    D[Kein Krebs] --&gt;|0.95| F[Negativ]\n\n\n\n\n\n\nNun vergegenw√§rtigen wir noch einmal unser Business Understanding: Wir wollen die Person mit Krebs identifizieren. Der Worst-Case ist, dass wir die Person mit Krebs nicht erkennen. Wir wollen also die Wahrscheinlichkeit, dass wir die Person mit Krebs erkennen maximieren.\nDas bedeutet, dass wir die Nullhypothese \\(H_0\\) formulieren, dass die Person keinen Krebs hat. Die Alternativhypothese \\(H_1\\) ist, dass die Person Krebs hat. Bei einem Hypothesentest k√∂nnen wir zwei Arten von Fehlern machen:\n\nTyp I Fehler (\\(\\alpha\\)): Wir lehnen die Nullhypothese ab, obwohl sie wahr ist.\nTyp II Fehler (\\(\\beta\\)): Wir akzeptieren die Nullhypothese, obwohl sie falsch ist.\n\noder als Tabelle\n\nTable of error types\n\n\n\n\n\n\n\n\nTable of error types\nNull hypothesis (H0) is\n\n\nTrue\nFalse\n\n\nDecision\nabout null\nhypothesis (H0)\nNot reject\nCorrect inference\n(true negative)\n(probability = 1-Œ±)\nType II error\n(false negative)\n(probability = Œ≤)\n\n\nReject\nType I error\n(false positive)\n(probability = Œ±)\nCorrect inference\n(true positive)\n(probability = 1-Œ≤)\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nDer kritischere Fehler ist der Typ I Fehler, da wir die Nullhypothese ablehnen, obwohl sie wahr ist. Das Signifikanzniveau \\(\\alpha\\) gibt die Wahrscheinlichkeit an, mit der wir die Nullhypothese f√§lschlicherweise ablehnen. Das Signifikanzniveau wird vor dem Test festgelegt und sollte m√∂glichst klein sein.\n√úblicherweise wird ein Signifikanzniveau von 5% oder 1% gew√§hlt. Das bedeutet, dass wir die Nullhypothese nur ablehnen, wenn die Wahrscheinlichkeit, dass die Nullhypothese wahr ist, kleiner als 5% oder 1% ist.\nIm Beispiel w√ºrde dies bedeuten, dass wir eine Person als gesund einstufen, obwohl sie Krebs hat. Dies kann fatale Folgen haben, wenn die Person nicht rechtzeitig behandelt wird.\n\n\n\n\n5.2.2 Am Beispiel eines Punktsch√§tzer f√ºr den Mittelwert einer Normalverteilung\nAngenommen wir wollen untersuchen, ob Studierende der Mechatronik im Schnitt intelligenter sind als der Durchschnitt. Wie haben bereits etabliert, dass die durchschnittliche Intelligenz in der Bev√∂lkerung bei einem IQ von 100 liegt. Nehmen wir nun wohlwollend an, dass der IQ der Mechatronikstudierenden normalverteilt ist mit einem Mittelwert von 110 und einer Standardabweichung von 15:\n\\[\nX \\sim \\mathcal{N}(110, 15).\n\\]\nWir haben eine Stichprobe von 100 Studierenden und wollen wissen, ob der Mittelwert signifikant von 100 abweicht.\n\n# Hypothesentest\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Stichprobe der Mechatronikstudierenden\nmu = 110\nsigma = 15\n\n# Nullhypothese\nmu_0 = 100\n\n# Stichprobe\nn = 100\nX = np.random.normal(mu, sigma, n)\n\n# Sch√§tzer f√ºr den Mittelwert und Standardabweichung\nX_hat = np.mean(X)\nmu_hat = np.std(X)\n\n# Normale Verteilung aus der Sch√§tzung\nx = np.linspace(90, 130, 100)\ny = norm.pdf(x, X_hat, mu_hat)\n\n# Plot der gesch√§tzten Verteilung und Nullhypothese\n\nplt.plot(x, y, label='Verteilung der Stichprobe')\nplt.axvline(mu_0, color='r', linestyle='--', label='Nullhypothese')\nplt.legend()\nplt.title('Verteilung der Stichprobe und Nullhypothese')\nplt.show()\n\n\n\n\n\n\n\n\nWir k√∂nnen nun sehen, dass die Stichprobe der Mechatronikstudierenden eine h√∂here Intelligenz aufweist als der Durchschnitt von 100. Wahrscheinlich werden wir auch einige Mechatronikstudierende finden, die weniger intelligent sind als der Durchschnitt.\nWas wir nun aber testen wollen, ist ob der Mittelwert der Stichprobe signifikant von 100 abweicht. Dazu brauchen wir jetzt einen Sch√§tzer des Mittelwertes und der Standardabweichung der Stichprobe. Die Ziehungen sind unab√§ngig und identisch verteilt, daher k√∂nnen wir den zentralen Grenzwertsatz anwenden. Dieser besagt, dass der Sch√§tzer f√ºr den Mittelwert gegeben ist durch\n\\[\n\\hat{\\mu}=\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i,\n\\]\ndessen Standardfehler ist gegeben durch\n\\[\nSE(\\hat{\\mu})=\\sigma_{\\hat{\\mu}} = \\frac{\\sigma}{\\sqrt{n}}.\n\\]\nDies k√∂nnen wir nun anhand der Stichprobe berechnen und visualisieren.\n\n# Sch√§tzer f√ºr den Mittelwert und Standardabweichung\nX_hat = np.mean(X)\nmu_hat = np.std(X)/np.sqrt(n)\n\n# Normale Verteilung aus der Sch√§tzung\nx = np.linspace(90, 130, 100)\ny = norm.pdf(x, X_hat, mu_hat)\n\n# mit zweiseitigem Signifikanzniveau von 5%\nalpha = 0.05\nz = norm.ppf(1-alpha/2)\n\n# Plot der gesch√§tzten Verteilung und Nullhypothese\n\nplt.plot(x, y, label='Verteilung der Stichprobe')\nplt.axvline(mu_0, color='r', linestyle='--', label='Nullhypothese')\nplt.axvline(X_hat + z*mu_hat, color='g', linestyle='--', label='Signifikanzniveau')\nplt.axvline(X_hat - z*mu_hat, color='g', linestyle='--')\nplt.fill_between(x, y, where=(x &gt;= X_hat - z*mu_hat) & (x &lt;= X_hat + z*mu_hat), alpha=0.1, color='g')\n\nplt.legend()\nplt.title('Verteilung von Mittelwert-Sch√§tzer und Nullhypothese')\nplt.show()\n\nprint(f'Mittelwert der Stichprobe: {X_hat:.2f}')\nprint(f'Standardabweichung der Stichprobe: {mu_hat:.2f}')\n\n\n\n\n\n\n\n\nMittelwert der Stichprobe: 108.26\nStandardabweichung der Stichprobe: 1.46\n\n\nAn Fig. k√∂nnen wir erkennen, dass die Wahrscheinlichkeit, dass der Mittelwert der Stichprobe von 100 abweicht, sehr hoch ist. Der gr√ºner Bereich, der das Signifikanzniveau darstellt, liegt weit rechts von der Nullhypothese.\n\n\n5.2.3 T-Verteilung\nF√ºr endlich gro√üe Stichproben-Umf√§nge funktionert unsere Ann√§herung des Strichprobenmittelwerts durch eine Normalverteilung. F√ºr kleine Stichproben-Umf√§nge ist die T-Verteilung besser geeignet.\nDie T-Verteilung ist gegeben durch\n\\[\nt = \\frac{\\bar{X} - \\mu_0}{\\frac{S}{\\sqrt{n}}},\n\\]\nund h√§ngt vorallem vom Stichprobenumfang \\(n\\) ab. Je kleiner der Stichprobenumfang, desto breiter ist die T-Verteilung. F√ºr gro√üe Stichprobenumf√§nge konvergiert die T-Verteilung gegen die Normalverteilung. \\(S\\) ist die Stichprobenstandardabweichung. Man bezeichnet \\(df = n-1\\) als die Freiheitsgrade der T-Verteilung.\n\n\n\n\n\n\nNote\n\n\n\nDie T-Verteilung wurde von William Sealy Gosset entwickelt. Er arbeitete f√ºr die Guinness-Brauerei und entwickelte die T-Verteilung, um die Qualit√§t des Bieres zu verbessern. Um die Geheimhaltung der Brauerei zu wahren, ver√∂ffentlichte er seine Ergebnisse unter dem Pseudonym Student. Dies f√ºhrte zur Bezeichnung der T-Verteilung als Student‚Äôs T-Verteilung.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Interferenz</span>"
    ]
  },
  {
    "objectID": "statistics/interference_advanced.html",
    "href": "statistics/interference_advanced.html",
    "title": "6¬† Tests",
    "section": "",
    "text": "6.1 T-Test\nNach dem zuvor beschreibenen Prinzip der Hypothesentests, gibt es verschiedene Tests, die auf unterschiedliche Fragestellungen zugeschnitten sind. In diesem Abschnitt werden einige dieser Tests vorgestellt.\nWir werden uns nur mit einigen typischen Tests besch√§ftigen. Es gibt noch viele weitere Tests, die auf spezielle Fragestellungen zugeschnitten sind. Die hier vorgestellten Tests sind jedoch die wichtigsten und werden in der Praxis am h√§ufigsten verwendet.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Tests</span>"
    ]
  },
  {
    "objectID": "statistics/interference_advanced.html#t-test",
    "href": "statistics/interference_advanced.html#t-test",
    "title": "6¬† Tests",
    "section": "",
    "text": "6.1.1 One-Sample Student‚Äôs T-Test\nBeim One-Sample Student‚Äôs T-Test wird die Mittelwert einer Stichprobe mit einem vorgegebenen Wert verglichen. Der Test wird verwendet, wenn die Varianz der Grundgesamtheit unbekannt ist.\nWir m√∂chten den Typ I Fehler (\\(\\alpha\\)) festlegen und dann sicherstellen, dass die Wahrscheinlichkeit, dass wir die Nullhypothese f√§lschlicherweise ablehnen, kleiner oder gleich \\(\\alpha\\) ist.\nUm zu beurteilen, wie wahrscheinlich die Stichprobe ist, wenn die Nullhypothese wahr ist, wird die Teststatistik \\(t\\) berechnet. Diese berechnet den Unterschied zwischen \\(\\bar{x}\\) und \\(\\mu_0\\) in Einheiten der Standardabweichung der Stichprobe. W√∂rtlich bedeutet die Teststatistik: Um wie viele Standardfehler unterscheidet sich der Stichprobenmittelwert \\(\\bar{x}\\) vom vorgegebenen Wert \\(\\mu_0\\).\n\\[\nt = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}\n\\]\nwobei \\(\\bar{x}\\) der Stichprobenmittelwert, \\(s\\) die Stichprobenstandardabweichung und \\(n\\) die Stichprobengr√∂√üe ist.\nDie Teststatistik \\(t\\) folgt einer t-Verteilung mit \\(n-1\\) Freiheitsgraden.\nWenn die Teststatistik \\(t\\) gr√∂√üer ist als der kritische Wert \\(t_{\\alpha}\\), dann lehnen wir die Nullhypothese ab. Der kritische Wert \\(t_{\\alpha}\\) wird aus der t-Verteilungstabelle abgelesen.\n\n\n6.1.2 Beispiel: Intelligenz von Mechatronikstudierenden\n\n# Hypothesentest\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Stichprobe der Mechatronikstudierenden\nmu = 110\nsigma = 15\n\n# Nullhypothese\nmu_0 = 100\n\n# Stichprobe\nn = 100\nX = np.random.normal(mu, sigma, n)\n\n# Sch√§tzer f√ºr den Mittelwert und Standardabweichung\nX_hat = np.mean(X)\nmu_hat = np.std(X)\n\n# Normale Verteilung aus der Sch√§tzung\nx = np.linspace(90, 130, 100)\n\n\n# Plot der gesch√§tzten Verteilung und Nullhypothese\n\nplt.hist(X, bins=20, alpha=0.5, label='Stichprobe', color='blue', edgecolor='black')\n\nplt.axvline(mu_0, color='r', linestyle='--', label='Nullhypothese')\nplt.legend()\nplt.title('Verteilung der Stichprobe und Nullhypothese')\nplt.xlabel('Wert')\nplt.ylabel('H√§ufigkeit')\nplt.show()\n\n\n\n\n\n\n\n\n\nDefinition der Hypothesen:\n\n\\(H_0: \\mu_x = 100\\) (Der Mittelwert der Mechatronikstudierenden ist gleich dem Durchschnitt)\n\\(H_1: \\mu_x &gt; 100\\) (Der Mittelwert der Mechatronikstudierenden ist gr√∂√üer als der Durchschnitt)\n\nDefinition des Signifikanzniveaus: \\(\\alpha = 0.05\\)\n\nDie Wahrscheinlichkeit, dass wir die Nullhypothese f√§lschlicherweise ablehnen, soll maximal 5% betragen oder \\(P(H_1 | H_0) \\leq 0.05\\).\n\nBerechnung des Teststatistik \\(t\\):\n\n\n\\(T = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}= 6.50\\)\n\n\nX_bar = np.mean(X)\ns = np.std(X)\nt = (X_bar - mu_0) / (s / np.sqrt(n))\nprint(f't = {t}')\n\nt = 7.291769280165263\n\n\n\nBerechnung des kritischen Wertes \\(t_{\\alpha}\\):\n\n\nZun√§chst m√ºssen wir uns entscheiden, ob wir ein- oder zweiseitig testen. Da wir wissen, dass der Mittelwert gr√∂√üer ist, w√§hlen wir einen einseitigen Test.\nAus der t-Verteilungstabelle k√∂nnen wir f√ºr \\(n-1 = 99\\) Freiheitsgrade und \\(\\alpha = 0.05\\) den kritischen Wert \\(t_{\\alpha} = 1.660\\) ablesen.\n\n\nfrom scipy.stats import t as t_table\n\nalpha = 0.05\nt_alpha = t_table.ppf(1 - alpha, n-1)\nprint(f't_alpha = {t_alpha}')\n\nt_alpha = 1.6603911559963895\n\n\nDies k√∂nnen wir, wie folgt visualisieren:\n\nx = np.linspace(-7, 7, 100)\ny = t_table.pdf(x, n-1)\nplt.plot(x, y, label='t-Verteilung')\nplt.fill_between(x, 0, y, where=(x &gt; t_alpha), color='red', alpha=0.5, label='Ablehnungsbereich')\n\nplt.axvline(t, color='black', linestyle='--', label='T')\nplt.legend()\nplt.title('t-Verteilung und Ablehnungsbereich')\nplt.xlabel('t')\nplt.ylabel('Dichte')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAuf den ersten Blick kann es so wirken, als h√§tten wir eine Verteilung, um unsere Null-Hypothese gefittet, f√ºr die wir ja gar keine Daten haben (vgl. Beispiel im vorherigen Kapitel). Tats√§chlich haben wird jedoch eine neue Zufallsvariable \\(T\\) definiert, die die Differenz zwischen dem Stichprobenmittelwert und dem Mittelwert der Nullhypothese in Einheiten der Standardabweichung der Stichprobe angibt. Diese Zufallsvariable \\(T\\) folgt einer t-Verteilung, die wir aus der Stichprobe berechnen k√∂nnen. Die Daten aus der Stichprobe k√∂nnen wir zum Fitten der Verteilung nutzen, da wir ja annehmen, dass alles was wird auf der Stichprobe basiert, auch auf der Grundgesamtheit basiert.\n\n\n\nDie Entscheidung basiert auf \\(T &gt; t_{\\alpha}\\),\n\n\nda \\(6.50 &gt; 1.660\\) ist, lehnen wir die Nullhypothese ab.\nDie Wahrscheinlichkeit, dass wir einen Mittelwert von 110 erhalten, wenn der Mittelwert tats√§chlich 100 betr√§gt, ist deutlich kleiner als 5%.\nDie Wahrscheinlichkeit, dass wir die Nullhypothese f√§lschlicherweise ablehnen, obwohl Sie wahr ist \\(P(\\text{Entscheidung f√ºr }H_1 | H_0) &lt; 0.05\\)\nWir k√∂nnen diese Wahrscheinlichkeit auch genau bestimmen, indem wir sie z.B. aus der t-Verteilung berechnen bzw. aus der Tabelle ablesen. \\(P(\\text{Entscheidung f√ºr }H_1 | H_0) = 1.63\\cdot 10^{-9}\\)\nWir k√∂nnen unsere Null-Hypothese also mit einer Wahrscheinlichkeit von 99.999999837% ablehnen. Vorerst ist unsere These, dass die Mechatronikstudierenden intelligenter sind als der Durchschnitt, nicht falsifiziert.\n\n\np_value = 1 - t_table.cdf(t, n-1)\nprint(f'p-value = {p_value}')\n\np-value = 3.813482862824458e-11\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWir bezeichen dern Wert \\(p\\) als p-Wert. Der p-Wert gibt die Wahrscheinlichkeit an, dass wir die Nullhypothese f√§lschlicherweise ablehnen, wenn sie tats√§chlich wahr ist. Je kleiner der p-Wert, desto unwahrscheinlicher ist es, dass wir die Nullhypothese f√§lschlicherweise ablehnen.\n\n\n\n6.1.2.1 Zweiseitiger Test\nWir sind bisher davon ausgegaben, dass die Intelligenz von Mechantronikstudierenden entweder gleich oder gr√∂√üer als der Durchschnitt ist. Deswegen, haben wir den Ablehnungsbereich nur f√ºr \\(T &gt; t_{\\alpha}\\) definiert und einen einseitigen Test durchgef√ºhrt. Wir k√∂nnten aber auch einen Studiengang untersuchen, √ºber den wir weniger wissen. In diesem Fall, k√∂nnten wir auch einen zweiseitigen Test durchf√ºhren, der einen Ablehnungsbereich f√ºr \\(T &gt; t_{\\alpha}\\) und \\(T &lt; -t_{\\alpha}\\) definiert.\n\nt_alpha = t_table.ppf(1 - alpha/2, n-1)\nprint(f't_alpha = {t_alpha}')\n\nt_alpha = 1.9842169515086827\n\n\n\nx = np.linspace(-7, 7, 100)\ny = t_table.pdf(x, n-1)\nplt.plot(x, y, label='t-Verteilung')\nplt.fill_between(x, 0, y, where=(x &gt; t_alpha), color='red', alpha=0.5, label='Ablehnungsbereich')\nplt.fill_between(x, 0, y, where=(x &lt; -t_alpha), color='red', alpha=0.5)\n\nplt.axvline(t, color='black', linestyle='--', label='T')\nplt.legend()\nplt.title('t-Verteilung und Ablehnungsbereich')\nplt.xlabel('t')\nplt.ylabel('Dichte')\nplt.show()\n\n\n\n\n\n\n\n\nDie Rechte Grenze des Ablehnungsbereichs ist \\(t_{\\alpha} = 1.984\\) wandert nun etwas nach rechts, da die Fl√§che unter der Kurve nun nur noch \\(0.025\\) betr√§gt. Wenn wir zweiseitig testen, testen wir also streger, da wir extremere Werte beobachten m√ºssen, um die Nullhypothese abzulehnen.\nAm Vorgehen √§ndert sich jedoch nichts. Wir berechnen die Teststatistik \\(T\\) und vergleichen sie mit den kritischen Werten \\(t_{\\alpha}\\) und \\(-t_{\\alpha}\\), die wird nun aber aus der zweiseitigen t-Verteilungstabelle ablesen.\n\nDefinition der Hypothesen:\n\n\\(H_0: \\mu_x = 100\\) (Der Mittelwert der Mechatronikstudierenden ist gleich dem Durchschnitt)\n\\(H_1: \\mu_x \\neq 100\\) (Der Mittelwert der Mechatronikstudierenden ist gr√∂√üer als der Durchschnitt)\n\nDefinition des Signifikanzniveaus: \\(\\alpha = 0.05\\)\n\nDie Wahrscheinlichkeit, dass wir die Nullhypothese f√§lschlicherweise ablehnen, soll maximal 5% betragen oder \\(P(H_1 | H_0) \\leq 0.05\\).\n\nBerechnung des Teststatistik \\(t\\):\n\n\n\\(T = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}= 6.50\\)\n\n\nX_bar = np.mean(X)\ns = np.std(X)\nt = (X_bar - mu_0) / (s / np.sqrt(n))\nprint(f't = {t}')\n\nt = 7.291769280165263\n\n\n\nBerechnung des kritischen Wertes \\(t_{\\alpha}\\):\n\n\nNun m√ºssen wir uns entscheiden, ob wir ein- oder zweiseitig testen. In diesem Fall m√ºssen wir einen zweiseitigen Test durchf√ºhren.\nAus der t-Verteilungstabelle k√∂nnen wir f√ºr \\(n-1 = 99\\) Freiheitsgrade und \\(\\alpha = 0.05\\) den kritischen Wert \\(t_{\\alpha} = 1.984\\) ablesen.\n\n\nDie Entscheidung basiert auf \\(T &gt; t_{\\alpha}\\)\n\n\nda \\(6.50 &gt; 1.984\\) ist, lehnen wir die Nullhypothese weiterhin ab.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Tests</span>"
    ]
  },
  {
    "objectID": "statistics/interference_advanced.html#zwei-stichproben-t-test",
    "href": "statistics/interference_advanced.html#zwei-stichproben-t-test",
    "title": "6¬† Tests",
    "section": "6.2 Zwei Stichproben T-Test",
    "text": "6.2 Zwei Stichproben T-Test\nH√§ufig ist man daran interessiert, ob sich die Mittelwerte zweier Stichproben unterscheiden. Auch in diesem Fall kann man einen T-Test einsetzen und die Teststatistik \\(t\\) berechnen (vgl. Fig. ?fig-sec-statistics-hypothesistests-two-sample-t-test)\nVorraussetzung ist jedoch, dass die Varianzen der beiden Stichproben gleich sind und die Stichproben unabh√§ngig voneinander sind.\n {#fig-sec-statistics-hypothesistests-two-sample-t-test}\n\nPr√ºfen der Vorraussetzungen:\n\nDie Varianzen der beiden Stichproben sind gleich\nDie Stichproben sind unabh√§ngig\n\nDefinition der Hypothesen:\n\n\\(H_0: \\mu_1 = \\mu_2\\) (Die Mittelwerte der beiden Stichproben sind gleich)\n\\(H_1: \\mu_1 \\neq \\mu_2\\) (Die Mittelwerte der beiden Stichproben sind ungleich)\n\nBerechnung der Teststatistik \\(t\\):\n\n\n\\(T = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\)\nwobei \\(s_p = \\sqrt{\\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}}\\) die gepoolte Standardabweichung ist.\n\n\nBerechnung des kritischen Wertes \\(t_{\\alpha}\\):\n\n\nAus der t-Verteilungstabelle k√∂nnen wir f√ºr \\(n_1 + n_2 - 2\\) Freiheitsgrade und \\(\\alpha = 0.05\\) den kritischen Wert \\(t_{\\alpha}\\) ablesen.\n\n\nDie Entscheidung basiert auf \\(T &gt; t_{\\alpha}\\) oder \\(T &lt; -t_{\\alpha}\\)\n\n\n\n\n\n\n\nTest auf Differenz der Erwartungswerte\n\n\n\nStatt auf Gleichheit der Mittelwerte zu testen, k√∂nnte man auch auf einen bestimmten Unterschied \\(\\omega_0\\) testen. In diesem Fall, w√ºrde die Nullhypothese \\(H_0: \\mu_1 - \\mu_2 = \\omega_0\\) lauten. Die Teststatistik \\(T\\) w√ºrde dann wie folgt berechnet werden: \\[\nT = \\frac{\\bar{x}_1 - \\bar{x}_2 - \\omega_0}{s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}.\n\\]\nDie Freiheitsgrade der t-Verteilung w√§ren weiterhin \\(n_1 + n_2 - 2\\).\n\n\n\n\n\n\n\n\nTest bei gepaarten Stichproben\n\n\n\nIn manchen F√§llen, haben wir zwei Stichproben, die nicht unabh√§ngig voneinander sind. Ein Beispiel w√§re, wenn wir ein Experiment haben, bei dem wir die gleiche Bauteil vor und nach einer Behandlung messen. In diesem Fall, k√∂nnen wir die Differenz der beiden Stichproben berechnen und dann einen One-Sample T-Test durchf√ºhren.\nZum Beispiel k√∂nnte man ein Verfahren zum H√§rten eines metallischen Bauteils untersuchen. Im Experiment w√ºrde man den H√§rtungsgrad vor und nach der Behandlung messen.\nTabelle: H√§rtungsgrad in HR (Rockwell) vor und nach der Behandlung\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nVor der Behandlung\n49.1\n49.2\n49.3\n49.4\n49.5\n49.6\n49.7\n49.8\n49.0\n50.0\n\n\nNach der Behandlung\n50.2\n50.3\n50.3\n50.2\n50.7\n50.7\n50.8\n50.9\n51.0\n51.1\n\n\nDifferenz\n1.1\n1.1\n1.0\n0.9\n1.2\n1.1\n1.1\n1.1\n1.0\n1.1\n\n\n\nDie durchschnittliche Differenz der Stichproben betr√§gt \\(\\bar{d} = 1.07\\) und die Standardabweichung der Differenz betr√§gt \\(s_d = 0.08\\). Wir k√∂nnten nun einen One-Sample T-Test durchf√ºhren, um zu testen, ob die Behandlung den H√§rtungsgrad signifikant erh√∂ht.\n\nimport numpy as np\nd = np.array([1.1, 1.1, 1.0, 0.9, 1.2, 1.1, 1.1, 1.1, 1.0, 1.1])\nd_bar = np.mean(d)\ns_d = np.std(d)\nn = len(d)\nprint(f'd_bar = {d_bar}, s_d = {s_d}, n = {n}')\nt = d_bar / (s_d / np.sqrt(n))\nprint(f't = {t}')\n\nd_bar = 1.0699999999999998, s_d = 0.07810249675906655, n = 10\nt = 43.323033664571994\n\n\n\nPr√ºfen der Vorraussetzungen:\n\nDie Differenz der Stichproben ist normalverteilt\nDie Stichproben sind gepaart\n\nDefinition der Hypothesen:\n\n\\(H_0: \\mu = 0\\) (Die Behandlung hat keinen Effekt)\n\\(H_1: \\mu &gt; 0\\) (Die Behandlung erh√∂ht den H√§rtungsgrad)\n\nBerechnung der Teststatistik \\(t\\):\n\n\\(T = \\frac{\\bar{x}}{s / \\sqrt{n}} = \\frac{1.07}{0.08/\\sqrt{10}} = 43.2\\)\n\nBerechnung des kritischen Wertes \\(t_{\\alpha}\\):\n\n\nAus der t-Verteilungstabelle k√∂nnen wir f√ºr \\(n-1 = 9\\) Freiheitsgrade und \\(\\alpha = 0.05\\) den kritischen Wert \\(t_{\\alpha} = 1.833\\) ablesen.\n\n::: {#1a5ea724 .cell .styled-output execution_count=10} ``` {.python .cell-code} from scipy.stats import t as t_table alpha = 0.05\nt_alpha = t_table.ppf(1 - alpha, n-1) print(f‚Äôt_alpha = {t_alpha}‚Äô) ```\n::: {.cell-output .cell-output-stdout} t_alpha = 1.8331129326536335 ::: :::\n\nEntscheidung basiert auf \\(T &gt; t_{\\alpha}\\)\n\n\nDa \\(43.2 &gt; 1.833\\) ist, lehnen wir die Nullhypothese ab. Die Behandlung hat den H√§rtungsgrad signifikant erh√∂ht.\n\n\n6.3 Chi-Quadrat-Test\nNicht alle Test basieren auf der Normalverteilung oder einer T-Verteilung. Einige Sch√§tzer folgen einer anderen Verteilung, wie z.B. der Chi-Quadrat(\\(\\mathcal{X}^2\\))-Verteilung. Der Chi-Quadrat-Test ist ein statistischer Test, der f√ºr verschiedene Fragestellungen eingesetzt werden kann.\n\nVerteilungstest (auch Anpassungstest genannt): Hier wird gepr√ºft, ob vorliegende Daten auf eine bestimmte Weise verteilt sind.\nUnabh√§ngigkeitstest: Hier wird gepr√ºft, ob zwei Merkmale stochastisch unabh√§ngig sind.\nHomogenit√§tstest: Hier wird gepr√ºft, ob zwei oder mehr Stichproben derselben Verteilung bzw. einer homogenen Grundgesamtheit entstammen.\n\n\n6.3.1 Verteilungs-Test\nDer Chi-Quadrat-Test kann verwendet werden, um zu pr√ºfen, ob zwei oder mehr Stichproben derselben Verteilung entstammen. Als Beispiel mit mechantronischem Anwendungsfall k√∂nnten wir die Qualit√§t eines Lieferanten mit denen unserer bisherigen vergleichen. Wir k√∂nnen dabei die H√§ufigkeiten von OK-Teilen und verschiedenen Arten von Fehlern z√§hlen und dann pr√ºfen, ob der Lieferant die gleiche Qualit√§t liefert.\n\n\n\nLieferant\nOK\nFehler 1\nFehler 2\nFehler 3\n\n\n\n\nA\n100\n10\n5\n5\n\n\nBisherige\n0.81\n0.08\n0.06\n0.05\n\n\n\nWir k√∂nnen die beobachteten H√§ufigkeiten in einer Kontingenztafel darstellen und dann die erwarteten H√§ufigkeiten berechnen, wenn die beiden Lieferanten die gleiche Qualit√§t liefern w√ºrden.\n\n\n\nLieferant\nOK\nFehler 1\nFehler 2\nFehler 3\nSumme\n\n\n\n\nA (Observed)\n100\n10\n5\n5\n120\n\n\nBisherige (Expected)\n97.2\n9.6\n7.2\n6.0\n120\n\n\n\n\n√úberpr√ºfen der Vorraussetzungen:\n\nDie Stichproben sind unabh√§ngig\nDie erwarteten H√§ufigkeiten sind gr√∂√üer als 5\n\nDefintion der Hypothesen:\n\n\\(H_0\\): Der neue Lieferant liefert die gleiche Qualit√§t (Identische Verteilung)\n\\(H_1\\): Die neue Lieferanten liefert abweichende Qualit√§t\n\n\n\n\n\n\nBerechnung der Teststatistik \\(Z\\)\n\n\nDie Test-Statistik \\(Z\\) wird √ºber die Summe der quadrierten Differenzen zwischen beobachteten \\(O_i\\) und erwarteten H√§ufigkeiten \\(E_i\\) berechnet. Dabei liegt der Ablehnungsbereich immer rechts. \\[\nZ = \\sum{\\frac{(O_i - E_i)^2}{E_i}}\n\\]\n\n\nimport numpy as np\nfrom scipy.stats import chi2\nO = np.array([100, 10, 5, 5])\nE = np.array([0.81, 0.08, 0.06, 0.05]) * np.sum(O)\n\nprint(f'O = {O}, E = {E}')\n\nZ = np.sum((O - E)**2 / E)\nprint(f'Z = {Z}')\n\nO = [100  10   5   5], E = [97.2  9.6  7.2  6. ]\nZ = 0.9362139917695469\n\n\n\nBerechnung des kritischen Wertes \\(Z_{\\alpha}\\):\n\n\nDer kritische Wert \\(Z_{\\alpha}\\) wird aus der Chi-Quadrat-Verteilungstabelle abgelesen. Die Anzahl der Freiheitsgrade \\(df\\) entspricht der Anzahl der Kategorien minus 1.\nIn diesem Fall haben wir 4 Kategorien, also \\(df = 4 - 1 = 3\\) Freiheitsgrade.\nF√ºr \\(\\alpha = 0.05\\) betr√§gt der kritische Wert \\(Z_{\\alpha} = 7.815\\).\n\nDie Chi-Quadrat-Verteilung ist asymmetrisch und hat eine hohe Wahrscheinlichkeit f√ºr Werte nahe 0. Abbildung ?fig-sec-statistics-hypothesistests-chi2-distribution zeigt die Chi-Quadrat-Verteilung und den Ablehnungsbereich f√ºr \\(\\alpha = 0.05\\). Der Ablehnungsbereich ist rot markiert.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nx = np.linspace(0, 20, 100)\ny = chi2.pdf(x, 3)\nplt.plot(x, y, label='Chi-Quadrat-Verteilung')\nplt.fill_between(x, 0, y, where=(x &gt; 7.815), color='red', alpha=0.5, label='Ablehnungsbereich')\n\nplt.axvline(Z, color='black', linestyle='--', label='Z')\nplt.legend()\nplt.title('Chi-Quadrat-Verteilung und Ablehnungsbereich')\nplt.xlabel('Z')\nplt.ylabel('Dichte')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nDie Entscheidung basiert auf \\(Z &gt; Z_{\\alpha}\\)\n\n\nDa \\(Z = 0.936 &lt; 7.815 =  Z_{\\alpha}\\) ist, k√∂nnen wir die Nullhypothese nicht ablehnen. Der neue Lieferant liefert die gleiche Qualit√§t wie der bisherige Lieferant.\n\n\n\n\n\n\n\nTest auf bestimmte Verteilung\n\n\n\nAnstelle einer Empirischen Grundgesamtheit, k√∂nnten wir auch eine bestimmte Verteilung annehmen und pr√ºfen, ob die Daten dieser Verteilung entsprechen. In diesem Fall, w√ºrden wir die erwarteten H√§ufigkeiten basierend auf der angenommenen Verteilung berechnen und dann den Chi-Quadrat-Test durchf√ºhren.\n\n\n\n\n6.3.2 Unabh√§ngigkeitstest\nDer Chi-Quadrat-Test kann auch verwendet werden, um zu pr√ºfen, ob zwei Merkmale \\(X\\) mit \\(m\\) Kategorien und \\(Y\\) mit \\(k\\) Kategorien stochastisch unabh√§ngig sind.\nHierzu kann man eine Kontingenztafel erstellen, die die H√§ufigkeiten der Kombinationen der beiden Merkmale enth√§lt:\n\n\n\n\n\\( Y_1 \\)\n\\( Y_2 \\)\n\\(\\dots\\)\n\\( Y_r \\)\n\\( \\sum \\)\n\n\n\\( X_1 \\)\n\\( n_{11} \\)\n\\( n_{12} \\)\n\\(\\dots\\)\n\\( n_{1r} \\)\n\\( n_{1.} \\)\n\n\n\\( X_2 \\)\n\\( n_{21} \\)\n\\( n_{22} \\)\n\\(\\dots\\)\n\\( n_{2r} \\)\n\\( n_{2.} \\)\n\n\n\\( \\vdots \\)\n\\( \\vdots \\)\n\\( \\vdots \\)\n\\( \\ddots \\)\n\\( \\vdots \\)\n\\( \\vdots \\)\n\n\n\\( X_m \\)\n\\( n_{m1} \\)\n\\( n_{m2} \\)\n\\(\\dots\\)\n\\( n_{mr} \\)\n\\( n_{m.} \\)\n\n\n\\( \\sum \\)\n\\( n_{.1} \\)\n\\( n_{.2} \\)\n\\(\\dots\\)\n\\( n_{.r} \\)\n\\( n \\)\n\n\n\nF√ºr die Zeilen und Spalten der Kontingenztafel werden die Randh√§ufigkeiten berechnet:\n\\[\nn_{i.} = \\sum_{j=1}^{r} n_{ij} \\quad \\text{und} \\quad n_{.j} = \\sum_{i=1}^{m} n_{ij}\n\\]\nDie erwarteten H√§ufigkeiten \\(p_{ij}\\) f√ºr jede Zelle der Kontingenztafel werden berechnet als das Produkt der Randh√§ufigkeiten geteilt durch die Gesamtanzahl der Beobachtungen:\n\\[\np_{ij} = \\frac{n_{i.} \\cdot n_{.j}}{n},\n\\]\nund die relativen Randh√§ufigkeiten als\n\\[\np_{i.} = \\frac{n_{i.}}{n} \\quad \\text{und} \\quad p_{.j} = \\frac{n_{.j}}{n}.\n\\]\nAus den Rechenregeln der Wahrscheinlichkeit in Kapitel Chapter 3 folgt f√ºr unabh√§ngige Ereignisse, dass die Wahrscheinlichkeit des gemeinsamen Eintretens der beiden Ereignisse gleich dem Produkt der Einzelwahrscheinlichkeiten ist:\n\\[\nP(A \\cap B) = P(A) \\cdot P(B).\n\\]\nF√ºr die Kontingenztafel bedeutet dies, dass die erwarteten H√§ufigkeiten \\(p_{ij}\\) gleich dem Produkt der Randh√§ufigkeiten \\(p_{i.}\\) und \\(p_{.j}\\) sind, wenn die beiden Merkmale unabh√§ngig voneinander sind. Wenn dies f√ºr alle Zellen nahezu zutrifft, dann k√∂nnen wir die Nullhypothese annehmen, dass die beiden Merkmale unabh√§ngig voneinander sind.\nDie Testgr√∂√üe \\(\\mathcal{X}^2\\) wird dann berechnet als die Summe der quadrierten Differenzen zwischen beobachteten und erwarteten H√§ufigkeiten, normiert durch die erwarteten H√§ufigkeiten:\n\\[\n\\mathcal{X}^2 = \\sum_{i=1}^{m} \\sum_{j=1}^{r} \\frac{(n_{ij} - p_{ij})^2}{p_{ij}}.\n\\]\nDie Testgr√∂√üe \\(\\mathcal{X}^2\\) folgt einer Chi-Quadrat-Verteilung mit \\((m-1)(r-1)\\) Freiheitsgraden.\n\n\n6.3.3 Homogenit√§tstest\nAuf √§hnliche Weise kann der Chi-Quadrat-Test auch verwendet werden, um zu pr√ºfen, ob zwei oder mehr Stichproben derselben Verteilung bzw. einer homogenen Grundgesamtheit entstammen. In diesem Fall wird eine Kontingenztafel erstellt, die die H√§ufigkeiten der Kombinationen der Stichproben enth√§lt. Die Testgr√∂√üe \\(\\mathcal{X}^2\\) wird dann berechnet als die Summe der quadrierten Differenzen zwischen beobachteten und erwarteten H√§ufigkeiten, normiert durch die erwarteten H√§ufigkeiten.\n\n\n\n6.4 Kolmogorov-Smirnov-Test\nDer \\(\\mathcal{X}^2\\)-Test funktionier auf der Basis von H√§ufigkeiten und ist daher nur f√ºr diskrete Daten geeignet. Der Kolmogorov-Smirnov-Test hingegen kann auch f√ºr kontinuierliche Daten verwendet werden. Der Kolmogorov-Smirnov-Test, kann z.B. angewendet werden, um zu pr√ºfen ob,\n\nzwei Zufallsvariablen eine identische Verteilung besitzen oder\neine Zufallsvariable einer zuvor angenommenen Wahrscheinlichkeitsverteilung folgt.\n\n\n\n6.5 √úbersicht √ºber Statistische Tests\nGrunds√§tzlich setzen wir statischtische Tests ein, wenn wir eine Hypothese √ºber eine Grundgesamtheit aufstellen und diese Hypothese auf Basis einer Stichprobe √ºberpr√ºfen wollen. Die Hypothese wird in eine Nullhypothese \\(H_0\\) und eine Alternativhypothese \\(H_1\\) aufgeteilt. Die Nullhypothese ist die Hypothese, die wir widerlegen wollen, w√§hrend die Alternativhypothese die Hypothese ist, die wir annehmen, wenn die Nullhypothese widerlegt wird.\nTest erm√∂glichen und zu unterschieden, ob die durch uns auf Grund der Stichprobe getroffene Aussage auf die Grundgesamtheit √ºbertragbar oder m√∂glicherweise nur Zufall sind.\n\n\n\n\n\n\n\nInductiveload, based on original work by Jhguch. n.d. ‚ÄúTwo Sample t-Test.‚Äù https://commons.wikimedia.org/wiki/File:Two_sample_ttest.svg.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Tests</span>"
    ]
  },
  {
    "objectID": "statistics/interference_advanced.html#chi-quadrat-test",
    "href": "statistics/interference_advanced.html#chi-quadrat-test",
    "title": "6¬† Tests",
    "section": "6.3 Chi-Quadrat-Test",
    "text": "6.3 Chi-Quadrat-Test\nNicht alle Test basieren auf der Normalverteilung oder einer T-Verteilung. Einige Sch√§tzer folgen einer anderen Verteilung, wie z.B. der Chi-Quadrat(\\(\\mathcal{X}^2\\))-Verteilung. Der Chi-Quadrat-Test ist ein statistischer Test, der f√ºr verschiedene Fragestellungen eingesetzt werden kann.\n\nVerteilungstest (auch Anpassungstest genannt): Hier wird gepr√ºft, ob vorliegende Daten auf eine bestimmte Weise verteilt sind.\nUnabh√§ngigkeitstest: Hier wird gepr√ºft, ob zwei Merkmale stochastisch unabh√§ngig sind.\nHomogenit√§tstest: Hier wird gepr√ºft, ob zwei oder mehr Stichproben derselben Verteilung bzw. einer homogenen Grundgesamtheit entstammen.\n\n\n6.3.1 Verteilungs-Test\nDer Chi-Quadrat-Test kann verwendet werden, um zu pr√ºfen, ob zwei oder mehr Stichproben derselben Verteilung entstammen. Als Beispiel mit mechantronischem Anwendungsfall k√∂nnten wir die Qualit√§t eines Lieferanten mit denen unserer bisherigen vergleichen. Wir k√∂nnen dabei die H√§ufigkeiten von OK-Teilen und verschiedenen Arten von Fehlern z√§hlen und dann pr√ºfen, ob der Lieferant die gleiche Qualit√§t liefert.\n\n\n\nLieferant\nOK\nFehler 1\nFehler 2\nFehler 3\n\n\n\n\nA\n100\n10\n5\n5\n\n\nBisherige\n0.81\n0.08\n0.06\n0.05\n\n\n\nWir k√∂nnen die beobachteten H√§ufigkeiten in einer Kontingenztafel darstellen und dann die erwarteten H√§ufigkeiten berechnen, wenn die beiden Lieferanten die gleiche Qualit√§t liefern w√ºrden.\n\n\n\nLieferant\nOK\nFehler 1\nFehler 2\nFehler 3\nSumme\n\n\n\n\nA (Observed)\n100\n10\n5\n5\n120\n\n\nBisherige (Expected)\n97.2\n9.6\n7.2\n6.0\n120\n\n\n\n\n√úberpr√ºfen der Vorraussetzungen:\n\nDie Stichproben sind unabh√§ngig\nDie erwarteten H√§ufigkeiten sind gr√∂√üer als 5\n\nDefintion der Hypothesen:\n\n\\(H_0\\): Der neue Lieferant liefert die gleiche Qualit√§t (Identische Verteilung)\n\\(H_1\\): Die neue Lieferanten liefert abweichende Qualit√§t\n\n\n\n\n\n\nBerechnung der Teststatistik \\(Z\\)\n\n\nDie Test-Statistik \\(Z\\) wird √ºber die Summe der quadrierten Differenzen zwischen beobachteten \\(O_i\\) und erwarteten H√§ufigkeiten \\(E_i\\) berechnet. Dabei liegt der Ablehnungsbereich immer rechts. \\[\nZ = \\sum{\\frac{(O_i - E_i)^2}{E_i}}\n\\]\n\n\nimport numpy as np\nfrom scipy.stats import chi2\nO = np.array([100, 10, 5, 5])\nE = np.array([0.81, 0.08, 0.06, 0.05]) * np.sum(O)\n\nprint(f'O = {O}, E = {E}')\n\nZ = np.sum((O - E)**2 / E)\nprint(f'Z = {Z}')\n\nO = [100  10   5   5], E = [97.2  9.6  7.2  6. ]\nZ = 0.9362139917695469\n\n\n\nBerechnung des kritischen Wertes \\(Z_{\\alpha}\\):\n\n\nDer kritische Wert \\(Z_{\\alpha}\\) wird aus der Chi-Quadrat-Verteilungstabelle abgelesen. Die Anzahl der Freiheitsgrade \\(df\\) entspricht der Anzahl der Kategorien minus 1.\nIn diesem Fall haben wir 4 Kategorien, also \\(df = 4 - 1 = 3\\) Freiheitsgrade.\nF√ºr \\(\\alpha = 0.05\\) betr√§gt der kritische Wert \\(Z_{\\alpha} = 7.815\\).\n\nDie Chi-Quadrat-Verteilung ist asymmetrisch und hat eine hohe Wahrscheinlichkeit f√ºr Werte nahe 0. Abbildung ?fig-sec-statistics-hypothesistests-chi2-distribution zeigt die Chi-Quadrat-Verteilung und den Ablehnungsbereich f√ºr \\(\\alpha = 0.05\\). Der Ablehnungsbereich ist rot markiert.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nx = np.linspace(0, 20, 100)\ny = chi2.pdf(x, 3)\nplt.plot(x, y, label='Chi-Quadrat-Verteilung')\nplt.fill_between(x, 0, y, where=(x &gt; 7.815), color='red', alpha=0.5, label='Ablehnungsbereich')\n\nplt.axvline(Z, color='black', linestyle='--', label='Z')\nplt.legend()\nplt.title('Chi-Quadrat-Verteilung und Ablehnungsbereich')\nplt.xlabel('Z')\nplt.ylabel('Dichte')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nDie Entscheidung basiert auf \\(Z &gt; Z_{\\alpha}\\)\n\n\nDa \\(Z = 0.936 &lt; 7.815 =  Z_{\\alpha}\\) ist, k√∂nnen wir die Nullhypothese nicht ablehnen. Der neue Lieferant liefert die gleiche Qualit√§t wie der bisherige Lieferant.\n\n\n\n\n\n\n\nTest auf bestimmte Verteilung\n\n\n\nAnstelle einer Empirischen Grundgesamtheit, k√∂nnten wir auch eine bestimmte Verteilung annehmen und pr√ºfen, ob die Daten dieser Verteilung entsprechen. In diesem Fall, w√ºrden wir die erwarteten H√§ufigkeiten basierend auf der angenommenen Verteilung berechnen und dann den Chi-Quadrat-Test durchf√ºhren.\n\n\n\n\n6.3.2 Unabh√§ngigkeitstest\nDer Chi-Quadrat-Test kann auch verwendet werden, um zu pr√ºfen, ob zwei Merkmale \\(X\\) mit \\(m\\) Kategorien und \\(Y\\) mit \\(k\\) Kategorien stochastisch unabh√§ngig sind.\nHierzu kann man eine Kontingenztafel erstellen, die die H√§ufigkeiten der Kombinationen der beiden Merkmale enth√§lt:\n\n\n\n\n\\( Y_1 \\)\n\\( Y_2 \\)\n\\(\\dots\\)\n\\( Y_r \\)\n\\( \\sum \\)\n\n\n\\( X_1 \\)\n\\( n_{11} \\)\n\\( n_{12} \\)\n\\(\\dots\\)\n\\( n_{1r} \\)\n\\( n_{1.} \\)\n\n\n\\( X_2 \\)\n\\( n_{21} \\)\n\\( n_{22} \\)\n\\(\\dots\\)\n\\( n_{2r} \\)\n\\( n_{2.} \\)\n\n\n\\( \\vdots \\)\n\\( \\vdots \\)\n\\( \\vdots \\)\n\\( \\ddots \\)\n\\( \\vdots \\)\n\\( \\vdots \\)\n\n\n\\( X_m \\)\n\\( n_{m1} \\)\n\\( n_{m2} \\)\n\\(\\dots\\)\n\\( n_{mr} \\)\n\\( n_{m.} \\)\n\n\n\\( \\sum \\)\n\\( n_{.1} \\)\n\\( n_{.2} \\)\n\\(\\dots\\)\n\\( n_{.r} \\)\n\\( n \\)\n\n\n\nF√ºr die Zeilen und Spalten der Kontingenztafel werden die Randh√§ufigkeiten berechnet:\n\\[\nn_{i.} = \\sum_{j=1}^{r} n_{ij} \\quad \\text{und} \\quad n_{.j} = \\sum_{i=1}^{m} n_{ij}\n\\]\nDie erwarteten H√§ufigkeiten \\(p_{ij}\\) f√ºr jede Zelle der Kontingenztafel werden berechnet als das Produkt der Randh√§ufigkeiten geteilt durch die Gesamtanzahl der Beobachtungen:\n\\[\np_{ij} = \\frac{n_{i.} \\cdot n_{.j}}{n},\n\\]\nund die relativen Randh√§ufigkeiten als\n\\[\np_{i.} = \\frac{n_{i.}}{n} \\quad \\text{und} \\quad p_{.j} = \\frac{n_{.j}}{n}.\n\\]\nAus den Rechenregeln der Wahrscheinlichkeit in Kapitel Chapter 3 folgt f√ºr unabh√§ngige Ereignisse, dass die Wahrscheinlichkeit des gemeinsamen Eintretens der beiden Ereignisse gleich dem Produkt der Einzelwahrscheinlichkeiten ist:\n\\[\nP(A \\cap B) = P(A) \\cdot P(B).\n\\]\nF√ºr die Kontingenztafel bedeutet dies, dass die erwarteten H√§ufigkeiten \\(p_{ij}\\) gleich dem Produkt der Randh√§ufigkeiten \\(p_{i.}\\) und \\(p_{.j}\\) sind, wenn die beiden Merkmale unabh√§ngig voneinander sind. Wenn dies f√ºr alle Zellen nahezu zutrifft, dann k√∂nnen wir die Nullhypothese annehmen, dass die beiden Merkmale unabh√§ngig voneinander sind.\nDie Testgr√∂√üe \\(\\mathcal{X}^2\\) wird dann berechnet als die Summe der quadrierten Differenzen zwischen beobachteten und erwarteten H√§ufigkeiten, normiert durch die erwarteten H√§ufigkeiten:\n\\[\n\\mathcal{X}^2 = \\sum_{i=1}^{m} \\sum_{j=1}^{r} \\frac{(n_{ij} - p_{ij})^2}{p_{ij}}.\n\\]\nDie Testgr√∂√üe \\(\\mathcal{X}^2\\) folgt einer Chi-Quadrat-Verteilung mit \\((m-1)(r-1)\\) Freiheitsgraden.\n\n\n6.3.3 Homogenit√§tstest\nAuf √§hnliche Weise kann der Chi-Quadrat-Test auch verwendet werden, um zu pr√ºfen, ob zwei oder mehr Stichproben derselben Verteilung bzw. einer homogenen Grundgesamtheit entstammen. In diesem Fall wird eine Kontingenztafel erstellt, die die H√§ufigkeiten der Kombinationen der Stichproben enth√§lt. Die Testgr√∂√üe \\(\\mathcal{X}^2\\) wird dann berechnet als die Summe der quadrierten Differenzen zwischen beobachteten und erwarteten H√§ufigkeiten, normiert durch die erwarteten H√§ufigkeiten.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Tests</span>"
    ]
  },
  {
    "objectID": "statistics/interference_advanced.html#kolmogorov-smirnov-test",
    "href": "statistics/interference_advanced.html#kolmogorov-smirnov-test",
    "title": "6¬† Tests",
    "section": "6.4 Kolmogorov-Smirnov-Test",
    "text": "6.4 Kolmogorov-Smirnov-Test\nDer \\(\\mathcal{X}^2\\)-Test funktionier auf der Basis von H√§ufigkeiten und ist daher nur f√ºr diskrete Daten geeignet. Der Kolmogorov-Smirnov-Test hingegen kann auch f√ºr kontinuierliche Daten verwendet werden. Der Kolmogorov-Smirnov-Test, kann z.B. angewendet werden, um zu pr√ºfen ob,\n\nzwei Zufallsvariablen eine identische Verteilung besitzen oder\neine Zufallsvariable einer zuvor angenommenen Wahrscheinlichkeitsverteilung folgt.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Tests</span>"
    ]
  },
  {
    "objectID": "statistics/interference_advanced.html#√ºbersicht-√ºber-statistische-tests",
    "href": "statistics/interference_advanced.html#√ºbersicht-√ºber-statistische-tests",
    "title": "6¬† Tests",
    "section": "6.5 √úbersicht √ºber Statistische Tests",
    "text": "6.5 √úbersicht √ºber Statistische Tests\nGrunds√§tzlich setzen wir statischtische Tests ein, wenn wir eine Hypothese √ºber eine Grundgesamtheit aufstellen und diese Hypothese auf Basis einer Stichprobe √ºberpr√ºfen wollen. Die Hypothese wird in eine Nullhypothese \\(H_0\\) und eine Alternativhypothese \\(H_1\\) aufgeteilt. Die Nullhypothese ist die Hypothese, die wir widerlegen wollen, w√§hrend die Alternativhypothese die Hypothese ist, die wir annehmen, wenn die Nullhypothese widerlegt wird.\nTest erm√∂glichen und zu unterschieden, ob die durch uns auf Grund der Stichprobe getroffene Aussage auf die Grundgesamtheit √ºbertragbar oder m√∂glicherweise nur Zufall sind.",
    "crumbs": [
      "Statistik",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Tests</span>"
    ]
  },
  {
    "objectID": "statistics/tutorial_2.html",
    "href": "statistics/tutorial_2.html",
    "title": "Tutorial 2: Understanding Patterns in Grid Loads",
    "section": "",
    "text": "Objective\nAgain, we work witht the Global Energy Forecasting Competition Hong, Pinson, and Fan (2014) data, you have cleaned before. This time, we want to make a deeper analysis of the data and try to find patterns in the data. In particular, we want to find out about the following characteristics of the data:",
    "crumbs": [
      "Statistik",
      "Tutorial 2: Understanding Patterns in Grid Loads"
    ]
  },
  {
    "objectID": "statistics/tutorial_2.html#objective",
    "href": "statistics/tutorial_2.html#objective",
    "title": "Tutorial 2: Understanding Patterns in Grid Loads",
    "section": "",
    "text": "Weekday Effects: Grid load varies by day of the wee, as this changes the behavior of private households and industries. You want to find out, whether there is a difference in the load between weekdays and weekends. This is important, for later forecasting steps, as you might want to include this information in your model or use different models for weekdays and weekends.\n\nVisualize the load of each Weekday in a Boxplot, Violin-Plot or a Histogram and color the weekends differently.\nAdd holidays to the plot and see, if there is a difference in the load on holidays.\nMake a statistical test to see, if the load is different between weekdays and weekends. You can use either a t-test to compare the means or the Kolmogorov-Smirnov (KS) Test to compare the distributions.\nIs there a significant difference (\\(\\alpha&gt;0.05\\)) in the load on weekends?\n\nExtreme Values: Extreme values are values that are far away from the mean of the data. In electricity grids, peak loads, which are the maximum load in a certain time period, are important to know, as they determine the capacity of the grid. If the peak load get to high, this might lead to equipment failure and blackouts. You want to find out, if there are extreme values in the data and how they are distributed.\n\nIdentify the peak loads for each day in the data and visualize them in a histogram.\nFit a Generalized Extreme Value Distribution to the extreme values and compare it to a fitted normal distribution in a plot. Use (from scipy.stats import genextreme)\nHow would You use this distribution to predict the probability peak loads higher, than a certain threshold?\n\n\n\n\n\n\nHong, Tao, Pierre Pinson, and Shu Fan. 2014. ‚ÄúGlobal Energy Forecasting Competition 2012.‚Äù International Journal of Forecasting 30 (2): 357‚Äì63. https://doi.org/https://doi.org/10.1016/j.ijforecast.2013.07.001.",
    "crumbs": [
      "Statistik",
      "Tutorial 2: Understanding Patterns in Grid Loads"
    ]
  },
  {
    "objectID": "regression/index.html",
    "href": "regression/index.html",
    "title": "Regressions-Analyse",
    "section": "",
    "text": "Im Allgemeinen kann die Regressionsanalyse als eine Sammlung von Werkzeugen verstanden werden, die dazu verwendet werden, eine Beziehung zwischen einer abh√§ngigen Variable \\(Y\\)$ (auch Zielvariable, Antwortvariable oder Label genannt) und der unabh√§ngigen Variable \\(X\\) (auch Regressor, Pr√§diktoren, Kovariaten, erkl√§rende Variable oder Feature genannt) zu sch√§tzen oder festzustellen.\nWenn wir eine Regressionsfunktion \\(f\\) als Modell f√ºr die Beziehung zwischen \\(X\\) und \\(Y\\) annehmen, k√∂nnen wir die Regressionsanalyse als die Suche nach den Parametern \\(\\theta\\) verstehen, die die Funktion \\(f\\) am besten an die Daten anpassen.\nDas Problem der Regressionsanalyse kann also wie folgt formuliert werden: \\[\nY = f(X, \\theta) + \\epsilon,\n\\]\nwobei \\(\\theta\\) durch die Optimierung f√ºr eine gute Anpassung von \\(f\\) an die Daten gefunden wird. In der Regel erhalten wir dabei einen Fehlerterm \\(\\epsilon\\), der ‚Äì wenn wir Gl√ºck haben ‚Äì normalverteilt mit einem Erwartungswert von null und konstanter Varianz ist.\nRegessionen sind ein m√§chtiges Werkzeug f√ºr die Interpetation von Daten und die Vorhersage von Werten. In der Praxis gibt es viele verschiedene Arten von Regressionsmodellen, die sich in ihrer Komplexit√§t und den Annahmen, die sie machen, unterscheiden. In diesem Kapitel werden wir uns auf die lineare Regression konzentrieren, die eine der einfachsten und am h√§ufigsten verwendeten Formen der Regressionsanalyse ist und sich f√ºr Zusammenh√§nge zwischen einer abh√§ngigen und einer unabh√§ngigen Variablen mit intervallskalierten Daten eignet.\nZuerst f√ºhren wir eine einfache Lineare Regression mit einer unabh√§ngigen Variable durch. Anschlie√üend erweitern wir das Modell auf mehrere unabh√§ngige und auch kategorische Variablen. Hierzu nutzen wird die Matrix-Schreibweise.\nErweiterte Themen Intercepts und Regularisierung\nDiskussion von Statistischem Lernen und Resampling\nLogistische Regression als Beispiel f√ºr Klassifikation",
    "crumbs": [
      "Regressions-Analyse"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "This concludes our introduction to the basis of data science with a focus on engineering topics.\nThese notes introduced the main concepts with a clear focus of accurate mathematical representation and close illustration with programmatic examples.\nIf we need to sum up the main concept that is present in large sections of these notes it is that the correct representation is key in finding good concepts of computer based processing, To this extend the second key concept is to make sure the concepts can be handled via a computer and the student, so theory and programmatic application go hand in hand.\nBoth concepts stay true if we move to more evolved data science methods in further classes.",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Carroll, Lewis. 2015. Alice‚Äôs Adventures in Wonderland\nUnfolded.\n\n\nDATAtab. retrieved 2025. ‚ÄúKorrelationskoeffizient Tutorial\nImage.‚Äù https://datatab.de/assets/tutorial/Korrelationskoeffizient.png.\n\n\nDiez, David M., Christopher D. Barr, and Mine Cetinkaya-Rundel. 2019.\nOpenIntro Statistics. 4th ed. Boston, MA: OpenIntro. \\url{https://www.openintro.org/stat/textbook.php?stat_book=os}.\n\n\nFrisch, Max. 1957. Homo Faber. Ein Bericht. Frankfurt am Main:\nSuhrkamp.\n\n\nHong, Tao, Pierre Pinson, and Shu Fan. 2014. ‚ÄúGlobal Energy\nForecasting Competition 2012.‚Äù International Journal of\nForecasting 30 (2): 357‚Äì63. https://doi.org/https://doi.org/10.1016/j.ijforecast.2013.07.001.\n\n\nHyndman, RJ. 2018. Forecasting: Principles and Practice.\nOTexts.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, et al.\n2013. An Introduction to Statistical Learning. Vol. 112.\nSpringer.\n\n\nKozyrkov, Cassie. 2018. ‚ÄúWhat on Earth Is Data Science?‚Äù\nmedium.com. \\url{https://kozyrkov.medium.com/what-on-earth-is-data-science-eb1237d8cb37}.\n\n\nMatthes, Eric. 2023. Python Crash Course - a Hands-on, Project-Based\nIntroduction to Programming. 3rd ed. No Starch Press. https://ehmatthes.github.io/pcc_3e/.\n\n\nRedAndr and Mikhail Ryazanov. 2011. ‚ÄúSatirical diagram illustrating the influence of pirates\ndecreasing on global warming as per Pastafarian beliefs.‚Äù\nhttps://commons.wikimedia.org/wiki/File:PiratesVsTemp(en).svg.\n\n\nShearer, Colin. 2000. ‚ÄúThe CRISP-DM Model: The New Blueprint for\nData Mining.‚Äù Journal of Data Warehousing 5 (4): 13‚Äì22.\n\n\nWickham, Hadley. 2014. ‚ÄúTidy Data.‚Äù Journal of\nStatistical Software 59: 1‚Äì23.\n\n\nWikimedia Commons contributors. retrieved 2025. ‚ÄúCRISP-DM\nProcess Diagram.‚Äù https://upload.wikimedia.org/wikipedia/commons/b/b9/CRISP-DM_Process_Diagram.png.",
    "crumbs": [
      "References"
    ]
  }
]