---

marp: true
theme: beams
author: Julian Huber
size: 16:9
footer: Julian Huber - Data Science
headingDivider: 2

# Strg+[ ] for Options
---

<!-- paginate: true -->



# 2 Data Science

<!-- _class: title -->

Dr. Julian Huber
*Management Center Innsbruck*




## 2.6 Learning

<center>

![h:500](images/UbersichtDS.JPG)

</center>

---

## 2.6.1 Gradient Descent

### üéØ Learning objectives

You will be able to 
* explain how gradient descent works describing it in an pseudo algorithm
* spot common problems to gradient descend and can provide solutions
* use learning curves to identify over-fitting

---

### Cost Functions

![bg right:30% w:400](images/ErrorPlaneRegression.png)

- Most models train, by minimizing a cost function on the training data to get good estimators for the parameters $\hat{Œ≤}_i$
 $$\hat{y} ‚âà \hat{Œ≤}_0 + \hat{Œ≤}_1 \cdot x_1$$

- A typical cost function is the $RSS$ (i.e., the sum of squared prediction errors) over all $n$ observations
$$RSS=\sum^n_{j=1}(y_j-\hat{y_j})^2$$

---


#### Minimizing Cost Functions

* We saw that there is an analytical solution to minimize $RSS$
    * so we can just calculate the optimal parameters $\beta_i$ 
* But we could also calculate the result of the cost function for different parameter values to approach a minimum in the cost function

![h:400](images/0_MJ4EJ3I79E6PFUO4.gif)

###### https://medium.com/analytics-vidhya/cost-function-explained-in-less-than-5-minutes-c5d8a44b918c

---

#### üß† Minimizing Complex Cost Functions

- $\theta_i$ ... parameters in a model, that is no linear regression (think $\beta_i$)
- $J(\vec{\theta})$ ... cost function in any machine learning model (think $RSS$) 

<center>

![](images/0_DpI20ifcOhit4Uvh.gif)

</center>

###### https://medium.com/analytics-vidhya/cost-function-explained-in-less-than-5-minutes-c5d8a44b918c

* We aim to find a global minimum
* How can we do this?


---

### Gradient Descent Heuristic

<!-- _class: none -->

![bg right:45% h:500](images/tex/gradient_dec.svg)

1) Start at a random position in the cost function
2) Calculate the slope of the cost function at the current position
3) If the slope is negative, move to the right
4) Repeat


---

üß†

- Example with only one parameter $\theta$ (`theta`) and
- only one local minimum of the cost function $J$ that is the global minimum, where $J' = 0$
    ```
    start at random position theta in the cost function

    while J'(theta)>0:
        Calculate the slope of J: J'(theta) at the current position
        If slope is negative:
            move theta to right by step width alpha
        If slope is positive:
            move theta to left by step width alpha
    ```
* the step with $\alpha$ is called learning rate

---


#### Gradients

- multi-dimensional derivatives
- gives us the direction of the steepest incline
- which weight / parameter $\theta$ of the model has the biggest influence on the cost

$$\nabla J = \frac{\partial J}{\partial \theta} = \begin{bmatrix} \frac{\partial J}{\partial \theta_0} \\ ... \\ \frac{\partial J}{\partial \theta_p} \end{bmatrix}$$

* with only one $\theta$

$$\nabla J (\theta) = \frac{dJ}{d \theta} = J'$$

---

#### Example with a single $\theta=\theta_1$

<!-- _class: none -->

* Example data

    Data points: $[4,2]$ and $[4,4]$
    $y = \begin{bmatrix} y_1 \\ y_2 \end{bmatrix} = \begin{bmatrix} 2 \\ 4 \end{bmatrix}$,     $X = \begin{bmatrix} x_{1,1} \\ x_{2,1} \end{bmatrix} = \begin{bmatrix} 4 \\ 4 \end{bmatrix}$

* We want to fit a line with no intercept

$$\hat{y}_j=\theta_1 \cdot x_{j,1}$$

![bg right:33% w:500](images/tex/gradient_dec_ex_1.svg)

---

<!-- _class: none -->

* we can write down $RSS$ as the cost function and plug in the model $\hat{y}_j$:
    * $J(\theta) = RSS=\sum^n_{j=1}(y_j-\hat{y}_j)^2$
    * $=\sum^n_{j=1}(y_j-\theta_1 \cdot x_{j,1})^2$

* we can calculate the derivative
    * $\nabla J (\theta) = \frac{dJ}{d \theta}$
    * $=\sum^n_{j=1} -2x_{j,1}(y_j-\theta_1 \cdot x_{j,1})$


![bg right:33% w:500](images/tex/gradient_dec_ex_1.svg)

---

<!-- _class: none -->

- what if we use flat line ($\theta=0$)
    * $J(\theta = 0) =\sum^n_{j=1}(y_j-\theta_1 \cdot x_{j,1})^2 =(2-0)^2+(4-0)^2 = 20$
    * $\nabla J(\theta = 0) =\sum^n_{j=1} -2x_{j,1}(y_j-\theta_1 \cdot x_{j,1})$
    $=-(2 \cdot 4 (2)+ 2 \cdot 4 (4) =-48$
* We make a large error ($J(\theta)=20$)
* The slope is negative ($J'(\theta)=-48$), so we must increase $\theta$


![bg right:33% w:500](images/tex/gradient_dec_ex_2.svg)

---

<!-- _class: none -->

- what if we use use line with incline of $1$ ($\theta=1$), which means the learning rate $\alpha=1$
    * $J(\theta = 1) =\sum^n_{j=1}(y_j-\theta_1 \cdot x_{j,1})^2$ 
    * $=(2-1 \cdot 4)^2+(4-1\cdot 4)^2 =4$
    * $\nabla J(\theta =1) =\sum^n_{j=1} -2x_{j,1}(y_j-\theta_1 \cdot x_{j,1})$
    $=-(2 \cdot 4 (2-4)+ 2 \cdot 4 (4-4) =16$
* We make a much smaller error ($J(\theta)=4$)
* The slope is now positive ($J'(\theta)=16$), so we must decrease $\theta$

![bg right:33% w:500](images/tex/gradient_dec_ex_3.svg)

---

#### üß† Key Takeaways

* if we have any cost function $J(\theta)$ (error measure) that is derivable, we have an algorithm to estimate *good* parameters $\vec{\theta}$
* this algorithm converges on a local minimum of the cost function, depending on the starting parameters
* we are not guaranteed to find a global minimum

![bg right w:620](images/GDS_4.png)

###### https://www.fromthegenesis.com/gradient-descent-part1/

---

### Teaching Computers

* Machine Learning is about teaching computers to learn from data
* In many cases, we want to minimize the error of a model
* Gradient Descent is a general algorithm to minimize errors
* There are many variations of Gradient Descent to improve the learning process

---

#### üß† Learning Rates

* the slope/gradients shows the direction but, we must decide for a step width (learning rate $\alpha$) 
* Learning rate is too big: We can overshoot the target
* Learning rate is too small: Slow learning

<center>

![](images/GDS_3.png)

</center>

###### https://www.fromthegenesis.com/gradient-descent-part1/

---

#### üß† Adaptable Learning Rates

* Steps are larger in steeper areas
* Steps get smaller over time 

![bg right](images/perceptron_learning_ratelocalminima.png)

###### https://wiki.tum.de/display/lfdv/Adaptive+Learning+Rate+Method?preview=/23573655/25008837/perceptron_learning_rate%20local%20minima.png



---

#### üß† Other Improvements

* Momentum strategy: consider direction of the last steps like a ball rolling down a hill that has inertia so that it does not stop at every local minimum
* Stochastic Gradient Descent: Jump Around based on a subset of training data!

<center>

![](images/Illustration-of-jumps-within-the-search-space-of-local-minima_W640.jpg)

</center>

###### https://www.researchgate.net/publication/2295939_Lecture_Notes_in_Computer_Science/figures?lo=1

---

### When to stop learning?

- Learning Curves plot the development of the training error over the steps (iterations of gradient descent)

<center>

![h:300](images/ak7oj.jpg)

</center>

* The loss (value of the cost function ($J$)) will never be zero
* except for a perfect (over-)fit on the training data

---

#### üß† Solution: Stopping criteria

* maximum number of iterations
* early-stopping: no (big) improvement for $x$ iterations



###### https://ai.stackexchange.com/questions/22369/why-the-cost-loss-starts-to-increase-for-some-iterations-during-the-training-pha

---

#### Different Learning Algorithms 

* for different machine learning models use different algorithms
* many follow the basic principle of gradient descent
* they can differ in both speed and accuracy
* take the default or test them during model selection

![bg right h:500](images/Comparison-of-Adam-to-Other-Optimization-Algorithms-Training-a-Multilayer-Perceptron.webp)

###### https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/#:~:text=Adam%20is%20a%20replacement%20optimization,sparse%20gradients%20on%20noisy%20problems.


---

### ‚úçÔ∏è Task

- Next, we look into an implementation of gradient descent

- Homework: [2.6.1 Learning and Accuracy Functions - Gradient Descent](https://github.com/jhumci/BLT_BDS/blob/main/6_Data_Science.ipynb)
- In class: [2.6.2 Gradient Descent in Python](https://github.com/jhumci/BLT_BDS/blob/main/6_Data_Science.ipynb)

![bg right](images/Magic-Wand.webp)


---


## 2.6.2 Accuracy Measures and Learning Curves

### üéØ Learning objectives

You will be able to 

* use different accuracy measures
* use learning curves to identify over-fitting

---

### What is a good model?

![bg right h:650](images/1_eFaSeepbLfF_lESM7FgZKw.webp)

* A good model makes an accurate prediction
* Models with a **low flexibility** (**high bias**) are not accurate because they are **too simple** do model the real world
* Models with a **high flexibility** (**high variance**) are not accurate because they are too complex and **overfit** the training data


###### https://medium.com/@ivanreznikov/stop-using-the-same-image-in-bias-variance-trade-off-explanation-691997a94a54

---

#### üß† Cost and Accuracy Measures

* *Residual Sum of Squares*: Squared error for the whole data set with $n$ observations

$$\text{RSS}=\sum^n_{i=1}(y_i-\hat{y_i})^2$$

* *Mean Squared Error*: Squared error for the average observation

$$\text{MSE}=\frac{1}{n}\sum^n_{i=1}(y_i-\hat{y_i})^2$$

* *Root Mean Squared Error*: $\text{MSE}$ corrected for the dimension 

$$\text{RMSE}=\sqrt{\frac{1}{n}\sum^n_{i=1}(y_i-\hat{y_i})^2}$$

###### https://scikit-learn.org/stable/modules/classes.html#regression-metrics


---

#### üß† Cost and Accuracy Measures

* *Mean Absolute Error*: keeps the unit of the predicted variable

$$\text{MAE}=\frac{1}{n}\sum^n_{i=1}|y_i-\hat{y_i}|$$

* *Mean Absolute Percentage Error*: allows comparison between different data sets

$$\text{MAPE}=\frac{1}{n}\sum^n_{i=1}|\frac{y_i-\hat{y_i}}{y_i}|$$

* not defined for $y_i=0$, biased for small $y_i$




---

### üß† Learning Curves

* A learning curve is a plot of model learning **performance over experience** or time (e.g., number of iterations of gradient descent or amount of training data).
* To really understand how a model behaves, the data must be split in a training an validation set
    * **Training** Learning Curve: Learning curve calculated from the training dataset that gives an idea of how well the model is learning
    * **Validation** Learning Curve: Learning curve calculated from a hold-out validation dataset that gives an idea of how well the model is generalizing



###### https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/

---

#### The Learning Curve Plot

* Learning Curve: Line plot of learning performance (y-axis) 
    * Loss/Score can be any accuracy measure
* over experience (x-axis).
    * for instance number of iterations 
    * size of the training set
    * duration of training

![bg right h:400](images/Learning_Curves.png)

###### https://upload.wikimedia.org/wikipedia/commons/2/24/Learning_Curves_%28Naive_Bayes%29.png

---

##### üß† Examples

![bg left h:400](images/LearningUnderfit.png)

* Model too flexible (high variance) and over-fits the data
* small training error but large validation error

---

##### üß† Examples

![bg left h:400](images/UnderfitNeedsTraining.png)

* Model has the right flexible, but training was stopped to early
* We would expect a better fit with more training

---

##### üß† Examples

![bg left h:400](images/Overfirt.png)

* Model is very flexible and over-fits the training data 
* after some time so the loss on unseen validation data increases a little (should be stopped earlier)

---

##### üß† Examples

![bg left h:400](images/GoodFit.png)

* Example for a good fit
* Usually we would expect a better fit on the training set than the validation set 


---

##### üß† Examples

![bg left h:400](images/unrep.png)

* Training set might be to small
* Data fits the training set well but this does not translate to the validation set

---

##### üß† Examples

![bg left h:400](images/unRepValidation.png)

* Validation set might be to small
* Depending on the selection of the values in the set the loss fluctuates a lot (high variance)

---


### ‚úçÔ∏è Case Study

- we train different model on synthetic data set
- we use training curve, to see how the models behave during training

![bg right h:400](images/Learning_data.png)

[2.6.3 Accuracy Measures and Learning Curves](https://github.com/jhumci/BLT_BDS/blob/main/6_Data_Science.ipynb)

‚åõ  25 min

