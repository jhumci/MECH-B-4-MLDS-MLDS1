---

marp: true
theme: beams
author: Julian Huber
size: 16:9
footer: Julian Huber - Data Science
headingDivider: 2

# Strg+[ ] for Options
---

<!-- paginate: true -->



# 2 Data Science

<!-- _class: title -->

Dr. Julian Huber
*Management Center Innsbruck*




## 2.5 Resampling and Model Development

<center>

![h:500](images/UbersichtDS.JPG)

</center>

---

## 2.5.1 Model Development

* **Data Preparation**: üêß
* **Feature Engineering**: New binary variables (```isFemale```)
* **Model Development**: Linear Regression comparing different predictors
* **Model Evaluation / Prediction**: on test-set


![bg right:53% w:770](images/CRISPDM/Folie3.JPG)

---

### Data Preparation

* Load the data from file or database
* Clean the data (e.g., missing values, outliers)
* Make sure what question You want to answer
* Make plots to get an understanding of the data

![bg right:40% w:500](images/bianry_pengus.png)

---

### Feature Engineering

* Create binary variables from categories
* Normalize and scale variables is necessary
* Create new features from existing ones

![](images/dummy_vars.png)

---

### Model Development

* Split the data in training and test or decide for another resampling approach
* Decide what models could work (e.g., is a linear model appropriate?)
* Decide how You want to measure if a model is good (e.g., $R^2$, $MSE$)
* Train models and compare their accuracy on the validation-set

```Python
train, test = train_test_split(penguins_cleaned, test_size=0.2, random_state=11)
```

<center>

![](images/train-and-test-datasets-in-machine-learning.png)

</center>

---

### Model Evaluation / Prediction

* Evaluate the models on a unseen test-set
* Calculate the prediction error (e.g., $MSE$, Residuals)
* Visualize the results
* Evaluate strength and weaknesses of the best models

![bg right:40% w:500](images/residuals_model.png)


---


## 2.5.2 Resampling Methods 

### üéØ Learning objectives

You will be able to 

* what questions to evaluate based on training, validation and test-sets
* perform LOO and k-fold cross-validation to create models that do not over-fit the training data

---


### üß† Model & Feature Selection

- we want to find the best model for the data
- models can differ in
    - predictors / features that go into the model
    - model type (linear regression, decision tree, etc...)
    - models power to (over-)fit the data
    - hyper parameters (parameters, that tweak the model)
* first, we focus in the the predictors / features

---

#### Finding the best predictors / features

- We have $6$ possible predictor variables in the data set to predict `body_mass_g`:
    - `species, island, bill_length_mm, bill_depth_mm, flipper_length_mm, sex`
* Encoding the categorical variables binary we have $p=8$ possible predictors
    - *isAdelie*, *isGentoo* , *fromTorgersen*, *fromBiscoe*, bill_length_mm, bill_depth_mm, flipper_length_mm, *isFemale*
* from that we can build ${2^p} = {2^8}= 256$ possible linear models:
    * $M_0: \text{body mass g}=\beta_0$
    * $M_1: \text{body mass g}=\beta_0 + \beta_1  \cdot \text{isAdelie}$
    * $M_2: \text{body mass g}=\beta_0+ \beta_2 \cdot \text{isGentoo}$
    * ...


---

#### What we found out in the last case study

* It is hard to predict what is the best model in advance
* We compared models $M_i$ in different ways
    * Fit on the training set ($R^2$, $RSS$)
    * Prediction accuracy 
        * on the training 
        * and test-set ($MSE$)
    * Evaluation only on the training set can lead to over fitting

---

##### Over-fitting the Training Data

<center>

![h:500](images/ForecastOverfittingFlexibility.png)

</center>

---

* What we really want to know: What will be the best model on new unseen data?
    * We need to **select models** on a set that is **not the training data**
    * After selecting a model, we still want to test how well it performs on unseen data
* validation-set
    * A set on which we compare trained models to find
    * the best features and ML Algorithms for the task

---

##### üß† We must divide the data into three Parts

<center>

![h:500](images/1_RJS8yV5mBDqrRu7THooH-w.png)

</center>



###### https://www.sxcco.com/?category_id=4128701

---

|      Training-Set      |           Validation-Set:           | Test-Set |
|:----------------------:|:-----------------------------------:|:--------:|
| for fitting the models | or selecting the models (e.g., which parameters to include)            |    hold-out set of data, to prove that we selected a good model for any data      |



---

#### How do we split the data?

* Often, we do not have much data, but ...
* if we choose a **small training set**
    * We only use a small proportion of the data to learn
    * If we have powerful models on sparse data, they tend to **over-fit**
* **small validation / test-set**
    * set can be very special by chance
    * **misleading results**

![bg right:33% w:300](images/1_RJS8yV5mBDqrRu7THooH-w.png)



###### https://www.sxcco.com/?category_id=4128701

---

### üß† The Validation-Set Approach

- split the data once
- $70\%$ / $15\%$ / $15\%$ are common proportions

```python
data = [3, 7, 13, 15, 22, 25, 50, 91]

training = [3, 7, 13, 50]
validation = [15, 91]
test = [22, 25]
```

* note, that we can shuffle the folds randomly - training is not the first three entries!

---

#### Problems with the Validation-Set Approach

<center>

![](images/TrainingTestSplit.png)

</center>

- as we saw in linear regression, the parameters can vary based on the training data
- test-set $MSE$ vs polynomial complexity of the model 
- Left: test-set $MSE$ one split, Right: test-set $MSE$ with ten different splits
* Results depend on how we split the data

---

### üß† Leave-One-Out Cross-Validation (LOO)

- only put one observation in the test or validation-set
- repeat this for all observations

```python
data = [3, 7, 13, 15, 22, 25, 50, 91]

test = [22, 25]

training_1 = [7, 13, 15, 50, 91]
validation_1 = [3]

training_2 = [3, 13, 15, 50, 91]
validation_2 = [7]

...
```
---

##### üß† Averaging the Cross-Validation results

```
data = [3, 7, 13, 15, 22, 25, 50, 91]
```

* With $n$ observations in the original data set
* We get $n$ models $M_l$ with different prediction errors on the validation-sets
* We can still calculate the $MSE$ the same way
* $MSE^{CV}=\frac{1}{n}\sum_{l=1}^n MSE_l$


---

### üß† k-fold Cross-Validation

- instead of $n$ folds with length $1$, the data is split in $k$ folds of equal length


```
data = [3, 7, 13, 15, 22, 25, 50, 91]

test = [22, 25]

training_fold_1 = [13, 15, 50, 91]
validation_fold_1 = [3, 7]

training_fold_2 = [3, 7, 50, 91]
validation_fold_2 = [13, 15]

...
```

---

<center>

![](images/0_64aOXOe2OsUzbiEr.jpg)

</center>

###### https://medium.com/coders-mojo/quick-recap-most-important-projects-data-science-machine-learning-programming-tricks-and-c7d99a7a2391

---

#### Averaging the Cross-Validation results

* With $k$ folds
* we get $k$ different $MSE_l$s
* $MSE^{kCV}=\frac{1}{k}\sum_{l=1}^k{MSE_l}$

---

### What to use?

* Leave-One-Out (LOO) is a special case of n-folds ($n=k$)
* LOO is computationally more expensive
* LOO has a smaller bias (we use more data)
* LOO has a higher variance (the models we train are all 
very similar, while the test-set very different)
* üß† using **kFolds with $k = 5$** or $k = 10$ has been shown empirically 
to yield good results

---

### üß† A practical approach to Cross-Validation

<center>

![h:500](images/grid_search_workflow.png)

</center>

###### https://scikit-learn.org/stable/modules/cross_validation.html

---



| Training-Set | Validation-Set | Test-Set |
|---|---|---|
| used to train the models | used to select the model, predictors $\vec{X}$ and/or parameters | used to prove the models performance |
| 5-fold CV (with Validation) | 5-fold CV (with Training) | $15\%$ cut out at the beginning |

* note that the data should be shuffled before the split
* there are some cases, where this will not work (e.g. temporal data)


---

## 2.5.3 Feature Selection

* given we have ${2^p} = {2^8}= 256$ possible linear models:
    * $M_0: \text{body mass g}=\beta_0$
    * $M_1: \text{body mass g}=\beta_0 + \beta_1 \cdot \text{isAdelie}$
    * $M_2: \text{body mass g}=\beta_0+ \beta_2 \cdot \text{isGentoo}$
    * ...
* how can we reliably find the best one?
* we use 5-fold cross-validation to find the best features (predictors) and parameters

--- 

### Option 1: Best Subset Selection

* brute force: train all possible models on the training set
* compare the models performance on the validation-set
* select the best model (e.g, based on validation $MSE$)
* With 5-fold cross-validation You will train $2^8 \cdot 5$ different models


---

### ü§ì Option 2:  Forward Selection

* start he the null model ($\text{body mass g}=\beta_0$)
* calculate the best model with only one predictor $X_a$
* calculate the best model containing $X_a$ and one further predictor
* ...

<center>

![](images/1_YjD-RRim3R2zzJNkwAFVUw.png)

</center>

###### https://medium.com/codex/what-are-three-approaches-for-variable-selection-and-when-to-use-which-54de12f32464

---

### ü§ì Option 3:  Backward Selection

* inversion of the forward selection
* start with all predictors
* drop the predictor with the smallest decrease in model accuracy


---

### ü§ì More Options: 

* [Automated Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html)
* [Regularization](https://en.wikipedia.org/wiki/Regularization_(mathematics)) can shrink unimportant parameters

---


### ü§ì Case Study

Try to find the best possible linear model for predicting the penguin weight using
- best subset selection and 
- cross-validation


![bg right:35% h:720](images/Dl16mkGWsAEgEAC.jpg)

###### https://www.scoopnest.com/user/AFP/1035147372572102656-do-you-know-your-gentoo-from-your-adelie-penguins-infographic-on-10-of-the-world39s-species-after

[2.5.1 Cross-Validation and Model Selection - Resampling](https://github.com/jhumci/BLT_BDS/blob/main/5_Data_Science_short.ipynb)

‚åõ 40 minutes



