# Regressions-Analyse {.unnumbered}

Im Allgemeinen kann die Regressionsanalyse als eine Sammlung von Werkzeugen verstanden werden, die dazu verwendet werden, eine Beziehung zwischen einer abhängigen Variable  $Y$ (auch Zielvariable, Antwortvariable oder Label genannt) und der unabhängigen Variable $X$ (auch Regressor, Prädiktoren, Kovariaten, erklärende Variable oder Feature genannt) zu schätzen oder festzustellen.

Wenn wir eine Regressionsfunktion $f$ als Modell für die Beziehung zwischen $X$ und $Y$ annehmen, können wir die Regressionsanalyse als die Suche nach den Parametern $\theta$ verstehen, die die Funktion $f$ am besten an die Daten anpassen.

Das Problem der Regressionsanalyse kann also wie folgt formuliert werden:
$$
Y = f(X, \theta) + \epsilon,
$$

wobei $\theta$ durch die Optimierung für _eine gute Anpassung_ von $f$ an die Daten gefunden wird. In der Regel erhalten wir dabei einen Fehlerterm $\epsilon$, der – wenn wir Glück haben – normalverteilt mit einem Erwartungswert von null und konstanter Varianz ist.

Regessionen sind ein mächtiges Werkzeug für die _Interpetation von Daten_ und die _Vorhersage von Werten_. In der Praxis gibt es viele verschiedene Arten von Regressionsmodellen, die sich in ihrer Komplexität und den Annahmen, die sie machen, unterscheiden. In diesem Kapitel werden wir uns zunächst auf die lineare Regression konzentrieren, die eine der einfachsten und am häufigsten verwendeten Formen der Regressionsanalyse ist und sich für Zusammenhänge zwischen einer abhängigen und einer unabhängigen Variablen mit intervallskalierten Daten eignet.


Zunächst werden wir eine einfache lineare Regression mit einer einzigen unabhängigen Variable durchführen. Dieses Modell beschreibt eine lineare Beziehung zwischen der abhängigen Variable $Y$ und einer unabhängigen Variable $X$. Anschließend erweitern wir das Modell, um mehrere unabhängige Variablen sowie kategorische Variablen zu berücksichtigen. Um diese komplexeren Modelle mathematisch effizient zu formulieren, werden wir die Matrix-Schreibweise verwenden, die eine kompakte Darstellung der Beziehungen zwischen mehreren Variablen ermöglicht.

Im nächsten Abschnitt behandeln wir fortgeschrittene Konzepte wie den Intercept (Achsenabschnitt) und die Regularisierung. Der Intercept repräsentiert den Wert der abhängigen Variable, wenn alle unabhängigen Variablen gleich null sind, und spielt eine zentrale Rolle in der Interpretation des Modells. Regularisierungstechniken, wie z. B. Ridge- oder Lasso-Regression, werden eingeführt, um Überanpassung (Overfitting) zu vermeiden und die Stabilität sowie die Vorhersagegenauigkeit des Modells zu verbessern, insbesondere bei hochdimensionalen Daten.

Ein weiterer Schwerpunkt liegt auf der Diskussion statistischer Lernverfahren und Resampling-Techniken. Statistische Lernverfahren umfassen Ansätze, wie Modelle aus Daten lernen können, und deren Evaluation. Resampling-Methoden, wie z. B. Kreuzvalidierung oder Bootstrapping, werden vorgestellt, um die Robustheit und Verallgemeinerungsfähigkeit von Regressionsmodellen zu überprüfen und die Modellleistung auf neuen Daten zu schätzen.

Logistische Regression für Klassifikationsaufgaben
Abschließend betrachten wir die logistische Regression als ein Beispiel für ein Regressionsmodell, das für Klassifikationsaufgaben verwendet wird. Im Gegensatz zur linearen Regression, die kontinuierliche Zielvariablen vorhersagt, modelliert die logistische Regression die Wahrscheinlichkeit, dass eine Beobachtung zu einer bestimmten Kategorie gehört. Dies macht sie besonders nützlich für binäre Klassifikationsprobleme, wie z. B. die Vorhersage, ob ein Ereignis eintritt oder nicht.