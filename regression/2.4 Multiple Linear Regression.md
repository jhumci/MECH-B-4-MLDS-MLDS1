---

marp: true
theme: beams
author: Julian Huber
size: 16:9
footer: Julian Huber - Data Science
headingDivider: 2

# Strg+[ ] for Options
---

<!-- paginate: true -->



# 2 Data Science

<!-- _class: title -->

Dr. Julian Huber
*Management Center Innsbruck*




## 2.4 Multiple Regression





### üéØ Learning objectives

You will be able to 

* implement and interpret Linear Regression models with multiple independent variables
* Students can calculate the VIF to remove colinear predictors
* name different approaches for variable selection

---


## 2.4.1 Multiple Linear Regression

### Need f√ºr Multiple Regression

<center>

![h:300](images/tv.png)

</center>


> How do the total sales relate to the three different advertising budgets?

$$\text{sales} = Œ≤_0 + Œ≤_1  \text{TV} + Œ≤_2  \text{radio} + Œ≤_3  \text{newspaper} + \epsilon$$
$$\hat{y} = \hat{Œ≤}_0 + \hat{Œ≤}_1 X_1 + \hat{Œ≤}_2 X_2 + ¬∑ ¬∑ ¬∑ + \hat{Œ≤}_p X_p$$

---

### üß† Interpreting Multiple Regression

<center>

![](images/MultipleRegressionModel.png)

</center>

* only newspaper adds show no significant correlation to sales

---

#### Take care when comparing different models

<center>

![](images/newspaper.png)

</center>

* If we only consider newspaper adds, they have a significant correlation
* the single equation ignores the other two media in forming estimates for the regression coefficients


---

### Visualization

* We can only visualize two predictors at once
* The regression plane stays linear is a generalization of the regression line to multiple dimensions
* Hower, if we plot only one predictor and the predicted values, the graph could be non-linear


![bg right h:500](images/RegressionPlane.png)



---

## 2.4.2 Variable Selection

<center>

![h:150](images/MultipleRegressionModel.png)

</center>



* which predictors should we include in the model?
    * $\text{sales} = Œ≤_0 + Œ≤_1 √ó \text{TV} + Œ≤_2 √ó \text{radio} + Œ≤_3 √ó \text{newspaper} + \epsilon$
    * $\text{sales} = Œ≤_0 + Œ≤_1 √ó \text{TV} + Œ≤_2 √ó \text{radio} + \epsilon$
    * $\text{sales} = Œ≤_0 + Œ≤_1 √ó \text{TV}  + Œ≤_3 √ó \text{newspaper} + \epsilon$
    * $\text{sales} = Œ≤_0  + Œ≤_2 √ó \text{radio} + Œ≤_3 √ó \text{newspaper} + \epsilon$
    * $\text{sales} = Œ≤_0 + Œ≤_1 √ó \text{TV} + \epsilon$
    * $\text{sales} = Œ≤_0 + Œ≤_2 √ó \text{radio} + \epsilon$
    * $\text{sales} = Œ≤_0 + Œ≤_3 √ó \text{newspaper} + \epsilon$

---

### üß† Model and Variable Selection

* there are $2^p$ models that contain subsets of $p$ variables.
* there are different criteria for model quality ($MSE$, $R^2$, ...)
* Approaches:
    * **Forward selection**: Start with null model (only intercept $\beta_0$) and add variables
    * **Backward selection**: Start with all variables and remove variables
* More in the next session

---

### ü§ì Collinearity

* two or more predictor variables are closely related to one another
    * e.g. weight in kg ($X_1$) and weight in pounds ($X_2$) as an extreme case
* makes difficult to separate out the individual effects
    * $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon$
* reduces the accuracy of the estimates of the regression coefficients, as it is unclear which variable is responsible for the effect
* You can spot them in scatter plot matrices

![bg right:40% w:520](images/0_Cf5INlQldwN89wnF.png)

###### https://medium.com/analytics-vidhya/new-aspects-to-consider-while-moving-from-simple-linear-regression-to-multiple-linear-regression-dad06b3449ff

---

#### ü§ì Variance inflation factor (VIF).

* An alternative to the scatter plot ist that VIF, that compares how much a variable $j$ adds the to problem:  $VIF_j=\frac{1}{1-R_j^2}$
* The smallest possible value for VIF is 1, which means that the predictor is not correlated with any other predictors at all ($R_j^2 = 0$).
which indicates the complete absence of collinearity.
* $VIF > 5 \text{ or } 10$ indicates a problematic amount of collinearity.

* What to do:
    * Check VIF for all possible predictors
    * drop the predictor with the highest $VIF$

###### $R_j^2$ is the result of regressing $j$ based on all other predictors

---

#### Case Study

If we would like to create a model, that predicts the flipper length not only from bill length but also bill depth, we need to define a model with multiple predictors.


![bg right:35% h:720](images/Dl16mkGWsAEgEAC.jpg)

###### https://www.scoopnest.com/user/AFP/1035147372572102656-do-you-know-your-gentoo-from-your-adelie-penguins-infographic-on-10-of-the-world39s-species-after

---

#### ‚úçÔ∏è Case Study

[2.4.1 Multiple Linear Regression](https://github.com/jhumci/BLT_BDS/blob/main/4_Data_Science.ipynb)

‚åõ 25 minutes

---


## 2.4.3 Qualitative Predictors and further Extensions

### üéØ Learning objectives

You will be able to 

* integrate qualitative predictors in regression models
* model non-linear relationships with linear regression
* name common problems when working with linear regression


---

### üß† Qualitative Predictors

* so far the models only included quantitative (interval scaled) predictors
    $$\text{body height} = \beta_0  + \beta_1 \cdot \text{femur length} + \epsilon$$
* how do we deal with qualitative data
    $$\text{body height} = \beta_0  + \beta_1 \cdot \text{femur length} + \beta_2 \cdot  \text{sex} + \epsilon$$
* where $\text{sex}$ indicates a categorical variable that describes a qualitative attribute

---

#### üß† Predictors with only Two Levels

- $x_{i,1}$ femur length of person $i$

- $x_{i,2} = \begin{cases} 1 & \text{if ith person is female} \\ 0 & \text{if ith person is male} \end{cases}$

$$y_i = \beta_0 + \beta_1 x_{i,1} + \beta_2 x_{i,2}  + \epsilon= \begin{cases} \beta_0 + \beta_1 x_{i,1} + \beta_2 x_{i,2}  + \epsilon & \text{if $i$th person is female}\\ \beta_0 + \beta_1 x_{i,1}  + \epsilon & \text{if $i$th person is male}\\\end{cases}$$

---


#### üß† Interpretation of $\beta_2$

$$y_i = \beta_0 + \beta_1 x_{i,1} + \beta_2 x_{i,2}  + \epsilon= \begin{cases} \beta_0 + \beta_1 x_{i,1} + \beta_2 x_{i,2}  + \epsilon & \text{if $i$th person is female}\\ \beta_0 + \beta_1 x_{i,1}  + \epsilon & \text{if $i$th person is male}\\\end{cases}$$

* if $\beta_2$ is negative:
    * a woman will be shorter with the same femur length

---


#### üß† Qualitative Predictors with more than two Levels

- if the data contains the following ethnicities: Asian, Caucasian, African American
- we would need *two* additional variables:

$$x_{i,3} = \begin{cases} 1 & \text{if $i$th person is Asian}\\ 0 & \text{if $i$th person is not Asian}\\\end{cases}$$

$$x_{i,4} = \begin{cases} 1 & \text{if $i$th person is Caucasian}\\ 0 & \text{if $i$th person is not Caucasian}\\\end{cases}$$

* in the models base line, the person is African American so $x_3 = x_4 = 0$

---

#### Example: Input Features that are not numerical

<center>

| id | body height | femur length | sex | ethnicity |
|---|---|---|---|---|
| 1 | 185 | 49 | male | caucasian |
| 2 | 176 | 43 | female | asian |
| 3 | 179 | 33 | female | african american |
|  | ... | ... | ... | ... |

</center>


* most models only can work with numerical variables

---

##### Dummy Encoding

<center>

| id | body height | femur length | is_female | is_asian | is_caucasian |
|---|---|---|---|---|---|
| 1 | 185 | 49 | 0 | 0 | 1 | 
| 2 | 176 | 43 | 1 | 1 |0| 
| 3 | 179 | 33 | 1 | 0 |0| 
|  | ... | ... | ... | ...  | ...| 

</center>

* $n-1$ new variables for each category in the original variable
* there is a baseline model (e.g., is male and african american)
* common in linear regression

---

##### One-Hot Encoding

<!-- _class : tinytable -->

<center>

| id | body height | femur length | is_female | is_male | is_asian | is_caucasian | is_african_american |
|---|---|---|---|---|---|---|---|
| 1 | 185 | 49 | 0 | 1 | 0 | 1 | 0 |
| 2 | 176 | 43 | 1 | 0 | 1 | 0 | 0 |
| 3 | 179 | 33 | 1 | 0 | 0 |0| 1 |
|  | ... | ... | ... | ... | ...  | ... | ... |

</center>

* $n$ new variables for each category in the original variable
* common in other machine learning models

---



##### üß† Example of a Regression Table with Dummy Encoding

<center>

![](images/regressionCategories.png)

</center>


* the only predictor in $X$ is ethnicity (three classes)
* the baseline is African American, where the intercept (predicted value $Y$ is $531$)
* for both other ethnicity, the model expects a lower value
* however, the correlation is not significant ($p>0.05$)


---

#### ‚úçÔ∏è Case Study

[2.4.2 Applications of Linear Regression](https://github.com/jhumci/BLT_BDS/blob/main/4_Data_Science.ipynb)

---

## ü§ì 2.4.4 Extensions of the Linear Model

* Linear Regression is a simple, yet powerful tool
    * it's the go-to tool in research to evaluate correlation in data
    * it's a commonly used model a a baseline for more sophisticated models
* There are two common extensions, that make it even more powerful


![bg right:43% h:720](images/linear-regression-i-have-no-idea-what-i-am-doing.jpg)


---

### ü§ì Removing the Additive Assumption 

<center>

![h:300](images/tv.png)

</center>

> In reality, is it better to spent all the marketing budget on TV or to split it?

* this model is purely additive
    $$\text{sales} = Œ≤_0 + Œ≤_1 √ó \text{TV} + Œ≤_2 √ó \text{radio} + Œ≤_3 √ó \text{newspaper} + \epsilon$$
* we assume the effects on the sales to be independent
* in reality, we expect synergies (e.g, when people see the ad twice)

---

#### ü§ì by adding Interaction Terms


* We can model this, using an interaction term ($\text{TV} √ó \text{radio}$)
    $\text{sales} = Œ≤_0 + Œ≤_1 √ó \text{TV} + Œ≤_2 √ó \text{radio} + Œ≤_3 √ó \text{newspaper} + Œ≤_4 √ó \text{TV} √ó \text{radio} + \epsilon$
* Interpretation: There is an additional positive effect, if we increase both 
(TV and radio adds)

---

#### ü§ì Interactions with Qualitative Predictors

<center>

![h:300](images/interaction.png)

</center>

> prediction of bank account balance from income

* Left Model: 
    $\text{balance}  = Œ≤_0 + Œ≤_1  \text{income} + Œ≤_2  \text{is student}$
* Right Model: 
    $\text{balance}  = Œ≤_0 + Œ≤_1  \text{income} + Œ≤_2  \text{is student} + Œ≤_3 \text{is student} \cdot \text{income}$


---

### ü§ì Modeling Non-Linear Relationships with Linear Models

<center>

![h:400](images/non-linear-regression.png)

</center>

> the linear model is no good predictor for the relationship


---

#### ü§ì Linear Regression Model can capture Non-Linear Relationships

- polynomial regression:
    
$$mpg = Œ≤_0 + Œ≤_1 √ó \text{horsepower} + Œ≤_2 √ó \text{horsepower}^2 + \epsilon$$

<center>

![](images/horsepower.png)

</center>


* Drawbacks
    * we have to decide for the degree of the polynom (parametric model)
    * it is harder to interpret
    * there are more flexible models, e.g., neural networks and decision trees



---

### ‚úçÔ∏è Case Study: Data Science Project with Linear Regression

<center>

![h:600](images/CRISPDM/Folie3.JPG)

</center>

---

Imagine researching penguin in the Antarctic caused Your scale to freeze an break. However, You still want to get an estimate of the penguins weight.

Luckily, Your colleagues left You with a data set (```train```) with all the other variables you can measure. 

Create a multiple linear regression model to predict the penguins weight. Compare the Mean Squared Error of the model in the ```test``` set with your colleagues.

![bg right:35% h:720](images/Dl16mkGWsAEgEAC.jpg)

###### https://www.scoopnest.com/user/AFP/1035147372572102656-do-you-know-your-gentoo-from-your-adelie-penguins-infographic-on-10-of-the-world39s-species-after

---



[2.4.3 Case Study](https://github.com/jhumci/BLT_BDS/blob/main/4_Data_Science.ipynb)

‚åõ 45 minutes

