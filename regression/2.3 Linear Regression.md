---

marp: true
theme: beams
author: Julian Huber
size: 16:9
footer: Julian Huber - Data Science
headingDivider: 2

# Strg+[ ] for Options
---

<!-- paginate: true -->



# 2 Data Science

<!-- _class: title -->

Dr. Julian Huber
*Management Center Innsbruck*




## 2.3 Linear Regression

<center>

![h:500](images/UbersichtDS.JPG)

</center>

---

### üéØ Learning objectives

You will be able to 

- Students can implement a Linear Regression in Python
- Students can interpret coefficients and p-values of an regression model
- Students describe the accuracy of an model using $R^2$


---

### Motivation



<center>

![h:400](images/tv.png)

</center>

* Linear Regression is supervised and parametric
* Interpretation:
    * Is there a relationship between advertising budget and sales?
    * How strong is the relationship between advertising budget and sales?


---

<center>

![h:400](images/tv.png)

</center>

* Interpretation:
    * Which media contribute to sales?
    * How accurately can we estimate the effect of each medium on sales?
    * How accurately can we predict future sales?
    * Is the relationship linear?
    * Is there synergy among the advertising media?

---

### üß† Linear Regression with a Single Predictor

* A linear model can be noted in the matrix form as, and we assume that thr error term $\epsilon$ is normally distributed with a mean of $0$ and a constant variance $œÉ^2$:
$$Y= X \beta + \epsilon$$
$$\epsilon \sim N(0,œÉ^2)$$
* If we have only one predictor, the model simplifies to:
$$Y ‚âà Œ≤_0 + Œ≤_1 \cdot X_1$$
$$\text{sales} ‚âà Œ≤_0 + Œ≤_1 \cdot \text{TV}$$

* coefficients or parameters:
    * $Œ≤_0$ ... intercept
    * $Œ≤_1$ ... slope

* We estimate the parameters based on training data
 $$Y ‚âà \hat{Œ≤}_0 + \hat{Œ≤}_1 \cdot X_1$$

---

#### üß† Estimating the Coefficients

![bg left h:300](images/RSS.png)

* model: $\hat{y}_i = \hat{Œ≤}_0 + \hat{Œ≤}_1 \cdot x_{i,1}$
* prediction error: $e_i = y_i‚àí\hat{y}_i$
* residual sum of squares (RSS) over all $n$ observations
    $RSS = e^2_1 + e^2_2 + ¬∑ ¬∑ ¬∑ + e^2_n$
* best model: $\min(RSS)$


---

‚úçÔ∏è What is the relationship between $MSE$ and $RSS$?

* Mean Squared Error: $MSE=\frac{1}{n}\sum_{i=1}^n(y_i‚àí\hat{y}_i)^2$
* Residual Sum of Squares: $RSS= \sum_{i=1}^n(y_i‚àí\hat{y}_i)^2$





---

##### Analytical Solution

- to find the parameters that have the minimal $RSS$ given the training data ($X,Y$)

$$\hat{Œ≤}_1 = \frac{\sum_{i=1}^n{(x_i-\bar{x})(y_i-\bar{y})}}{\sum_{i=1}^n{(x_i-\bar{x})^2}}$$

$$\hat{Œ≤}_0 = \bar{y}-\beta_1\bar{x}$$

- $\bar{x_j} = \frac{1}{n}\sum_{i=1}^n x_{i,j}$
- $\bar{y} = \frac{1}{n}\sum_{i=1}^n y_{i}$

![bg right:33% w:400](images/ErrorPlaneRegression.png)

###### You can find the proof in any textbook or wikipedia. Later, we will discuss alternative algorithm to find the solution

---

##### :nerd_face: Problem Setup

We want to find the coefficients **$\hat{Œ≤}_0$** and **$\hat{Œ≤}_1$** of a simple linear regression model:

$$y_i = Œ≤_0 + Œ≤_1x_i + \epsilon_i$$

- **Goal**: Minimize the Residual Sum of Squares (RSS):
  $$RSS = \sum_{i=1}^n \left(y_i - (Œ≤_0 + Œ≤_1x_i)\right)^2$$

---

##### :nerd_face: Step 1: Definitions

Key Quantities
- **Mean of $x$**:
  $$\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i$$
- **Mean of $y$**:
  $$\bar{y} = \frac{1}{n}\sum_{i=1}^n y_i$$

---

##### :nerd_face: Step 2: Deriving $\hat{Œ≤}_1$

1. **Start with RSS**:
   $$RSS = \sum_{i=1}^n \left(y_i - (\beta_0 + \beta_1x_i)\right)^2$$

2. **Minimize RSS w.r.t. $\beta_1$**:
   $$\frac{\partial RSS}{\partial \beta_1} = 0$$

Differentiate the RSS with respect to $Œ≤_1$:

$$\frac{\partial RSS}{\partial Œ≤_1} = \frac{\partial}{\partial Œ≤_1} \sum_{i=1}^n \left(y_i - Œ≤_0 - Œ≤_1x_i\right)^2$$

---

2.1. **Chain Rule**:
   $$\frac{\partial RSS}{\partial Œ≤_1} = -2 \sum_{i=1}^n x_i \left(y_i - Œ≤_0 - Œ≤_1x_i\right)$$

2.2. **Set the derivative to zero** (necessary condition for a minimum):
   $$-2 \sum_{i=1}^n x_i \left(y_i - Œ≤_0 - Œ≤_1x_i\right) = 0$$

---



2.3. **Simplify**:
   $$\sum_{i=1}^n x_i y_i - \beta_0 \sum_{i=1}^n x_i - \beta_1 \sum_{i=1}^n x_i^2 = 0$$

2.4. **Substitute** $\beta_0 = \bar{y} - \beta_1\bar{x}$ (from earlier derivation).

2.5. **Rearrange terms** to isolate $\beta_1$:
   $$\hat{Œ≤}_1 = \frac{\sum_{i=1}^n{(x_i-\bar{x})(y_i-\bar{y})}}{\sum_{i=1}^n{(x_i-\bar{x})^2}}$$


---

##### :nerd_face: Step 3: Deriving $\hat{Œ≤}_0$

1. Using the relationship:
   $$\hat{Œ≤}_0 = \bar{y} - \hat{Œ≤}_1\bar{x}$$

2. Substitute $\hat{Œ≤}_1$:
   - Ensures the line passes through $(\bar{x}, \bar{y})$.

---

##### :nerd_face: Summary

- **Slope ($\hat{Œ≤}_1$)**:
  $$\hat{Œ≤}_1 = \frac{\sum_{i=1}^n{(x_i-\bar{x})(y_i-\bar{y})}}{\sum_{i=1}^n{(x_i-\bar{x})^2}}$$

- **Intercept ($\hat{Œ≤}_0$)**:
  $$\hat{Œ≤}_0 = \bar{y} - \hat{Œ≤}_1\bar{x}$$


---


##### Same Data, different Solutions

![bg right w:640](images/Different_Paramters_Same_Data.png)

- red: true relationship of the simulated data
- grey: random sample drawn from the true relationship
- blue: estimated regressions lines based on different samples (training sets)
* How sure can we be about the values of $\beta_0$ and $\beta_1$?

---

#### Standard Error $SE(\beta), \sigma(\beta)$ of the Coefficients

* is a measure for the **average amount** that an estimate (e.g., $\hat{\beta}_0$) **differs from the actual value of $\beta_0$** and depends on the variance $œÉ^2$ in the data and the number of observations $n$ on the sample , where $œÉ$ is the standard deviation of the error term $\epsilon$.
    * The larger the variance $œÉ^2$ the larger the standard error
    * The larger the sample size $n$ the smaller the standard error


---

#### Normal Distribution

<center>

![h:250](images/Standard_deviation_diagram.svg.png)

</center>

* We assume the $\beta$ to be normally distributed around the true value $\beta$ with a standard deviation $SE(\beta)$
* üß† there is approximately a 95% ($95.4=2 \cdot (13.6 + 34.1)$) chance that the interval
    $$Œ≤_i ¬± 2 ¬∑ SE(\hat{\beta}_i)$$
    will contain the true value of $Œ≤_i$. (given a large enough sample size $n$ and normal distributed errors)
* This is used as a confidence interval.

---

##### Example TV advertising data

$$\text{sales} ‚âà \hat{Œ≤}_0 + \hat{Œ≤}_1 \cdot \text{TV}$$
$$\text{sales} ‚âà 7.0325 + 0.0475 \cdot \text{TV}$$

* $95 \%$ confidence interval for the intercept $\hat{Œ≤}_0$ is $[6.130, 7.935]$
* $95 \%$ confidence interval for $\hat{Œ≤}_1$  $[0.042, 0.053]$

* without any advertising, sales will, on average, fall between $6{,}130$ and $7{,}940$ units.
* for each $1{,}000$ $ increase in TV advertising, there will be an average increase in sales of between $42$ and $53$ units.

![bg right:35% h:200](images/tv.png)

---

## Hypothesis Tests on Parameters

* We can use a linear model to test hypotheses about the relationship between the predictor and the predicted variable
> Is there really a influence of advertising ($X$) on sales ($Y$)? 

<center>

![h:200](images/tv.png)

</center>

* $H_0$: There is no relationship between $X$ and $Y$
    $H_0: Œ≤_1 = 0$
* $H_a$: There is some relationship between $X$ and $Y$
    $H_a: Œ≤_1 \ne 0$


---

### ü§ì Is $\hat{Œ≤}_1$ far enough from $0$ that we can reject $H_0$?

* From the data we know / can calculate:
    * $\hat{Œ≤}_1 = 0.0475$
    * $SE(\hat{\beta}_1)=0.0027$
    * So, we are pretty sure that the real $\beta_1$ is in $[0.042,0.053]$
* how sure are we?
    * based on the $SE$ more then $95\%$, which is high enough to reject $H_0$ with the standard significance level of $5\%$
    * alternatively, we can use a t-Test to calculate, how likely it is have a $\beta_1$ of $0$ given our data:
    * $$t=\frac{\hat{Œ≤}_1-0}{SE(\hat{Œ≤}_1)}=17.6$$

![bg right:25% h:250](images/Student_t_pdf.png)

###### https://www.geogebra.org/m/b85v7zww, $df = n-p-1$

---

ü§ì

![bg right:25% h:250](images/Student_t_pdf.png)

* To interpret the t-Value, we can look up the probability of observing a $t>17.6$ or $t<17.6$ in a t-distribution with $n-p-1$ degrees of freedom, where $n$ is the number of observations and $p$ is the number of predictors in the model
* the probability of observing a $t>17.6$ or $t<17.6$, given the real $\beta_1=0$ is very low
* we call this probability the $p$-value
* a small p-value indicates that it is unlikely to observe such a substantial association between the predictor and response due to chance
* We reject the null hypothesis $H_0: Œ≤_1 = 0$ ‚Äî that is, we declare a relationship to exist between $X$ and $Y$ ‚Äî if the $p$-value is small enough (usually $5\%$)

---

### üß† Regression Tables

$$\text{sales} ‚âà Œ≤_0 + Œ≤_1 \cdot \text{TV}$$

 After fitting the model, the models parameters can be found in a regression table.

<center>

![](images/regressionTable.png)

</center>

* $Œ≤_0=7.0325$, $SE(\hat{\beta}_0)=0.4578$
* $Œ≤_1=0.0475$, $SE(\hat{\beta}_1)=0.0027$
* as $p<0.05$, we say: TV adds have a significant positive correlation with sales


---

‚úçÔ∏è Real Regression Tables 

<center>

![](images/FemurLengthRegression.png)

</center>

* what is the predicted variable?
* how large is the intercept?
* is there a significant influence of femur length on the predicted variable?

###### OLS - Ordinary least squares = Regular Linear Regression

---


## Assessing the Accuracy of the Model

> Does the model fit the data?

---

### ü§ì Residual Standard Error

$$Y = Œ≤_0 + Œ≤_1 \cdot X + \epsilon$$

* There will always be an error $\epsilon$, even if we know $Œ≤_0$ and $Œ≤_1$ perfectly
* Residual Standard Error ($RSE$) is an estimate of the standard deviation of $\epsilon$
* average amount that the response will deviate from the true regression line

![bg left:45% h:300](images/RSS.png)

---

$$RSE=\sqrt{\frac{1}{n-2}RSS}=\sqrt{\frac{1}{n-2}(e^2_1 + e^2_2 + ¬∑ ¬∑ ¬∑ + e^2_n)}$$
$$=\sqrt{\frac{1}{n-2}\sum_{j=0}^n(y_j‚àí\hat{y}_j)^2}$$

* $RSE=0$ ... perfect fit

---

<center>

![h:400](images/tv.png)

</center>


$$RSE_{TV}=3.260$$
> even if the model was correct and the true values of the unknown coefficients $Œ≤_0$ and $Œ≤_1$ were known exactly, any prediction of sales on the basis of TV advertising would still be off by about $3.260$ units on average.

---

#### üß† $R^2$ Statistic

* $RSE$ provides an absolute measure of the fit (e.g., $3.260$ units)
* $R^2$ provides a proportion between $0$ and $1$

$$R^2 = \frac{TSS-RSS}{TSS} = 1- \frac{RSS}{TSS}$$

* where $TSS=\sum{(y_j-\bar{y})^2}$ is the total sum of squares in relation to the sample mean $\bar{y}$
* $RSS=\sum{(y_j-\hat{y}_j)^2}$ is the residual sum of squares based on the model


---

<!-- _class: white -->

* $R^2$ measures the proportion of variability in $Y$ that can be explained using $X$ and the linear model (instead of just using the sample mean)
*  $R^2 \approx 0$: regression did not explain much of the variability in the response
* in the TV-example $R^2=0.61$: two-thirds of the variability in sales is explained by a linear regression on TV

![bg right w:500](images/ANOVA-graphically.png)


---

##### üß† Interpretation of $R^2$

<center>

![](images/2742052271.jpg)

</center>

###### https://www.datasciencecentral.com/wp-content/uploads/2021/10/2742052271.jpg

---

üß† Be careful!

<center>

![](images/r2_vs_CVRMSE.png)

</center>



* $R^2$ is not a good measure for prediction accuracy!
* $RSS$ or $MSE$ is identical, still $R^2$ is very different
* Especially as it is calculated on the test set

###### https://www.enmanreg.org/r2/r2_vs_cvrmse/

---

<center>

![](images/FemurLengthRegression.png)

</center>


---


### ‚úçÔ∏è Case Study

<center>

![](images/penguingraphic.jpg.webp)

</center>



###### https://ocean.si.edu/ocean-life/seabirds/penguins

---


![bg right:64% h:720](images/Penguin_Scatter.png)

- Is there any relationship between two variables (for any species and penguin), where You do not expect a linear correlation?
* If so (or not), is the model a good predictor?

[2.3 Linear Regression](https://github.com/jhumci/BLT_BDS/blob/main/3_Data_Science.ipynb)

‚åõ 35 minutes

